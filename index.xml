<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> </title>
    <link>http://blog.michaelhamrah.com/</link>
    <language>en-us</language>
    <author>Michael Hamrah</author>
    <rights>(C) 2015</rights>
    <updated>2015-06-27 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Fleet Unit Files for Kubernetes on CoreOS</title>
          <link>http://blog.michaelhamrah.com/2015/06/fleet-unit-files-for-kubernetes-on-coreos/</link>
          <pubDate>Sat, 27 Jun 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/06/fleet-unit-files-for-kubernetes-on-coreos/</guid>
          <description>&lt;p&gt;As I&amp;#8217;ve been leveraging CoreOS more and more for running docker containers, the limitations of Fleet have become apparent.&lt;/p&gt;

&lt;p&gt;Despite the benefits of &lt;a href=&#34;http://blog.michaelhamrah.com/2015/04/easy-scaling-with-fleet-and-coreos/&#34;&gt;dynamic unit files via the Fleet API&lt;/a&gt; there is still a need for fine-grained scheduling, discovery, and more complex dependencies across containers. Thus I&amp;#8217;ve been exploring Kubernetes. Although young, it shows promise for practical usage. The &lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes&#34;&gt;Kubernetes repository&lt;/a&gt; has a plethora of &amp;#8220;getting started&amp;#8221; examples across a variety of environments. There are a few CoreOS related already, but they embed the kubernetes units in a cloud-config file, which may not be what you want.&lt;/p&gt;

&lt;p&gt;My preference is to separate the CoreOS cluster setup from the Kubernetes installation. Keeping your CoreOS cloud-config minimal has many benefits, especially on AWS where you can&amp;#8217;t easily update your cloud-config. Plus, you may already have a CoreOS cluster and you just want to deploy Kubernetes on top of it.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/mhamrah/kubernetes-coreos-units&#34;&gt;Fleet Unit Files for Kubernetes on CoreOS are on GitHub&lt;/a&gt;. The unit files makes a few assumptions, mainly you are running with a &lt;a href=&#34;https://coreos.com/docs/cluster-management/setup/cluster-architectures/#production-cluster-with-central-services&#34;&gt;production setup using central services&lt;/a&gt;. For AWS &lt;a href=&#34;http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/&#34;&gt;you can use Cloudformation to manage multiple sets of CoreOS roles as distinct stacks, and join them together via input parameters&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Networking Basics: Understanding CIDR notation and Subnets: what’s up with /16 and /24?</title>
          <link>http://blog.michaelhamrah.com/2015/05/networking-basics-understanding-cidr-notation-and-subnets-whats-up-with-16-and-24/</link>
          <pubDate>Tue, 12 May 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/05/networking-basics-understanding-cidr-notation-and-subnets-whats-up-with-16-and-24/</guid>
          <description>&lt;p&gt;An IP address (specifically, an IPv4 address), like 192.168.1.51, is really just 32 bits of data. 32 ordinary bits, like a 32 bit integer, but represented a little differently than a normal int32. These 32 bits are split up into 4 8-bit blocks, with each block represented as a number with a dot in between. This is called dotted-decimal notation: 192.168.1.51. Curious why an IP address never has a number in it greater than 255? That&amp;#8217;s the maximum value for 8 bits of data: you can only represent 0 to 255 with 8 bits.&lt;/p&gt;

&lt;p&gt;The IP address 192.168.1.51 in binary is 11000000 10101000 00000001 00110011, or just 11000000101010000000000100110011. If this were an int32 it would be 3232235827. Why not just use this number as an address? By breaking up the address into blocks we can logically group related ip addresses together. This is helpful for routing and subnets (as in sub-networks) which is where we usually see &lt;a href=&#34;http://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing&#34;&gt;CIDR notation&lt;/a&gt;. It lets us specify a common range of IP address by saying some bits in the IP address are fixed, while others can change. When we add a slash and a number between 0 and 32 after an IP, like 192.168.1.&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;24&lt;/sub&gt;, we specify which bits of the address are fixed and which can be changed.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s take a look at Docker networking. Docker creates a subnet for all containers on a machine. It uses CIDR notation to specify a range of IP address for containers, usually 172.17.42.&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;16&lt;/sub&gt;. The /16 tells us the first sixteen bits in the dotted-decimal ip address are fixed. So any ip address, in binary, must begin with 1010110000010001. We are &amp;#8220;masking&amp;#8221; the first 16 bits of the address. Docker could assign a container of 172.23.42.1 but could not assign it 172.32.42.1. That&amp;#8217;s because the first four bits in 23 are the same as 17 (0001) but the first four bits 32 are different (0010).&lt;/p&gt;

&lt;p&gt;Another way to specify a range of IP addresses is with a &amp;#8220;subnet mask&amp;#8221;. Instead of simply saying 172.17.42.&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;16&lt;/sub&gt;, we could give the ip address of 172.17.42.1 and a subnet mask of 255.255.0.0. We often see this with tools like &lt;code&gt;ifconfig&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;255.255.0.0 in binary is 11111111 11111111 00000000 00000000. The first sixteen bits are 1 telling us which bits in the corresponding IP address are fixed. Sometimes you see a subnet mask of 255.240.0.0: that&amp;#8217;s the same as an ip range of /12. The number 240 is 11110000 in binary, masking the first 12 bits in the subnet mask 255.240.0.0 (8 1&amp;#8217;s for 255, and 4 1&amp;#8217;s for 240). You could never have a subnet of 239, because there is a 0 in the middle of the binary number 239 (11101111), defeating the purpose of a mask.&lt;/p&gt;

&lt;p&gt;A /16 range gives us 2 8 bit blocks to play with, or 65535 combinations: 2^16, or 256*256. But the 0 and 255 numbers are reserved in the IP address space, so we really only get 254 * 254 combinations, or 64516 addresses. This is important when setting up networks: if you want to make sure you have enough IP addresses for all the things in your network. A software defined network like &lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;Flannel&lt;/a&gt;, which allows you to create a private subnet for docker containers across hosts, uses a /16 subnet for a cluster and /24 range per node. So an entire cluster can only have 64516 ip address. More specifically, it can only have 254 nodes running 254 containers. Large networks, like those at most companies, use the reserved address space 10.0.0.0/8. The first 8 bits are fixed giving us 24 bits to play with: or 16,387,064 possible addresses (because we can&amp;#8217;t use 0 or 255). These are usually broken up into several subnet works, like 10.252.0.0/16 and 10.12.0.0/16, carving up smaller subnets from the larger address space.&lt;/p&gt;

&lt;p&gt;Subnet masks and CIDR notation play prominent roles in a variety of areas beyond specifying subnets. They are heavily used in routing tables to specify where traffic for a particular IP should go. They are used extensively in AWS and other cloud providers to specify firewall rules for security groups as well as their VPC product. Understanding CIDR notion and subnet masks make other aspects of networking: interfaces, gateways, routing tables much easier to understand.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Easy Scaling with Fleet and CoreOS</title>
          <link>http://blog.michaelhamrah.com/2015/04/easy-scaling-with-fleet-and-coreos/</link>
          <pubDate>Fri, 10 Apr 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/04/easy-scaling-with-fleet-and-coreos/</guid>
          <description>&lt;p&gt;One element of a successful production deployment is the ability to easily scale the number of instances your process is running. Many cloud providers, both on the PaaS and IaaS front, offer such functionality: AWS Auto Scaling Groups, Heroku&amp;#8217;s process size, Marathon&amp;#8217;s instance count. I was hoping for something similar in the CoreOS world. &lt;a href=&#34;http://deis.io&#34;&gt;Deis&lt;/a&gt;, the PaaS-on-CoreOS service, offers Heroku-like scaling, but I don&amp;#8217;t want to commit to the Deis layer nor its build pack approach (for no other reason than personal preference). Fleet, CoreOS&amp;#8217;s distributed systemd service, offers service templating, but you cannot say &amp;#8220;now run three instances of service x&amp;#8221;. Being programmers we can do whatever we want, and luckily, we&amp;#8217;re only a little bash script away from replicating the &amp;#8220;scale to x instances&amp;#8221; functionality of popular providers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.michaelhamrah.com/2015/03/deploying-docker-containers-on-coreos-with-the-fleet-api/&#34;&gt;You&amp;#8217;ll want to enable the Fleet HTTP Api&lt;/a&gt; for this script to work. You can easily port this to the Fleet CLI, but I much prefer the http api because it doesn&amp;#8217;t involve ssh, and provides more versatility into how and where you run the script.&lt;/p&gt;

&lt;p&gt;Conceptually the flow is straightforward:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Given a process we want to set the number of running instances to some &lt;code&gt;desired_count&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;desired_count&lt;/code&gt; is less than &lt;code&gt;current_count&lt;/code&gt;, scale down.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;desired_count&lt;/code&gt; is more than &lt;code&gt;current_count&lt;/code&gt;, scale up.&lt;/li&gt;
&lt;li&gt;If they are the same, do nothing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fleet offers service templating so you can have a service unit named &lt;code&gt;my_awesome_app@.service&lt;/code&gt; with specific copies named &lt;code&gt;my_awesome_app@1, my_awesome_app@2, my_awesome_app@N&lt;/code&gt; representing specific running instances. Currently Fleet doesn&amp;#8217;t offer a way to group these related services together but we can easily pattern match on the service name to control specific running instances. The steps are straightforward:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Query the Fleet API for all instances&lt;/li&gt;
&lt;li&gt;Filter by all services matching the specified name&lt;/li&gt;
&lt;li&gt;See how many instances we have running for the given service&lt;/li&gt;
&lt;li&gt;Destroy or create instances using specific service names until we match the &lt;code&gt;desired_size&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these steps are easily achievable with Fleet&amp;#8217;s HTTP Api (or fleetctl) and a little bash. To give our script some context, let&amp;#8217;s start with how we want to use the script. Ideally it will look like this:&lt;/p&gt;

&lt;pre class=&#34;toolbar-overlay:false syntax bash&#34;&gt;./scale-fleet my_awesome_app 5
&lt;/pre&gt;

&lt;p&gt;First, let&amp;#8217;s set up our script &lt;code&gt;scale-fleet&lt;/code&gt; and set the command line arguments:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

FLEET_HOST=&amp;lt;YOUR FLEET API HOST&gt;

# You may want to consider cli flags 
SERVICE_NAME=$1
DESIRED_SIZE=$2
&lt;/pre&gt;

&lt;p&gt;Next we want to query the Fleet API and filter on all units with a prefix of &lt;code&gt;SERVICE_NAME&lt;/code&gt; which have a process number. This will give us an array of units matching &lt;code&gt;my_awesome_app@1.service&lt;/code&gt;, not the base template of &lt;code&gt;my_awesome_app@.service&lt;/code&gt;. These are the units we will either add to or destroy as appropriate. The latest 1.5 version of jq supports regex expressions, but as of this writing 1.4 is the common release version, so we&amp;#8217;ll parse the json response with jq, and then filter with grep. Finally some bash trickery will parse the result into an array we can loop through later.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Curls the API and filter on a specific pattern, storing results in an array
INSTANCES=($(curl -s $FLEET_HOST/fleet/v1/units | jq &#34;.units[].name | select(startswith(\&#34;$SERVICE@\&#34;))&#34; | grep &#39;\w@\d\.service&#39;))

# A bash trick to get size of array
CURRENT_SIZE=${#INSTANCES[@]}
echo &#34;Current instance count for $SERVICE is: $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;Next let&amp;#8217;s scaffold the various scenarios for matching &lt;code&gt;CURRENT_SIZE&lt;/code&gt; with &lt;code&gt;DESIRED_SIZE&lt;/code&gt;, which boils down to some if statements.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;if [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; then
  echo &#34;doing nothing, current size is equal desired size&#34;
elif [[ $DESIRED_SIZE &amp;lt; $CURRENT_SIZE ]]; then
  echo &#34;going to scale down instance $CURRENT_SIZE&#34;
  # More stuff here
else 
  echo &#34;going to scale up to $DESIRED_SIZE&#34;
  # More stuff here
fi
&lt;/pre&gt;

&lt;p&gt;When the desired size equals the current size we don&amp;rsquo;t need to do anything. Scaling down is easy, we simply loop, deleting the specific instance, until the desired and current states match. You can drop in the following snippet for scaling down:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;until [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; do
    curl -X DELETE $FLEET_HOST/fleet/v1/units/${SERVICE}@${CURRENT_SIZE}.service

    let CURRENT_SIZE = CURRENT_SIZE-1
  done
  echo &#34;new instance count is $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;Scaling up is a bit trickier. Unfortunately you can&amp;rsquo;t simply create a new unit from a template like you can with the fleetctl CLI. But you can do exactly what the fleetctl does: copy the body from the base template and create a new one with the specific full unit name. With the body we can loop, creating instances, until our current size matches the desired size. Let&amp;rsquo;s walk it through step-by-step:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;echo &#34;going to scale up to $desired_size&#34;
 # Get payload by parsing the options field from the base template
 # And build our new payload for PUTing later
 payload=`curl -s $FLEET_HOST/fleet/v1/units/${SERVICE}@.service | jq &#39;. | { &#34;desiredState&#34;:&#34;launched&#34;, &#34;options&#34;: .options }&#39;`

 #Loop, PUTing our new template with the appropriate name
 until [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; do
   let current_size=current_size+1

   curl -X PUT -d &#34;${payload}&#34; -H &#39;Content-Type: application/json&#39; $FLEET_HOST/fleet/v1/units/${SERVICE}@${CURRENT_SIZE}.service 
 done
 echo &#34;new instance count is $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;With our script in place we can scale away:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Scale up to 5 instances
$ ./scale-fleet my_awesome_app 5

# Scale down
$ ./scale-fleet my_awesome_app 3
&lt;/pre&gt;

&lt;p&gt;Because this all comes down to a simple bash script you can easily run it from a variety of places. It can be part of a parameterized Jambi job to scale manually with a UI, part of an &lt;a href=&#34;http://github.com/hashicorp/envconsul&#34;&gt;envconsul&lt;/a&gt; setup with a key set in &lt;a href=&#34;consul.io&#34;&gt;Consul&lt;/a&gt;, or it can fit into a larger script that reads performance characteristics from some monitoring tool and reacts accordingly. You can also combine this with AWS Cloudformation or another cloud provider: if you&amp;rsquo;re CPU&amp;rsquo;s hit a certain threshold, you can scale the specific worker role running your instances, and have your &lt;code&gt;desired_size&lt;/code&gt; be some factor of that number.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been on a bash kick lately. It&amp;rsquo;s a versatile scripting language that easily portable. The syntax can be somewhat mystic, but as long as you have a shell, you have all you need to run your script.&lt;/p&gt;

&lt;p&gt;The final, complete script is here:&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Managing CoreOS Clusters on AWS with CloudFormation</title>
          <link>http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/</link>
          <pubDate>Wed, 25 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/</guid>
          <description>&lt;p&gt;Personally, I find CloudFormation a somewhat annoying tool, yet I haven&amp;#8217;t replaced it with anything else. Those json files can get so ugly and unwieldy. Alternatives exist; you can try an abstraction like &lt;a href=&#34;https://github.com/cloudtools/troposphere&#34;&gt;troposphere&lt;/a&gt; or &lt;a href=&#34;https://jclouds.apache.org&#34;&gt;jclouds&lt;/a&gt;, or ditch cfn completely with something like &lt;a href=&#34;https://www.terraform.io&#34;&gt;terraform&lt;/a&gt;. These are interesting tools but somehow I find myself sticking with the straight-up json approach, the aws cli, and some bash scripting: the pieces are already there, they just need to be strung together. In the end it&amp;#8217;s not that bad, and there are some tools and techniques I&amp;#8217;ve picked up which really help out. I recently applied these to managing CoreOS clusters with CFN, and wanted to share a simplified version of the approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://coreos.com/docs/running-coreos/cloud-providers/ec2/&#34;&gt;CoreOS provides a default CloudFormation template&lt;/a&gt; which is a great start for cluster experimentation. But scaling out, where nodes are coming and going, can be disastrous for etcd&amp;#8217;s quorum consensus if you&amp;#8217;re not careful. You just don&amp;#8217;t want to remove nodes from a formed etcd cluster. &lt;a href=&#34;https://coreos.com/docs/cluster-management/setup/cluster-architectures/&#34;&gt;CoreOS&amp;#8217;s cluster documentation&lt;/a&gt; has a section on production configuration: you want a core set of nodes for running central services, with various worker nodes for specific purposes. We can elaborate this with a short-list of requirements:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;You want to tag sets of instances with specific roles so you can group dependencies and isolate apps when needed.&lt;/strong&gt;&lt;/em&gt; Although possible, it&amp;#8217;s unrealistic to actually run any app on any node. More likely you want to group apps into front-facing and back-facing and treat those nodes differently. For instance, you could map the IP&amp;#8217;s of front-facing nodes to a Route53 endpoint.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;You want a cluster of heterogeneous instances for different workloads&lt;/strong&gt;&lt;/em&gt; Certain apps require certain characteristics. Even though you&amp;#8217;re running everything in docker containers, you still want to have c4&amp;#8217;s for compute-intensive loads, r3&amp;#8217;s for memory-intensive loads, etc. Look at your applications and map them to a system topology. You can also scale these groups of instances differently, but you want to see your entire system as a whole: not as independent, discrete parts.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;At some point, you&amp;#8217;ll need to update the configuration of your instances. You want to do this surgically, without accidentally destroying your cluster&lt;/strong&gt;&lt;/em&gt;. You may be one bad cfn update from relaunching an auto scaling group or misconfiguring an instance which causes a replacement. Just like normal instances you want to apply updates and reconfiguration of nodes in a sane, logical way. If you only had one cfn template for your entire cluster, it&amp;#8217;s all or nothing. That&amp;#8217;s not a choice we want to make.&lt;/p&gt;

&lt;p&gt;CoreOS won&amp;#8217;t let you forget about the underlying nodes; it just adds a little abstraction so you don&amp;#8217;t need to deal with specific nodes as much.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m assuming you&amp;#8217;re familiar with CloudFormation and the basics of a template. For our setup we&amp;#8217;ll start with the &lt;a href=&#34;https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template&#34;&gt;us-east-1 hvm CoreOS template&lt;/a&gt; and modify it along the way. This template create a straight-up CoreOS cluster launched in an Auto Scaling Group, uses a LaunchConfig&amp;#8217;s UserData to set some Cloud-Config settings. Like most templates you need a few parameters to launch. The non-default ones are your keypair and the etcd Discovery Url for forming the cluster. We are going to launch this stack with the CLI (who needs user interfaces?)&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s create a bash script, &lt;code&gt;coreos-cfn.sh&lt;/code&gt;, to call our create stack (don&amp;#8217;t forget to chmod +x). We need a DiscoveryUrl so we&amp;#8217;ll get a new one in our script and pass it as a parameter to CFN.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash 

DISCOVERY_URL=`curl -s -w &#34;\n&#34; https://discovery.etcd.io/new`
#Check to make sure the above command worked, or exit
[[ $? -ne 0 ]] &amp;#038;&amp;#038; echo &#34;Could not generate discovery url.&#34; &amp;#038;&amp;#038; exit 1

if [ -z &#34;$COREOS_KEYPAIR&#34; ]; then
  KEYPAIR=yourkey.pem
fi

# Create the CloudFormation stack
aws cloudformation create-stack \
    --stack-name coreos-test \
    --template-body file://coreos-stable-hvm.template \
    --capabilities CAPABILITY_IAM \
    --tags Key=Name,Value=CoreOS \
    --parameters \
        ParameterKey=DiscoveryURL,ParameterValue=${DISCOVERY_URL} \
        ParameterKey=KeyPair,ParameterValue=${KEYPAIR}
&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-z $KEYPAIR&lt;/code&gt; tests to see if there&amp;#8217;s a keypair set as an environment variable; if not, it uses the specified one. If you run &lt;code&gt;coreos-cfn.sh&lt;/code&gt; you should see the CLI spit out the ARN for the stack. Before we do that, let&amp;#8217;s make two minor tweaks.&lt;/p&gt;

&lt;p&gt;There are two key pieces of information we want to remember from this cluster: The DiscoveryUrl, so can access cluster state, and the AutoScalingGroup, so we can easily inspect instances in the future. Because the DiscoveryUrl is a parameter the aws cli will remember it for you. We need to add the auto scaling group as an output:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;&#34;Outputs&#34;: {
    &#34;AutoScalingGroup&#34; : {
      &#34;Value&#34;: { &#34;Ref&#34;: &#34;CoreOSServerAutoScale&#34; }
    }
  }
&lt;/pre&gt;

&lt;p&gt;After launching the cluster we can use the CLI and some jq to get back these parameters. It&amp;#8217;s a simple built-in storage mechanism of AWS, and all you need is the original stack name:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Get back the DiscoveryURL: Describe the stack, select the parameter list
DISCOVERY_URL=`aws cloudformation describe-stacks --stack-name coreos-test | \
  jq -r &#39;[.Stacks[].Parameters[]][] | select (.ParameterKey == &#34;DiscoveryURL&#34;) | .ParameterValue&#39;`

# Get back the auto-scaling-group-id
LEADER_ASG=`aws cloudformation describe-stacks --stack-name coreos-test | \
  jq -r &#39;[.Stacks[].Outputs[]][] | select (.OutputKey == &#34;AutoScalingGroup&#34;) | .OutputValue&#39;`

echo &#34;Discovery Url is $DISCOVERY_URL and Leader ASG is $LEADER_ASG&#34;
&lt;/pre&gt;

&lt;p&gt;Why is this important? Because now we can either inspect the state of the cluster via the disovery url service, or query the ASG to inspect running nodes directly:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Query AWS for Leader Nodes
$aws ec2 describe-instances --filters Name=tag-value,Values=$LEADER_ASG | \
  jq &#39;.Reservations[].Instances[].NetworkInterfaces[].PrivateIpAddress&#39;

# Inspect the Discovery Url for nodes, trimming port. 
$ `curl -s $DISCOVERY_URL | jq &#39;.node.nodes[].value[0:-5]&#39;

# Taking the latter one step further, we can build an Etcd Peers string using Jq, xargs and tr
$ ETCD_PEERS=`curl -s $DISCOVERY_URL | jq &#39;.node.nodes[].value[0:-5]&#39; | xargs -I{}  echo &#34;{}:4001&#34; | tr &#34;\\n&#34; &#34;,&#34;`
# Drop the last ,
$ ETCD_PEERS=${ETCD_PEERS%?}
&lt;/pre&gt;

&lt;p&gt;Armed with this information we are now able to spin up new CoreOS nodes and have it use our CoreOS leader cluster for management. The &lt;a href=&#34;https://coreos.com/docs/cluster-management/setup/cluster-architectures/&#34;&gt;CoreOS Cluster Architecture page&lt;/a&gt; has the specific &lt;code&gt;cloud-config&lt;/code&gt; settings which amount to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Disable etcd, we don&amp;#8217;t need it&lt;/li&gt;
&lt;li&gt;Set etcd peer settings to a comma delimited list of nodes for Fleet, Locksmith&lt;/li&gt;
&lt;li&gt;Set environment variables for fleet and etcd in start scripts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;#8217;ll make the etcd peer list a parameter for our template. We can duplicate our leader template, replace the &lt;code&gt;UserData&lt;/code&gt; portion of the &lt;code&gt;LaunchConfig&lt;/code&gt; with the updated settings from the link above, and add &lt;code&gt;{ Ref: }&lt;/code&gt; parameters where appropriate. Let&amp;#8217;s also add a metadata parameter as well:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;&#34;Parameters&#34;: {
    &#34;EtcdPeers&#34; : {
      &#34;Description&#34; : &#34;A comma delimited list of etcd endpoints to use for state management.&#34;,
      &#34;Type&#34; : &#34;String&#34;
    },
    &#34;FleetMetadata&#34; : {
      &#34;Description&#34; : &#34;A comma delimited list of key=value attributes to apply for fleet&#34;,
      &#34;Type&#34; : &#34;String&#34;
    }
  }
&lt;/pre&gt;

&lt;p&gt;We can use the &lt;code&gt;Ref&lt;/code&gt; functionality to pass these to our &lt;code&gt;UserData&lt;/code&gt; script of the &lt;code&gt;LaunchConfig&lt;/code&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;//other config above
  &#34;UserData&#34; : { &#34;Fn::Base64&#34;:
          { &#34;Fn::Join&#34;: [ &#34;&#34;, [
            &#34;#cloud-config\n\n&#34;,
            &#34;coreos:\n&#34;,
            &#34;  fleet:\n&#34;,
            &#34;    metadata: &#34;, { &#34;Ref&#34;: &#34;FleetMetadata&#34; }, &#34;\n&#34;,
            &#34;    etcd_servers: $&#34;, { &#34;Ref&#34;: &#34;EtcdPeers&#34; }, &#34;\n&#34;,
            &#34;  locksmith:\n&#34;,
            &#34;    endpoint: &#34;, { &#34;Ref&#34;: &#34;EtcdPeers&#34; }, &#34;\n&#34;
            ] ]
          }

// Other config below
&lt;/pre&gt;

&lt;p&gt;Finally we need a bash script which lets us inspect the existing stack information to pass as parameters to this new template. I also appreciate a CLI tool with a sane set of explicit flags. When I launch a secondary set of CoreOS nodes, I&amp;#8217;d like something simple to set the name, type, metadata and where I want to join to:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ launch-worker-group.sh -n r3-workers -t r3.large -j coreos-test -m &#34;instancetype=r3,role=worker&#34;
&lt;/pre&gt;

&lt;p&gt;Bash has a flag-parsing abilities in its &lt;code&gt;getopts&lt;/code&gt; function which we&amp;#8217;ll simply use to set variables:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

while getopts n:j:m:s: FLAG; do
  case $FLAG in
    n)  STACK_NAME=${OPTARG};;
    j)  JOIN=${OPTARG};;
    m)  METADATA=${OPTARG};;
    t)  INSTANCE_TYPE =${OPTARG};;
    [?])
      print &gt;&amp;#038;2 &#34;Usage: $0 [ -n stack-name ] [ -j join to leader] [ -m fleet-metadata ] [ -t instance-type ]&#34;
      exit 1;;
  esac
done

shift $((OPTIND-1))

# You can set defaults, too:
if [ -z $INSTANCE_TYPE ]; then 
  INSTANCE_TYPE =&#34;m3.medium&#34;
fi
&lt;/pre&gt;

&lt;p&gt;With this in place it&amp;#8217;s just a matter of calling the AWS CLI with our new template and updated parameters. The only thing we&amp;#8217;re doing differently than the original script is using CloudFormation&amp;#8217;s json parameter functionality. This allows for more structured data in variables. Otherwise the comma-delimited list for etcd peers will throw off the CLI call.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;DISCOVERY_URL=`aws cloudformation describe-stacks --stack-name $JOIN | \
  jq -r &#39;[.Stacks[].Parameters[]][] | select (.ParameterKey == &#34;DiscoveryURL&#34;) | .ParameterValue&#39;`
# Taking the latter one step further, we can build an Etcd 
# Peers string using jq, xargs and tr to flatten
ETCD_PEERS=`curl -s $DISCOVERY_URL | jq &#39;.node.nodes[].value[0:-5]&#39; | \
  xargs -I{}  echo &#34;{}:4001&#34; | tr &#34;\\n&#34; &#34;,&#34;`

# Drop the last ,
ETCD_PEERS=${ETCD_PEERS%?}

 # Create the CloudFormation stack
 aws cloudformation create-stack \
    --stack-name STACK_NAME \
    --template-body file://coreos-worker-hvm.template \
    --capabilities CAPABILITY_IAM \
    --tags Key=Name,Value=CoreOS Key=Role,Value=Worker \
    --parameters &#34;[
      { \&#34;ParameterKey\&#34;:\&#34;FleetMetadata\&#34;,\&#34;ParameterValue\&#34;:\&#34;${METADATA}\&#34; },
      { \&#34;ParameterKey\&#34;:\&#34;InstanceType\&#34;,\&#34;ParameterValue\&#34;:\&#34;${INSTANCE_TYPE}\&#34; },
      { \&#34;ParameterKey\&#34;:\&#34;EtcdPeers\&#34;,\&#34;ParameterValue\&#34;:\&#34;${ETCD_PEERS%?}\&#34; },
      { \&#34;ParameterKey\&#34;:\&#34;KeyPair\&#34;,\&#34;ParameterValue\&#34;:\&#34;${KEYPAIR}\&#34; }
    ]&#34;
&lt;/pre&gt;

&lt;p&gt;And launch it! This will create a new stack for your worker nodes with whatever metadata you want, with whatever instance type you want.&lt;/p&gt;

&lt;p&gt;There are a few ways to extend this. For one, we haven&amp;#8217;t dealt with updating or destroying the stack. You can create separate shell scripts or combine them together with flags for determining which action to take. I prefer the latter as it keeps all related scripts in one file, but you can break out accordingly. You can use the AWS CLI and the Stack Name to query for private ip&amp;#8217;s and update Route 53 accordingly, bypassing the need for an ELB.&lt;/p&gt;

&lt;p&gt;You can do a lot with bash and other CLI tools like jq. You don&amp;#8217;t need to scour GitHub for open source tools, or frameworks that have bells and whistles. The core components are there, you just need to glue them together. Yes, your scripts may get out of hand, but at that point it&amp;#8217;s worth looking for alternatives because there&amp;#8217;s probably a specific problem you need to solve. Remember, be opinionated and let those choices guide you. At some point in the future I may be raving about Terraform; friends say it&amp;#8217;s a great tool, but it&amp;#8217;s just not one that I need-or particularly want-to use now.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Slimming down Dockerfiles: Decrease the size of Gitlab’s CI runner from 900 to 420 mb</title>
          <link>http://blog.michaelhamrah.com/2015/03/slimming-down-dockerfiles-decrease-the-size-of-gitlabs-ci-runner-from-900-to-420-mb/</link>
          <pubDate>Sun, 22 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/slimming-down-dockerfiles-decrease-the-size-of-gitlabs-ci-runner-from-900-to-420-mb/</guid>
          <description>

&lt;p&gt;I&amp;#8217;ve been leveraging Gitlab CI for our continuous integration needs, running both the CI site and CI runners on our CoreOS cluster in docker containers. It&amp;#8217;s working well. On the runner side, after cloning the ci-runner repositroy and running a &lt;code&gt;docker build -t base-runner .&lt;/code&gt; , I was a little disappointed with the size of the runner. It weighed in at 900MB, a fairly hefty size for something that should be a lightweight process. I&amp;#8217;ve built the &lt;a href=&#34;https://github.com/gitlabhq/gitlab-ci-runner/blob/master/Dockerfile&#34;&gt;ci-runner dockerfile&lt;/a&gt; with the name &amp;#8220;base-runner&amp;#8221;:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;base-runner      latest      aaf8a1c6a6b8    2 weeks ago    901.1 MB
&lt;/pre&gt;

&lt;p&gt;The dockerfile is well documented and organized, but I immediately noticed some things which cause dockerfile bloat. There are some great resources on slimming down docker files, including &lt;a href=&#34;http://www.centurylinklabs.com/optimizing-docker-images/&#34;&gt;optimizing docker images&lt;/a&gt; and the &lt;a href=&#34;https://github.com/gliderlabs/docker-alpine&#34;&gt;docker-alpine&lt;/a&gt; project. The advice comes down to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use the smallest possible base layer (usually Ubuntu is not needed)&lt;/li&gt;
&lt;li&gt;Eliminate, or at least reduce, layers&lt;/li&gt;
&lt;li&gt;Avoid extraneous cruft, usually due to excessive packages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#8217;s make some minor changes to see if we can slim down this image. At the top of the dockerfile, we see the usual apt-get commands:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;# Update your packages and install the ones that are needed to compile Ruby
RUN apt-get update -y
RUN apt-get upgrade -y
RUN apt-get install -y curl libxml2-dev libxslt-dev libcurl4-openssl-dev libreadline6-dev libssl-dev patch build-essential zlib1g-dev openssh-server libyaml-dev libicu-dev

# Download Ruby and compile it
RUN mkdir /tmp/ruby
RUN cd /tmp/ruby &amp;#038;&amp;#038; curl --silent ftp://ftp.ruby-lang.org/pub/ruby/2.0/ruby-2.0.0-p481.tar.gz | tar xz
RUN cd /tmp/ruby/ruby-2.0.0-p481 &amp;#038;&amp;#038; ./configure --disable-install-rdoc &amp;#038;&amp;#038; make install
&lt;/pre&gt;

&lt;p&gt;Each &lt;code&gt;RUN&lt;/code&gt; command creates a separate layer, and nothing is cleaned up. These artifacts will stay with the container unnecessarily. Running another &lt;code&gt;RUN rm -rf /tmp&lt;/code&gt; won&amp;#8217;t help, because the history is still there. We need things gone and without a trace. We can &amp;#8220;flatten&amp;#8221; these commands and add some cleanup commands while preserving readability:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Update your packages and install the ones that are needed to compile Ruby
# Download Ruby and compile it
RUN apt-get update -y &amp;#038;&amp;#038; 
    apt-get upgrade -y &amp;#038;&amp;#038; 
    apt-get install -y curl libxml2-dev libxslt-dev libcurl4-openssl-dev libreadline6-dev libssl-dev patch build-essential zlib1g-dev openssh-server libyaml-dev libicu-dev &amp;#038;&amp;#038; 
    mkdir /tmp/ruby &amp;#038;&amp;#038; 
    cd /tmp/ruby &amp;#038;&amp;#038; curl --silent ftp://ftp.ruby-lang.org/pub/ruby/2.0/ruby-2.0.0-p481.tar.gz | tar xz &amp;#038;&amp;#038; 
    cd /tmp/ruby/ruby-2.0.0-p481 &amp;#038;&amp;#038; ./configure --disable-install-rdoc &amp;#038;&amp;#038; make install &amp;#038;&amp;#038; 
    apt-get clean &amp;#038;&amp;#038; 
    rm -rf /var/lib/apt/lists/* /tmp/*
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s only one run command, and the last two lines cleanup the apt-get downloads and &lt;code&gt;tmp&lt;/code&gt; space. Let&amp;#8217;s see how we well we do:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;$ docker images | grep base-runner
base-runner      latest      2a454f84e4e8      About a minute ago   566.9 MB
&lt;/pre&gt;

&lt;p&gt;Not bad; with one simple modification we went from 902mb to 566mb. This change comes at the cost of build speed. Because there&amp;#8217;s no previously cached layer, we always start from the beginning. When creating docker files, I usually start with multiple run commands so history is preserved while I&amp;#8217;m working on the file, but then concatenate everything at the end to minimize cruft.&lt;/p&gt;

&lt;p&gt;566mb is a good start, but can we do better? The goal of this build is to install the ci-runner. This requires Ruby and some dependencies, all documented on the ci-runner&amp;#8217;s readme. As long as we&amp;#8217;re meeting those requirements, we&amp;#8217;re good to go. Let&amp;#8217;s switch to debian:wheezy. We&amp;#8217;ll also need to tweak the locale setting for debian. Our updated dockerfile starts with this:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;# gitlab-ci-runner

FROM debian:wheezy
MAINTAINER Michael Hamrah &amp;lt;m@hamrah.com&gt;

# Get rid of the debconf messages
ENV DEBIAN_FRONTEND noninteractive

# Update your packages and install the ones that are needed to compile Ruby
# Download Ruby and compile it
RUN apt-get update -y &amp;#038;&amp;#038; 
    apt-get upgrade -y &amp;#038;&amp;#038; 
    apt-get install -y locales curl libxml2-dev libxslt-dev libcurl4-openssl-dev libreadline6-dev libssl-dev patch build-essential zlib1g-dev openssh-server libyaml-dev libicu-dev &amp;#038;&amp;#038; 
    mkdir /tmp/ruby &amp;#038;&amp;#038; 
    cd /tmp/ruby &amp;#038;&amp;#038; curl --silent ftp://ftp.ruby-lang.org/pub/ruby/2.0/ruby-2.0.0-p481.tar.gz | tar xz &amp;#038;&amp;#038; 
    cd /tmp/ruby/ruby-2.0.0-p481 &amp;#038;&amp;#038; ./configure --disable-install-rdoc &amp;#038;&amp;#038; make install &amp;#038;&amp;#038; 
    apt-get clean &amp;#038;&amp;#038; 
    rm -rf /var/lib/apt/lists/* /tmp/*
&lt;/pre&gt;

&lt;p&gt;Let&amp;#8217;s check this switch:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;base-runner      latest      40a1465ebaed      3 minutes ago      490.3 MB
&lt;/pre&gt;

&lt;p&gt;Better. A slight modification can slim this down some more; the dockerfile builds ruby from source. Not only does this take longer, it&amp;#8217;s not needed: we can just include the &lt;code&gt;ruby&lt;/code&gt; and &lt;code&gt;ruby-dev&lt;/code&gt; packages; on debian:wheezy these are good enough for running the ci-runner. By removing the install-from-source commands we can get the image down to:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;base-runner      latest      bb4e6306811d      About a minute ago   423.6 MB
&lt;/pre&gt;

&lt;p&gt;This now more than 50% less then the original, with a minimal amount of tweaking.&lt;/p&gt;

&lt;h1 id=&#34;pushing-even-further:ef76312dca5f3c1992b296f85b019da1&#34;&gt;Pushing Even Further&lt;/h1&gt;

&lt;p&gt;Normally I&amp;#8217;m not looking for an absolute minimal container. I just want to avoid excessive bloat, and some simple commands can usually go a long way. I also find it best to avoid packages in favor of pre-built binaries. As an example I do a lot of work with Scala, and have an sbt container for builds. If I were to install the SBT package from debian I&amp;#8217;d get a container weighing in at a few hundred megabytes. That&amp;#8217;s because the SBT package pulls in a lot of dependencies: java, for one. But if I already have a jre, all I really need is the sbt jar file and a bash script to launch. That considerably shrinks down the dockerfile size.&lt;/p&gt;

&lt;p&gt;When selecting a base image, it&amp;#8217;s important to realize what you&amp;#8217;re getting. A linux distribution is simply the linux kernel and an opinionated configuration of packages, tools and binaries. Ubuntu uses aptitude for package management, Fedora uses Yum. Centos 6 uses a specific version of the kernel, while version 7 uses another. You get one set of packages with Debian, another with Ubuntu. That&amp;#8217;s the power of specific communities: how frequently things are updated, how well they&amp;#8217;re maintained, and what you get out-of-box. A docker container jails a process to only see a specific part of the filesystem; specifically, the container&amp;#8217;s file system. Using a distribution ensures that the required libraries and support binaries are there, in place, where they should be. But major distributions aren&amp;#8217;t designed to run specific applications; their general-purposes servers that are designed to run a variety of apps and processes. If you want to run a single process there&amp;#8217;s a lot that comes with an OS you don&amp;#8217;t need.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s a re-emergence of lightweight linux distributions in the docker world first popularized with embedded systems. You&amp;#8217;re probably familiar with &lt;a href=&#34;https://registry.hub.docker.com/_/busybox/&#34;&gt;busybox&lt;/a&gt; useful for running one-off bash commands. Because of busybox&amp;#8217;s embedded roots, it&amp;#8217;s not quite intended for packages. &lt;a href=&#34;https://www.alpinelinux.org&#34;&gt;Alpine Linux&lt;/a&gt; is another alternative which features its own &lt;a href=&#34;https://registry.hub.docker.com/_/alpine/&#34;&gt;official registry&lt;/a&gt;. It&amp;#8217;s still very small, based on busybox, and has its own package system. I tried getting gitlab&amp;#8217;s ci-runner working with alpine, but unfortunately some of the ruby gems used by ci-runner require GNU packages which aren&amp;#8217;t available, and I didn&amp;#8217;t want to compile them manually. In terms of time/benefit, I can live with 400mb and move on to something else. For most things you can probably do a lot with Alpine and keep your containers really small: great for doing continuous deploys to a bunch of servers.&lt;/p&gt;

&lt;p&gt;The bottom line is know what you need. If you want a minimal container, build up, rather than slim down. You usually need the runtime for your application (if any), your app, and supporting dependencies. Know those dependencies, and avoid cruft when you can.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Book Review: Go Programming Blueprints, and the beauty of a language.</title>
          <link>http://blog.michaelhamrah.com/2015/03/book-review-go-programming-blueprints-and-the-beauty-of-a-language/</link>
          <pubDate>Fri, 20 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/book-review-go-programming-blueprints-and-the-beauty-of-a-language/</guid>
          <description>&lt;p&gt;Just over two years ago my wife and I [traveled around Asia for several months)[thegreatbigadventure.tumblr.com]. I didn’t do any programming while I was gone &lt;a href=&#34;http://blog.michaelhamrah.com/2013/04/six-months-of-computer-science-without-computers/&#34;&gt;but I did a great deal of reading&lt;/a&gt;, gaining a new-found appreciation for programming and technology. I became deeply interested in Scala and Go for their respective approachs to statically typed languages. Scala for its functional programming aspects and Go for its refreshing and intentionally succinct approach to interfaces, types and its anti-inheritance. The criticism I most often here with Scala; that’s it too open, too free-for-fall in its paradigms is in stark contrast to the main criticisms I hear of Go: it’s too limiting, too constrained.&lt;/p&gt;

&lt;p&gt;Since returning a majority of my time is focused on Scala, yet I still keep a hand in the Go cookie jar. Both languages are incredibly productive, and I appreciate FP the more I use it and understand it. Scala’s criticism is legitimate; it can be a chaotic language. However, my personal opinion is the language shouldn’t constrain you: it’s the discipline of the programmer to write code well, not the language. A bad programmer is going to destroy any language; a good programmer can make any code beautiful. More importantly, no language is magical. A language is a tool, and it’s up to the programmer to use it effectively.&lt;/p&gt;

&lt;p&gt;Learning a language is more than just knowing how to write a class or function. Learning a language is about composing these together effectively and using the ecosystem around the language. Scala’s benefit is the ecosystem around the JVM; idiomatic Scala is contentious debate, as you have the functional programmers on one side and the more lenient anti-javaists on the other (Martin Odersky’s talk &lt;a href=&#34;https://www.youtube.com/watch?v=ecekSCX3B4Q&#34;&gt;Scala: The Simple Parts&lt;/a&gt; is a great overview of where Scala shines). Go, on the other hand, is truly effective when you embrace its opinions and leverage its ecosystem: understanding imports and go get, writing small, independent modules, reusing these modules, embracing interfaces, and understanding the power of goroutines.&lt;/p&gt;

&lt;p&gt;Last summer I had the great pleasure of being a technical reviewer for Mat Ryer’s &lt;a href=&#34;http://bit.ly/GoBb&#34;&gt;Go Programming Blueprints&lt;/a&gt;. I’ve read a great deal of programming books in my career and appreciated Mat’s approach to showcasing the power and simplicity of Go. It’s not for beginners programmers, but if you have some experience, not even with Go, you can kick-start a working knowledge easily with Mat’s book. My favorite aspect is it explains how to write idiomatic Go to build applications. One example application composes discrete services and links them with bitly’s NSQ library, another uses a routing library on top of Go’s httpRequest handler. The book isn’t just isolated to web programs, there’s a section on writing CLI apps which link together with standard in and standard out. For those criticizing Go’s terseness Mat’s book exemplifies what you can do with those terse systems: write scalable, composable apps that are also maintainable and readable. The books shows why so many exciting new tools are written in Go: you can do a lot with little, and they compile to statically linked, minimal binaries.&lt;/p&gt;

&lt;p&gt;As you develop your craft of writing code, you develop certain opinions on the way code should work. When your language is inline with your opinions, or you develop opinions based on the language, you are effectively using that language. If you are learning a new language, like Go, but still applying your existing opinions on how to develop applications (say, by wishing the language had Generics), you struggle. Worse, you are attempting to shape a new language to the one you know, effectively programming in the old language. You should embrace what the language offers, and honor its design decisions. Mat’s book shows how to apply Go’s design decisions effectively. The language itself will evolve and grow, but it will do it in a way that enhances and honors its design decisions. And if you still don’t like it, or Scala, well there’s always Rust.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Deploying Docker Containers on CoreOS with the Fleet API</title>
          <link>http://blog.michaelhamrah.com/2015/03/deploying-docker-containers-on-coreos-with-the-fleet-api/</link>
          <pubDate>Tue, 17 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/deploying-docker-containers-on-coreos-with-the-fleet-api/</guid>
          <description>

&lt;p&gt;I&amp;#8217;ve been spending a lot more time with CoreOS in search of a docker-filled utopian PaaS dreams. I haven&amp;#8217;t found quite what I&amp;#8217;m looking for, but with some bash scripting, a little http, good tools and a lotta love I&amp;#8217;m coming close. There are no shortage of solutions to this problem, and honestly, nobody&amp;#8217;s really figured this out yet in an easy, fluid, turn-key type of way. You&amp;#8217;ve probably read about CoreOS, Mesos, Marathon, Kubernetes&amp;#8230; maybe even dug into Deis, Flynn, Shipyard. You&amp;#8217;ve spun up a cluster, and are like&amp;#8230; This is great, now what.&lt;/p&gt;

&lt;p&gt;What I want is to go from an app on my laptop to running in a production environment with minimal fuss. I don&amp;#8217;t want to re-invent the wheel; there are too many people solving this problem in a similar way. I like CoreOS because it provides a bare-bones docker runtime with a solid set of low-level tools. Plus, a lot of people I&amp;#8217;m close with have been using it, so the cross-pollination of ideas helps overcome some hurdles.&lt;/p&gt;

&lt;p&gt;One of these hurdles is how you launch containers on a cluster. I really like &lt;a href=&#34;https://github.com/mesosphere/marathon&#34;&gt;Marathon&amp;#8217;s&lt;/a&gt; http api for Mesos, but I also like the simplicity of CoreOS as a platform. CoreOS&amp;#8217;s distributed init system is &lt;a href=&#34;https://github.com/coreos/fleet&#34;&gt;Fleet&lt;/a&gt;, which leverages systemd for running a process on a CoreOS node (it doesn&amp;#8217;t have to be a container). It has some nice features, but having to constantly write similar systemd files and run fleetctl to manage containers is somewhat annoying.&lt;/p&gt;

&lt;p&gt;Turns out, &lt;a href=&#34;https://github.com/coreos/fleet/blob/master/Documentation/api-v1.md&#34;&gt;Fleet has an http API&lt;/a&gt;. It&amp;#8217;s not quite as nice as Marathon&amp;#8217;s; you can&amp;#8217;t easily scale to N number of instances, but it does come close. There are a few examples of using the API to launch containers, but I wanted a more end-to-end solution that eliminated boilerplate.&lt;/p&gt;

&lt;h2 id=&#34;activate-the-fleet-api:f1075c253a77011bd480830af8403bf8&#34;&gt;Activate the Fleet API&lt;/h2&gt;

&lt;p&gt;The Fleet API isn&amp;#8217;t enabled out-of-the-box. That makes sense as the API is currently unsecured, so you shouldn&amp;#8217;t enable it unless you have the proper VPC set up. &lt;a href=&#34;https://coreos.com/docs/launching-containers/config/fleet-deployment-and-configuration/&#34;&gt;CoreOS has good documentation on getting the API running&lt;/a&gt;. For a quick start you can drop the following yaml snippet into your cloudconfig&amp;#8217;s units section:&lt;/p&gt;

&lt;pre class=&#34;syntax yaml&#34;&gt;- name: fleet.socket
  drop-ins:
    - name: 30-ListenStream.conf
      content: |
        [Socket]
        ListenStream=8080
        Service=fleet.service
        [Install]
        WantedBy=sockets.target
&lt;/pre&gt;

&lt;h2 id=&#34;exploring-the-api:f1075c253a77011bd480830af8403bf8&#34;&gt;Exploring the API&lt;/h2&gt;

&lt;p&gt;With the API enabled, it&amp;#8217;s time to get to work. The &lt;a href=&#34;https://github.com/coreos/fleet/blob/master/Documentation/api-v1.md&#34;&gt;API has some simple documentation&lt;/a&gt; but offers enough to get started. I personally like the minimal approach, although I wish it was more feature-rich (it is v1, and better than nothing).&lt;/p&gt;

&lt;p&gt;You can do a lot with curl, bash and jq. First, let&amp;#8217;s see what&amp;#8217;s running. All these examples assume you have a FLEET_ENDPOINT environment variable set with the host and port:&lt;/p&gt;

&lt;p&gt;On a side note, environment variables are key to reuse the same functionality across environments. In my opinion, they aren&amp;#8217;t used nearly enough. Check out the &lt;a href=&#34;http://12factor.net/config&#34;&gt;twelve-factor app&amp;#8217;s config section&lt;/a&gt; to understand the importance of environment variables.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -s $FLEET_ENDPOINT/fleet/v1/units | jq &#39;.units[] | { name: .name, currentState: .currentState}&#39;
&lt;/pre&gt;

&lt;p&gt;Sure, you can get the same data by running &lt;code&gt;fleetctl list-units&lt;/code&gt;, but the http command doesn&amp;#8217;t involve ssh, which can be a plus if you have a protected network, are are running from an application or CI server.&lt;/p&gt;

&lt;h2 id=&#34;creating-containers:f1075c253a77011bd480830af8403bf8&#34;&gt;Creating Containers&lt;/h2&gt;

&lt;p&gt;Instead of crafting a fleet template and running &lt;code&gt;fleetctl start sometemplate&lt;/code&gt; , we want to launch new units via http. This involves PUTting a resource to the /units/ endpoint under the name of your unit (it&amp;#8217;s actually /fleet/v1/units, it took me forever to find the path prefix). The Fleet API will build a corresponding systemd unit from the json payload, and the content closely corresponds to what you can do with &lt;a href=&#34;https://coreos.com/docs/launching-containers/launching/fleet-unit-files/&#34;&gt;a fleet unit file&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The schema takes in a &lt;code&gt;desiredState&lt;/code&gt; and an array of &lt;code&gt;options&lt;/code&gt; which specify the &lt;code&gt;section&lt;/code&gt;, &lt;code&gt;name&lt;/code&gt;, and &lt;code&gt;value&lt;/code&gt; for each line. Most Fleet templates follow a similar pattern as exemplified with &lt;a href=&#34;https://coreos.com/docs/launching-containers/launching/launching-containers-fleet/&#34;&gt;the launching containers with Fleet guide&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Cleanup potentially running containers&lt;/li&gt;
&lt;li&gt;Pull the container&lt;/li&gt;
&lt;li&gt;Run the container&lt;/li&gt;
&lt;li&gt;Define X-Fleet parameters, like conflicts.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Again we&amp;#8217;ll use curl, but writing json on the command line is really annoying. So let&amp;#8217;s create a &lt;code&gt;unit.json&lt;/code&gt; for our payload defining the tasks for CoreOS&amp;#8217;s apache container:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;{
  &#34;desiredState&#34;: &#34;launched&#34;,
  &#34;options&#34;: [
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/usr/bin/docker kill %p-i%&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/usr/bin/docker rm %p-%i&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker pull coreos/%p&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStart&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker run --rm --name %pi-%i -p 80 coreos/%p /usr/sbin/apache2ctl -D FOREGROUND&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStop&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker stop %p-%i&#34;
    },
    {
      &#34;section&#34;: &#34;X-Fleet&#34;,
      &#34;name&#34;: &#34;Conflicts&#34;,
      &#34;value&#34;: &#34;%p@*.service&#34;
    }
  ]
}
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a couple of things of note in this snippet:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We&amp;#8217;re adding a &amp;#8220;-&amp;#8221; in front of the docker kill and docker rm commands of the ExecStartPre tasks. This tells to Fleet to continue if there&amp;#8217;s an error; these tasks are precautionary to remove an existing phantom container if it will conflict with the newly launched one.&lt;/li&gt;
&lt;li&gt;We&amp;#8217;re using Fleet&amp;#8217;s systemd placeholders %p and %i to replace actual values in our template with values from the template name. This provides a level of agnosticism in our template; we can easily reuse this template to launch different containers by changing the name. Unfortunately this doesn&amp;#8217;t quite work in our example because it&amp;#8217;s apache specific, but if you were running a container with an entry point command specified, it would work fine. You&amp;#8217;ll also want to manage containers under your own namespace, either in a private or public registry.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can launch this file with curl:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -d @unit.json -w &#34;%{http_code}&#34; -H &#39;Content-Type: application/json&#39; $FLEETCTL_ENDPOINT/fleet/v1/units/apache@1.service
&lt;/pre&gt;

&lt;p&gt;If all goes well you&amp;#8217;ll get back a &lt;code&gt;201 Created&lt;/code&gt; response. Try running the &lt;code&gt;list units&lt;/code&gt; curl command to see your container task.&lt;/p&gt;

&lt;p&gt;We can run &lt;code&gt;fleetctl cat apache@1&lt;/code&gt; to view the generated systemd unit:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;[Service]
ExecStartPre=-/usr/bin/docker kill %p-%I
ExecStartPre=-/usr/bin/docker rm %p-%i
ExecStartPre=/usr/bin/docker pull coreos/%p
ExecStart=/usr/bin/docker run --rm --name %pi-%i -p 80 coreos/%p /usr/sbin/apache2ctl -D FOREGROUND
ExecStop=/usr/bin/docker stop %p-%i

[X-Fleet]
Conflicts=%p@*.service
&lt;/pre&gt;

&lt;p&gt;Want to launch a second task? Just post again, but change the instance number from 1 to 2:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -d @unit.json -w &#34;%{http_code}&#34; -H &#39;Content-Type: application/json&#39; $FLEETCTL_ENDPOINT/fleet/v1/units/apache@2.service
&lt;/pre&gt;

&lt;p&gt;When you&amp;#8217;re done with your container, you can simple issue a delete command to tear it down:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X DELETE -w &#34;%{http_code}&#34; $FLEET_ENDPOINT/fleet/v1/units/apache@1.service
&lt;/pre&gt;

&lt;h2 id=&#34;deploying-new-versions:f1075c253a77011bd480830af8403bf8&#34;&gt;Deploying New Versions&lt;/h2&gt;

&lt;p&gt;Launching individual containers is great, but for continuous delivery, you need deploy new versions with no downtime. The example above used systemd&amp;#8217;s placeholders for providing the name of the container, but left the apache commands in place. Let&amp;#8217;s use another CoreOS example container from the &lt;a href=&#34;https://coreos.com/blog/zero-downtime-frontend-deploys-vulcand/&#34;&gt;zero downtime frontend deploys&lt;/a&gt; blog post. This &lt;code&gt;coreos/example&lt;/code&gt; container uses an entrypoint and tagged docker versions to go from a v1 to a v2 version of the app. Instead of creating multiple, similar, fleet unit files like that blog post, can we make an agnostic http call that works across versions? Yes we can.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s conceptually figure out how this would work. We don&amp;#8217;t want to change the json payload across versions, so the body must be static. We could use some form of templating or find-and-replace, but let&amp;#8217;s try and avoid that complexity for now. Can we make due with the options provided us? We know that the %p parameter lets us pass in the template name to our body. So if we can specify the name and version of the container we want to launch in the name of the unit file we PUT, we&amp;#8217;re good to go.&lt;/p&gt;

&lt;p&gt;So we want to:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -d @unit.json -w &#34;%{http_code&#34;} -H &#39;Content-Type: application/json&#39; $FLEETCTL_ENDPOINT/fleet/v1/units/example:1.0.0@1.service
&lt;/pre&gt;

&lt;p&gt;I tried this with the above snippet, but replaced the pull and run commands above with the following:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;{
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStart&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker run --rm --name %p-%i -p 80 coreos/%p&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStop&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker stop %p-%i&#34;
    },
&lt;/pre&gt;

&lt;p&gt;Unfortunately, this didn&amp;#8217;t work because the colon, :, in example:1.0.0 make the name invalid for a container. I could forego the name, but then I wouldn&amp;#8217;t be able to easily stop, kill or rm the container. So we need to massage the %p parameter a little bit. Luckily, bash to the rescue.&lt;/p&gt;

&lt;p&gt;Unfortunately, systemd is a little wonky when it comes to scripting in a unit file. It&amp;#8217;s relatively hard to create and access environment variables, you need fully-qualified paths, and multiple lines for arbitrary scripts are discouraged. After googling how exactly to do bash scripting in a systemd file, or why an environment variable wasn&amp;#8217;t being set, I began to understand the frustration in the community on popular distros switching to systemd. But we can still make do with what we have by launching a &lt;code&gt;/bin/bash&lt;/code&gt; command instead of the vanilla &lt;code&gt;/usr/bin/docker&lt;/code&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;{
  &#34;desiredState&#34;: &#34;launched&#34;,
  &#34;options&#34;: [
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker kill $APP-%i\&#34;&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker rm $APP-%i\&#34;&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker pull coreos/%p&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStart&#34;,
      &#34;value&#34;: &#34;/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker run --name $APP-%i -h $APP-%i -p 80 --rm coreos/%p\&#34;&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStop&#34;,
      &#34;value&#34;: &#34;/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker stop $APP-%i&#34;
    },
    {
      &#34;section&#34;: &#34;X-Fleet&#34;,
      &#34;name&#34;: &#34;Conflicts&#34;,
      &#34;value&#34;: &#34;%p@*.service&#34;
    }
  ]
}
&lt;/pre&gt;

&lt;p&gt;and we can submit with:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X PUT -d @unit.json -H &#39;Content-Type: application/json&#39;  $FLEET_ENDPOINT/fleet/v1/units/example:1.0.0@1.service
&lt;/pre&gt;

&lt;p&gt;More importantly, we can easily launch multiple containers of version two simultaneously:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X PUT -d @unit.json -H &#39;Content-Type: application/json&#39;  $FLEET_ENDPOINT/fleet/v1/units/example:2.0.0@1.service
curl -X PUT -d @unit.json -H &#39;Content-Type: application/json&#39;  $FLEET_ENDPOINT/fleet/v1/units/example:2.0.0@2.service
&lt;/pre&gt;

&lt;p&gt;and then destroy version one:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X DELETE -w &#34;%{http_code}&#34; $FLEET_ENDPOINT/fleet/v1/units/example:1.0.0@1.service
&lt;/pre&gt;

&lt;h2 id=&#34;more-jq-and-bash-fun:f1075c253a77011bd480830af8403bf8&#34;&gt;More jq and bash fun&lt;/h2&gt;

&lt;p&gt;Let&amp;#8217;s say you do start multiple containers, and you want to cycle them out and delete them. In our above example, we&amp;#8217;ve started two containers. How will we easily go from v2 to v3, and remove the v3 nodes? The marathon API has a simple &amp;#8220;scale&amp;#8221; button which does just that. Can we do the same for CoreOS? Yes we can.&lt;/p&gt;

&lt;p&gt;Conceptually, let&amp;#8217;s think about what we want. We want to select all containers running a specific version, grab the full unit file name, and then curl a DELETE operation to that endpoint. We can use the Fleet API to get our information, jq to parse the response, and the bash pipe operator with xargs to call our curl command.&lt;/p&gt;

&lt;p&gt;Stringing this together like so:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -s $FLEET_ENDPOINT/fleet/v1/units | jq &#39;.units[] | .name | select(startswith(&#34;example:1.0.0&#34;))&#39; | xargs -t -I{} curl -s -X DELETE $FLEET_ENDPOINT/fleet/v1/units/{}
&lt;/pre&gt;

&lt;p&gt;jq provides some very powerful json processing. We are pulling out the name field, and only selecting elements which start with our specific app and version, and then piping that to xargs. The -I{} flag for xargs is a substitution trick I learned. This allows you to do string placements rather than pass the field as an argument.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:f1075c253a77011bd480830af8403bf8&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I can pretty much guarantee no matter what you pick to run your Docker PaaS, it won&amp;#8217;t do exactly what you want. I can also guarantee that there will be a lot to learn: new apis, new commands, new tools. It&amp;#8217;s going to feel like pushing a round peg in a square hole. But that&amp;#8217;s okay; part of the experience is formulating opinions on how you want things to work. It&amp;#8217;s a blend of learning the patterns and practices of a tool versus configuring it to work the way you want. Always remember a few things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Keep It Simple&lt;/li&gt;
&lt;li&gt;Think about how it should work conceptually&lt;/li&gt;
&lt;li&gt;You can do a lot with command line.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With an API-enabled CoreOS cluster, you can easily plug deployment of containers to whatever build flow you use: your laptop, a github web hook, jenkins, or whatever flow you wish. Because all the above commands are bash, you can replace any part with a bash variable and execute appropriately. This makes parameterizing these commands into functions easy.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Adding Http Server-Side Events to Akka-Streams</title>
          <link>http://blog.michaelhamrah.com/2015/01/adding-http-server-side-events-to-akka-streams/</link>
          <pubDate>Sun, 18 Jan 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/01/adding-http-server-side-events-to-akka-streams/</guid>
          <description>

&lt;p&gt;In my last blog post we pushed messages from RabbitMq to the console using Akka-Streams. We used the &lt;a href=&#34;https://github.com/ScalaConsultants/reactive-rabbit&#34;&gt;reactive-rabbit&lt;/a&gt; library to create an Akka-Streams &lt;code&gt;Source&lt;/code&gt; for our &lt;em&gt;streams-playground&lt;/em&gt; queue and mapped the stream to a &lt;code&gt;println&lt;/code&gt; statement before dropping it into an empty &lt;code&gt;Sink&lt;/code&gt;. All Akka-Streams need both a &lt;code&gt;Source&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; to be runnable; we created a complete stream blueprint to be run later.&lt;/p&gt;

&lt;p&gt;Printing to the console is somewhat boring, so let&amp;#8217;s take it up a notch. The excellent &lt;a href=&#34;spray.io&#34;&gt;Spray Web Service&lt;/a&gt; library is being merged into Akka as &lt;a href=&#34;http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M2/scala/http/index.html&#34;&gt;Akka-Http&lt;/a&gt;. It&amp;#8217;s essentially Spray built with Akka-Streams in mind. The routing dsl, immutable request/response model, and high-performance http server are all there; think of it as Spray vNext. Check out Mathias Doenitz&amp;#8217;s &lt;a href=&#34;http://spray.io/scaladays2014/#/&#34;&gt;excellent slide deck on kaka-http from Scala days&lt;/a&gt; to learn more on this evolution of Spray; it also highlights the back-pressure functionality Akka-Streams will give you for Http.&lt;/p&gt;

&lt;p&gt;Everyone&amp;#8217;s familiar with the Request/Response model of Http, but to show the power of Akka-Streams we&amp;#8217;ll add Heiko Seeberger&amp;#8217;s &lt;a href=&#34;https://github.com/hseeberger/akka-sse&#34;&gt;Akka-SSE&lt;/a&gt; library which brings Server-Side Events to Akka-Http. &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/eventsource/basics/&#34;&gt;Server-Side Events&lt;/a&gt; are a more efficient form of long-polling that&amp;#8217;s a lighter protocol to the bi-directional WebSocket API. It allows the client to easily register a handler which the server can then push events to. Akka-SSE adds an SSE-enabled completion marshaller to Akka-Http so your response can be SSE-aware. Instead of printing messages to the console, we&amp;#8217;ll push those messages to the browser with SSE. This shows one of my favorite features of stream-based programming: we simply connect the specific pipes to create more complex flows, without worrying about the how; the framework handles that for us.&lt;/p&gt;

&lt;h2 id=&#34;changing-the-original-example:a9237168f3920272915cff712cbdae5e&#34;&gt;Changing the Original Example&lt;/h2&gt;

&lt;p&gt;If you&amp;#8217;re interested in the code, simply &lt;a href=&#34;https://github.com/mhamrah/streams-playground&#34;&gt;clone the original repo&lt;/a&gt; with &lt;code&gt;git clone https://github.com/mhamrah/streams-playground.git&lt;/code&gt; and then &lt;code&gt;git checkout adding-sse&lt;/code&gt; to get to this step in the repo.&lt;/p&gt;

&lt;p&gt;To modify the original example we&amp;#8217;re going to remove the &lt;code&gt;println&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; calls from &lt;code&gt;RabbitMqConsumer&lt;/code&gt; so we can plug in our enhanced &lt;code&gt;Source&lt;/code&gt; to the Akka-Http sink.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;def consume() = {
    Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
  }
&lt;/pre&gt;

&lt;p&gt;This is now a partial flow: we build up the original RabbitMq &lt;code&gt;Source&lt;/code&gt; with our map function to get the message body. Now the &amp;#8220;other end&amp;#8221; of the stream needs to be connected, which we defer until later. This is the essence of stream composition. There are multiple ways we can cut this up: our &lt;code&gt;map&lt;/code&gt; call could be the only thing in this function, with our &lt;code&gt;RabbitMq&lt;/code&gt; source defined elsewhere.&lt;/p&gt;

&lt;h2 id=&#34;adding-akka-http:a9237168f3920272915cff712cbdae5e&#34;&gt;Adding Akka-Http&lt;/h2&gt;

&lt;p&gt;If you&amp;#8217;re familiar with Spray, Akka-Http won&amp;#8217;t look that much different. We want to create an &lt;code&gt;Actor&lt;/code&gt; for our http service. There are just a few different traits we extend our original Actor from, and a different way plug our routing functions into the Akka-Streams pipeline.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;class HttpService
  extends Actor
  with Directives
  with ImplicitFlowMaterializer
  with SseMarshalling {
  // implementation
  // ...
}
&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;Directives&lt;/code&gt; gives us the routing dsl, similar to &lt;a href=&#34;http://spray.io/documentation/1.2.2/spray-routing/&#34;&gt;spray-routing&lt;/a&gt; (the functions are pretty much the same). Because Akka-Http uses Akka-Streams, we need an implicit &lt;code&gt;FlowMaterializer&lt;/code&gt; in scope to run the stream. &lt;code&gt;ImplicitFlowMaterializer&lt;/code&gt; provides a default. Finally, the &lt;code&gt;SseMarshalling&lt;/code&gt; trait from Heiko Seeberger&amp;#8217;s library provides the SSE functionality we want for our app. &lt;em&gt;If you&amp;#8217;re interested in a robust Akka-Streams sample, Heiko&amp;#8217;s &lt;a href=&#34;https://github.com/hseeberger/reactive-flows&#34;&gt;Reactive-Flows&lt;/a&gt; is worth checking out.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;##Binding to Http&lt;/p&gt;

&lt;p&gt;Within our actor body we&amp;#8217;ll create our http stream by binding a routing function to an http port. This is a little different than Spray; there&amp;#8217;s just some syntactical sugar so we can plug our routing function directly into the http pipeline:&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;//need an ExecutionContext for Futures
    import context.dispatcher

    //There&#39;s no receive needed, this is implicit
    //by our routing dsl.
    override def receive: Receive = Actor.emptyBehavior

    //We bind to an interface and create a 
    //Flow with our routing function
    Http()(context.system)
      .bind(Config.interface, Config.port)
      .startHandlingWith(route)

    //Simple composition of basic routes
    private def route: Route = sse ~ assets

    //Defined later
    private def see: Route = ???
    private def assets: Route = ???
&lt;/pre&gt;

&lt;p&gt;If we weren&amp;#8217;t using the Routing DSL we&amp;#8217;d need to explicitly handling HttpRequest messages in our receive partial function. But the &lt;code&gt;startHandlingWith&lt;/code&gt; call will do this for us; like spray-routing it takes in a routing function, and will call the appropriate route handler. New http requests will be pumped into the route handler and completed with the completion function at the end of the route.&lt;/p&gt;

&lt;p&gt;##Adding SSE&lt;/p&gt;

&lt;p&gt;The last piece of the puzzle is adding a specific route for SSE. We need two pieces for SSE support: first, an implicit function which converts the type produced from our &lt;code&gt;Source&lt;/code&gt; to an &lt;code&gt;Sse.Message&lt;/code&gt;; in this case, we need to go from a &lt;code&gt;String&lt;/code&gt; to an &lt;code&gt;Sse.Message&lt;/code&gt;. Secondly we need a route where a client can subscribe to the stream of server-side events.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;//Convert a String (our RabbitMq output) to an SSE Message
 implicit def stringToSseMessage(event: String): Sse.Message = {
      Sse.Message(event, Some(&#34;published&#34;))
    }

 //add a route for our sse endpoint.
 private def sse: Route = {
      path(&#34;messages&#34;) {
        get {
          complete {
            RabbitMqConsumer.consume
          }
        }
      }
    }
&lt;/pre&gt;

&lt;p&gt;In order for SSE to work in the browser we need to produce a stream of SSE messages with a specific content-type: &lt;code&gt;Content-Type: text/event-stream&lt;/code&gt;. That&amp;#8217;s what Akka-SSE provides: the SSE Message case classes and serialization to &lt;code&gt;text/event-stream&lt;/code&gt;. Our implicit function &lt;code&gt;stringToSseMessage&lt;/code&gt; allows the Scala types to align so the &amp;#8220;stream pipes&amp;#8221; can be attached together. In our case, we produce a stream of &lt;code&gt;String&lt;/code&gt;s, our RabbitMq message body. We need to produce a stream of &lt;code&gt;SSE.Messages&lt;/code&gt; so we add a simple conversion function. When a new client connects, they&amp;#8217;ll attach themselves to the consuming RabbitMq &lt;code&gt;Source&lt;/code&gt;. Akka-Http lets you natively complete a route with a &lt;code&gt;Flow&lt;/code&gt;; Akka-Sse simply completes that &lt;code&gt;Flow&lt;/code&gt; with the proper Http response for SSE.&lt;/p&gt;

&lt;h2 id=&#34;trying-it-out:a9237168f3920272915cff712cbdae5e&#34;&gt;Trying It Out&lt;/h2&gt;

&lt;p&gt;Fire up SBT and run &lt;code&gt;~reStart&lt;/code&gt;, ensuring you have RabbitMq running and set up a queue named &lt;code&gt;streams-playground&lt;/code&gt; (&lt;a href=&#34;https://github.com/mhamrah/streams-playground/blob/master/README.md&#34;&gt;see the README&lt;/a&gt;). In your console, try a simple &lt;code&gt;curl&lt;/code&gt; command:&lt;/p&gt;

&lt;pre class=&#34;bash&#34;&gt;$ curl http://localhost:8080/messages
&lt;/pre&gt;

&lt;p&gt;The curl command won&amp;#8217;t return. Start sending messages via the RabbitMq Admin console and you&amp;#8217;ll see the SSE output in action:&lt;/p&gt;

&lt;pre class=&#34;bash&#34;&gt;$ curl localhost:8080/messages
event:published
data:woot!

event:published
data:another message!
&lt;/pre&gt;

&lt;p&gt;Close the curl command, and fire up your browser at &lt;code&gt;http://localhost:8080&lt;/code&gt; you&amp;#8217;ll see a simple web page (served from the &lt;code&gt;assets&lt;/code&gt; route). Continue sending messages via RabbitMq, and those messages will be added to the dom. Most modern browsers natively support SSE with the &lt;code&gt;EventSource&lt;/code&gt; object. The following gist creates an event listener on the &lt;code&gt;&#39;published&#39;&lt;/code&gt; event, which is produced from our &lt;code&gt;implicit string =&amp;gt; sse&lt;/code&gt; function above:&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s also handlers for opening the initial sse connection and any errors produced. You could also add more events; our simple conversion only goes from a &lt;code&gt;String&lt;/code&gt; to one specific SSE of type &lt;code&gt;published&lt;/code&gt;. You could map a set of case classes&amp;#8211;preferably an algebraic data type&amp;#8211;to a set of events for the client. Most modern browsers support &lt;code&gt;EventStream&lt;/code&gt;; there&amp;#8217;s no need a for an additional framework or library. The gist above includes a test I copied from the &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/eventsource/basics/&#34;&gt;html5 rocks page on SSE&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;a-naive-implementation:a9237168f3920272915cff712cbdae5e&#34;&gt;A Naive Implementation&lt;/h2&gt;

&lt;p&gt;If you open up multiple browsers to localhost, or &lt;code&gt;curl http://localhost:8080/messages&lt;/code&gt; a few times, you&amp;#8217;ll notice that a published message only goes to one client. This is because our initial RabbitMq &lt;code&gt;Source&lt;/code&gt; only consumes one message from a queue, and passes that down the stream pipeline. That single message will only go to one of the connected clients; there&amp;#8217;s no fanout or broadcasting. You can do that with either RabbitMq or Akka-Streams, try experimenting for yourself!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>A Gentle Introduction To Akka Streams</title>
          <link>http://blog.michaelhamrah.com/2015/01/a-gentle-introduction-to-akka-streams/</link>
          <pubDate>Tue, 13 Jan 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/01/a-gentle-introduction-to-akka-streams/</guid>
          <description>

&lt;p&gt;I&amp;#8217;m happy to see stream-based programming emerge as a paradigm in many languages. Streams have been around for a while: take a look at the good &amp;#8216;ol | operator in Unix. Streams offer an interesting conceptual model to processing pipelines that is very functional: you have an input, you produce an output. You string these little functions together to build bigger, more complex pipelines. Most of the time you can make these functions asynchronous and parallelize them over input data to maximize throughput and scale. With a Stream, handling data is almost hidden behind the scenes: it just &lt;em&gt;flows&lt;/em&gt; through &lt;em&gt;functions&lt;/em&gt;, producing a new output from some input. In the case of an Http server, the Request-Response model across all clients is a Stream-based process: You map a Request to a Response, passing it through various functions which act on an input. Forget about MVC, it&amp;#8217;s all middleware. No need to set variables, iterate over collections, orchestrate function calls. Just concatenate stream-enabled functions together, and run your code. Streams offer a succinct programming model for a process. The fact it also scales is a nice bonus.&lt;/p&gt;

&lt;p&gt;Stream based programming is possible in a variety of languages, and I encourage you to explore this space. There&amp;#8217;s an excellent &lt;a href=&#34;https://github.com/substack/stream-handbook&#34;&gt;stream handbook for Node&lt;/a&gt;, &lt;a href=&#34;https://github.com/matz/streem&#34;&gt;an exploratory stream language from Yukihiro &amp;#8220;Matz&amp;#8221; Matsumoto of Ruby fame&lt;/a&gt;, &lt;a href=&#34;https://spark.apache.org/streaming/&#34;&gt;Spark Streaming&lt;/a&gt; and of course &lt;a href=&#34;http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M2/index.html&#34;&gt;Akka-Streams&lt;/a&gt; which joins the existing &lt;a href=&#34;https://github.com/scalaz/scalaz-stream&#34;&gt;scalaz-stream&lt;/a&gt; library for Scala. Even Go&amp;#8217;s &lt;a href=&#34;http://golang.org/pkg/net/http/#HandleFunc&#34;&gt;HttpHandler function&lt;/a&gt; is Stream-esque: you can easily wrap one function around another, building up a flow, and manipulate the Response stream accordingly.&lt;/p&gt;

&lt;h2 id=&#34;why-akka-streams:9c0e63de68271e30d1a6e002245492be&#34;&gt;Why Akka-Streams?&lt;/h2&gt;

&lt;p&gt;Akka-Streams provide a higher-level abstraction over Akka&amp;#8217;s existing actor model. The Actor model provides an excellent primitive for writing concurrent, scalable software, but it still is a primitive; it&amp;#8217;s not hard to find a few critiques of the model. So is it possible to have your cake and eat it too? Can we abstract the functionality we want to achieve with Actors into a set of function calls? Can we treat Actor Messages as Inputs and Outputs to Functions, with type safety? Hello, Akka-Streams.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s an excellent &lt;a href=&#34;http://www.typesafe.com/activator/template/akka-stream-scala&#34;&gt;activator template for Akka-Streams&lt;/a&gt; offering an in-depth tutorial on several aspects of Akka-Streams. For a more a gentler introduction, read on.&lt;/p&gt;

&lt;h2 id=&#34;the-recipe:9c0e63de68271e30d1a6e002245492be&#34;&gt;The Recipe&lt;/h2&gt;

&lt;p&gt;To cook up a reasonable dish, we are going to consume messages from &lt;a href=&#34;https://www.rabbitmq.com&#34;&gt;RabbitMq&lt;/a&gt; with the &lt;a href=&#34;https://github.com/ScalaConsultants/reactive-rabbit&#34;&gt;reactive-rabbit&lt;/a&gt; library and output them to the console. The code is on &lt;a href=&#34;https://github.com/mhamrah/streams-playground&#34;&gt;GitHub&lt;/a&gt;. If you&amp;#8217;d like to follow along, &lt;code&gt;git clone&lt;/code&gt; and then &lt;code&gt;git checkout intro&lt;/code&gt;; hopefully I&amp;#8217;ll build up more functionality in later posts so the master branch may differ.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s start with a code snippet:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;object RabbitMqConsumer {
 def consume(implicit flowMaterializer: FlowMaterializer) = {
    Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
      .foreach(println(_))
  }
}
&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;We use a RabbitMq connection to consume messages off of a queue named &lt;code&gt;streams-playground&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;For each message, we pull out the message and decode the bytes as a UTF-8 string&lt;/li&gt;
&lt;li&gt;We print it to the console&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-ingredients:9c0e63de68271e30d1a6e002245492be&#34;&gt;The Ingredients&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;Source&lt;/code&gt; is something which produces exactly one output. If you need something that generates data, you need a &lt;code&gt;Source&lt;/code&gt;. Our source above is produced from the &lt;code&gt;connection.consume&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;Sink&lt;/code&gt; is something with exactly one input. A &lt;code&gt;Sink&lt;/code&gt; is the final stage of a Stream process. The &lt;code&gt;.foreach&lt;/code&gt; call is a Sink which writes the input (_) to the console via &lt;code&gt;println&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;Flow&lt;/code&gt; is something with exactly one input and one output. It allows data to flow through a function: like calling &lt;code&gt;map&lt;/code&gt; which also returns an element on a collection. The &lt;code&gt;map&lt;/code&gt; call above is a &lt;code&gt;Flow&lt;/code&gt;: it consumes a &lt;code&gt;Delivery&lt;/code&gt; message and outputs a &lt;code&gt;String&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to actually run something using Akka-Streams you must have both a &lt;code&gt;Source&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; attached to the same pipeline. This allows you to create a &lt;code&gt;RunnableFlow&lt;/code&gt; and begin processing the stream. Just as you can compose functions and classes, you can compose streams to build up richer functionality. It&amp;#8217;s a powerful abstraction allowing you to build your processing logic independently of its execution. Think of stream libraries where you &amp;#8220;plug in&amp;#8221; parts of streams together and customize accordingly.&lt;/p&gt;

&lt;h2 id=&#34;a-simple-flow:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Simple Flow&lt;/h2&gt;

&lt;p&gt;You&amp;#8217;ll notice the above snippet requires an &lt;code&gt;implicit flowMaterializer: FlowMaterializer&lt;/code&gt;. A &lt;code&gt;FlowMaterializer&lt;/code&gt; is required to actually run a &lt;code&gt;Flow&lt;/code&gt;. In the snippet above &lt;code&gt;foreach&lt;/code&gt; acts as both a &lt;code&gt;Sink&lt;/code&gt; and a &lt;code&gt;run()&lt;/code&gt; call to run the flow. If you look at the Main.scala file you&amp;#8217;ll see I start the stream easily in one call:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;implicit val flowMaterializer = FlowMaterializer()
  RabbitMqConsumer.consume
&lt;/pre&gt;

&lt;p&gt;Create a queue named &lt;code&gt;streams-playground&lt;/code&gt; via the RabbitMq Admin UI and run the application. You can use publish messages in the RabbitMq Admin UI and they will appear in the console. Try some UTF-8 characters, like åßç∂!&lt;/p&gt;

&lt;h2 id=&#34;a-variation:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Variation&lt;/h2&gt;

&lt;p&gt;The original snippet is nice, but it does require the implicit FlowMaterializer to build and run the stream in &lt;code&gt;consume&lt;/code&gt;. If you remove it, you&amp;#8217;ll get a compile error. Is there a way to separate the definition of the stream with the running of the stream? Yes, by simply removing the &lt;code&gt;foreach&lt;/code&gt; call. &lt;code&gt;foreach&lt;/code&gt; is just syntactical sugar for a &lt;code&gt;map&lt;/code&gt; with a &lt;code&gt;run()&lt;/code&gt; call. By explicitly setting a &lt;code&gt;Sink&lt;/code&gt; without a call to &lt;code&gt;run()&lt;/code&gt; we can construct our stream blueprint producing a new object of type &lt;code&gt;RunnableFlow&lt;/code&gt;. Intuitively, it&amp;#8217;s a &lt;code&gt;Flow&lt;/code&gt; which can be &lt;code&gt;run()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the variation:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;def consume() = {
     Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
      .map(println(_))
      .to(Sink.ignore) //won&#39;t start consuming until run() is called!
  }
&lt;/pre&gt;

&lt;p&gt;We got rid of our &lt;code&gt;flowMaterializer&lt;/code&gt; implicit by terminating our Stream with a &lt;code&gt;to()&lt;/code&gt; call and a simple Sink.ignore which discards messages. This stream will not be run when called. Instead we must call it explicitly in Main.scala:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;implicit val flowMaterializer = FlowMaterializer()
  RabbitMqConsumer.consume().run()
&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;ve separated out the entire pipeline into two stages: the build stage, via the &lt;code&gt;consume&lt;/code&gt; call, and the run stage, with &lt;code&gt;run()&lt;/code&gt;. Ideally you&amp;#8217;d want to compose your stream processing as you wire up the app, with each component, like RabbitMqConsumer, providing part of the overall stream process.&lt;/p&gt;

&lt;h2 id=&#34;a-counter-example:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Counter Example&lt;/h2&gt;

&lt;p&gt;As an alternative, explore the &lt;a href=&#34;http://www.rabbitmq.com/tutorials/tutorial-one-java.html&#34;&gt;rabbitmq tutorials&lt;/a&gt; for Java examples. Here&amp;#8217;s a snippet from the site:&lt;/p&gt;

&lt;pre class=&#34;lang:java&#34;&gt;QueueingConsumer consumer = new QueueingConsumer(channel);
    channel.basicConsume(QUEUE_NAME, true, consumer);

    while (true) {
      QueueingConsumer.Delivery delivery = consumer.nextDelivery();
      String message = new String(delivery.getBody());
      System.out.println(&#34; [x] Received &#39;&#34; + message + &#34;&#39;&#34;);
    }
&lt;/pre&gt;

&lt;p&gt;This is typical of an imperative style. Our flow is controlled by the while loop, we have to explicitly manage variables, and there&amp;#8217;s no flow control. We could separate out the body from the while loop, but we&amp;#8217;d have a crazy function signature. Alternatively on the Akka side there&amp;#8217;s the solid &lt;a href=&#34;https://github.com/sstone/amqp-client&#34;&gt;amqp-client library&lt;/a&gt; which provides an Actor based model over RabbitMq:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;// create an actor that will receive AMQP deliveries
  val listener = system.actorOf(Props(new Actor {
    def receive = {
      case Delivery(consumerTag, envelope, properties, body) =&gt; {
        println(&#34;got a message: &#34; + new String(body))
        sender ! Ack(envelope.getDeliveryTag)
      }
    }
  }))

  // create a consumer that will route incoming AMQP messages to our listener
  // it starts with an empty list of queues to consume from
  val consumer = ConnectionOwner.createChildActor(conn, Consumer.props(listener, channelParams = None, autoack = false))
&lt;/pre&gt;

&lt;p&gt;You get the concurrency primitives via configuration over the actor system, but we still enter imperative-programming land in the Actor&amp;#8217;s &lt;code&gt;receive&lt;/code&gt; blog (sure, this can be refactored to some degree). In general, if we can model our process as a set of streams, we achieve the same benefits we get with functional programming: clear composition on what is happening, not how it&amp;#8217;s doing it.&lt;/p&gt;

&lt;p&gt;Streams can be applied in a variety of contexts. I&amp;#8217;m happy to see the amazing and powerful &lt;a href=&#34;http://spray.io&#34;&gt;spray.io&lt;/a&gt; library for Restful web services will be merged into Akka as a stream enabled http toolkit. It&amp;#8217;s also not hard to find out what&amp;#8217;s been done with &lt;a href=&#34;https://github.com/scalaz/scalaz-stream#projects-using-scalaz-stream&#34;&gt;scalaz-streams&lt;/a&gt; or the plethora of tooling already available in other languages.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Don’t Sweat Choice in Tech: Be Opinionated</title>
          <link>http://blog.michaelhamrah.com/2015/01/dont-sweat-choice-in-tech-be-opinionated/</link>
          <pubDate>Mon, 12 Jan 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/01/dont-sweat-choice-in-tech-be-opinionated/</guid>
          <description>&lt;p&gt;Recently I jumped back into some front-end development. My focus is primarily on backend systems and APIs so I welcomed the opportunity to hack on a UI. I keep tabs on the front-end world and a new project is a good opportunity for a test-drive or to level-up on an existing toolkit. The caveat, however, is the dreaded &lt;a href=&#34;http://techcrunch.com/2014/10/18/you-too-may-be-a-victim-of-developaralysis/&#34;&gt;developaralysis&lt;/a&gt;. We have so many choices that discerning the difference, picking the &amp;#8220;right one&amp;#8221;, and learning it becomes an overwhelming endeavor. Should I try out &lt;a href=&#34;http://gulpjs.com&#34;&gt;gulp&lt;/a&gt;? What about test-driving &lt;a href=&#34;http://facebook.github.io/react/&#34;&gt;react&lt;/a&gt;? Or should I go with the usual bootstrap/angular combo I&amp;#8217;ve come to know well?&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s hard to balance the cost of time in the present for the potential&amp;#8211;and I emphasize potential&amp;#8211;benefit of speed and simplicity later when choosing something new. Time is limited; do I need an exploration of browserify, amd, and umd when all I really need is a simple &lt;code&gt;script&lt;/code&gt; tag? Browserify looks cool, but what&amp;#8217;s the return on that investment? The flood of options occurs at every level of experience; it&amp;#8217;s endearing to overhear a debate amongst new developers on whether to learn rails or node first. It&amp;#8217;s definitely not helpful when &lt;a href=&#34;http://mashable.com/2014/01/21/learn-programming-languages/&#34;&gt;sites offer laundry lists of languages you should learn&lt;/a&gt;. C# and Java, really? I&amp;#8217;m surprised assembly wasn&amp;#8217;t on the list. Judging the nuances of NoSQL options is just as entertaining.&lt;/p&gt;

&lt;p&gt;My programming career, now inching the 15-year mark, has seen its fair share of languages and frameworks. Happily I no longer think about ASP.NET view state or the server-control lifecycle. These were instrumental at one time, and even though they are long gone, those experiences helped shape my current opinions on how I want to develop (or, in this particular case, not to develop) software. I didn&amp;#8217;t realize I had a choice back then on how I develop: ASP.NET seemed a given. That horrible windows-on-web paradigm pushed me to an intense focus on MVC, now a staple of many web frameworks. In turn, with the advent of APIs and more complex, task-focused UX, I am keenly interested in stream-based programming emerging in &lt;a href=&#34;http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M2/scala/stream-index.html&#34;&gt;Scala&lt;/a&gt; and &lt;a href=&#34;https://github.com/substack/stream-handbook&#34;&gt;Node&lt;/a&gt;. What I&amp;#8217;ve come to realize in the tumultuous world of programming, and with constantly needing to level-up, is that frameworks and languages are only part of the equation. The most important part is me, and you: the developer. Steve Ballmer got it right: &lt;a href=&#34;http://vimeo.com/6668315&#34;&gt;it&amp;#8217;s about developers&lt;/a&gt;. Languages and frameworks help us do things but our opinions on how we want to do them is what moves us forward. When a framework matches your opinions, getting stuff done is simple and intuitive. When you feel like your jumping through hoops it&amp;#8217;s time to try something different.&lt;/p&gt;

&lt;p&gt;My small foray back into the front-end world was met with the usual whirlwind of information. Not to mention the usual upgrade of tools, so any answers I find on google will be outdated:&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;
  &lt;p&gt;
    &amp;#8220;What&amp;#8217;s bower?&amp;#8221; &amp;#8220;A package manager, install it with npm.&amp;#8221; &amp;#8220;What&amp;#8217;s npm?&amp;#8221; &amp;#8220;A package manager, you can install it with brew&amp;#8221; &amp;#8220;What&amp;#8217;s brew?&amp;#8221; &amp;#8230;
  &lt;/p&gt;
  
  &lt;p&gt;
    — Stefan Baumgartner (@ddprrt) &lt;a href=&#34;https://twitter.com/ddprrt/status/529909875347030016&#34;&gt;November 5, 2014&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I just want to throw together a web site. Grunt vs. Gulp? Wait, there&amp;#8217;s this thing called &lt;a href=&#34;https://github.com/broccolijs/broccoli&#34;&gt;Broccoli&lt;/a&gt;? Is there something different than &lt;a href=&#34;http://getbootstrap.com&#34;&gt;Bootstrap&lt;/a&gt; that&amp;#8217;s less bootstrappy? Are people still using &lt;a href=&#34;http://html5boilerplate.com&#34;&gt;h5bp&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;It seemed even the simple go-to of &lt;a href=&#34;http://yeoman.io&#34;&gt;&lt;code&gt;yo angular&lt;/code&gt;&lt;/a&gt; was fraught with peril: what are all these questions I have to answer? A massive amount of files were generated. Yes, all important and I know all required for various things, but it&amp;#8217;s information overload. Why is &lt;code&gt;unresolved&lt;/code&gt; added to the body tag? What happens if I hack the meta-viewport settings? What is &lt;code&gt;build:js&lt;/code&gt; doing? Should I put on the blinders and ignore? Maybe I should try the possible simplicity of &lt;a href=&#34;http://blog.keithcirkel.co.uk/why-we-should-stop-using-grunt/&#34;&gt;just using npm&lt;/a&gt;. I was in it: developaralysis.&lt;/p&gt;

&lt;p&gt;Stop. Relax. Breathe. I already knew the simple answer to navigating the awesome amount of choice: opinion. Forget about existing, pre-conceived notions of software. You need to do something: how would you do it? Chances are someone&amp;#8217;s had a similar idea and &lt;a href=&#34;https://github.com/explore&#34;&gt;wrote some software&lt;/a&gt;. Don&amp;#8217;t even know what you need? Then start with something that&amp;#8217;s easy to learn. If it doesn&amp;#8217;t work out, you&amp;#8217;ll have formed an opinion on what you wanted to happen. This is learning by fire.&lt;/p&gt;

&lt;p&gt;Opinions have given birth to some of the most widely used software in the world. &lt;a href=&#34;http://en.wikipedia.org/wiki/Ruby_(programming_language)#Early_concept&#34;&gt;Yukihiro Matsumoto created Ruby from his dissatisfaction with other OOP languages&lt;/a&gt;. &lt;a href=&#34;http://www.aosabook.org/en/nginx.html&#34;&gt;Nginx was spawned by dissatisfaction in threaded web-servers&lt;/a&gt;. You may not be ready to write a new language, framework, or web server, but your opinions can still shape what you learn and where you invest your time.&lt;/p&gt;

&lt;p&gt;Nobody asked me a decade ago how I wanted to write web software. If they did I doubt I would have come up with anything similar to ASP.NET webforms, if I could even have put together a semi-coherent answer. Yet I was a full-time ASP.NET webforms developer, and that&amp;#8217;s how I was writing software. Eventually my teammates and I realized this way of programming was utter crap. We asked ourselves that simple question, which we should have asked a lot earlier: How do we want to do this?&lt;/p&gt;

&lt;p&gt;At any level it&amp;#8217;s important to develop opinions on how you want to achieve goals. Beginners may seem they have a difficult spot because there&amp;#8217;s so little grounding to formulate opinions: any answer may appear &amp;#8220;too simple&amp;#8221;. But the spectrum is the same for experience developers as well. There&amp;#8217;s always &amp;#8220;something else&amp;#8221; to know and factor in behind the curtain. You&amp;#8217;re constantly peeling layers off of the onion.&lt;/p&gt;

&lt;p&gt;Don&amp;#8217;t sweat the plethora of choice which exist. Nothing is perfect, and stagnation is the worst option. Take a moment and develop an opinion on how you want to solve a particular problem. Poke holes in your solution. See if somebody else has a similar idea, or a similar experience. Try something out: don&amp;#8217;t like how it happened or the result? Did you leverage the tool correctly? Okay, great, now you have the basis for something better. Develop your suite of go-to tooling. You can keep tabs on the eco-system, and cross-pollinate ideas across similar veins. Choice is a good thing: like a breadth-first search, letting you still run forward if you want.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Go Style Directory Layout for Scala with SBT</title>
          <link>http://blog.michaelhamrah.com/2014/12/go-style-directory-layout-for-scala-with-sbt/</link>
          <pubDate>Sun, 07 Dec 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/12/go-style-directory-layout-for-scala-with-sbt/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve come to appreciate Go&amp;#8217;s directory layout where &lt;em&gt;test&lt;/em&gt; and &lt;em&gt;build&lt;/em&gt; files are located side-by-side. This promotes a conscience testing priority. It also enables easy navigation to usage of a particular class/trait/object along with the implementation. After reading through the &lt;a href=&#34;http://www.scala-sbt.org/0.13.2/docs/Howto/defaultpaths.html&#34;&gt;getting-better-every-day sbt documentation&lt;/a&gt; I noticed you can easily change default directories for sources, alleviating the folder craziness of default projects. Simply add a few lines to your build.sbt:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;//Why do I need a Scala folder? I don&#39;t!
//Set the folder for Scala sources to the root &#34;src&#34; folder
scalaSource in Compile := baseDirectory.value / &#34;src&#34;

//Do the same for the test configuration. 
scalaSource in Test := baseDirectory.value / &#34;src&#34;

//We&#39;ll suffix our test files with _test, so we can exclude
//then from the main build, and keep the HiddenFileFilter
excludeFilter in (Compile, unmanagedSources) := HiddenFileFilter || &#34;*_test.scala&#34;

//And we need to re-include them for Tests 
excludeFilter in (Test, unmanagedSources) := HiddenFileFilter
&lt;/pre&gt;

&lt;p&gt;Although breaking from the norm of java-build tools may cause confusion, if you like the way something works, go for it; don&amp;#8217;t chain yourself to past practices. I never understood the class-to-file relationship of java sources, and I absolutely &lt;em&gt;hate&lt;/em&gt; navigating one-item folders. Thankfully Scala improved the situation, but the sbt maven-like defaults are still folder-heavy. IDEs make the situation easier, but I prefer simple text editors; and to paraphrase Dan North, &amp;#8220;Your fancy IDE is a painkiller for your shitty language&amp;#8221;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Running Consul on CoreOS</title>
          <link>http://blog.michaelhamrah.com/2014/11/running-consul-on-coreos/</link>
          <pubDate>Sat, 29 Nov 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/11/running-consul-on-coreos/</guid>
          <description>&lt;p&gt;I&amp;#8217;m a big fan of &lt;a href=&#34;consul.io&#34;&gt;Consul&lt;/a&gt;, Hashicorp&amp;#8217;s service discovery tool. I&amp;#8217;ve also become a fan of CoreOS, the cluster framework for running docker containers. Even though CoreOS comes with etcd for service discovery I find the feature set of Consul more compelling. And as a programmer I know I can have my cake and eat it too.&lt;/p&gt;

&lt;p&gt;My first take was to modify my &lt;a href=&#34;github.com/mhamrah/ansible-consul&#34;&gt;ansible-consul&lt;/a&gt; fork to run consul natively on CoreOS. Although this could work I find it defeats CoreOS&amp;#8217;s container-first approach with &lt;a href=&#34;https://github.com/coreos/fleet&#34;&gt;fleet&lt;/a&gt;. Jeff Lindsay created &lt;a href=&#34;https://github.com/progrium/docker-consul&#34;&gt;a consul docker container&lt;/a&gt; which does the job well. I created two fleet service files: one for launching the consul container and another for service discovery. At first the service discovery aspect seemed weird; I tried to pass ip addresses via the &amp;#8211;join parameter or use &lt;code&gt;ExecStartPost&lt;/code&gt; for running the join command. However I took a cue from the CoreOS cluster setup: sometimes you need a third party to get stuff done. In this case we the built in etcd server to manage the join ip address to kickstart the consul cluster.&lt;/p&gt;

&lt;p&gt;The second fleet service file acts as a sidekick:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For every running consul service there&amp;#8217;s a sidekick process&lt;/li&gt;
&lt;li&gt;The sidekick process writes the current IP to a key only if that key doesn&amp;#8217;t exist&lt;/li&gt;
&lt;li&gt;The sidekick process uses the value of that key to join the cluster with &lt;code&gt;docker exec&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The sidekick process removes the key if the consul service dies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two service files are below, but you should tweak for your needs.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You only need a 3 or 5 node server cluster. If your CoreOS deployment is large, use some form of restriction for the server nodes. You can do the same for the client nodes.&lt;/li&gt;
&lt;li&gt;The discovery script could be optimized. It will try and join whatever ip address is listed in the key. This avoids a few split brain scenarios, but needs to be tested.&lt;/li&gt;
&lt;li&gt;If you want DNS to work properly you need to set some Docker daemon options. Read the docker-consul README.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Clustering Akka Applications with Docker — Version 3</title>
          <link>http://blog.michaelhamrah.com/2014/11/clustering-akka-applications-with-docker-version-3/</link>
          <pubDate>Thu, 27 Nov 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/11/clustering-akka-applications-with-docker-version-3/</guid>
          <description>&lt;p&gt;The SBT Native Packager plugin now offers first-class Docker support for building Scala based applications. My last post involved combining SBT Native Packager, SBT Docker, and a custom start script to launch our application. We can simplify the process in two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Although the SBT Docker plugin allows for better customization of Dockerfiles it&amp;#8217;s unnecessary for our use case. SBT Native Packager is enough.&lt;/li&gt;
&lt;li&gt;A separate start script was required for IP address inspection so TCP traffic can be routed to the actor system. I recently contributed an update for &lt;a href=&#34;https://github.com/sbt/sbt-native-packager/pull/411&#34;&gt;better ENTRYPOINT support within SBT Native Packager&lt;/a&gt; which gives us options for launching our app in a container.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With this PR we can now add our IP address inspection snippet to our build removing the need for extraneous files. We could have added this snippet to &lt;code&gt;bashScriptExtraDefines&lt;/code&gt; but that is a global change, requiring &lt;code&gt;/sbin/ifconfig eth0&lt;/code&gt; to be available wherever the application is run. This is definitely infrastructure bleed-out and must be avoided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;The new code, on GitHub,&lt;/a&gt; uses a shell with ENTRYPOINT exec mode to set our environment variable before launching the application:&lt;/p&gt;

&lt;pre class=&#34;code scala&#34;&gt;dockerExposedPorts in Docker := Seq(1600)

dockerEntrypoint in Docker := Seq(&#34;sh&#34;, &#34;-c&#34;, &#34;CLUSTER_IP=`/sbin/ifconfig eth0 | grep &#39;inet addr:&#39; | cut -d: -f2 | awk &#39;{ print $1 }&#39;` bin/clustering $*&#34;)
&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;$*&lt;/code&gt; allows for command-line parameters to be honored when launching the container. Because the app leverages the Typesafe Config library we can also set via Java system properties:&lt;/p&gt;

&lt;pre class=&#34;code bash&#34;&gt;docker run --rm -i -t --name seed mhamrah/clustering:0.3 -Dclustering.cluster.name=example-cluster
&lt;/pre&gt;

&lt;p&gt;Launching the cluster is exactly as before:&lt;/p&gt;

&lt;pre class=&#34;code bash&#34;&gt;docker run --rm -d --name seed mhamrah/clustering:0.3
docker run --rm -d --name member1 --link seed:seed mhamrah/clustering:0.3
&lt;/pre&gt;

&lt;p&gt;For complex scripts it may be too messy to overload the ENTRYPOINT sequence. For those cases simply bake your own docker container as a base and use the ENTRYPOINT approach to call out to your script. SBT Native Packager will still upload all your dependencies and its bash script to &lt;code&gt;/opt/docker/bin/&amp;lt;your app&amp;gt;&lt;/code&gt;. The Docker &lt;code&gt;WORKDIR&lt;/code&gt; is set to &lt;code&gt;/opt/docker&lt;/code&gt; so you can drop the &lt;code&gt;/opt/docker&lt;/code&gt; as above.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Accelerate Team Development with your own SBT Plugin Defaults</title>
          <link>http://blog.michaelhamrah.com/2014/10/accelerate-team-development-with-your-own-sbt-plugin-defaults/</link>
          <pubDate>Mon, 13 Oct 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/10/accelerate-team-development-with-your-own-sbt-plugin-defaults/</guid>
          <description>

&lt;p&gt;My team manages several Scala services built with SBT. The setup of these projects are very similar, from included plugins, dependencies, and build-and-deploy configurations. At first we simply copied and paste these settings across projects but as the number of services increased the hunt-and-change strategy became laborious. Time to optimize.&lt;/p&gt;

&lt;p&gt;I heard of a few teams that created their own sbt plugins for default settings but couldn&amp;#8217;t find information on how this looked. The recent change to &lt;a href=&#34;http://www.scala-sbt.org/0.13/docs/Plugins.html&#34;&gt;AutoPlugins&lt;/a&gt; also didn&amp;#8217;t help existing documentation. I found Will Sargent&amp;#8217;s excellent post on &lt;a href=&#34;tersesystems.com/2014/06/24/writing-an-sbt-plugin&#34;&gt;writing an sbt plugin&lt;/a&gt; helpful but it wasn&amp;#8217;t what I was looking for. I want a plugin which included other plugins and set defaults for those plugins. The goal is to &amp;#8220;drop in&amp;#8221; this plugin and automatically have a set of defaults: using &lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;sbt-native-packager&lt;/a&gt;, a configured &lt;a href=&#34;https://github.com/sbt/sbt-release&#34;&gt;sbt-release&lt;/a&gt; and our nexus artifact server good-to-go.&lt;/p&gt;

&lt;h2 id=&#34;file-locations:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;File Locations&lt;/h2&gt;

&lt;p&gt;As an sbt refresher anything in the &lt;code&gt;project/&lt;/code&gt; folder relates to the build. If you want to develop your own plugin just for the current project you can simply add your .scala files to &lt;code&gt;project/&lt;/code&gt;. If you want to develop your own plugin as a standalone project you put those files in the &lt;code&gt;src/&lt;/code&gt; directory as usual. I mistakenly thought an sbt plugin project only required files in the &lt;code&gt;project/&lt;/code&gt; folder. Silly me.&lt;/p&gt;

&lt;h2 id=&#34;sbt-builds:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;SBT Builds&lt;/h2&gt;

&lt;p&gt;It&amp;#8217;s important to note that the project folder&amp;#8211;and the build itself&amp;#8211;is separate from how your source code is built. SBT uses Scala 2.10, so anything in the &lt;code&gt;project/&lt;/code&gt; folder will be built against 2.10 even if your project is set to 2.11. Thus when developing your plugin use Scala 2.10 to match sbt.&lt;/p&gt;

&lt;h2 id=&#34;dependencies:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;Dependencies&lt;/h2&gt;

&lt;p&gt;Usually when you include a plugin you specify it in the &lt;code&gt;project/plugins.sbt&lt;/code&gt;, right? But what if you&amp;#8217;re developing a plugin that uses other plugins? Your code is in &lt;code&gt;src/&lt;/code&gt; so it won&amp;#8217;t pick up anything in &lt;code&gt;project/&lt;/code&gt; as that only relates to your build. So you need to add whatever plugin you want as a &lt;code&gt;dependency&lt;/code&gt; in your build so its available in within your project, just like any other dependency. But there&amp;#8217;s a trick with sbt plugins. Originally I had the usual in &lt;code&gt;build.sbt&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;libraryDependencies += &amp;quot;com.typesafe.sbt&amp;quot; % &amp;quot;sbt-native-packager&amp;quot; % &amp;quot;0.8.0-M2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but kept getting unresolved dependency errors. This made no sense to me as the plugin is clearly available. It turns out if you want to include an sbt plugin as a project dependency you need to specify it in a special way, explicitly setting the sbt and scala version you want:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;libraryDependencies += sbtPluginExtra(&amp;quot;com.typesafe.sbt&amp;quot; % &amp;quot;sbt-native-packager&amp;quot; % &amp;quot;0.8.0-M2&amp;quot;, sbtV = &amp;quot;0.13&amp;quot;, scalaV = &amp;quot;2.10&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With that, your dependency will resolve and you can use include anything under sbt-native-packager when developing your plugin.&lt;/p&gt;

&lt;h2 id=&#34;specifying-your-plugin-defaults:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;Specifying your Plugin Defaults&lt;/h2&gt;

&lt;p&gt;With your separate project and dependencies satisfied you can now create your plugin which uses other plugins and defaults settings specific to you. This part is easy and follows the usual documentation. Declare an object which extends AutoPlugin and override &lt;code&gt;projectSettings&lt;/code&gt; or &lt;code&gt;buildSettings&lt;/code&gt;. This class looks exactly like it would if you were setting things manually in your build.&lt;/p&gt;

&lt;p&gt;For instance, here&amp;#8217;s how we&amp;#8217;d set the &lt;code&gt;java_server&lt;/code&gt; archetype as the default in our plugin:&lt;/p&gt;

&lt;pre class=&#34;code scala&#34;&gt;package com.hamrah.plugindefaults

import sbt._
import Keys._
import com.typesafe.sbt.SbtNativePackager._

object PluginDefaults extends AutoPlugin {
 override lazy val projectSettings = packageArchetype.java_server
}
&lt;/pre&gt;

&lt;p&gt;You can concatenate any other settings you want to project settings, like scalaVersion, scalacOptions, etc.&lt;/p&gt;

&lt;h2 id=&#34;using-the-plugin:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;Using the Plugin&lt;/h2&gt;

&lt;p&gt;You can build and publish your plugin to a repo and include it like you would any other plugin. Or you can include it locally for testing by putting this in your sbt file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lazy val root = project.in( file(&amp;quot;.&amp;quot;) ).dependsOn( defaultPluginSettings )
lazy val defaultPluginSettings = uri(&amp;quot;file:///&amp;lt;full path to your plugin directory&amp;gt;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your default settings can be explicitly added to your project if not automatically imported with a simple:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PluginDefaults.projectSettings
//or
settings = PluginDefaults.projectSettings // in a .scala file
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;in-closing:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;In Closing&lt;/h2&gt;

&lt;p&gt;As an FYI there could be better ways to do this. A lot of the above was trial and error, but works. If you have feedback or better suggestions please leave a comment!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Accessing the Docker Host Server Within a Container</title>
          <link>http://blog.michaelhamrah.com/2014/06/accessing-the-docker-host-server-within-a-container/</link>
          <pubDate>Sun, 29 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/accessing-the-docker-host-server-within-a-container/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/userguide/dockerlinks/#working-with-links-names&#34;&gt;Docker links&lt;/a&gt; are a great way to link two containers together but sometimes you want to know more about the host and network from within a container. You have a couple of options:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can access the Docker host by the container&amp;#8217;s gateway.&lt;/li&gt;
&lt;li&gt;You can access the Docker host by its ip address from within a container.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-gateway-approach:d6e669ce9206f072ff43b8fdf5b03a0c&#34;&gt;The Gateway Approach&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dotcloud/docker/issues/1143&#34;&gt;This GitHub Issue&lt;/a&gt; outlines the solution. Essentially you&amp;#8217;re using netstat to parse the gateway the docker container uses to access the outside world. This is the docker0 bridge on the host.&lt;/p&gt;

&lt;p&gt;As an example, we&amp;#8217;ll run a simple docker container which returns the hostname of the container on port 8080:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run -d -p 8080:8080 mhamrah/mesos-sample
&lt;/pre&gt;

&lt;p&gt;Next we&amp;#8217;ll run /bin/bash in another container to do some discovery:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run -i -t ubuntu /bin/bash
#once in, install curl:
apt-get update
apt-get install -y curl
&lt;/pre&gt;

&lt;p&gt;We can use the following command to pull out the gateway from netstat:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;netstat -nr | grep &#39;^0\.0\.0\.0&#39; | awk &#39;{print $2}&#39;
#returns 172.17.42.1 for me.
&lt;/pre&gt;

&lt;p&gt;We can then curl our other docker container, and we should get that docker container&amp;#8217;s hostname:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl 172.17.42.1:8080
# returns 00b019ce188c
&lt;/pre&gt;

&lt;p&gt;Nothing exciting, but you get the picture: it doesn&amp;#8217;t matter that the service is inside another container, we&amp;#8217;re accessing it via the host, and we didn&amp;#8217;t need to use links. We just needed to know the port the other service was listening on. If you had a service running on some other port&amp;#8211;say Postgres on 5432&amp;#8211;not running in a Docker container&amp;#8211;you can access it via &lt;code&gt;172.17.42.1:5432&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have docker installed in your container you can also query the docker host:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# In a container with docker installed list other containers running on the host for other containers:
docker -H tcp://172.17.42.1:2375 ps
CONTAINER ID        IMAGE                         COMMAND                CREATED              STATUS              PORTS                     NAMES
09d035054988        ubuntu:14.04                  /bin/bash              About a minute ago   Up About a minute   0.0.0.0:49153-&gt;8080/tcp   angry_bardeen
00b019ce188c        mhamrah/mesos-sample:latest   /opt/delivery/bin/de   8 minutes ago        Up 8 minutes        0.0.0.0:8080-&gt;8080/tcp    suspicious_colden
&lt;/pre&gt;

&lt;p&gt;You can use this for some hakky service-discovery.&lt;/p&gt;

&lt;h2 id=&#34;the-ip-approach:d6e669ce9206f072ff43b8fdf5b03a0c&#34;&gt;The IP Approach&lt;/h2&gt;

&lt;p&gt;The gateway approach is great because you can figure out a way to access a host from entirely within a container. You also have the same access via the host&amp;#8217;s ip address. I&amp;#8217;m using boot2docker, and the boot2docker ip address is &lt;code&gt;192.168.59.103&lt;/code&gt; and I can accomplish the same tasks as the gateway approach:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Docker processes, via ip:
docker -H tcp://192.168.59.103:2375 ps
# Other docker containers, via ip:
curl 192.168.59.103:8080
&lt;/pre&gt;

&lt;p&gt;Although there&amp;#8217;s no way to introspect the host&amp;#8217;s ip address (AFAIK) you can pass this in via an environment variable:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker@boot2docker:~$  docker run -i -t -e DOCKER_HOST=192.168.59.103 ubuntu /bin/bash
root@07561b0607f4:/# env
HOSTNAME=07561b0607f4
DOCKER_HOST=192.168.59.103
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PWD=/
&lt;/pre&gt;

&lt;p&gt;If the container knows the ip address of its host, you can broadcast this out to other services via the container&amp;#8217;s application. Useful for service discovery tools run from within a container where you want to tell others the host IP so others can find you.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Service Discovery Options with Marathon and Deimos</title>
          <link>http://blog.michaelhamrah.com/2014/06/service-discovery-options-with-marathon-and-deimos/</link>
          <pubDate>Sun, 29 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/service-discovery-options-with-marathon-and-deimos/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve become a fan of Mesos and Marathon: combined with &lt;a href=&#34;https://github.com/mesosphere/deimos&#34;&gt;Deimos&lt;/a&gt; you can create a DIY PaaS for launching and scaling Docker containers across a number of nodes. Marathon supports a bare-bones service-discovery mechanism through its task API, but it would be nice for containers to register themselves with some service discovery tool themselves. In order to achieve this containers need to know their host ip address and the port Marathon assigned them so they could tell other interested services where they can be found.&lt;/p&gt;

&lt;p&gt;Deimos allows default parameters to be passed in when executing &lt;code&gt;docker run&lt;/code&gt; and Marathon adds assigned ports to a container&amp;#8217;s environment variables. If a container has this information it can register it with a service discovery tool.&lt;/p&gt;

&lt;p&gt;Here we assign the host&amp;#8217;s IP address as a default run option in our &lt;a href=&#34;https://github.com/mesosphere/deimos/#configuration&#34;&gt;Deimos config file&lt;/a&gt;.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#/etc/deimos.cfg
[containers.options]
append: [&#34;-e&#34;, &#34;HOST_IP=192.168.33.12&#34;]
&lt;/pre&gt;

&lt;p&gt;Now let&amp;#8217;s launch our mesos-sample container to our Mesos cluster via Marathon:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;// Post to http://192.168.33.12/v2/apps
{
  &#34;container&#34;: {
    &#34;image&#34;: &#34;docker:///mhamrah/mesos-sample&#34;
  },
  &#34;cpus&#34;: &#34;1&#34;,
  &#34;id&#34;: &#34;www&#34;,
  &#34;instances&#34;: 1,
  &#34;mem&#34;: 512,
  &#34;ports&#34;: [0],
  &#34;uris&#34;: [],
  &#34;cmd&#34;: &#34;&#34;
}
&lt;/pre&gt;

&lt;p&gt;Once our app is launch, we can inspect all the environment variables in our container with the &lt;code&gt;/env&lt;/code&gt; endpoint from &lt;code&gt;mhamrah/mesos-sample&lt;/code&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;curl http://192.168.33.12:31894/env
[ {
  &#34;HOSTNAME&#34; : &#34;a4305981619d&#34;
}, {
  &#34;PORT0&#34; : &#34;31894&#34;
}, {
  &#34;PATH&#34; : &#34;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&#34;
}, {
  &#34;PWD&#34; : &#34;/tmp/mesos-sandbox&#34;
}, {
  &#34;PORTS&#34; : &#34;31894&#34;
}, {
  &#34;HOST_IP&#34; : &#34;192.168.33.12&#34;
}, {
  &#34;PORT&#34; : &#34;31894&#34;
}]
&lt;/pre&gt;

&lt;p&gt;With this information some startup script could use the &lt;code&gt;PORT&lt;/code&gt; (or &lt;code&gt;PORT0&lt;/code&gt;) and &lt;code&gt;HOST_IP&lt;/code&gt; to register itself for direct point-to-point communication in a cluster.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Setting up a Multi-Node Mesos Cluster running Docker, HAProxy and Marathon with Ansible</title>
          <link>http://blog.michaelhamrah.com/2014/06/setting-up-a-multi-node-mesos-cluster-running-docker-haproxy-and-marathon-with-ansible/</link>
          <pubDate>Thu, 26 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/setting-up-a-multi-node-mesos-cluster-running-docker-haproxy-and-marathon-with-ansible/</guid>
          <description>

&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;With Mesos 0.20 Docker support is now native, and Deimos has been deprecated. The ansible-mesos-playbook has been updated appropriately, and most of this blog post still holds true. There are slight variations with how you post to Marathon.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf&#34;&gt;Google Omega Paper&lt;/a&gt; has given birth to cloud vNext: cluster schedulers managing containers. You can make a bunch of nodes appear as one big computer and deploy anything to your own private cloud; just like Docker, but across any number of nodes. &lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes&#34;&gt;Google&amp;#8217;s Kubernetes&lt;/a&gt;, &lt;a href=&#34;flynn.io&#34;&gt;Flynn&lt;/a&gt;, &lt;a href=&#34;https://github.com/coreos/fleet&#34;&gt;Fleet&lt;/a&gt; and &lt;a href=&#34;http://mesos.apache.org/&#34;&gt;Apache Mesos&lt;/a&gt;, originally from Twitter, are implementations of Omega with the goal of abstracting away discrete nodes and optimizing compute resources. Each implementation has its own tweak, but they all follow the same basic setup: leaders, for coordination and scheduling; some service discovery component; some underlying cluster tool (like Zookeeper); followers, for processing.&lt;/p&gt;

&lt;p&gt;In this post we&amp;#8217;ll use &lt;a href=&#34;http://www.ansible.com/home&#34;&gt;Ansible&lt;/a&gt; to install a multi-node Mesos cluster using packages from &lt;a href=&#34;http://mesosphere.io/&#34;&gt;Mesosphere&lt;/a&gt;. Mesos, as a cluster framework, allows you to run a variety of cluster-enabled software, including &lt;a href=&#34;http://spark.apache.org/&#34;&gt;Spark&lt;/a&gt;, &lt;a href=&#34;https://github.com/mesosphere/storm-mesos&#34;&gt;Storm&lt;/a&gt; and &lt;a href=&#34;https://github.com/mesos/hadoop&#34;&gt;Hadoop&lt;/a&gt;. You can also run &lt;a href=&#34;https://github.com/jenkinsci/mesos-plugin&#34;&gt;Jenkins&lt;/a&gt;, schedule tasks with &lt;a href=&#34;https://github.com/airbnb/chronos&#34;&gt;Chronos&lt;/a&gt;, even run &lt;a href=&#34;https://github.com/mesosphere/elasticsearch-mesos&#34;&gt;ElasticSearch&lt;/a&gt; and &lt;a href=&#34;https://github.com/mesosphere/cassandra-mesos&#34;&gt;Cassandra&lt;/a&gt; without having to double to specific servers. We&amp;#8217;ll also set up &lt;a href=&#34;https://github.com/mesosphere/marathon&#34;&gt;Marathon&lt;/a&gt; for running services with &lt;a href=&#34;https://github.com/mesosphere/deimos&#34;&gt;Deimos&lt;/a&gt; support for Docker containers.&lt;/p&gt;

&lt;p&gt;Mesos, even with Marathon, doesn&amp;#8217;t offer the holistic integration of some other tools, namely Kubernetes, but at this point it&amp;#8217;s easier to set up on your own set of servers. Although young Mesos is one of the oldest projects of the group and allows more of a DIY approach on service composition.&lt;/p&gt;

&lt;h3 id=&#34;tl-dr:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/mhamrah/ansible-mesos-playbook&#34;&gt;The playbook is on github, just follow the readme!&lt;/a&gt;&lt;/em&gt;. If you want to simply try out Mesos, Marathon, and Docker &lt;a href=&#34;http://mesosphere.io/learn/run-docker-on-mesosphere/&#34;&gt;mesosphere has an excellent tutorial to get you started on a single node&lt;/a&gt;. This tutorial outlines the creation of a more complex multi-node setup.&lt;/p&gt;

&lt;h3 id=&#34;system-setup:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;System Setup&lt;/h3&gt;

&lt;p&gt;The system is divided into two parts: a set of masters, which handle scheduling and task distribution, with a set of slaves providing compute power. Mesos uses Zookeeper for cluster coordination and leader election. A key component is service discovery: you don&amp;#8217;t know which host or port will be assigned to a task, which makes, say, accessing a website running on a slave difficult. The Marathon API allows you to query task information, and we use this feature to configure HAProxy frontend/backend resources.&lt;/p&gt;

&lt;p&gt;Our masters run:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Zookeeper&lt;/li&gt;
&lt;li&gt;Mesos-Master&lt;/li&gt;
&lt;li&gt;HAProxy&lt;/li&gt;
&lt;li&gt;Marathon&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and our slaves run:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mesos-Slave&lt;/li&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;li&gt;Deimos, the Mesos -&amp;gt; Docker bridge&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ansible:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Ansible&lt;/h3&gt;

&lt;p&gt;Ansible works by running a playbook, composed of roles, against a set of hosts, organized into groups. My &lt;a href=&#34;https://github.com/mhamrah/ansible-mesos-playbook&#34;&gt;Ansible-Mesos-Playbook&lt;/a&gt; on GitHub has an example hosts file with some EC2 instances listed. You should be able to replace these with your own EC2 instances running Ubuntu 14.04, our your own private instances running Ubuntu 14.04. Ansible allows us to pass node information around so we can configure multiple servers to properly set up our masters, zookeeper set, point slaves to masters, and configure Marathon for high availability.&lt;/p&gt;

&lt;p&gt;We want at least three servers in our master group for a proper zookeeper quorum. We use host variables to specify the zookeeper id for each node.&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;[mesos_masters]
ec2-54-204-214-172.compute-1.amazonaws.com zoo_id=1
ec2-54-235-59-210.compute-1.amazonaws.com zoo_id=2
ec2-54-83-161-83.compute-1.amazonaws.com zoo_id=3
&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/mhamrah/ansible-mesos&#34;&gt;mesos-ansible&lt;/a&gt; playbook will use nodes in the &lt;code&gt;mesos_masters&lt;/code&gt; for a variety of configuration options. First, the &lt;code&gt;/etc/zookeeper/conf/zoo.cfg&lt;/code&gt; will list all master nodes, with &lt;code&gt;/etc/zookeeper/conf/myid&lt;/code&gt; being set appropriately. It will also set up upstart scripts in &lt;code&gt;/etc/init/mesos-master.conf&lt;/code&gt;, &lt;code&gt;/etc/init/mesos-slave.conf&lt;/code&gt; with default configuration files in &lt;code&gt;/etc/defaults/mesos.conf&lt;/code&gt;. Mesos 0.19 supports external executors, so we use Deimos to run docker containers. This is only required on slaves, but the configuration options are set in the shared &lt;code&gt;/etc/defaults/mesos.conf&lt;/code&gt; file.&lt;/p&gt;

&lt;h3 id=&#34;marathon-and-haproxy:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Marathon and HAProxy&lt;/h3&gt;

&lt;p&gt;The playbook leverages an &lt;code&gt;ansible-marathon&lt;/code&gt; role to install a custom build of marathon with Deimos support. If Mesos is the OS for the data center, Marathon is the init system. Marathoin allows us to &lt;code&gt;http post&lt;/code&gt; new tasks, containing docker container configurations, which will run on Mesos slaves. With HAProxy we can use the masters as a load balancing proxy server routing traffic from known hosts (the masters) to whatever node/port is running the marathon task. HAProxy is configured via a cron job running &lt;a href=&#34;https://github.com/mhamrah/ansible-marathon/blob/master/files/haproxy_dns_cfg&#34;&gt;a custom bash script&lt;/a&gt;. The script queries the marathon API and will route to the appropriate backend by matching a host header prefix to the marathon job name.&lt;/p&gt;

&lt;h3 id=&#34;mesos-followers-slaves:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Mesos Followers (Slaves)&lt;/h3&gt;

&lt;p&gt;The slaves are pretty straightforward. We don&amp;#8217;t need any host variables, so we just list whatever slave nodes you&amp;#8217;d like to configure:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;[mesos_slaves]
ec2-54-91-78-105.compute-1.amazonaws.com
ec2-54-82-227-223.compute-1.amazonaws.com 
&lt;/pre&gt;

&lt;p&gt;Mesos-Slave will be configured with Deimos support.&lt;/p&gt;

&lt;h3 id=&#34;the-result:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;The Result&lt;/h3&gt;

&lt;p&gt;With all this set up you can set up a wildcard domain name, say &lt;code&gt;*.example.com&lt;/code&gt;, to point to all of your master node ip addresses. If you launch a task like &amp;#8220;www&amp;#8221; you can visit www.example.com and you&amp;#8217;ll hit whatever server is running your application. Let&amp;#8217;s try launching a simple web server which returns the docker container&amp;#8217;s hostname:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;POST to one of our masters:

POST /v2/apps

{
  &#34;container&#34;: {
    &#34;image&#34;: &#34;docker:///mhamrah/mesos-sample&#34;
  },
  &#34;cpus&#34;: &#34;.25&#34;,
  &#34;id&#34;: &#34;www&#34;,
  &#34;instances&#34;: 4,
  &#34;mem&#34;: 512,
  &#34;ports&#34;: [0],
  &#34;uris&#34;: []
}
&lt;/pre&gt;

&lt;p&gt;We run four instances allocating 25% of a cpu with an application name of &lt;code&gt;www&lt;/code&gt;. If we hit &lt;code&gt;www.example.com&lt;/code&gt;, we&amp;#8217;ll get the hostname of the docker container running on whatever slave node is hosting the task. Deimos will inspect whatever ports are &lt;code&gt;EXPOSE&lt;/code&gt;d in the docker container and assign a port for Mesos to use. Even though the config script only works on port 80 you can easily reconfigure for your own needs.&lt;/p&gt;

&lt;p&gt;To view marathon tasks, simply go to one of your master hosts on port 8080. Marathon will proxy to the correct master. To view mesos tasks, navigate to port 5050 and you&amp;#8217;ll be redirected to the appropriate master. You can also inspect the STDOUT and STDERR of Mesos tasks.&lt;/p&gt;

&lt;h3 id=&#34;notes:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Notes&lt;/h3&gt;

&lt;p&gt;In my testing I noticed, on rare occasion, the cluster didn&amp;#8217;t have a leader or marathon wasn&amp;#8217;t running. You can simply restart zookeeper, mesos, or marathon via ansible:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#Restart Zookeeper
ansible mesos_masters -a &#34;sudo service zookeeper restart&#34;
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a high probability something won&amp;#8217;t work. Check the logs, it took me a while to get things working: grepping &lt;code&gt;/var/log/syslog&lt;/code&gt; will help, along with &lt;code&gt;/var/log/upstart/mesos-master.conf&lt;/code&gt;, &lt;code&gt;mesos-slave.conf&lt;/code&gt; and &lt;code&gt;marathon.conf&lt;/code&gt;, along with the &lt;code&gt;/var/log/mesos/&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;what-8217-s-next:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;What&amp;#8217;s Next&lt;/h3&gt;

&lt;p&gt;Cluster schedulers are an exciting tool for running production applications. It&amp;#8217;s never been easier to build, package and deploy services on public, private clouds or bare metal servers. Mesos, with Marathon, offers a cool combination for running docker containers&amp;#8211;and other mesos-based services&amp;#8211;in production. &lt;a href=&#34;https://engineering.twitter.com/university/videos/docker-mesos&#34;&gt;This Twitter U video highlights how OpenTable uses Mesos for production&lt;/a&gt;. The HAProxy approach, albeit simple, offers a way to route traffic to the correct container. HAProxy will detect failures and reroute traffic accordingly.&lt;/p&gt;

&lt;p&gt;I didn&amp;#8217;t cover inter-container communication (say, a website requiring a database) but you can use your service-discovery tool of choice to solve the problem. The Mesos-Master nodes provide good &amp;#8220;anchor points&amp;#8221; for known locations to look up stuff; you can always query the marathon api for service discovery. Ansible provides a way to automate the install and configuration of mesos-related tools across multiple nodes so you can have a serious mesos-based platform for testing or production use.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Akka Clustering with SBT-Docker and SBT-Native-Packager</title>
          <link>http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/</link>
          <pubDate>Thu, 19 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/</guid>
          <description>

&lt;p&gt;Since my last post on &lt;a href=&#34;http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/&#34;&gt;akka clustering with docker containers&lt;/a&gt; a new plugin, &lt;a href=&#34;https://github.com/marcuslonnberg/sbt-docker&#34;&gt;SBT-Docker&lt;/a&gt;, has emerged which allows you to build docker containers directly from SBT. I&amp;#8217;ve updated my &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;akka-docker-cluster-example&lt;/a&gt; to leverage these two plugins for a smoother docker build experience.&lt;/p&gt;

&lt;h2 id=&#34;one-step-build:9dc58615474f52923afa41a9d5040e47&#34;&gt;One Step Build&lt;/h2&gt;

&lt;p&gt;The approach is basically the same as the previous example: we use SBT Native Packager to gather up the appropriate dependencies, upload them to the docker container, and create the entrypoint. I decided to keep the start script approach to &amp;#8220;prep&amp;#8221; any environment variables required before launching. With SBT Docker linked to Native Packager all you need to do is fire&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;docker
&lt;/pre&gt;

&lt;p&gt;from sbt and you have a docker container ready to launch or push.&lt;/p&gt;

&lt;h2 id=&#34;understanding-the-build:9dc58615474f52923afa41a9d5040e47&#34;&gt;Understanding the Build&lt;/h2&gt;

&lt;p&gt;SBT Docker requires a dockerfile defined in your build. I want to pass in artifacts from native packager to docker. This allows native packager to focus on application needs while docker is focused on docker. Docker turns into just another type of package for your app.&lt;/p&gt;

&lt;p&gt;We can pass in arguments by mapping the appropriate parameters to a function which returns the Dockerfile. In build.spt:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;// Define a dockerfile, using parameters from native-packager
dockerfile in docker &amp;lt;&amp;lt;= (name, stagingDirectory in Universal) map {
  case(appName, stageDir) =&gt;
    val workingDir = s&#34;/opt/${appName}&#34;
    new Dockerfile {
      //use java8 base box
      from(&#34;relateiq/oracle-java8&#34;)
      maintainer(&#34;Michael Hamrah&#34;)
      //expose our akka port
      expose(1600)
      //upload native-packager staging directory files
      add(stageDir, workingDir)
      //make files executable
      run(&#34;chmod&#34;, &#34;+x&#34;, s&#34;/opt/${appName}/bin/${appName}&#34;)
      run(&#34;chmod&#34;, &#34;+x&#34;, s&#34;/opt/${appName}/bin/start&#34;)
      //set working directory
      workDir(workingDir)
      //entrypoint into our start script
      entryPointShell(s&#34;bin/start&#34;, appName, &#34;$@&#34;)
    }
}
&lt;/pre&gt;

&lt;h3 id=&#34;linking-sbt-docker-to-sbt-native-packager:9dc58615474f52923afa41a9d5040e47&#34;&gt;Linking SBT Docker to SBT Native Packager&lt;/h3&gt;

&lt;p&gt;Because we&amp;#8217;re relying on Native Packager to assemble our runtime dependencies we need to ensure the native packager files are &amp;#8220;staged&amp;#8221; before docker tries to upload them. Luckily it&amp;#8217;s easy to create dependencies with SBT. We simply have docker depend on the native packager&amp;#8217;s stage task:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;docker &amp;lt;&amp;lt;= docker.dependsOn(com.typesafe.sbt.packager.universal.Keys.stage.in(Compile))
&lt;/pre&gt;

&lt;h3 id=&#34;adding-additional-files:9dc58615474f52923afa41a9d5040e47&#34;&gt;Adding Additional Files&lt;/h3&gt;

&lt;p&gt;The last step is to add our start script to the native packager build. Native packager has a &lt;code&gt;mappings&lt;/code&gt; key where we can add files to our package. I kept the start script in the docker folder and I want it in the bin directory within the docker container. Here&amp;rsquo;s the mapping:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;mappings in Universal += baseDirectory.value / &#34;docker&#34; / &#34;start&#34; -&gt; &#34;bin/start&#34;
&lt;/pre&gt;

&lt;p&gt;With this setting everything will be assembled as needed and we can package to any type we want. Setting up a cluster with docker is &lt;a href=&#34;http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/&#34;&gt;the same as before&lt;/a&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run --name seed -i -t clustering
docker run --name c1 -link seed:seed -i -t clustering
&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s interesting to note SBT Native Packager also has docker support, but it&amp;rsquo;s undocumented and doesn&amp;rsquo;t allow granular control over the Dockerfile output. Until SBT Native Packager fully supports docker output the SBT Docker plugin is a nice tool to package your sbt-based apps.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Spray Directives: Creating Your Own, Simple Directive</title>
          <link>http://blog.michaelhamrah.com/2014/05/spray-directives-creating-your-own-simple-directive/</link>
          <pubDate>Sat, 24 May 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/05/spray-directives-creating-your-own-simple-directive/</guid>
          <description>&lt;p&gt;The &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/&#34;&gt;spray-routing&lt;/a&gt; package provides an excellent dsl for creating restful api&amp;#8217;s with Scala and Akka. This dsl is powered by &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/key-concepts/directives/&#34;&gt;directives&lt;/a&gt;, small building blocks you compose to filter, process and compose requests and responses for your API. Building your own directives lets you create reusable components for your application and better organize your application.&lt;/p&gt;

&lt;p&gt;I recently refactored some code in a Spray API to leverage custom directives. The &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/advanced-topics/custom-directives/&#34;&gt;Spray documentation provides a good reference on custom directives&lt;/a&gt; but I found myself getting hung up in a few places.&lt;/p&gt;

&lt;p&gt;As an example we&amp;#8217;re going to write a custom directive which produces a UUID for each request. Here&amp;#8217;s how I want to use this custom directive:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;generateUUID { uuid =&gt;
  path(&#34;foo&#34;) {
   get {
     //log the uuid, pass it to your app, or maybe just return it
     complete { uuid.toString }
   }
  }
}
&lt;/pre&gt;

&lt;p&gt;Usually you leverage existing directives to build custom directives. I (incorrectly) started with the &lt;code&gt;provide&lt;/code&gt; directive to provide a value to an inner route:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;import spray.routing._
import java.util.UUID
import Directives._

trait UuidDirectives {
  def generateUuid: Directive1[UUID] = {
    provide(UUID.randomUUID)
  }
}
&lt;/pre&gt;

&lt;p&gt;Before I explain what&amp;#8217;s wrong, let&amp;#8217;s dig into the code. First, generateUuid is a function which returns a Directive1 wrapping a UUID value. Directive1 is just a type alias for &lt;code&gt;Directive[UUID :: HNil]&lt;/code&gt;. Directives are centered around a feature of the shapeless library called heterogeneous lists, or HLists. An &lt;code&gt;HList&lt;/code&gt; is simply a list, but each element in the list can be a different, specific type. Instead of a generic &lt;code&gt;List[Any]&lt;/code&gt;, your list can be composed of specific types of list of String, Int, String, UUID. The first element of this list is a String, not an Any, and the second is an Int, with all the features of an Int. In the directive above I just have an &lt;code&gt;HList&lt;/code&gt; with one element: &lt;code&gt;UUID&lt;/code&gt;. If I write &lt;code&gt;Directive[UUID :: String :: HNil]&lt;/code&gt; I have a two element list of &lt;code&gt;UUID&lt;/code&gt; and String, and the compiler will throw an error if I try to use this directive with anything other a &lt;code&gt;UUID&lt;/code&gt; and a String. HLists sound like a lightweight case class, but with an &lt;code&gt;HList&lt;/code&gt;, you get a lot of list-like features. HLists allow the compiler to do the heavy lifting of type safety, so you can have strongly-typed functions to compose together.&lt;/p&gt;

&lt;p&gt;Provide is a directive which (surprise surprise) will provide a value to an inner route. I thought this would be perfect for my directive, and the corresponding test ensures it works:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;import org.scalatest._
import org.scalatest.matchers._
import spray.testkit.ScalatestRouteTest
import spray.http._
import spray.routing.Directives._

class UuidDirectivesSpec
  extends FreeSpec
  with Matchers
  with UuidDirectives
  with ScalatestRouteTest {

  &#34;The UUID Directive&#34; - {
    &#34;can generate a UUID&#34; in {
      Get() ~&gt; generateUuid { uuid =&gt; complete(uuid.toString) } ~&gt; check  {
        responseAs[String].size shouldBe 36
      }
    }
  }
}
&lt;/pre&gt;

&lt;p&gt;But there&amp;#8217;s an issue! Spray directives are classes are composed when instantiated via an apply() function. The &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/advanced-topics/understanding-dsl-structure/&#34;&gt;Spray docs on understanding the dsl structure&lt;/a&gt; explains it best, but in summary, generateUuid will only be called once when the routing tree is built, not on every request.&lt;/p&gt;

&lt;p&gt;A better unit test shows the issue:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;will generate different UUID per request&#34; in {
      //like the runtime, instantiate route once
      val uuidRoute =  generateUuid { uuid =&gt; complete(uuid.toString) }

      var uuid1: String = &#34;&#34;
      var uuid2: String = &#34;&#34;
      Get() ~&gt; uuidRoute ~&gt; check  {
        responseAs[String].size shouldBe 36
        uuid1 = responseAs[String]
      }
      Get() ~&gt; uuidRoute ~&gt; check  {
        responseAs[String].size shouldBe 36
        uuid2 = responseAs[String]
      }
      //fails!
      uuid1 shouldNot equal (uuid2)
    }
  }
&lt;/pre&gt;

&lt;p&gt;The fix is simple: we need to use the &lt;code&gt;extract&lt;/code&gt; directive which applies the current RequestContext to our route so it&amp;#8217;s called on every request. For our UUID directive we don&amp;#8217;t need anything from the request, just the function which is run for every request:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;trait UuidDirectives {
  def generateUuid: Directive[UUID :: HNil] = {
    extract(ctx =&gt;
        UUID.randomUUID)
  }
}
&lt;/pre&gt;

&lt;p&gt;With our randomUUID call wrapped in an extract directive we have a unique call per request, and our tests pass!&lt;/p&gt;

&lt;p&gt;In a following post we&amp;#8217;ll add some more complexity to our custom directive, stay tuned!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Spray Directives: Custom Directives, Part Two: flatMap</title>
          <link>http://blog.michaelhamrah.com/2014/05/spray-directives-custom-directives-part-two-flatmap/</link>
          <pubDate>Sat, 24 May 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/05/spray-directives-custom-directives-part-two-flatmap/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;http://blog.michaelhamrah.com/2014/05/spray-directives-creating-your-own-simple-directive/&#34;&gt;Our last post covered custom Spray Directives&lt;/a&gt;. We&amp;#8217;re going to expand our UUID directive a little further. Generating a unique ID per request is great, but what if we want the client to pass in an existing unique identifier to act as a correlation id between systems?&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll modify our existing directive by checking to see if the client supplied a correlation-id request-header using the existing &lt;code&gt;optionalHeaderValueByName&lt;/code&gt; directive:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;def generateUuid: Directive[UUID :: HNil] = {
    optionalHeaderValueByName(&#34;correlation-id&#34;) {
      case Some(value) =&gt; provide(UUID.fromString(value))
      case None =&gt; provide(UUID.randomUUID)
    }
  }
&lt;/pre&gt;

&lt;p&gt;Unfortunately this code doesn&amp;#8217;t compile! We get an error because Spray is looking for a Route, which is a function of RequestContext =&amp;gt; Unit:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;[error]  found   : spray.routing.Directive1
[error]     (which expands to)  spray.routing.Directive[shapeless.::]
[error]  required: spray.routing.RequestContext =&gt; Unit
[error]       case Some(value) =&gt; provide(UUID.fromString(value))
&lt;/pre&gt;

&lt;p&gt;What do we do? &lt;code&gt;flatMap&lt;/code&gt; comes to the rescue. Here&amp;#8217;s the deal: we need to transform one directive (&lt;code&gt;optionalHeaderValueByName&lt;/code&gt;) into another directive (one that provides a UUID). We do this by using flatMap to focus on the value in the first directive (the option returned from &lt;code&gt;optionalHeaderValueByName&lt;/code&gt;) and return another value (the UUID). With &lt;code&gt;flatMap&lt;/code&gt; we are basically &amp;#8220;repackaging&amp;#8221; one value into another package.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the updated code which properly compiles:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;def generateUuid: Directive[UUID :: HNil] = {
    //use flatMap to match on the Option returned and provide
    //a new value
    optionalHeaderValueByName(&#34;correlation-id&#34;).flatMap {
      case Some(value) =&gt; provide(UUID.fromString(value))
      case None =&gt; provide(UUID.randomUUID)
    }
  }
&lt;/pre&gt;

&lt;p&gt;and the test:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;can extract a uuid value from the header&#34; in {
      val uuid = java.util.UUID.randomUUID.toString

      Get() ~&gt; addHeader(&#34;correlation-id&#34;, uuid) ~&gt; uuidRoute ~&gt; check {
        responseAs[String] shouldEqual uuid
      }
    }
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a small tweak we&amp;#8217;ll make to our UUID directive to show another example of directive composition. If the client doesn&amp;#8217;t supply a UUID, and we call generateUUID multiple times, we&amp;#8217;ll get different uuids for the same request. This defeats the purpose of a single correlation id, and prevents us from extracting a uuid multiple times per request. A failing test shows the issue:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;can extract the same uuid twice per request&#34; in {
      var uuid1: String =&#34;&#34;
      var uuid2: String = &#34;&#34;
      Get() ~&gt; generateUuid { uuid =&gt;
        {
          uuid1 = uuid.toString
          generateUuid { another =&gt;
            uuid2 = another.toString
            complete(&#34;&#34;)
          }
        }
      } ~&gt; check {
        //fails
        uuid1 shouldEqual uuid2
      }
    }
&lt;/pre&gt;

&lt;p&gt;To fix the issue, if we generate a UUID, we will add it to the request header as if the client supplied it. We&amp;#8217;ll use the mapRequest directive to add the generated UUID to the header.&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;def generateUuid: Directive[UUID :: HNil] = {
    optionalHeaderValueByName(&#34;correlation-id&#34;).flatMap {
      case Some(value) =&gt; provide(UUID.fromString(value))
      case None =&gt;
        val id = UUID.randomUUID
        mapRequest(r =&gt; r.withHeaders(r.headers :+ RawHeader(&#34;correlation-id&#34;, id.toString))) &amp;#038; provide(id)
    }
  }
&lt;/pre&gt;

&lt;p&gt;In my first version I had the mapRequest call and the provide call on separate lines (there was no &amp;amp;). mapRequest was never being called, and it was because mapRequest was not being returned as a value- only the provide directive is returned. We need to &amp;#8220;merge&amp;#8221; these two directives with the &amp;amp; operator. &lt;code&gt;mapRequest&lt;/code&gt; is a no-op returning a Directive0 (a Directive with a Nil HList) so combining it with provide yields a Directive1[UUID], which is exactly what we want.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Running an Akka Cluster with Docker Containers</title>
          <link>http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/</link>
          <pubDate>Sun, 23 Mar 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/</guid>
          <description>

&lt;p&gt;&lt;strong&gt;Update! You can now use SBT-Docker with SBT-Native Packager for a better sbt/docker experience. &lt;a href=&#34;http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/&#34;&gt;Here&amp;#8217;s the new approach&lt;/a&gt; with an &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;updated GitHub repo&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We recently upgraded our vagrant environments to use &lt;a href=&#34;http://docker.io&#34;&gt;docker&lt;/a&gt;. One of our projects relies on &lt;a href=&#34;http://doc.akka.io/docs/akka/2.3.0/common/cluster.html&#34;&gt;akka&amp;#8217;s cluster functionality&lt;/a&gt;. I wanted to easily run an akka cluster locally using docker as sbt can be somewhat tedious. &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;The example project is on github&lt;/a&gt; and the solution is described below.&lt;/p&gt;

&lt;p&gt;The solution relies on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;Sbt Native Packager&lt;/a&gt; to package dependencies and create a startup file.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library for configuring the app&amp;#8217;s ip address and seed nodes. We setup cascading configurations that will look for docker link environment variables if present.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example/blob/master/bin/dockerize&#34;&gt;A simple bash script&lt;/a&gt; to package the app and build the docker container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;#8217;m a big fan of &lt;a href=&#34;http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library and the environment variable overrides come in handy for providing sensible defaults with optional overrides. It&amp;#8217;s the preferred way we configure our applications in upper environments.&lt;/p&gt;

&lt;p&gt;The tricky part of running an akka cluster with docker is knowing the ip address each remote node needs to listen on. An akka cluster relies on each node listening on a specific port and hostname or ip. It also needs to know the port and hostname/ip of a seed node the cluster. As there&amp;#8217;s no catch-all binding we need specific ip settings for our cluster.&lt;/p&gt;

&lt;p&gt;A simple bash script within the container will figure out the current IP for our cluster configuration and &lt;a href=&#34;http://docs.docker.io/en/latest/use/working_with_links_names/&#34;&gt;docker links&lt;/a&gt; pass seed node information to newly launched nodes.&lt;/p&gt;

&lt;h2 id=&#34;first-step-setup-application-configuration:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;First Step: Setup Application Configuration&lt;/h2&gt;

&lt;p&gt;The configuration is the same as that of a normal cluster, but I&amp;#8217;m using substitution to configure the ip address, port and seed nodes for the application. For simplicity I setup a &lt;code&gt;clustering&lt;/code&gt; block with defaults for running normally and environment variable overrides:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;clustering {
 ip = &#34;127.0.0.1&#34;
 ip = ${?CLUSTER_IP}
 port = 1600
 port = ${?CLUSTER_PORT}
 seed-ip = &#34;127.0.0.1&#34;
 seed-ip = ${?CLUSTER_IP}
 seed-ip = ${?SEED_PORT_1600_TCP_ADDR}
 seed-port = 1600
 seed-port = ${?SEED_PORT_1600_TCP_PORT}
 cluster.name = clustering-cluster
}

akka.remote {
    log-remote-lifecycle-events = on
    netty.tcp {
      hostname = ${clustering.ip}
      port = ${clustering.port}
    }
  }
  cluster {
    seed-nodes = [
       &#34;akka.tcp://&#34;${clustering.cluster.name}&#34;@&#34;${clustering.seed-ip}&#34;:&#34;${clustering.seed-port}
    ]
    auto-down-unreachable-after = 10s
  }
}
&lt;/pre&gt;

&lt;p&gt;As an example the &lt;code&gt;clustering.seed-ip&lt;/code&gt; setting will use &lt;em&gt;127.0.0.1&lt;/em&gt; as the default. If it can find a _CLUSTER&lt;em&gt;IP&lt;/em&gt; or a &lt;em&gt;SEED_PORT_1600_TCP_ADDR&lt;/em&gt; override it will use that instead. You&amp;#8217;ll notice the latter override is using docker&amp;#8217;s environment variable pattern for linking: that&amp;#8217;s how we set the cluster&amp;#8217;s seed node when using docker. You don&amp;#8217;t need the _CLUSTER&lt;em&gt;IP&lt;/em&gt; in this example but that&amp;#8217;s the environment variable we use in upper environments and I didn&amp;#8217;t want to change our infrastructure to conform to docker&amp;#8217;s pattern. The cascading settings are helpful if you&amp;#8217;re forced to follow one pattern depending on the environment. We do the same thing for the ip and port of the current node when launched.&lt;/p&gt;

&lt;p&gt;With this override in place we can use substitution to set the seed nodes in the akka cluster configuration block. The expression &lt;code&gt;&amp;quot;akka.tcp://&amp;quot;${clustering.cluster.name}&amp;quot;@&amp;quot;${clustering.seed-ip}&amp;quot;:&amp;quot;${clustering.seed-port}&lt;/code&gt; builds the proper akka URI so the current node can find the seed node in the cluster. Seed nodes avoid potential split-brain issues during network partitions. You&amp;#8217;ll want to run more than one in production but for local testing one is fine. On a final note the cluster-name setting is arbitrary. Because the name of the actor system and the uri must match I prefer not to hard code values in multiple places.&lt;/p&gt;

&lt;p&gt;I put these settings in resources/reference.conf. We could have named this file application.conf, but I prefer bundling configurations as reference.conf and reserving application.conf for external configuration files. A setting in application.conf will override a corresponding reference.conf setting and you probably want to manage application.conf files outside of the project&amp;#8217;s jar file.&lt;/p&gt;

&lt;h2 id=&#34;second-sbt-native-packager:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Second: SBT Native Packager&lt;/h2&gt;

&lt;p&gt;We use the native packager plugin to build a runnable script for our applications. For docker we just need to run &lt;code&gt;universal:stage&lt;/code&gt;, creating a folder with all dependencies in the &lt;code&gt;target/&lt;/code&gt; folder of our project. We&amp;#8217;ll move this into a staging directory for uploading to the docker container.&lt;/p&gt;

&lt;h2 id=&#34;third-the-dockerfile-and-start-script:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Third: The Dockerfile and Start script&lt;/h2&gt;

&lt;p&gt;The dockerfile is pretty simple:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;FROM dockerfile/java

MAINTAINER Michael Hamrah m@hamrah.com

ADD tmp/ /opt/app/
ADD start /opt/start
RUN chmod +x /opt/start

EXPOSE 1600

ENTRYPOINT [ &#34;/opt/start&#34; ]
&lt;/pre&gt;

&lt;p&gt;We start with Dockerfile&amp;#8217;s java base image. We then upload our staging &lt;code&gt;tmp/&lt;/code&gt; folder which has our application from sbt&amp;#8217;s native packager output and a corresponding executable start script described below. I opted for &lt;code&gt;ENTRYPOINT&lt;/code&gt; instead of &lt;code&gt;CMD&lt;/code&gt; so the container is treated like an executable. This makes it easier to pass in command line arguments into the sbt native packager script in case you want to set java system properties or override configuration settings via command line arguments.&lt;/p&gt;

&lt;p&gt;The start script is how we tee up the container&amp;#8217;s IP address for our cluster application:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

CLUSTER_IP=&lt;code&gt;/sbin/ifconfig eth0 | grep &#39;inet addr:&#39; | cut -d: -f2 | awk &#39;{ print $1}&#39;&lt;/code&gt; /opt/app/bin/clustering $@
&lt;/pre&gt;

&lt;p&gt;The script sets an inline environment variable by parsing &lt;code&gt;ifconfig&lt;/code&gt; output to get the container&amp;#8217;s ip. We then run the &lt;em&gt;clustering&lt;/em&gt; start script produced from sbt native packager. The &lt;code&gt;$@&lt;/code&gt; lets us pass along any command line settings set when launching the container into the sbt native packager script.&lt;/p&gt;

&lt;h2 id=&#34;fourth-putting-it-together:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Fourth: Putting It Together&lt;/h2&gt;

&lt;p&gt;The last part is a simple bash script named &lt;code&gt;dockerize&lt;/code&gt; to orchestrate each step. By running this script we run sbt native packager, move files to a staging directory, and build the container:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

echo &#34;Build docker container&#34;

#run sbt native packager
sbt universal:stage

#cleanup stage directory
rm -rf docker/tmp/

#copy output into staging area
cp -r target/universal/stage/ docker/tmp/

#build the container, remove intermediate nodes
docker build -rm -t clustering docker/

#remove staged files
rm -rf docker/tmp/
&lt;/pre&gt;

&lt;p&gt;With this in place we simply run&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;bin/dockerize
&lt;/pre&gt;

&lt;p&gt;to create our docker container named clustering.&lt;/p&gt;

&lt;h2 id=&#34;running-the-application-within-docker:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Running the Application within Docker&lt;/h2&gt;

&lt;p&gt;With our clustering container built we fire up our first instance. This will be our seed node for other containers:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -i -t -name seed clustering
2014-03-23 00:20:39,918 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:20:40,392 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:20:40,403 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:20:40,418 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:20:41,404 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
&lt;/pre&gt;

&lt;p&gt;Next we fire up a second node. Because of our reference.conf defaults all we need to do is link this container with the name &lt;em&gt;seed&lt;/em&gt;. Docker will set the environment variables we are looking for in the bundled reference.conf:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -name c1 -link seed:seed -i -t clustering
2014-03-23 00:22:49,332 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:22:49,788 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:22:49,797 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:22:50,238 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
2014-03-23 00:22:50,249 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:22:50,803 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.3:1600
&lt;/pre&gt;

&lt;p&gt;You&amp;#8217;ll see the current leader discovering new nodes and the appropriate broadcast messages sent out. We can even do this a third time and all nodes will react:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -name c2 -link seed:seed -i -t clustering
2014-03-23 00:24:52,768 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:24:53,224 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:24:53,235 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:24:53,470 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
2014-03-23 00:24:53,472 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.3:1600
2014-03-23 00:24:53,478 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:24:55,401 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.4:1600
&lt;/pre&gt;

&lt;p&gt;Try killing a node and see what happens!&lt;/p&gt;

&lt;h2 id=&#34;modifying-the-docker-start-script:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Modifying the Docker Start Script&lt;/h2&gt;

&lt;p&gt;There&amp;#8217;s another reason for the docker start script: it opens the door for different seed discovery options. Container linking works well if everything is running on the same host but not when running on multiple hosts. Also setting multiple seed nodes via docker links will get tedious via environment variables; it&amp;#8217;s doable but we&amp;#8217;re getting into coding-cruft territory. It would be better to discover seed nodes and set that configuration via command line parameters when launching the app.&lt;/p&gt;

&lt;p&gt;The start script gives us control over how we discover information. We could use &lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;etcd&lt;/a&gt;, &lt;a href=&#34;http://www.serfdom.io/&#34;&gt;serf&lt;/a&gt; or even zookeeper to manage how seed nodes are set and discovered, passing this to our application via environment variables or additional command line parameters. Seed nodes can easily be set via system properties set via the command line:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;-Dakka.cluster.seed-nodes.0=akka.tcp://ClusterSystem@host1:2552
-Dakka.cluster.seed-nodes.1=akka.tcp://ClusterSystem@host2:2552
&lt;/pre&gt;

&lt;p&gt;The start script can probably be configured via sbt native packager but I haven&amp;#8217;t looked into that option. Regardless this approach is (relatively) straight forward to run akka clusters with docker. The &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;full project is on github&lt;/a&gt;. If there&amp;#8217;s a better approach I&amp;#8217;d love to know!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Testing Akka’s FSM: Using setState for unit testing</title>
          <link>http://blog.michaelhamrah.com/2014/03/testing-akkas-fsm-using-setstate-for-unit-testing/</link>
          <pubDate>Fri, 21 Mar 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/03/testing-akkas-fsm-using-setstate-for-unit-testing/</guid>
          <description>&lt;p&gt;I wrote &lt;a href=&#34;http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/&#34;&gt;about Akka&amp;#8217;s Finite State Machine&lt;/a&gt; as a way to model a process. One thing I didn&amp;#8217;t discuss was testing an FSM. Akka has &lt;a href=&#34;http://doc.akka.io/docs/akka/snapshot/scala/testing.html&#34;&gt;great testing support&lt;/a&gt; and FSM&amp;#8217;s can easily be tested using the &lt;a href=&#34;http://doc.akka.io/api/akka/snapshot/index.html#akka.testkit.TestFSMRef&#34;&gt;&lt;code&gt;TestFSMRef&lt;/code&gt;&lt;/a&gt; class.&lt;/p&gt;

&lt;p&gt;An FSM is defined by its states and the data stored between those states. For each state in the machine you can match on both an incoming message and current state data. Our previous example modeled a process to check data integrity across two systems. We&amp;#8217;ll continue that example by adding tests to ensure the FSM is working correctly. &lt;a href=&#34;http://blog.michaelhamrah.com/2013/02/embracing-test-driven-development-for-speed/&#34;&gt;These should have been before we developed the FSM&lt;/a&gt; but late tests are (arguably) better than no tests.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s important to test combinations of messages against various states and data. You don&amp;#8217;t want to be in a position to run through a state machine to the state you want for every test. Luckily, there&amp;#8217;s a handy &lt;code&gt;setState&lt;/code&gt; method to explicitly set the current state and data of the FSM. This lets you &amp;#8220;fast forward&amp;#8221; the FSM to the exact state you want to test against.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s say we want to test a &lt;code&gt;DataRetrieved&lt;/code&gt; message in the &lt;code&gt;PendingComparison&lt;/code&gt; state. We also want to test this message against various &lt;code&gt;Data&lt;/code&gt; combinations. We can set this state explicitly:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;The ComparisonEngine&#34; - {
  &#34;in the PendingComparison state&#34; - {
    &#34;when a DataRetrieved message arrives from the old system&#34; - {
      &#34;stays in PendingComparison with updated data when no other data is present&#34; in {
        val fsm = TestFSMRef(new ComparisonEngine())
        
        //set our initial state with setState
        fsm.setState(PendingComparison, ComparisonStatus(&#34;someid&#34;, None, None))

        fsm ! DataRetrieved(&#34;oldSystem&#34;, &#34;oldData&#34;)

        fsm.stateName should be (PendingComparison)
        fsm.stateData should be (ComparisonStatus(&#34;someid&#34;, Some(&#34;oldData&#34;), None))
      }
    }
  }
}
&lt;/pre&gt;

&lt;p&gt;It may be tempting to send more messages to continue verifying the FSM is working correctly. This will yield a large, unwieldy and brittle test. It will make refactoring difficult and make it harder to understand what &lt;em&gt;should&lt;/em&gt; be happening.&lt;/p&gt;

&lt;p&gt;Instead, to further test the FSM, be explicit about the current state and what should happen next. Add a test for it:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;when a DataRetrieved message arrives from the old system&#34; - {
  &#34;moves to AllRetrieved when data from the new system is already present&#34; in {
    val fsm = TestFSMRef(new ComparisonEngine())
        
    //set our initial state with setState
    fsm.setState(PendingComparison, ComparisonStatus(&#34;someid&#34;, None, Some(&#34;newData&#34;)))

    fsm ! DataRetrieved(&#34;oldSystem&#34;, &#34;oldData&#34;)
    fsm.stateName should be (AllRetrieved)
    fsm.stateData should be (ComparisonStatus(&#34;someid&#34;, Some(&#34;oldData&#34;), Some(&#34;newData&#34;)))
  }
}
&lt;/pre&gt;

&lt;p&gt;By mixing in ImplicitSender or using TestProbes we can also verify messages the FSM should be sending in response to incoming messages.&lt;/p&gt;

&lt;p&gt;Testing is an essential part of developing applications. Unit tests should be explicit and granular. For higher level orchestration integration tests, taking a black-box approach, provide ways to oversee entire processes. Don&amp;#8217;t let your code become too unwieldy to manage: use the tools at your disposal and good coding practices to stay lean. Akka&amp;#8217;s FSM provides ways of programming transitional behavior over time and Akka&amp;#8217;s FSM Testkit support provides a way of ensuring that code works over time.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Using Typesafe’s Config for Scala (and Java) for Application Configuration</title>
          <link>http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/</link>
          <pubDate>Sun, 23 Feb 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/</guid>
          <description>

&lt;p&gt;I recently leveraged &lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library to refactor configuration settings for a project. I was very pleased with the API and functionality of the library.&lt;/p&gt;

&lt;p&gt;The documentation is pretty solid so there&amp;#8217;s no need to go over basics. One feature I like is the clear hierarchy when specifying configuration values. I find it helpful to put as much as possible in a reference.conf file in the /resources directory for an application or library. These can get overridden in a variety of ways, primarily by adding an application.conf file to the bundled output&amp;#8217;s classpath. The &lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;sbt native packager&lt;/a&gt;, helpful for deploying applications, makes it easy to attach a configuration file to an output. This is helpful if you have settings which you normally wouldn&amp;#8217;t want to use during development, say using remote actors with akka. I find placing a reasonable set of defaults in a reference.conf file allows you to easily transport a configuration around while still overriding it as necessary. Otherwise you can get into copy and paste hell by duplicating configurations across multiple files for multiple environments.&lt;/p&gt;

&lt;h2 id=&#34;alternative-overrides:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Alternative Overrides&lt;/h2&gt;

&lt;p&gt;There are two other interesting ways you can override configuration settings: using environment variables or java system properties. The environment variable approach comes in very handy when pushing to cloud environments where you don&amp;#8217;t know what a configuration is beforehand. Using the ${?VALUE} pattern a property will only be set if a value exists. This allows you to provide an option for overriding a value without actually having to specify one.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s an example in a conf file using substitution leveraging this technique:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http {
 port = 8080
 port = ${?HTTP_PORT}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;re setting a default port of 8080. If the configuration can find a valid substitute it will replace the port value with the substitute; otherwise, it will keep it at 8080. The configuration library will look up its hierarchy for an HTTP_PORT value, checking other configuration files, Java system properties, and finally environment variables. Environment variables aren&amp;#8217;t perfect, but they&amp;#8217;re easy to set and leveraged in a lot of places. If you leave out the ? and just have ${HTTP_PORT} then the application will throw an exception if it can&amp;#8217;t find a value. But by using the ? you can override as many times as you want. This can be helpful when running apps on Heroku where environment variables are set for third party services.&lt;/p&gt;

&lt;h3 id=&#34;using-java-system-properties:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Using Java System Properties&lt;/h3&gt;

&lt;p&gt;Java system properties provide another option for setting config values. The shell script created by sbt-native-packager supports java system properties, so you can also set the http port via the command line using the -D flag:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/bash_script_from_native_packager -Dhttp.port=8081
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be helpful if you want to run an akka based application with a different log level to see what&amp;#8217;s going on in production:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/some_akka_app_script -Dakka.loglevel=debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately sbt run doesn&amp;#8217;t support java system properties so you can&amp;#8217;t tweak settings with the command line when running sbt. The &lt;a href=&#34;https://github.com/spray/sbt-revolver&#34;&gt;sbt-revolver&lt;/a&gt; plugin, which allows you to run your app in a forked JVM, does allow you to pass java arguments using the command line. Once you&amp;#8217;re set up with this plugin you can change settings by adding your Java overrides after &lt;code&gt;---&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;re-start --- -Dhttp.port=8081
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;with-c3p0:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;With c3p0&lt;/h3&gt;

&lt;p&gt;I was really excited to see that the &lt;a href=&#34;http://www.mchange.com/projects/c3p0/#c3p0_conf&#34;&gt;c3p0 connection pool library also supports Typesafe Config&lt;/a&gt;. So you can avoid those annoying xml-based files and merge your c3p0 settings directly with your regular configuration files. I&amp;#8217;ve migrated an application to a &lt;a href=&#34;docker.io&#34;&gt;docker&lt;/a&gt; based development environment and used this c3p0 feature with &lt;a href=&#34;http://docs.docker.io/en/latest/use/working_with_links_names/&#34;&gt;docker links&lt;/a&gt; to set mysql settings:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app {
 db {
  host = localhost
  host = ${?DB_PORT_3306_TCP_ADDR}
  port = &amp;quot;3306&amp;quot;
  port = ${?DB_PORT_3306_TCP_PORT}
 }
}

c3p0 {
 named-configs {
  myapp {
      jdbcUrl = &amp;quot;jdbc:mysql://&amp;quot;${app.db.host}&amp;quot;:&amp;quot;${app.db.port}&amp;quot;/MyDatabase&amp;quot;
  }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When I link a mysql container to my app container with &lt;code&gt;--link mysql:db&lt;/code&gt; Docker will inject the DB_PORT_3306_TCP_* environment variables which are pulled by the above settings.&lt;/p&gt;

&lt;h3 id=&#34;accessing-values-from-code:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Accessing Values From Code&lt;/h3&gt;

&lt;p&gt;One other practice I like is having a single &amp;#8220;Config&amp;#8221; class for an application. It can be very tempting to load a configuration node from anywhere in your app but that can get messy fast. Instead, create a config class and access everything you need through that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;object MyAppConfig {
  private val config =  ConfigFactory.load()

  private lazy val root = config.getConfig(&amp;quot;my_app&amp;quot;)

  object HttpConfig {
    private val httpConfig = config.getConfig(&amp;quot;http&amp;quot;)

    lazy val interface = httpConfig.getString(&amp;quot;interface&amp;quot;)
    lazy val port = httpConfig.getInt(&amp;quot;port&amp;quot;)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Type safety, Single Responsibility, and no strings all over the place.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;When dealing with configuration think about what environments you have and what the actual differences are between those environments. Usually this is a small set of differing values for only a few properties. Make it easy to change just those settings without changing&amp;#8211;or duplicating&amp;#8211;anything else. This could done via environment variables, command line flags, even loading configuration files from a url. Definitely avoid copying the same value across multiple configurations: just distill that value down to a lower setting in a hierarchy. By minimizing configuration files you&amp;#8217;ll be making your life a lot easier.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re developing an app for distribution, or writing a library, providing a well-documented configuration file (&lt;a href=&#34;https://github.com/spray/spray/blob/master/spray-can/src/main/resources/reference.conf&#34;&gt;spray&amp;#8217;s spray-can reference.conf is an excellent example&lt;/a&gt;) you can allow users to override defaults easily in a manner that is suitable for them and their runtimes.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Using Akka’s ClusterClient</title>
          <link>http://blog.michaelhamrah.com/2014/02/using-akkas-clusterclient/</link>
          <pubDate>Wed, 05 Feb 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/02/using-akkas-clusterclient/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve been spending some time implementing a feature which leverages Akka&amp;#8217;s &lt;a href=&#34;http://doc.akka.io/docs/akka/2.2.3/contrib/cluster-client.html&#34;&gt;ClusterClient&lt;/a&gt; (&lt;a href=&#34;http://doc.akka.io/api/akka/2.2.3/index.html#akka.contrib.pattern.ClusterClient&#34;&gt;api docs&lt;/a&gt;). A ClusterClient can be useful if:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You are running a service which needs to talk to another service in a cluster, but you don&amp;#8217;t that service to be in the cluster (cluster roles are another option for interconnecting services where separate hosts are necessary but I&amp;#8217;m not sold on them just yet).&lt;/li&gt;
&lt;li&gt;You don&amp;#8217;t want the overhead of running an http client/server interaction model between these services, but you&amp;#8217;d like similar semantics. Spray is a great akka framework for api services but you may not want to write a Spray API or use an http client library.&lt;/li&gt;
&lt;li&gt;You want to use the same transparency of local-to-remote actors but don&amp;#8217;t want to deal with remote actorref configurations to specific hosts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The documentation was a little thin on some specifics so getting started wasn&amp;#8217;t as smooth sailing as I&amp;#8217;d like. Here are some gotchas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You only need &lt;code&gt;akka.extensions = [&amp;quot;akka.contrib.pattern.ClusterReceptionistExtension&amp;quot;]&lt;/code&gt; on the Host (Server) Cluster. (If your client isn&amp;#8217;t a cluster, you&amp;#8217;ll get a runtime exception).&lt;/li&gt;
&lt;li&gt;&amp;#8220;Receptionist&amp;#8221; is the default name for the Host Cluster actor managing ClusterClient connections. Your ClusterClient connects first to the receptionist (via the set of initial contacts) then can start sending messages to actors in the Host Cluster. The name is configurable.&lt;/li&gt;
&lt;li&gt;The client actor system using the ClusterClient needs to have a Netty port open. You must use either actor.cluster.ClusterActorRefProvider or actor.remote.RemoteActorRefProvider. Otherwise the Host Cluster and ClusterClient can&amp;#8217;t establish proper communication. You can use the ClusterActorRefProvider on the client even you&amp;#8217;re not running a cluster.&lt;/li&gt;
&lt;li&gt;As a ClusterClient you wrap messages with a ClusterClient.send (or sendAll) message first. (I was sending vanilla messages and they weren&amp;#8217;t going through, but this is in the docs).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ClusterClients are worth checking out if you want to create physically separate yet interconnected systems but don&amp;#8217;t want to go through the whole load-balancer or http-layer setup. Just another tool in the Akka toolbelt!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>First Class Function Example in Scala and Go</title>
          <link>http://blog.michaelhamrah.com/2014/01/first-class-function-example-in-scala-and-go/</link>
          <pubDate>Mon, 20 Jan 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/01/first-class-function-example-in-scala-and-go/</guid>
          <description>&lt;p&gt;Go and Scala both make functions first-class citizens of their language. I recently had to recurse a directory tree in Go and came across the &lt;a href=&#34;http://golang.org/pkg/path/filepath/#Walk&#34;&gt;Walk&lt;/a&gt; function which exemplifies first-class functions. The Walk function talks a path to start a directory traversal and calls a function WalkFunc for everything it finds in the sub-tree:&lt;/p&gt;

&lt;pre class=&#34;syntax go&#34;&gt;func Walk(root string, walkFn WalkFunc) error &lt;/pre&gt;

&lt;p&gt;If you&amp;#8217;re coming from the &lt;a href=&#34;http://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html&#34;&gt;Kingdom of Nouns&lt;/a&gt; you may assume WalkFunc is a class or interface with a method for Walk to call. But that cruft is gone; WalkFunc is just a regular function with a defined signature given its own type, WalkFunc:&lt;/p&gt;

&lt;pre class=&#34;syntax go&#34;&gt;type WalkFunc func(path string, info os.FileInfo, err error) error
&lt;/pre&gt;

&lt;p&gt;Why is this interesting? I wasn&amp;#8217;t surprised Go would have a built-in method for crawling a directory tree. It&amp;#8217;s a pretty common scenario, and I&amp;#8217;ve written similar code many times before. What&amp;#8217;s uncommon about directory crawling is what you want to do with those files: open them up, move them around, inspect them. Separating the common from the uncommon is where first-class functions come into play. How much code have you had to write to just write the code you want?&lt;/p&gt;

&lt;p&gt;Scala hides the OOP-ness of its underlying runtime by compile-time tricks, putting a first-class function like:&lt;/p&gt;

&lt;pre class=&#34;syntax go&#34;&gt;val walkFunc = (file: java.io.File) =&gt; { /* do something with the file */ }
&lt;/pre&gt;

&lt;p&gt;into a class of &lt;a href=&#34;http://www.scala-lang.org/api/current/index.html#scala.Function1&#34;&gt;Function1&lt;/a&gt;. C# does something similar with its various &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/bb534960(v=vs.110).aspx&#34;&gt;function classes&lt;/a&gt; and delegate constructs. Go makes the interesting design decision of forcing function declarations outside of structs, putting an emphasis on stand-alone functions and struct extensibility. There are no classes in Go to encapsulate functions.&lt;/p&gt;

&lt;p&gt;We can write a walk method for our walkFunc in Scala by creating a method which takes a function as a parameter (methods and functions have nuanced differences in Scala, but don&amp;#8217;t worry about it):&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;object FileUtil {
  def walk(file: File, depth: Int, walkFunc: (File, Int) =&gt; Unit): Unit = {
    walkFunc(file, depth)
    Option(file.listFiles).map(_.map(walk(_, depth + 1, walkFunc)))
  }
}
&lt;/pre&gt;

&lt;p&gt;In our Scala walk function we added a depth parameter which tracks how deep you are in the stack. We&amp;#8217;re also wrapping the listFiles method in an Option to avoid a possible null pointer exception.&lt;/p&gt;

&lt;p&gt;We can tweak our walkFunc and use our Scala walk function:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;import FileUtil._
val walkFunc = (path: File, depth: Int) =&gt; { println(s&#34;$depth, ${path}&#34;) }
walk(new File(&#34;/path/to/dir&#34;), 0, walkFunc)
&lt;/pre&gt;

&lt;p&gt;Because typing (File, Int) =&amp;gt; Unit is somewhat obscure, type aliases come in handy. We can refactor this with a type alias:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;type WalkFunc = (File, Int) =&gt; Unit
&lt;/pre&gt;

&lt;p&gt;And update our walk method accordingly:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;def walk(file: File, depth: Int, walkFunc: WalkFunc): Unit = { ... }
&lt;/pre&gt;

&lt;p&gt;First class functions are powerful constructs making code flexible and succinct. If all you need is to call a function than pass that function as a parameter to your method. Just as classes have the &lt;a href=&#34;http://en.wikipedia.org/wiki/Single_responsibility_principle&#34;&gt;single responsibility principle&lt;/a&gt; functions can have them too; avoid doing too much at once like file crawling and file processing. Instead pass a file processor call to your file crawling function.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Programming Akka’s Finite State Machines in Scala</title>
          <link>http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/</link>
          <pubDate>Thu, 16 Jan 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/</guid>
          <description>&lt;p&gt;Over the past few months my team has been building a new suite of services using Scala and Akka. An interesting aspect of Akka&lt;/p&gt;

&lt;p&gt;we leverage is its &lt;a href=&#34;http://doc.akka.io/docs/akka/2.2.3/scala/fsm.html&#34;&gt;Finite State Machine&lt;/a&gt; support. Finite State Machines&lt;/p&gt;

&lt;p&gt;are a staple of computer programming although not often used in practice. A conceptual process can usually be represented with a finite state machine: there are a defined number of states with explicit transitions between states. If we have a vocabulary&lt;/p&gt;

&lt;p&gt;around these states and transitions we can program the state machine.&lt;/p&gt;

&lt;p&gt;A traditional implementation of an FSM is to check and maintain state explicitly via if/else conditions, use the state design pattern, or implement some other construct. Using Akka&amp;#8217;s FSM support, which explicitly defines states and offers transition hooks, allows us to easily implement our conceptual model of a process. FSM is built on top of Akka&amp;#8217;s actor model giving excellent concurrency controls so we can run many of these state machines simultaneously. You can implement your own FSM with Akka&amp;#8217;s normal actor behavior with the &lt;em&gt;become&lt;/em&gt; method to change the partial function handling messages. However FSM offers some nice hooks plus data management in addition to just changing behavior.&lt;/p&gt;

&lt;p&gt;As an example we will use Akka&amp;#8217;s FSM support to check data in two systems. Our initial process is fairly simplistic but provides a good overview of leveraging Finite State Machines. Say we are rolling out a new system and we want to ensure data flows to both the old and new system. We need a process which waits a certain&lt;/p&gt;

&lt;p&gt;amount of time for data to appear in both places. If data is found in both systems we will check the data for consistency,&lt;/p&gt;

&lt;p&gt;if data is never found after a threshold we will alert data is out of sync.&lt;/p&gt;

&lt;p&gt;Based on our description we have four states. We define our states using a sealed trait:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;sealed trait ComparisonStates
case object AwaitingComparison extends ComparisonStates
case object PendingComparison extends ComparisonStates
case object AllRetrieved extends ComparisonStates
case object DataUnavailable extends ComparisonStates
&lt;/pre&gt;

&lt;p&gt;Next we define the data we manage between state transitions. We need to manage an identifier with data from&lt;/p&gt;

&lt;p&gt;the old and new system. Again we use a sealed trait:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;sealed trait Data
case object Uninitialized extends Data
case class ComparisonStatus(id: String, oldSystem: Option[SomeData] = None, newSystem: Option[SomeData] = None) extends Data
&lt;/pre&gt;

&lt;p&gt;A state machine is just a normal actor with the FSM trait mixed in. We declare our&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;ComparisonEngine&lt;/pre&gt;

&lt;p&gt;actor with FSM support,&lt;/p&gt;

&lt;p&gt;specifying our applicable state and data types:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;class ComparisonEngine extends Actor with FSM[ComparisonStates, Data] {
}
&lt;/pre&gt;

&lt;p&gt;Instead of handling messages directly in a receive method FSM support creates an additional layer of messaging handling.&lt;/p&gt;

&lt;p&gt;When using FSM you match on both message and current state. Our FSM only handles two messages: &lt;em&gt;Compare(id: Int)&lt;/em&gt; and&lt;/p&gt;

&lt;p&gt;&lt;em&gt;DataRetrieved(system: String, someData: SomeData)&lt;/em&gt;. You can construct your data types and messages any way&lt;/p&gt;

&lt;p&gt;you please. I like to keep states abstract as we can generalize on message handling. This&lt;/p&gt;

&lt;p&gt;prevents us from dealing with too many states and state transitions.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s start implementing the body of our &lt;em&gt;ComparisonEngine&lt;/em&gt;. We will start with our initial state:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;startWith(AwaitingComparison, Uninitialized)

when(AwaitingComparison) {
  case Event(Compare(id), Uninitialized) =&gt;
    goto(PendingComparison) using ComparisonStatus(id)
}
&lt;/pre&gt;

&lt;p&gt;We simply declare our initial state is AwaitingComparison, and the only message we are willing to process is a Compare.&lt;/p&gt;

&lt;p&gt;When we receive this message we go to a new state&amp;#8211;PendingComparison&amp;#8211;and set some data. Notice how we aren&amp;#8217;t actually doing anything else?&lt;/p&gt;

&lt;p&gt;A great aspect of FSM is the ability to listen on state transitions. This allows us to separate state transition logic from state transition&lt;/p&gt;

&lt;p&gt;actions. When we transition from an initial state to a PendingComparison state we want to ask our two systems for data. We simply match&lt;/p&gt;

&lt;p&gt;on state transitions and add our applicable logic:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;onTransition {
    case AwaitingComparison -&gt; PendingComparison =&gt;
      nextStateData match {
        case ComparisonStatus(id, old, new) =&gt; {
          oldSystemChecker ! VerifyData(id)
          newSystemChecker ! VerifyData(id)
        }
      }
    }
&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;oldSystemChecker&lt;/em&gt; and &lt;em&gt;newSystemChecker&lt;/em&gt; are actors responsible for verifying data in their respective systems. These can be passed in to the FSM as constructor arguments, or you can have the FSM create the actors and supervise their lifecycle.&lt;/p&gt;

&lt;p&gt;These two actors will send a DataRetrieved message back to our FSM when data is present. Because we are now in the &lt;em&gt;PendingComparison&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;state we specify our new state transition actions against a set of possible scenarios:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;when(PendingComparison, stateTimeout = 15 minutes) {
  case Event(DataRetrieved(&#34;old&#34;, old), ComparisonStatus(id, _, None)) =&gt; {
    stay using ComparisonStatus(id, Some(old), None)
  }
  case Event(DataRetrieved(&#34;new&#34;, new), ComparisonStatus(id, None, _)) =&gt; {
    stay using ComparisonStatus(id, None, Some(new))
  }
  case Event(StateTimeout, c: ComparisonStatus) =&gt; {
    goto(IdUnavailable) using c
  }
  case Event(DataRetrieved(system, data), cs @ ComparisonStatus(_, _, _)) =&gt; {
    system match {
      case &#34;old&#34; =&gt; goto(AllRetrieved) using cs.copy(old = Some(data))
      case &#34;new&#34; =&gt; goto(AllRetrieved) using cs.copy(new = Some(data))
    }
  }
}
&lt;/pre&gt;

&lt;p&gt;Our snippet says we will wait 15 minutes for our systemChecker actors to return with data, otherwise, we&amp;#8217;ll timeout and go to the unavailable state. Either the old&lt;/p&gt;

&lt;p&gt;system or new system will return first, in which case, one set of data in our ComparisonStatus will be None. So we stay in the PendingComparison state until&lt;/p&gt;

&lt;p&gt;the other system returns. If our previous pattern matches do not match, we know the current message we are processing is the final message. Notice how we don&amp;#8217;t care how these actors are getting their data. That&amp;#8217;s the responsibility of the child actors.&lt;/p&gt;

&lt;p&gt;Once we have all our data,&lt;/p&gt;

&lt;p&gt;so we go to the AllRetrieved state with the data from the final message.&lt;/p&gt;

&lt;p&gt;There are a couple of ways we could have defined our states. We could have a state for the oldSystem returned or newSystem returned. I find it easier to&lt;/p&gt;

&lt;p&gt;create a generic &lt;em&gt;PendingComparison&lt;/em&gt; state to keep our pattern matching for pending comparisons consolidated in a single partial function.&lt;/p&gt;

&lt;p&gt;Our final states are pretty simple: we just stop our state machine!&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;when(IdUnavailable) {
  case Event(_, _) =&gt; {
    stop
  }
}
when(AllRetrieved) {
  case Event(_, _) =&gt; {
    stop
  }
}
&lt;/pre&gt;

&lt;p&gt;Our last step is to add some more onTransition checks to handle our final states:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;case PendingComparison -&gt; AllRetrieved =&gt;
    nextStateData match {
      case ComparisonStatus(id, old, new) =&gt; {
        //Verification logic
     }
   }
 case _ -&gt; IdUnavailable =&gt;
   nextStateData match {
     case ComparisonStatus(id, old, new) =&gt; {
      //Handle timeout
      }
   }
&lt;/pre&gt;

&lt;p&gt;We don&amp;#8217;t care how we got to the &lt;em&gt;AllRetrieved&lt;/em&gt; state; we just know we are there and we have the data we need. We can offload our verification logic&lt;/p&gt;

&lt;p&gt;to another actor or inline it within our FSM as necessary.&lt;/p&gt;

&lt;p&gt;Implementing processing workflows can be tricky involving a lot of boilerplate code. Conditions must be checked, timeouts handled, error handling implemented.&lt;/p&gt;

&lt;p&gt;The Akka FSM approach provides a foundation for implementing workflow based processes on top of Akka&amp;#8217;s great supervision support. We create a ComparisonEngine&lt;/p&gt;

&lt;p&gt;for every piece of data we need to check. If an engine dies we can supervise and restart. My favorite feature is the separation of what causes a state transition&lt;/p&gt;

&lt;p&gt;with what happens during a state transition. Combined with isolated behavior amongst actors this creates a cleaner, isolated and composable application to manage.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Overview on Web Performance and Scalability</title>
          <link>http://blog.michaelhamrah.com/2013/10/overview-on-web-performance-and-scalability/</link>
          <pubDate>Sat, 12 Oct 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/10/overview-on-web-performance-and-scalability/</guid>
          <description>&lt;p&gt;I recently gave a talk to some junior developers on performance and scalability. The talk is relatively high-level, providing an overview of non-programming topics which are important for performance and scalability. The &lt;a href=&#34;http://michaelhamrah.com/perf/#/&#34;&gt;original deck is here&lt;/a&gt; and on &lt;a href=&#34;https://speakerdeck.com/mhamrah/things-to-know-about-web-performance&#34;&gt;speaker deck&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A few months ago I also &lt;a href=&#34;http://michaelhamrah.com/spdy/&#34;&gt;gave a talk on spdy&lt;/a&gt; which is also on &lt;a href=&#34;https://speakerdeck.com/mhamrah/intro-to-spdy&#34;&gt;speaker deck&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Spray API Development: Getting Started with a Spray Web Service Using JSON</title>
          <link>http://blog.michaelhamrah.com/2013/06/scala-web-apis-up-and-running-with-spray-and-akka/</link>
          <pubDate>Sat, 22 Jun 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/06/scala-web-apis-up-and-running-with-spray-and-akka/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;spray.io&#34;&gt;Spray&lt;/a&gt; is a great library for building http api&amp;#8217;s with Scala. Just like &lt;a href=&#34;playframework.com&#34;&gt;Play!&lt;/a&gt; it&amp;#8217;s built with &lt;a href=&#34;akka.io&#34;&gt;Akka&lt;/a&gt; and provides numerous low and high level tools for http servers and clients. It puts Akka and Scala&amp;#8217;s asynchronous programming model first for high performance, composable application development.&lt;/p&gt;

&lt;p&gt;I wanted to highlight the &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-routing/&#34;&gt;spray-routing&lt;/a&gt; library which provides a nice DSL for defining web services. The routing library can be used with the standalone &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-can/#spray-can&#34;&gt;spray-can&lt;/a&gt; http server or in any servlet container.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll highlight a simple entity endpoint, unmarshalling Json data into an object and deferring actual process to another Akka actor. To get started with your own spray-routing project, I created a &lt;a href=&#34;https://github.com/n8han/giter8&#34;&gt;giter8&lt;/a&gt; template to bootstrap your app:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$g8 mhamrah/sbt -b spray&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://spray.io/documentation/&#34;&gt;The documentation&lt;/a&gt; is quite good and &lt;a href=&#34;https://github.com/spray/spray&#34;&gt;the source code is worth browsing&lt;/a&gt;. For a richer routing example check out &lt;a href=&#34;https://github.com/spray/spray/tree/release/1.1/examples/spray-routing/on-spray-can&#34;&gt;Spray&amp;#8217;s own routing project&lt;/a&gt; which shows off http-streaming and a few other goodies.&lt;/p&gt;

&lt;h2 id=&#34;creating-a-server:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Creating a Server&lt;/h2&gt;

&lt;p&gt;We are going to create three main structures: An actor which contains our Http Service, a trait which contains our route definition, and a Worker actor that will do the work of the request.&lt;/p&gt;

&lt;p&gt;The service actor is launched in your application&amp;#8217;s main method. Here we are using Scala&amp;#8217;s App class to launch our server feeding in values from &lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;typesafe config&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
val service= system.actorOf(Props[SpraySampleActor], &amp;quot;spray-sample-service&amp;quot;)
IO(Http) ! Http.Bind(service, system.settings.config.getString(&amp;quot;app.interface&amp;quot;), system.settings.config.getInt(&amp;quot;app.port&amp;quot;))

println(&amp;quot;Hit any key to exit.&amp;quot;)
val result = readLine()
system.shutdown()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because Spray is based on Akka, we are just creating a standard actor system and passing our service to &lt;a href=&#34;http://doc.akka.io/docs/akka/snapshot/scala/io.html&#34;&gt;Akka&amp;#8217;s new IO library&lt;/a&gt;. This is the high performance foundation for our service built on the spray-can server.&lt;/p&gt;

&lt;h2 id=&#34;the-service-actor:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;The Service Actor&lt;/h2&gt;

&lt;p&gt;Our service actor is pretty lightweight, as the functionality is deferred to our route definition in the HttpService trait. We only need to set the actorRefFactory and call runRoutes from our trait. You could simply set routes directly in this class, but the separation has its benefits, primarily for testing.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
class SpraySampleActor extends Actor with SpraySampleService with SprayActorLogging {
  def actorRefFactory = context
  def receive = runRoute(spraysampleRoute)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-service-trait-8211-spray-8217-s-routing-dsl:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;The Service Trait &amp;#8211; Spray&amp;#8217;s Routing DSL&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-routing/key-concepts/routes/&#34;&gt;Spray&amp;#8217;s Routing DSL&lt;/a&gt; is where Spray really shines. It is similar to Sinatra inspired web frameworks like Scalatra, but relies on composable function elements so requests pass through a series of actions similar to &lt;a href=&#34;http://unfiltered.databinder.net/&#34;&gt;Unfiltered&lt;/a&gt;. The result is an easy to read syntax for routing and the Dont-Repeat-Yourself of composable functions.&lt;/p&gt;

&lt;p&gt;To start things off, we&amp;#8217;ll create a simple get/post operation at the /entity path:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
trait SpraySampleService extends HttpService {
  val spraysampleRoute = {
    path(&amp;quot;entity&amp;quot;) {
      get { 
        complete(&amp;quot;list&amp;quot;)
      } ~
      post {
        complete(&amp;quot;create&amp;quot;)
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The path, get and complete operations are &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-routing/key-concepts/directives/#directives&#34;&gt;Directives&lt;/a&gt;, the building blocks of Spray routing. Directives take the current http request and process a particular action against it. The above snippet doesn&amp;#8217;t much except filter the request on the current path and the http action. The path directive also lets you pull out path elements:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
path (&amp;quot;entity&amp;quot; / Segment) { id =&amp;gt;
    get {
      complete(s&amp;quot;detail ${id}&amp;quot;)
    } ~
    post {
      complete(s&amp;quot;update ${id}&amp;quot;)
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a number ways to pull out elements from a path. Spray&amp;#8217;s unit tests are the best way to explore the possibilities.&lt;/p&gt;

&lt;p&gt;You can use curl to test the service so far:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!bash
curl -v http://localhost:8080/entity
curl -v http://localhost:8080/entity/1234
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;unmarshalling:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Unmarshalling&lt;/h2&gt;

&lt;p&gt;One of the nice things about Spray&amp;#8217;s DSL is how function composition allows you to build up request handling. In this snippet we use json4s support to unmarshall the http request into a JObject:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
/* We need an implicit formatter to be mixed in to our trait */
object Json4sProtocol extends Json4sSupport {
  implicit def json4sFormats: Formats = DefaultFormats
}

trait SpraySampleService extends HttpService {
  import Json4sProtocol._

  val spraysampleRoute = {
    path(&amp;quot;entity&amp;quot;) {
      /* ... */
      post {
        entity(as[JObject]) { someObject =&amp;gt;
          doCreate(someObject)
        }
      } 
     /* ... */
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We use the Entity to directive to unmarshall the request, which finds the implicit json4s serializer we specified earlier. SomeObject is set to the JObject produced, which is passed to our yet-to-be-built doCreate method. If Spray can&amp;#8217;t unmarshall the entity an error is returned to the client.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s a curl command that sets the http method to POST and applies the appropriate header and json body:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!bash
curl -v -X POST http://localhost:8080/entity -H &amp;quot;Content-Type: application/json&amp;quot; -d &amp;quot;{ \&amp;quot;property\&amp;quot; : \&amp;quot;value\&amp;quot; }&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;leveraging-akka-and-futures:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Leveraging Akka and Futures&lt;/h2&gt;

&lt;p&gt;We want to keep our route structure clean, so we defer actual work to another Akka worker. Because Spray is built with Akka this is pretty seamless. We need to create our ActorRef to send a message. We&amp;#8217;ll also implement our doCreate function called within the earlier POST /entity directive:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
//Our worker Actor handles the work of the request.
val worker = actorRefFactory.actorOf(Props[WorkerActor], &amp;quot;worker&amp;quot;)

def doCreate[T](json: JObject) = {
  //all logic must be in the complete directive
  //otherwise it will be run only once on launch
  complete {
    //We use the Ask pattern to return
    //a future from our worker Actor,
    //which then gets passed to the complete
    //directive to finish the request.
    (worker ? Create(json))
                .mapTo[Ok]
                .map(result =&amp;gt; s&amp;quot;I got a response: ${result}&amp;quot;)
                .recover { case _ =&amp;gt; &amp;quot;error&amp;quot; }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a couple of things going on here. Our worker class is looking for a Create message, which we send to the actor with the ask (?) pattern. The ask pattern lets us know the task completed so we call then tell the client. When we get the Ok message we simply return the result; in the case of an error we return a short message. The response future returned is passed to Spray&amp;#8217;s complete directive, which will then complete the request to the client. There&amp;#8217;s no blocking occurring in this snippet: we are just wiring up futures and functions.&lt;/p&gt;

&lt;p&gt;Our worker doesn&amp;#8217;t do much but out the message contents and return a random number:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
class WorkerActor extends Actor with ActorLogging {
import WorkerActor._

def receive = {
  case Create(json) =&amp;gt; {
    log.info(s&amp;quot;Create ${json}&amp;quot;)
    sender ! Ok(util.Random.nextInt(10000))
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can view how the entire request is handled &lt;a href=&#34;https://github.com/mhamrah/spray-sample/blob/master/src/main/scala/Boot.scala&#34;&gt;by viewing the source file&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;wrapping-up:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Wrapping Up&lt;/h2&gt;

&lt;p&gt;Reading the documentation and exploring the unit tests are the best way to understand the power of Spray&amp;#8217;s routing DSL. The performance of the standalone &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-can/&#34;&gt;spray-can&lt;/a&gt; service is outstanding, and the Akka platform adds resiliency through its lifecycle management tools. Akka&amp;#8217;s remoting feature allows systems to build out their app tiers. A project I&amp;#8217;m working on is using Spray and Akka to publish messages to a pub/sub system for downstream request handling. It&amp;#8217;s an excellent platform for high performance API development. &lt;a href=&#34;https://github.com/mhamrah/spray-sample&#34;&gt;Full spray-sample is on GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Updating Flickr Photos with Gpx Data using Scala: Getting Started</title>
          <link>http://blog.michaelhamrah.com/2013/05/updating-flickr-photos-with-gpx-data-using-scala-getting-started/</link>
          <pubDate>Sun, 05 May 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/05/updating-flickr-photos-with-gpx-data-using-scala-getting-started/</guid>
          <description>

&lt;p&gt;If you read this blog you know I&amp;#8217;ve just returned from six months of travels around Asia, documented on our tumblr, &lt;a href=&#34;http://thegreatbigadventure.tumblr.com&#34;&gt;The Great Big Adventure&lt;/a&gt; with photos on &lt;a href=&#34;http://flickr.com/hamrah&#34;&gt;Flickr&lt;/a&gt;. Even though my camera doesn&amp;#8217;t have a GPS, I realized toward the second half of the trip I could mark GPS waypoints and write a program to link that data later. I decided to write this little app in Scala, a language I&amp;#8217;ve been learning since my return. The app is still a work in progress, but instead of one long post I&amp;#8217;ll spread it out as I go along.&lt;/p&gt;

&lt;h2 id=&#34;the-workflow:57d2935fd637ebe85b97477296b70272&#34;&gt;The Workflow&lt;/h2&gt;

&lt;p&gt;When I took a photo I usually marked the location with a waypoint in my GPS. I accumulated a set of around 1000 of these points spread out over three gpx (xml) files. My plan is to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read in the three gpx files and combine them into a distinct list.&lt;/li&gt;
&lt;li&gt;For each day I have at least one gpx point, get all of my flickr images for that data.&lt;/li&gt;
&lt;li&gt;For each image, find the waypoint timestamp with the least difference in time.&lt;/li&gt;
&lt;li&gt;Update that image with the waypoint data on Flickr.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;getting-started:57d2935fd637ebe85b97477296b70272&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;If you&amp;#8217;re going to be doing anything with Scala, learning &lt;a href=&#34;http://scala-sbt.org&#34;&gt;sbt&lt;/a&gt; is essential. Luckily, it&amp;#8217;s pretty straightforward, but the documentation across the internet is somewhat inconsistent. As of this writing, &lt;a href=&#34;http://twitter.github.io/scala_school/sbt.html&#34;&gt;Twitter&amp;#8217;s Scala School SBT Documentation&lt;/a&gt;, which I used as a reference to get started, incorrectly states that SBT creates a template for you. It no longer does, with the preferred approach to use &lt;a href=&#34;https://github.com/n8han/giter8&#34;&gt;giter8&lt;/a&gt;, an excellent templating tool. I created &lt;a href=&#34;https://github.com/mhamrah/sbt.g8&#34;&gt;my own simplified version&lt;/a&gt; which is based off of the excellently documented &lt;a href=&#34;https://github.com/ymasory/sbt.g8&#34;&gt;template by Yuvi Masory&lt;/a&gt;. Some of the versions in build.sbt are a outdated, but it&amp;#8217;s worthwhile reading through the code to get a feel for the Scala and SBT ecosystem. The g8 project also contains a good working example of custom sbt commands (like g8-test). One gotcha with SBT: if you change your build.sbt file, you must call &lt;em&gt;reload&lt;/em&gt; in the sbt console. Otherwise, your new dependencies will not be picked up. For rubyists this is similar to running &lt;em&gt;bundle update&lt;/em&gt; after changing your gemfile.&lt;/p&gt;

&lt;h2 id=&#34;testing:57d2935fd637ebe85b97477296b70272&#34;&gt;Testing&lt;/h2&gt;

&lt;p&gt;I&amp;#8217;m a big fan of TDD, and strive for a test-first approach. It&amp;#8217;s easy to get a feel for the small stuff in the scala repl, but orchestration is what programming is all about, and TDD allows you to design and throughly test functionality in a repeatable way. The two main libraries are &lt;a href=&#34;https://code.google.com/p/specs/&#34;&gt;specs&lt;/a&gt; (actually, it&amp;#8217;s now &lt;a href=&#34;http://etorreborre.github.io/specs2/&#34;&gt;specs2&lt;/a&gt;) and &lt;a href=&#34;http://www.scalatest.org/&#34;&gt;ScalaTest&lt;/a&gt;. I originally went with specs2. It was fine, but I wasn&amp;#8217;t too impressed with the output and not thrilled with the matchers. I believe these are all customizable, but to get a better feel for the ecosystem I switched to ScalaTest. I like ScalaTest&amp;#8217;s default output better and the flexible composition of testing styles (I&amp;#8217;m using FreeSpec) and matchers (ShouldMatchers) provide a great platform for testing. Luckily, both specs2 and scalatest integrate with SBT which provides continuous testing and growl support, so you don&amp;#8217;t need to fully commit to either one too early.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Six Months of Computer Science Without Computers</title>
          <link>http://blog.michaelhamrah.com/2013/04/six-months-of-computer-science-without-computers/</link>
          <pubDate>Mon, 22 Apr 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/04/six-months-of-computer-science-without-computers/</guid>
          <description>

&lt;p&gt;A few weeks ago I returned from &lt;a href=&#34;http://thegreatbigadventure.tumblr.com&#34;&gt;a six month trip around Asia&lt;/a&gt;. I didn&amp;#8217;t have a computer while abroad, but I was able to catch up on several tech books I never had time for previously. Reading about programming without actually programming was an interesting and rewarding circumstance. It provided a unique mental model: it was no longer about &amp;#8220;how you do this&amp;#8221; but about &amp;#8220;why would you do this&amp;#8221;. Accomplishment of a task via implementation was not an end goal. The end goal was simply absorbing information; once read, it didn&amp;#8217;t need to be applied. It only needed to be reasoned about and hypothetically applied under a specific situation (which I usually did on a trek or on a beach). Before I would have been eager to try it out, hacking away, but without a computer, I couldn&amp;#8217;t. It was liberating. Given a problem, and a set of constraints, what&amp;#8217;s the ideal solution? I realize this is somewhat of an ivory-tower mentality, however, I also realized some of the best software has emerged from an idealism to solve problems in an opinionated way. Sometimes we are too consumed by the here-and-now we fail to step back for the bigger picture. Conversely, we hold onto our ideals and fail to adapt to changing circumstances.&lt;/p&gt;

&lt;p&gt;My favorite aspect of learning technology while traveling abroad did not come from any book or video. A large part of computer science is about optimizing systems under the pressure of constraints. Efficient algorithms, clean code, improving performance. The world is full of sub-optimal processes. Burmese hotels, the Lao transportation system, and Nepalese immigration to name a few. On a larger scale sun-optimal problems are created by geographic, socio-economic, or political constraints. People try the best they can to improve their way of life, and unfortunately, the processes are often &amp;#8220;implemented&amp;#8221; with a &amp;#8220;naïve&amp;#8221; solution. Some are also inspiring. It was powerful to see these systems up close, with cultural and historical factors so foreign. One thing is certain: when you optimize for efficiency, everyone wins.&lt;/p&gt;

&lt;p&gt;Below are a selection of books and resources I found particularly interesting. I encourage you to check them out, hopefully away from a computer in a foreign land:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://programmer.97things.oreilly.com/wiki/index.php/97_Things_Every_Programmer_Should_Know&#34;&gt;97 Things Every Programmer Should Know&lt;/a&gt; : A great selection of tidbits from a variety of sources. Nothing new for the experienced programmer, but reading through the sections is a great refresher to keep core principles fresh. Worthwhile to randomly select a chapter now and again for those &amp;#8220;oh yeah&amp;#8221; moments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://shop.oreilly.com/product/9780596510046.do&#34;&gt;Beautiful Code&lt;/a&gt; by Andy Oram and Greg Wilson: My favorite book. Not so much about code, but the insight about solving problems makes it a great read. I appreciate the intelligent thought process which went into some of the chapters. Python&amp;#8217;s hashtable implementation and debugging prioritization in the Linux kernel are two highlights.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://shop.oreilly.com/product/0636920022626.do&#34;&gt;Exploring Everyday Things with R and Ruby&lt;/a&gt; by Sau Sheong Chang: This is a short book with great content. You only need an elementary knowledge of programming and mathematics to appreciate the concepts. It&amp;#8217;s also a great way to get a taste of R. The book covers a variety of topics from statistics, machine learning, and simulations. My favorite aspect is how to use modeling to verify a hypothesis or create a simulation. The chapters involving emergent behavior are particularly interesting.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://shop.oreilly.com/product/0636920018483.do&#34;&gt;Machine Learning for Hackers&lt;/a&gt; by Drew Conway and John Myles White: I&amp;#8217;ve been interested in machine learning for a while, and I was very happy with this read. Far more technical and mathematical than &lt;em&gt;Exploring Everyday Things&lt;/em&gt;, this book digs into supervised and unsupervised learning and several aspects of statistics. If you&amp;#8217;re interested in data science and are comfortable with programming, this book is for you.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.manning.com/raychaudhuri/&#34;&gt;Scala in Action&lt;/a&gt; by Nilanjan Raychaudhuri: Scala and Go have been on my radar for a while as new languages to learn. It&amp;#8217;s funny to learn a new programming language without being able to test-drive it, but I appreciated the separation. My career has largely been focused on OOP: leveraging design patterns, class composition, SOLID principles, enterprise architecture. After reading this book I realize I was missing out on great functional programming paradigms I was only unconsciously using. Languages like Clojure and Haskell are gaining steam for a radically different approach to OOP, and Scala provides a nice balance between the two. It&amp;#8217;s also wonderfully expressive: traits, the type system, and for-comprehension are beautiful building blocks to managing complex behavior. Since returning I&amp;#8217;ve been doing Scala full-time and couldn&amp;#8217;t be happier. It&amp;#8217;s everything you need with a statically typed language with everything you want from a dynamic one (well, there&amp;#8217;s still no method-missing, at least not yet). I looked at a few Scala books and this is easily on the top of the list. Nilanjan does an excellent job balancing language fundamentals with applied patterns.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://shop.oreilly.com/product/0636920014348.do&#34;&gt;HBase: The Definitive Guide&lt;/a&gt; by Lars George: I&amp;#8217;ve been deeply interested in distributed databases and performance for some time. I purchased this book a few years ago when first exploring NoSQL databases. Since then, Cassandra has eclipsed the distributed hashtable family of databases (Riak, Hbase, Voldemort) but I found this book a great read. No matter what implementation you go with, this book will help you think in a column-orientated way, offering great tidbits into architectural tradeoffs which went into HBase&amp;#8217;s design. At the very least, this book will give you a solid foundation to compare against other BigTable/Dynamo clones.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.aosabook.org/en/index.html&#34;&gt;The Architecture of Open Source Applications&lt;/a&gt;: I was excited when I stumbled upon this website. It offers a plethora of information from elite contributors. The applied-practices and deep architectural insight are valuable lessons to learn from. &lt;a href=&#34;http://www.aosabook.org/en/nginx.html&#34;&gt;Andrew Alexeev on Nginx&lt;/a&gt;, &lt;a href=&#34;http://www.aosabook.org/en/distsys.html&#34;&gt;Kate Matsudaira on Scalable Web Architecture&lt;/a&gt; and &lt;a href=&#34;http://www.aosabook.org/en/zeromq.html&#34;&gt;Martin Sústrik on ZeroMQ&lt;/a&gt; are highlights.&lt;/p&gt;

&lt;h2 id=&#34;itunes-u:c1a63adcb7740f27e4178f1e17a10120&#34;&gt;iTunes U&lt;/h2&gt;

&lt;p&gt;I was also able to check out some courses on iTunes U while traveling. &lt;a href=&#34;http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2010/index.htm&#34;&gt;The MIT OCW Performance Engineering of Software Systems&lt;/a&gt; was my favorite. Prof. Saman Amarasinghe and Prof. Charles Leiserson were both entertaining lecturers, and the course provided great insight into memory management, parallel programming, hardware architecture, and bit hacking. I also watched several lectures on algorithms giving me a new found appreciation for Big-O notation (I wish I remembered more while on the job interview circuit). I&amp;#8217;ve been gradually neglecting the importance of algorithmic design since graduating ten years ago, but found revisiting sorting algorithms, dynamic programming, and graph algorithms refreshing. Focusing on how well code runs is as important as how well it&amp;#8217;s written. Like most things, there&amp;#8217;s a naïve brute-force solution and an elegant, efficient other solution. You may not know what the other solution is, but knowing there&amp;#8217;s one lurking behind the curtain will make you a better engineer.&lt;/p&gt;

&lt;p&gt;So, if you can (and you definitely can!) take a break, grab a book, read it distraction free, gaze out in space and think. You&amp;#8217;ll like what you&amp;#8217;ll find!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>SPDY Slide Deck</title>
          <link>http://blog.michaelhamrah.com/2013/04/spdy-slide-deck/</link>
          <pubDate>Sun, 14 Apr 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/04/spdy-slide-deck/</guid>
          <description>&lt;p&gt;I recently gave a talk on &lt;a href=&#34;http://www.chromium.org/spdy&#34;&gt;SPDY&lt;/a&gt;, the new protocol which will serve as the foundation for HTTP 2.0. SPDY introduces some interesting features to solve current limitations with how HTTP 1.1 sits on top of TCP. &lt;a href=&#34;http://www.michaelhamrah.com/spdy/&#34;&gt;Check out the deck for a high-level overview, with links.&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Choosing a Technology: You’re Asking the Wrong Question</title>
          <link>http://blog.michaelhamrah.com/2013/03/choosing-a-technology-youre-asking-the-wrong-question/</link>
          <pubDate>Wed, 27 Mar 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/03/choosing-a-technology-youre-asking-the-wrong-question/</guid>
          <description>&lt;p&gt;When making a choice in the tech world there are two wide-spread approaches: &amp;#8220;What&amp;#8217;s better, X or Y?&amp;#8221; and &amp;#8220;Should I use xyz?&amp;#8221;. The &amp;#8220;or&amp;#8221; debate is always an entertaining topic usually ending in an absurdly hilarious flame war. The &amp;#8220;Should I use xyz?&amp;#8221; is a subtler, more prevalent question in the tech community leading to an extensive amount of discourse. Fairly rational, usually with some good insight, but still a time consuming task. I&amp;#8217;ve fallen victim to both approaches when exploring a technology decision. What I realized is I&amp;#8217;m asking the wrong question. There are only two things I should ask:&lt;/p&gt;

&lt;p&gt;1) What problem do I need to a solve?&lt;/p&gt;

&lt;p&gt;2) How do I want to solve it?&lt;/p&gt;

&lt;p&gt;Once I take this approach I have an opinionated basis for decision-making and I have a clear direction in how to make that decision. Frameworks&amp;#8211;web or javascript&amp;#8211;are excellent examples on taking this approach. Most of these frameworks were born on the simple premise of solving a problem in an opinionated way. Backbone takes a bare-bones approach to a front-end, event-driven structure; Ember offers a robust, &amp;#8220;things just happen&amp;#8221; framework. Sinatra and co. offers an http-first approach to development. Rails and variants are opinionated in web application structure. Do you agree with that approach? Yes, excellent! No? Find something else or roll your own.&lt;/p&gt;

&lt;p&gt;Don&amp;#8217;t know the answer? That&amp;#8217;s okay too. Most beginners want to make the &amp;#8220;right&amp;#8221; choice on what to learn. But the thing is there is no &amp;#8220;right&amp;#8221; answer. For a beginner choosing python vs. ruby vs. php vs scala wastes effort. Just build something using something: you&amp;#8217;ll soon develop your own opinions, with &amp;#8220;how easy is this to learn&amp;#8221; probably the first. Next, when your rails codebase is out of control and you&amp;#8217;re drowning in method_missing issues maybe you&amp;#8217;ll want a more granular, service-orientated approach and the type-safety of Scala. Maybe not&amp;#8230; But you&amp;#8217;ll have a valid problem to solve and a reasonable opinion to go with it.&lt;/p&gt;

&lt;p&gt;I suggest reading &lt;a href=&#34;http://www.aosabook.org/en/nginx.html&#34;&gt;Andrew Alexeev&amp;#8217;s reason on why he built NGINX&lt;/a&gt; and &lt;a href=&#34;https://www.varnish-cache.org/trac/wiki/ArchitectNotes&#34;&gt;Poul-Henning Kamp&amp;#8217;s rationale on how you write a modern application&lt;/a&gt;. Like so many others these incredible open-source systems were born from a problem and the way someone wanted it solved. But those systems didn&amp;#8217;t happen overnight and the authors didn&amp;#8217;t start from scratch. They spent years encountering, learning, and dealing with problems in their respective spaces. They knew the problem domain well, they knew how they wanted the problem solved, and they solved it.&lt;/p&gt;

&lt;p&gt;So put your choice in a context and don&amp;#8217;t sweat the details which are irrelevant to the task at hand. When you need to know those details you&amp;#8217;ll know them, and when you hit problems you&amp;#8217;ll know how you want them solved.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Markdown Powered Resume with CSS Print Styles</title>
          <link>http://blog.michaelhamrah.com/2013/03/markdown-powered-resume-with-css-print-styles/</link>
          <pubDate>Sat, 23 Mar 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/03/markdown-powered-resume-with-css-print-styles/</guid>
          <description>&lt;p&gt;As much as I wish a LinkedIn profile could be a substitute for a resume, it&amp;#8217;s not, and I needed an updated resume. My previous resume was done some time ago with InDesign when I was on a design-tools kick. It worked well, but InDesign isn&amp;#8217;t the best choice for a straight forward approach to a resume and I was not interested in going back to word. So in honor of my friend Karthik&amp;#8217;s &lt;a href=&#34;http://kufli.blogspot.com/2013/02/evolution-of-my-resume-karthik.html&#34;&gt;programming themed resume&lt;/a&gt; I had an idea: program my resume. My requirements were simple:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Easy to edit: I should be able to update and output with minimal effort.&lt;/li&gt;
&lt;li&gt;Easy to design: Something simple, but not boilerplate.&lt;/li&gt;
&lt;li&gt;Export to Html and PDF: For easy distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;#8217;m a big fan of &lt;a href=&#34;http://daringfireball.net/projects/markdown/syntax&#34;&gt;Markdown&lt;/a&gt; and happy to see the prevalence of Markdown across the web, however fragmented. I use Markdown to publish this blog and felt it would work well for writing a resume. The only problem is layout: you have minimal control over structural html elements which can make aspects of design difficult. For writing articles this isn&amp;#8217;t a problem but when you need structural markup for CSS it can be limiting. Luckily I found &lt;a href=&#34;https://github.com/bhollis/maruku&#34;&gt;Maruku&lt;/a&gt;, a ruby-based markdown interpreter which supports &lt;a href=&#34;http://michelf.ca/projects/php-markdown/extra/&#34;&gt;PHP Markdown Extra&lt;/a&gt; and a &lt;a href=&#34;http://maruku.rubyforge.org/proposal.html&#34;&gt;new meta-data syntax&lt;/a&gt; for adding id, css, and div elements to a page. It does take away from Markdown&amp;#8217;s simplicity but adds enough structure for design. Combined with CSS I had everything I needed to fulfill my requirements.&lt;/p&gt;

&lt;p&gt;My &lt;a href=&#34;https://github.com/mhamrah/mlh.com/blob/master/michael-hamrah-resume.md&#34;&gt;markdown resume&lt;/a&gt; is on GitHub. I was surprised it rendered well with GitHub-Flavored Markdown despite the extraneous Maruku elements. I knew I was on the right track. Maruku lets you add your own stylesheets to the html output which I used for &lt;a href=&#34;http://www.michaelhamrah.com/michael-hamrah-resume.html&#34;&gt;posting online&lt;/a&gt;. One simple command gets me from markdown to ready-to-publish html. Exactly what I wanted.&lt;/p&gt;

&lt;p&gt;Markulu supports pdf output as well, but requires a heavy LaTex install which I wasn&amp;#8217;t happy with. I also wasn&amp;#8217;t impressed with the LaTex PDF output. Luckily there&amp;#8217;s an easy alternative: printing to PDF. I used some &lt;a href=&#34;https://github.com/mhamrah/mlh.com/blob/master/scss/resume.scss&#34;&gt;SASS media query overrides&lt;/a&gt; on top of Html 5 Boilerplate&amp;#8217;s default styles to control the print layout in the way I wanted. You can even specify page breaks and print margins via CSS. I favored Safari&amp;#8217;s pdf output over Chrome&amp;#8217;s for the sole reason Safari automatically embedded custom fonts in the final PDF.&lt;/p&gt;

&lt;p&gt;At the end of the day I realized I probably didn&amp;#8217;t need to add explicit divs to Markdown; I could have gotten the layout I wanted with just vanilla Markdown and CSS3 queries. I also could have a semantically better markup if I used HAML to add &lt;section&gt; tags instead of divs where appropriate, but HAML would have added a considerable amount of extraneous information to the markup. I&amp;#8217;m also not sure editing the raw HAML text would have been as easy as Markdown.&lt;/p&gt;

&lt;p&gt;At the end of the day, it&amp;#8217;s all a tradeoff. GitHub flavored markdown, Markdown Here and other interpreters support fenced code blocks; I like the idea of adding fenced blocks to get &lt;section&gt; elements to get semantic correctness and layout elements in the html output. Unfortunately there&amp;#8217;s no official Markdown spec and support is somewhat fragmented across various implementations, but &lt;a href=&#34;http://www.codinghorror.com/blog/2012/10/the-future-of-markdown.html&#34;&gt;hopefully it will come together soon&lt;/a&gt;. Until then, if you need it, you can always fork. Luckily I didn&amp;#8217;t have to take it that far.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Scalability comparison of WordPress with NGINX/PHP-FCM and Apache on an ec2-micro instance.</title>
          <link>http://blog.michaelhamrah.com/2013/03/scalability-comparison-of-wordpress-with-nginxphp-fcm-and-apache-on-an-ec2-micro-instance/</link>
          <pubDate>Sun, 17 Mar 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/03/scalability-comparison-of-wordpress-with-nginxphp-fcm-and-apache-on-an-ec2-micro-instance/</guid>
          <description>&lt;p&gt;For the past few years this blog ran apache + mod_php on an ec2-micro instance. It was time for a change; I&amp;#8217;ve enjoyed using nginx in other projects and thought I could get more out of my micro server. I went with a php-fpm/nginx combo and am very surprised with the results. The performance charts are below; for php the response times varied little under minimal load, but nginx handled heavy load far better than apache. Overall throughput with nginx was phenomenal from this tiny server. The result for static content was even more impressive: apache effectively died after ~2000 concurrent connections and 35k total pages killing the server; nginx handled the load to 10,000 very well and delivered 160k successful responses.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the &lt;a href=&#34;http://loader.io&#34;&gt;loader.io&lt;/a&gt; results from static content from &lt;a href=&#34;http://www.michaelhamrah.com&#34;&gt;http://www.michaelhamrah.com&lt;/a&gt;, comparing apache with nginx. I suggest clicking through and exploring the charts:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/f1c357b13b1f554eef534b79866eb5ce&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Apache only handled 33.5k successful responses up to about 1,300 concurrent connections, and died pretty quickly. Nginx did far better:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/9430bdfcab50f31dc66f3ea3014beb84&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;160k successful response with a 22% error rate and avg. response time of 142ms. Not too shabby. The apache run effectively killed the server and required a full reboot as ssh was unresponsive. Nginx barely hiccuped.&lt;/p&gt;

&lt;p&gt;The results of my wordpress/php performance is also interesting. I only did 1000 concurrent users hitting blog.michaelhamrah.com. Here&amp;#8217;s the apache result:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/210867953c97cdd2dd4308dce17bcae3&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;There was a 21% error rate with 13.7k request served and a 237ms average response time (I believe the lower average is due to errors). Overall not too bad for an ec2-micro instance, but the error rate was quite high and nginx again did far better:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/631e11ff9206c6c7a3820c891380c9a3&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;A total of 19k successes with a 0% error rate. The average response time was a little higher than apache, but nginx did serve far more responses. I also get a kick out of the response time line between the two charts. Apache is fairly choppy as it scales up, while nginx increases smoothly and evens out when the concurrent connections plateaus. That&amp;#8217;s what scalability should look like!&lt;/p&gt;

&lt;p&gt;There are plenty of guides online showing how to get set up with nginx/php-fpm. &lt;a href=&#34;http://codex.wordpress.org/Nginx&#34;&gt;The Nginx guide on WordPress Codex&lt;/a&gt; is the most thorough, but there&amp;#8217;s a &lt;a href=&#34;http://todsul.com/install-configure-php-fpm&#34;&gt;straightforward nginx/php guide on Tod Sul&lt;/a&gt;. I also relied on an &lt;a href=&#34;http://dak1n1.com/blog/12-nginx-performance-tuning&#34;&gt;nginx tuning guide from Dakini&lt;/a&gt; and &lt;a href=&#34;http://calendar.perfplanet.com/2012/using-nginx-php-fpmapc-and-varnish-to-make-wordpress-websites-fly/&#34;&gt;this nginx/wordpress tuning guide from perfplanet&lt;/a&gt;. They both have excellent information. I also think you should check out the &lt;a href=&#34;https://github.com/h5bp/server-configs/blob/master/nginx/nginx.conf&#34;&gt;html5 boilerplate nginx conf files&lt;/a&gt; which have great bits of information.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re setting this up yourself, start simple and work your way up. The guides above have varying degrees of information and various configuration options which may conflict with each other. Here&amp;#8217;s some tips:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Decide if you&amp;#8217;re going with a socket or tcp/ip connection between nginx + php-fcm. A socket connection is slightly faster and local to the system, but a tcp/ip is (marginally) easier to set up and good if you are spanning multiple nodes (you could create a php app farm to compliment an nginx front-facing web farm).&lt;/p&gt;

&lt;p&gt;I chose to go with the socket approach between nginx/php-fpm. It was relatively painless, but I did hit a snag passing nginx requests to php. I kept getting a &amp;#8220;no input file specified&amp;#8221; error. It turns out it was a simple permissions issue: the default php-fpm user was different the nginx user the webserver runs under. Which leads me to:&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Plan your users. Security issues are annoying, so make sure file and app permissions are all in sync.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check your settings! Read through default configuration options so you know what&amp;#8217;s going on. For instance you may end up running more worker processes in your nginx instance than available cpu&amp;#8217;s killing performance. Well documented configuration files are essential to tuning.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Plan for access and error logging. If things go wrong during the setup, you&amp;#8217;ll want to know what&amp;#8217;s going on and if your server is getting requests. You can turn access logs of later.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get your app running, test, and tune. If you do too many configuration settings at once you&amp;#8217;ll most likely hit a snag. I only did a moderate amount of tuning; nginx configuration files vary considerably, so again it&amp;#8217;s a good idea to read through the options and make your own call. Ditto for php-fcm.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I am really happy with the idea of running php as a separate process. Running php as a daemon has many benefits: you have a dedicate process you can monitor and recycle for php without effecting your web server. Pooling apps allows you to tune them individually. You&amp;#8217;re also not tying yourself to a particular web server; php-fpm can run fine with apache. In TCP mode you can even offload your web server to separate node. At the very least, you can distinguish php usage against web server usage.&lt;/p&gt;

&lt;p&gt;So my only question is why would anyone still use apache?&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to Handle a Super Bowl Size Spike in Web Traffic</title>
          <link>http://blog.michaelhamrah.com/2013/02/how-to-handle-a-super-bowl-size-spike-in-web-traffic/</link>
          <pubDate>Wed, 06 Feb 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/02/how-to-handle-a-super-bowl-size-spike-in-web-traffic/</guid>
          <description>

&lt;p&gt;I was shocked to learn the number of &lt;a href=&#34;http://www.yottaa.com/blog/bid/265815/Coke-SodaStream-the-13-Websites-That-Crashed-During-Super-Bowl-2013&#34;&gt;sites which failed to handle the spike in web traffic during the Super Bowl&lt;/a&gt;. Most of these sites served static content and should have scaled easily with the use of CDNs. Scaling sites, even dynamic ones, are achievable with well known tools and techniques.&lt;/p&gt;

&lt;h2 id=&#34;the-problem-is-simple:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;The Problem is Simple&lt;/h2&gt;

&lt;p&gt;At a basic level accessing a web page is when one computer, the client, connects to a server and downloads some content. A problem occurs when the number of people requesting content exceeds the ability to deliver content. It&amp;#8217;s just like a restaurant. When there are too many customers people must wait to be served. Staff becomes stressed and strained. Computers are the same. Excessive load causes things to break down.&lt;/p&gt;

&lt;h2 id=&#34;optimization-comes-in-three-forms:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Optimization Comes in Three Forms&lt;/h2&gt;

&lt;p&gt;To handle more requests there are three things you can do: produce (render) content faster, deliver (download) content faster and add more servers to handle more connections. Each of these solutions has a limit. Designing for these limits is architecting for scale.&lt;/p&gt;

&lt;p&gt;A page is composed of different types of content: html, css and js. This content is either dynamic (changes frequently) or static (changes infrequently). Static content is easier to scale because you create it once and deliver it repeatedly. The work of rendering is eliminated. Static content can be pushed out to CDNs or cached locally to avoid redownloading. Requests to origin servers are reduced or eliminated. You can also download content faster with small payload sizes. There is less to deliver if there is less markup and the content is compressed. Less to deliver means faster download.&lt;/p&gt;

&lt;p&gt;Dynamic content is trickier to cache because it is always changing. Reuse is difficult because pages must be regenerated for specific users at specific times. Scaling dynamic content involves database tuning, server side caching, and code optimization. If you can render a page quickly you can deliver more pages because the server can move on to new requests. Most often, at scale, you want to treat treat dynamic content like static content as best you can.&lt;/p&gt;

&lt;p&gt;Adding more servers is usually the easiest way to scale but breaks down quickly. The more servers you have the more you need to keep in sync and manage. You may be able to add more web servers, but those web servers must connect to database servers. Even powerful database servers can only handle so many connections and adding multiple database servers is complicated. You may be able to add specific types of servers, like cache servers, to achieve the results you need without increasing your entire topology.&lt;/p&gt;

&lt;p&gt;The more servers you have the harder it is to keep content fresh. You may feel increasing your servers will increase your load. It will become expensive to both manage and run. You may be able to achieve a similar result if you cut your response times which also gives the end user a better experience. If you understand the knobs and dials of your system you can tune properly.&lt;/p&gt;

&lt;h2 id=&#34;make-assumptions:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Make Assumptions&lt;/h2&gt;

&lt;p&gt;Don&amp;#8217;t be afraid to make assumptions about your traffic patterns. This will help you optimize for your particular situation. For most publicly facing websites traffic is anonymous. This is particularly true during spikes like the Super Bowl. Because you can deliver the same page to every anonymous user you effectively have static content for those users. Cache controls determine how long content is valid and powers HTTP accelerators and CDNs for distribution. You don&amp;#8217;t need to optimize for everyone; split your user base into groups and optimize for the majority. Even laxing cache rules on pages to a minute can shift the burden away from your application servers freeing valuable resources. Anonymous users will get the benefit of cached content with a quick download, dynamic users will have fast servers.&lt;/p&gt;

&lt;p&gt;You can also create specific rendering pipelines for anonymous and known users for highly dynamic content. If you can identify anonymous users early you may be able to avoid costly database queries, external API calls or page renders.&lt;/p&gt;

&lt;h2 id=&#34;understand-http:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Understand HTTP&lt;/h2&gt;

&lt;p&gt;HTTP powers the web. The better you understand HTTP the better you can leverage tools for optimizing the web. Specifically look at &lt;a href=&#34;http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html&#34;&gt;http cache headers&lt;/a&gt; which allow you to use web accelerators like Varnish and CDNs. The vary header will allow you to split anonymous and known users giving you fine grained control on who gets what. Expiration headers determine content freshness. The worst thing you can do is set cache headers to private on static content preventing browsers from caching locally.&lt;/p&gt;

&lt;h2 id=&#34;try-varnish-and-esi:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Try Varnish and ESI&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.varnish-cache.org&#34;&gt;Varnish&lt;/a&gt; is an HTTP accelerator. It caches dynamic content produced from your website for efficient delivery. Web frameworks usually have their own features for caching content, but Varnish allows you to bypass your application stack completely for faster response times. You can deliver a pre-rendered dynamic page as if it were a static page sitting in memory for a greater number of connections.&lt;/p&gt;

&lt;p&gt;Edge Side Includes allow you to mix static and dynamic content together. If a page is 90% similar for everyone, you can cache the 90% in Varnish and have your application server deliver the other 10%. This greatly reduces the work your app server needs to do. ESI&amp;#8217;s are just emerging into web frameworks. It will play a more prominent role in Rails 4.&lt;/p&gt;

&lt;h2 id=&#34;use-a-cdn-and-multiple-data-centers:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Use a CDN and Multiple Data Centers&lt;/h2&gt;

&lt;p&gt;You don&amp;#8217;t need to add more servers to your own data center. You can leverage the web to fan work out across the Internet. I talk more about CDN&amp;#8217;s, the importance of edge locations and latency in my post &lt;a href=&#34;http://www.michaelhamrah.com/blog/2012/01/building-for-the-web-understanding-the-network/&#34;&gt;Building for the Web: Understanding the Network&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Your application servers should be reserved for doing application-specific work which is unique to every request. There are more efficient ways of delivering the same content to multiple people than processing a request top-to-bottom via a web framework. Remember &amp;#8220;the same&amp;#8221; doesn&amp;#8217;t mean the same indefinitely; it&amp;#8217;s the same for whatever timeframe you specify.&lt;/p&gt;

&lt;p&gt;If you run Varnish servers in multiple data centers you can effectively create your own CDN. Your database and content may be on the east coast but if you run a Varnish server on the west coast an anonymous user in San Fransisco will have the benefit of a fast response time and you&amp;#8217;ve saved a connection to your app server. Even if Varnish has to deliver 10% dynamic content via an ESI on the east coast it can leverage the fast connection between data centers. This is much better then the end user hoping coast-to-coast themselves for an entire page.&lt;/p&gt;

&lt;p&gt;Amazon&amp;#8217;s Route 53 offers the ability to route requests to an optimal location. There are other geo-aware DNS solutions. If you have a multi-region setup you are not only building for resiliency your are horizontally scaling your requests across data centers. At massive scale even load balancers may become overloaded so round-robin via DNS becomes essential. DNS may be a bottleneck as well. If your DNS provider can&amp;#8217;t handle the flood of requests trying to map your URL to your IP address nobody can even get to your data center!&lt;/p&gt;

&lt;h2 id=&#34;use-auto-scaling-groups-or-alerting:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Use Auto Scaling Groups or Alerting&lt;/h2&gt;

&lt;p&gt;If you can take an action when things get rough you can better handle spikes. Auto scaling groups are a great feature of AWS when some threshold is maxed. If you&amp;#8217;re not on AWS good monitoring tools will help you take action when things hit a danger zone. If you design your application with auto-scaling in mind, leveraging load balancers for internal communication and avoiding state, you are in a better position to deal with traffic growth. Scaling on demand saves money as you don&amp;#8217;t need to run all your servers all the time. Pinterest gave a talk explaining how it saves money by reducing its server farm at night when traffic is low.&lt;/p&gt;

&lt;h2 id=&#34;compress-and-serialized-data-across-the-wire:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Compress and Serialized Data Across the Wire&lt;/h2&gt;

&lt;p&gt;Page sizes can be greatly reduced if you enable compression. Web traffic is mostly text which is easily compressible. A 100kb page is a lot faster to download than a 1mb page. Don&amp;#8217;t forget about internal communication as well. In todays API driven world using efficient serialization protocols like protocol buffers can greatly reduce network traffic. Most RPC tools support some form of optimal serialization. SOAP was the rage in the early 2000s but XML is one of the worst ways to serialize data for speed. Compressed content allows you to store more in cache and reduces network I/O as well.&lt;/p&gt;

&lt;h2 id=&#34;shut-down-features:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Shut Down Features&lt;/h2&gt;

&lt;p&gt;A performance bottleneck may be caused by one particular feature. When developing new features, especially on a high traffic site, the ability to shut down a misbehaving feature could be the quick solution to a bad problem. Most high-traffic websites &amp;#8220;leak&amp;#8221; new features by deploying them to only 10% of their users to monitor behavior. Once everything is okay they activate the feature everywhere. Similar to determining page freshness for caches, determining available features under load can keep a site alive. What&amp;#8217;s more important: one specific feature or the entire system?&lt;/p&gt;

&lt;h2 id=&#34;non-blocking-i-o:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Non-Blocking I/O&lt;/h2&gt;

&lt;p&gt;Asynchronous programming is a challenge and probably a last-resort for scaling. Sometimes servers break down without any visible threshold. You may have seen a slow request but memory, cpu, and network levels are all okay. This scenario is usually caused by blocking threads waiting on some form of I/O. Blocked threads are plugs that clog your application. They do nothing and prevent other things from happening. If you call external web services, run long database queries or perform disk I/O beware of synchronous operations. They are bottlenecks. Asynchronous based frameworks like node.js put asynchronous programming at the forefront of development making them attractive for handling numerous concurrent connections. Asynchronous programming also paves the way for queue-based architectures. If every request is routed through a queue and processed by a worker the queue will help even out spikes in traffic. The queue size will also determine how many workers you need. It may be trickier to code but it&amp;#8217;s how things scale.&lt;/p&gt;

&lt;h2 id=&#34;think-at-scale:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Think at Scale&lt;/h2&gt;

&lt;p&gt;When dealing with a high-load environment nothing can be off the table. What works for a few thousand users will grow out of control for a few million. Even small issues will become exponentially problematic.&lt;/p&gt;

&lt;p&gt;Scaling isn&amp;#8217;t just about the tools to deal with load. It&amp;#8217;s about the decisions you make on how your application behaves. The most important thing is determining page freshness for users. The decisions for an up-to-the-second experience for every user are a lot different than an up-to-the-minute experience for anonymous users. When dealing with millions of concurrent requests one will involve a lot of engineering complexity and the other can be solved quickly.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Embracing Test Driven Development for Speed</title>
          <link>http://blog.michaelhamrah.com/2013/02/embracing-test-driven-development-for-speed/</link>
          <pubDate>Mon, 04 Feb 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/02/embracing-test-driven-development-for-speed/</guid>
          <description>

&lt;p&gt;A few months ago I helped a developer looking to better embrace test driven development. The session was worthwhile and made me reflect on my journey with TDD.&lt;/p&gt;

&lt;p&gt;Writing tests is one thing. Striving for full test coverage, writing tests first and leveraging integration and unit tests is another. Some people find writing tests cumbersome and slow. Others may ignore tests for difficult scenarios or code spikes. When first working with tests I felt the same way. Over time I worked through issues and my feeling towards TDD changed. The pain was gone and I worked more effectively.&lt;/p&gt;

&lt;p&gt;TDD is about speed. Speed of development and speed of maintenance. Once you leverage TDD as a way to better produce code you&amp;#8217;ve unlocked the promise of TDD: Code more, debug less.&lt;/p&gt;

&lt;h2 id=&#34;stay-in-your-editor:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Stay In Your Editor&lt;/h2&gt;

&lt;p&gt;How many times have you verified something works by firing up your browser in development? Too many times. You build, you wait for the app to start, you launch the browser, you click a link, you fill in forms, you hit submit. Maybe there&amp;#8217;s a breakpoint you step through or some trace statements you output. How much time have you wasted going from coding to verifying your code works? Too much time.&lt;/p&gt;

&lt;p&gt;Stay in your editor. It has everything you need to get stuff done. Avoid the context switch. Avoid repetitive typing. Have one window for your code and another for your tests. Even on small laptops you can split windows to have both open at once. Gary Bernhardt, in an excellent &lt;a href=&#34;https://peepcode.com/products/play-by-play-bernhardt&#34;&gt;Peepcode&lt;/a&gt;, shows how he runs specs from within vim. Ryan Bates, in his screencast &lt;a href=&#34;http://railscasts.com/episodes/275-how-i-test&#34;&gt;How I Test&lt;/a&gt;, only uses the browser for UI design. If you leave your editor you are wasting time and suffering a context switch.&lt;/p&gt;

&lt;p&gt;Every language has some sort of continuous testing runtime. Detect a file change, run applicable tests. Take a look at &lt;a href=&#34;https://github.com/guard/guard&#34;&gt;Guard&lt;/a&gt;. Selenium and company are excellent browser testing tools. Jasmine works great for Javascript. Rspec and Capybara are a solid combination. Growl works well for notifications. By staying in your editor you are coding all that manual verification away. Once coded you can repeat indefinitely.&lt;/p&gt;

&lt;h2 id=&#34;start-with-tests:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Start with Tests&lt;/h2&gt;

&lt;p&gt;Test driven doesn&amp;#8217;t mean test after. This may be the hardest rule for newcomers to follow. We&amp;#8217;ve been so engrained to write code, to design classes, to focus on OOP. We know what we need to do. We just need to do it. Once code works we&amp;#8217;ll then write tests to ensure it always works. I&amp;#8217;ve done this bad practice myself.&lt;/p&gt;

&lt;p&gt;When you test last you&amp;#8217;re missing the &lt;em&gt;why&lt;/em&gt;. &lt;em&gt;Customer gets welcome email after signing up&lt;/em&gt; means nothing without context. If you know &lt;em&gt;why&lt;/em&gt; this is needed you are in a better position to define your required tests and start shaping your code. The notification could be a simple acknowledgement or part of some intricate flow. If you know the &lt;em&gt;why&lt;/em&gt; you are not driving blind. The what you will build and the how you will build it will follow. If you code the other way around, testing later, you&amp;#8217;re molding the problem to your solution. Define the problem first, then solve succinctly.&lt;/p&gt;

&lt;h2 id=&#34;start-with-failing-tests:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Start with Failing Tests&lt;/h2&gt;

&lt;p&gt;One of my favorite newbie mistakes is when a developer writes some code, then writes a test, watches the test pass, then is surprised when the code fails in the browser. But the test passed!&lt;/p&gt;

&lt;p&gt;Anyone can write a green test. It is the action of going from red to green which gives the test meaning. Something needs to work, it doesn&amp;#8217;t. Red state. You change your code, you make it work. Green state. Without the red state first you have no idea how you got to a green state. Was it a bug in your test? Did you test the right thing? Did you forget to assert something? Who knows.&lt;/p&gt;

&lt;p&gt;Combined with the &lt;em&gt;why&lt;/em&gt; going from red to green gives the code shape. You don&amp;#8217;t need to over-think class design. The code you write has purpose: it implements a need to make something work that doesn&amp;#8217;t. As your functionality becomes more complex, your code becomes more nimble. You deal with dependencies, spawning new tests and classes when cohesion breaks down. You stay focused on your goal: make something work. Combined with git commits you have a powerful history to branch and backtrack if necessary. As always, don&amp;#8217;t be afraid to refactor.&lt;/p&gt;

&lt;h2 id=&#34;testing-first-safeguards-agile-development:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Testing First Safeguards Agile Development&lt;/h2&gt;

&lt;p&gt;Testing first also acts as a safeguard. Too often developers will pull work from a backlog prematurely. They&amp;#8217;ll make assumptions, code to those assumptions, and have to make too many changes before release. If the first thing you do after pulling a story is ask yourself &amp;#8220;how can I verify this works&amp;#8221; you&amp;#8217;re thinking in terms of your end-user. You&amp;#8217;re writing acceptance tests. You understand what you need to deliver. BDD tools like &lt;a href=&#34;http://cukes.info/&#34;&gt;Cucumber&lt;/a&gt; put this paradigm in the foreground. You can achieve the same effect with vanilla integration tests.&lt;/p&gt;

&lt;h2 id=&#34;always-test-difficult-code:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Always Test Difficult Code&lt;/h2&gt;

&lt;p&gt;Most of the time not testing comes down to two reasons. The code is too hard to test or the code is not worth testing. There are other reasons, but they are all poor excuses. If you want to test code you can test code.&lt;/p&gt;

&lt;p&gt;Code shouldn&amp;#8217;t be too hard to test. Testing distributed, asynchronous systems is hard but still testable. When code is too hard to test you have the wrong abstraction. You&amp;#8217;re API isn&amp;#8217;t working. You aren&amp;#8217;t adhering to SOLID principles. Your testing toolkit isn&amp;#8217;t sufficient.&lt;/p&gt;

&lt;p&gt;Static languages can rely on dependency injection to handle mocking, dynamic languages can intercept methods. Tools like &lt;a href=&#34;https://www.relishapp.com/vcr/vcr&#34;&gt;VCR&lt;/a&gt; and Cassette can fake http requests for external dependencies. Databases can be tested in isolation or &lt;a href=&#34;https://github.com/nulldb/nulldb&#34;&gt;faked&lt;/a&gt;. Asynchronous code can be tricky to test but becomes easier when separating pre and post conditions (you can also block in unit tests to handle synchronization).&lt;/p&gt;

&lt;p&gt;The code you don&amp;#8217;t test, especially difficult code, will always bite you. Taking the time to figure out how to test will clean up the code and will give you incredible insight into how your underlying framework works.&lt;/p&gt;

&lt;h2 id=&#34;always-test-your-code:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Always Test Your Code&lt;/h2&gt;

&lt;p&gt;I worked with a developer that didn&amp;#8217;t write tests because the requirements, and thus code, were changing too much and dealing with the failing tests was tedious. It actually signified a red flag exposing larger issues in the organization but the point is a common one. Some developers don&amp;#8217;t test because code may be thrown out or it&amp;#8217;s just a spike and not worth testing.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re not testing first because it&amp;#8217;s a faster way to develop, realize that there is no such thing as throw away code (on the other hand, &lt;a href=&#34;http://code.dblock.org/treat-every-line-of-code-as-if-its-going-to-be-thrown-away-one-day&#34;&gt;all code is throw away code&lt;/a&gt;). Mixing good, tested code with untested code creates technical debt. If you put a drop of sewer in a barrel of wine you will have a barrel of sewer. The code has no &lt;em&gt;why&lt;/em&gt;. It may be just a spike but it could also turn out to be the next best thing. Then you&amp;#8217;re left retrofitting unit tests, fitting a square peg in a round hole.&lt;/p&gt;

&lt;h2 id=&#34;balancing-integration-and-unit-tests:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Balancing Integration and Unit Tests&lt;/h2&gt;

&lt;p&gt;Once you start testing first a lot of pieces fall into place. The balance between integration and unit tests is an interesting topic when dealing with code coverage. There will be overlap in code coverage but not in terms of covered functionality.&lt;/p&gt;

&lt;p&gt;Unit tests are the distinct pieces of your code. Integration tests are how those pieces fit together. You have a customer class and a customer page. The unit tests are the rules around the customer model or the distinct actions around the customer controller. The integration tests are how the end user interacts with those models top to bottom. &lt;a href=&#34;http://pivotallabs.com/cucumber-step-definitions-are-not-methods/&#34;&gt;Pivotal Labs talks about changing state in cucumber steps&lt;/a&gt; showing how integration tests monitor the flow of events in an application. Unit tests are for the discrete methods and properties which drive those individual events.&lt;/p&gt;

&lt;h2 id=&#34;automate:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Automate&lt;/h2&gt;

&lt;p&gt;Developing applications is much more than coding. Focusing on tools and techniques at your disposal will help you write code more effectively. Your IDE, command line skills, testing frameworks, libraries and development paradigms are as important as the code you right. They are your tools and become more powerful when used correctly.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Bus travel tips in Turkey</title>
          <link>http://blog.michaelhamrah.com/2012/09/bus-travel-tips-in-turkey/</link>
          <pubDate>Thu, 13 Sep 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/09/bus-travel-tips-in-turkey/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2012/09/20120913-205957.jpg&#34;&gt;&lt;img src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2012/09/20120913-205957.jpg?w=660&#34; alt=&#34;20120913-205957.jpg&#34; class=&#34;alignnone size-full&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ve been travelling around turkey for the past three weeks and have relied heavily on bus travel to get around. Travel books have great info, but there are a couple of more things to think about when dealing with buses in Turkey.&lt;/p&gt;

&lt;p&gt;First, there are several companies that serve various routes. Pamukkale, KamilKoc and Metro are the big ones. There are several more depending on where you are and where you&amp;#8217;re going. If you don&amp;#8217;t see the bus time you want, or if a bus is full, check with another company. Prices are fairly set so I don&amp;#8217;t think it&amp;#8217;s worth negotiating down. If you are booking through a tour operator, hotel or another reseller they will most likely book through another company, most likely one of the above.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s important to ask what type of bus you&amp;#8217;ll be taking. There are big coach buses, older buses, and minibuses. Ideally you want to be on a big coach bus, often referred to as a big bus. There is usually wifi and tv (turkish only) on big buses, but I haven&amp;#8217;t been on one yet with power. One did have USB outlets but was unable to charge the iPad. Minibuses and older buses may not have air conditioning, so it&amp;#8217;s important to ask. Big buses have the smoothest ride and the most legroom. If you don&amp;#8217;t like the bus you&amp;#8217;re getting at the time you want, see if there&amp;#8217;s another time with a better bus or go to another company. Always get your ticket from someone behind a desk. There will be plenty of people trying to sherpa you here and there, but just go right to the desk. At some otogars there are valets to help you. They may appear to be trying to sell you something. Just ask the right questions and you&amp;#8217;ll be fine. Turkish people are very nice and very helpful.&lt;/p&gt;

&lt;p&gt;The bus may make a lot of stops. We were on a minibus from Denizli to Fethiye and the bus stopped for anybody along the road. It was nuts! People would be waiting on the road no more than 50 meters away from each other and the bus would pull up, slow down, see if anybody needed to get on. Also, some buses will stop at rest areas every 45 minutes to an hour for breaks. On our way to Selcuk we had to stop at a rest area for 15 minutes even though we were only five minutes out from our destination. Ask if it is a direct bus and how many stops it will make. Usually the big coaches are better than the minibuses in terms of stopping. If possible, just avoid minibuses. The ride will most likely be bumpy as well unless it is a newer minibus or a tourist minibus.&lt;/p&gt;

&lt;p&gt;Seats are assigned on buses, so ask for a seat up in the front. Some bus companies have seat maps so you can see where you&amp;#8217;ll be seated. You don&amp;#8217;t need to rush onto the bus, just put your bags on, get on, and find your seat. Everyone is very nice and will gladly help you out. You&amp;#8217;ll also get tea or coffee on the bus with a snack. If the bus is really bumpy don&amp;#8217;t get anything hot. You&amp;#8217;ll probably spill it, need to drink it really quickly, than have to go to the bathroom. Most buses don&amp;#8217;t have bathrooms (they do make a lot of stops, so don&amp;#8217;t worry, but some restrooms cost 1TL). Another funny thing is that on minibuses in small towns there will be guy walking up and down with lemon or rose oil for your hands. A nice little refresher!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2012/09/20120919-164620.jpg&#34;&gt;&lt;img src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2012/09/20120919-164620.jpg?w=660&#34; alt=&#34;20120919-164620.jpg&#34; class=&#34;alignnone size-full&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Dolmuses are fantastic. These are little vans that go around towns to pick people up and drop them off along the way. They are extremely cheap, extremely frequent and should be leveraged. They are just as good as taxis and cost a lot less. They are great within cities to get to more remote areas and to travel among smaller towns. They are great on the Turqouise coast to explore different beaches. Essentially, you just wait outside on the road in the direction you want and a van with people will pull up. Hotel, pensyon and guest house operators are very helpful with Dolmus transport. Depending on where you are on the Lycian way you could even send your bags ahead to be picked up by your next stop.&lt;/p&gt;

&lt;p&gt;If you stick with the big buses and know your options bus travel in Turkey is a great and economical way to get around. Always bring earplugs and an eyemask, especially on night buses. There will always be a crying baby and someone reading with the light on.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Effective Caching Strategies: Understanding HTTP, Fragment and Object Caching</title>
          <link>http://blog.michaelhamrah.com/2012/08/effective-caching-strategies-understanding-http-fragment-and-object-caching/</link>
          <pubDate>Sat, 18 Aug 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/08/effective-caching-strategies-understanding-http-fragment-and-object-caching/</guid>
          <description>

&lt;p&gt;Caching is one of the most effective techniques to speed up a website and has become a staple of modern web architecture. Effective caching strategies will allow you to get the most out of your website, ease pressure on your database and offer a better experience for users. Yet as the old &lt;a href=&#34;http://martinfowler.com/bliki/TwoHardThings.html&#34;&gt;adage says&lt;/a&gt; caching&amp;#8211;especially invalidation&amp;#8211;is tricky. How to deal with dynamic pages, deciding what to cache, per-user personalization and invalidation are some of the challenges which come along with caching.&lt;/p&gt;

&lt;h3 id=&#34;caching-levels:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Caching Levels&lt;/h3&gt;

&lt;p&gt;There a three broad levels of caching:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html&#34;&gt;HTTP Caching&lt;/a&gt;&lt;/em&gt; allows for full-page caching via HTTP headers on URIs. This must be enabled on all static content and should be added to dynamic content when possible. It is the best form of caching, especially for dynamic pages, as you are serving generated html content and your application can effectively leverage reverse-proxies like &lt;a href=&#34;http://www.squid-cache.org/&#34;&gt;Squid&lt;/a&gt; and &lt;a href=&#34;https://www.varnish-cache.org/&#34;&gt;Varnish&lt;/a&gt;. &lt;a href=&#34;http://www.mnot.net/cache_docs/&#34;&gt;Mark Nottingham&amp;#8217;s great overview on HTTP Caching is worth a read&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Fragment Caching&lt;/em&gt; allows you to cache page fragments or partial templates. When you cannot cache an entire http response, fragment caching is your next best bet. You can quickly assemble your pages from pre-generated html snippets. For a page involving disparate dynamic content you can build your result page from cached html fragments for each section. For listing pages, like search results, you can build the page from html fragments for each id and not regenerate markup. For detail pages you can separate less-volatile or common sections from high-volatile or per-user sections.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Object Caching&lt;/em&gt; allows you to cache a full object (as in a model or viewmodel). When you must generate html for each user/request, or when your objects are shared across various views, object caching can be extremely helpful. It allows you to better deal with expensive queries and lessen hits to your database.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The goal is to make your response times as fast as possible while lessening load. The more html (or data) you can push closer to the end-user the better. HTTP caching is better than fragment caching: you are ready to return the rendered page. When combined with a CDN even dynamic pages can be pushed to edge locations for faster response times. Fragment caching is better than object caching: you already have the rendered html to build the page. Object caching is better than a database call: you already have the cached query result or denormalized object for your view. The deeper you get in the stack (the closer to the datastore) the more options you have to vary the output. Consequently the more expensive and longer the operation will take.&lt;/p&gt;

&lt;h3 id=&#34;break-content-down-cache-for-views:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Break Content Down; Cache for Views&lt;/h3&gt;

&lt;p&gt;A cache strategy is dependent on breaking content down to store and reuse later. The more granular you can get the more options you have to serve cached content. There are two main dimensions: what to cache and whom to cache for. It is difficult to HTTP cache a page with a &amp;#8220;Hello, {{ username }}&amp;#8221; in the header for all users. However if you break your users down into logged-in users and anonymous users you can easily HTTP cache your homepage for just anonymous users using the &lt;em&gt;vary&lt;/em&gt; http-header and defer to fragment caching for logged-in users.&lt;/p&gt;

&lt;p&gt;Cache key naming strategies allow you to vary the &lt;em&gt;what&lt;/em&gt; with the &lt;em&gt;who for&lt;/em&gt; in a robust way by creating multiple versions of the same resource. A cache key could include the role of the user and the page, such as &lt;em&gt;role:page:fragement:id&lt;/em&gt;, as in _anon:widget&lt;em&gt;detail:widget:1234&lt;/em&gt; and serve the &lt;em&gt;widget detail&lt;/em&gt; html fragment to anonymous users. The same widget could be represented in a search detail list via _anon:widget&lt;em&gt;search:widget:1234&lt;/em&gt;. When widget 1234 updates both keys are invalidated. Most people opt for object caching for an easy win with dynamic pages, specifically by caching via a primary key or id. This can be helpful, but if you break down your content into the &lt;em&gt;what&lt;/em&gt; and &lt;em&gt;who for&lt;/em&gt; with a good key naming strategy you can leverage fragment caching and save on rendering time.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;vary&lt;/em&gt; http header is very helpful for dealing with HTTP caching and is not used widely enough. By varying URIs based on certain headers (like authorization or a cookie value) you can cache different representations for the same resource in a similar way to creating multiple keys. Think of the cache key as the URI plus whatever is set in the &lt;em&gt;vary&lt;/em&gt; header. This opens up the power of HTTP caching for dynamic or per-user content.&lt;/p&gt;

&lt;p&gt;You are ready to deliver content quickly when you think about your cache in terms of views and not data. Cache a denormalized object with child associations for easy rendering without extra lookups. Store rendered html fragments for sections of a page that are common to users on otherwise specific content. &amp;#8220;Popular&amp;#8221; and &amp;#8220;Recent&amp;#8221; may be expensive queries; storing rendered html saves on processing time and can be injected into the main page. You can even reuse fragments across pages. A good cache key naming strategy allows for different representations of the same data which can easily be invalidated.&lt;/p&gt;

&lt;h3 id=&#34;cache-invalidation:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Cache Invalidation&lt;/h3&gt;

&lt;p&gt;Nobody likes stale data. As you think about caching think about what circumstances to invalidate the cache. Time-based expirations are convenient but can usually be avoided by invalidating caches on create and update commands. A good cache key naming strategy helps. Web frameworks usually have a notion of &amp;#8220;callbacks&amp;#8221; to perform secondary actions when a primary action takes place. A set of fragment and object caches for a widget could be invalidated when a record is updated. If cache values are granular enough you could invalidate sections of a page, like blog comments, when a comment is added and not expire the entire blog post.&lt;/p&gt;

&lt;p&gt;HTTP Etags provide a great mechanism for dealing with stale HTTP requests. Etags allow a more invalidation options than the basic if-modified-since headers. When dealing with Etags the most important thing is to avoid processing the entire request simply to generate the Etag to validate against (this saves network bandwidth but does not save processing time). Caching Etag values against URIs are a good way to see if an Etag is still valid to send the proper 304 NOT MODIFIED response as quickly as possible in the request cycle. Depending on your needs you can also cache sets of Etag values against URIs to handle various representations.&lt;/p&gt;

&lt;p&gt;If you must rely on time-based expiration try to add expiration callbacks to keep the cache fresh, especially for expensive queries in high-load scenarios.&lt;/p&gt;

&lt;h3 id=&#34;edge-side-includes-fragment-caching-for-http:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Edge Side Includes: Fragment Caching for HTTP&lt;/h3&gt;

&lt;p&gt;Edge Side Includes are a great way of pushing more dynamic content closer to users. ESIs essentially give you the benefits of fragment caching with the performance of HTTP caching. If you are considering using a tool like &lt;a href=&#34;http://www.squid-cache.org/&#34;&gt;Squid&lt;/a&gt; or &lt;a href=&#34;https://www.varnish-cache.org/&#34;&gt;Varnish&lt;/a&gt; ESIs are essential and will allow you to add customized content to otherwise similar pages. The &lt;em&gt;user panel&lt;/em&gt; in the header of a page is a classic example of an ESI usage. If the user panel is the only variant of an otherwise common page for all users, the common elements could be pulled from the reverse-proxy within milliseconds and the &amp;#8220;Welcome, {{USER}}&amp;#8221; injected dynamically as a fragment from the application server before sending everything to the client. This bypasses the application stack lightening load and decreasing processing time.&lt;/p&gt;

&lt;h3 id=&#34;distributed-or-centralized-caches-are-better:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Distributed or Centralized Caches are Better&lt;/h3&gt;

&lt;p&gt;Distributed and/or centralized caches are better than in-memory application server cache stores. By using a distributed cache like &lt;a href=&#34;http://memcached.org/&#34;&gt;Memcache&lt;/a&gt;, or a centralized cache store like &lt;a href=&#34;http://redis.io&#34;&gt;Redis&lt;/a&gt;, you can drop duplicate data caches to make caching and invalidating objects easier. Even though caching objects in a web app&amp;#8217;s memory space is convenient and reduces network i/o, it soon becomes impractical in a web farm. You do not want to build up caches per-server or steal memory space away from the web server. Nor do you want to have to hunt and gather objects across a farm to invalidate caches. If you do not want to support your own cache farm, there are plenty of SaaS services to deal with caching.&lt;/p&gt;

&lt;h3 id=&#34;compress-when-possible:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Compress When Possible&lt;/h3&gt;

&lt;p&gt;Compressing content helps. Memory is a far more valuable resource for web apps than cpu cycles. When possible, compress your serialized cache content. This lowers the memory footprint so you can put more stuff in cache, and lightens the transfer load (and time) between your cache server and application server. For HTTP caching the helpful &lt;em&gt;vary&lt;/em&gt; http header can also be used to cache content for browsers supporting compression and those that don&amp;#8217;t. For object caching, only store what you need in the cache. Even though compression helps reduce the footprint, not storing extraneous data further reduces the footprint and saves serialization time.&lt;/p&gt;

&lt;h3 id=&#34;nosql-to-the-rescue:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;NoSQL to the Rescue&lt;/h3&gt;

&lt;p&gt;One of the interesting trends I am reading about is how certain NoSQL stores are eliminating the need for separate cache farms. NoSQL solutions are beneficial for a variety of reasons even though they create significant data-modeling challenges. What NoSQL solutions lack in the flexibility of representing and accessing data (i.e. no joins, minimal search) they can make up in their distributed nature, fault-tolerance, end access efficiency. When you model your data for your views, putting the burden on storing data in the same way you want to get it out, you&amp;#8217;re essentially replacing your denormalized memory-caching tier with a more durable solution. Cassandra and other Dynamo/Bigtable type stores are key-value stores, similar to cache stores, with the value part offering some sort of structured data type (in the case of Cassandra, sorted lists via column families). MongoDb and Redis, (not Dynamo inspired) offer similar advantages; Redis&amp;#8217; sorted sets/sorted lists offer a variety of solutions for listing problems, MongoDb allows you to query objects.&lt;/p&gt;

&lt;p&gt;If you are okay with storing (and updating) multiple-versions of your data (again, you are caching for views) you can cut the two-layer approach of separate cache and data stores. The trick is storing everything you need to render a view for a given key. Searches could be handled by a search-server like Solr or ElasticSearch; listing results could be handled by maintaining your own index via a sorted-list value via another key. When using Cassandra you&amp;#8217;d get fast, masterless, and scalable persistant storage. In general this approach is only worthwhile if your views are well-defined. The worst thing you want to do is refactor your entire data model when your views change!&lt;/p&gt;

&lt;h3 id=&#34;how-web-frameworks-help:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;How Web Frameworks Help&lt;/h3&gt;

&lt;p&gt;There is always debate on differences between frameworks and languages. One of the things I always look for is how easy it is to add caching to your application. Rails offers great support for caching, and the &lt;a href=&#34;http://guides.rubyonrails.org/caching_with_rails.html&#34;&gt;Caching with Rails&lt;/a&gt; guide is worth a read no matter what framework or language you use. It easily supports fragment caching in views via content blocks, behind-the-scene action caching support, has a pluggable cache framework to use different stores, and most importantly has an extremely flexible invalidation framework via model observers and cache sweepers. When choosing any type of framework, &amp;#8220;how to cache&amp;#8221; should be a bullet point at the top of the list.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Web Services @ Getty Images</title>
          <link>http://blog.michaelhamrah.com/2012/02/web-services-getty-images/</link>
          <pubDate>Tue, 21 Feb 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/02/web-services-getty-images/</guid>
          <description>&lt;p&gt;I wrote a post for the Getty Images Technology blog on how we use SOA at Getty Images. As systems become more and more complex with unique scalability needs, SOA allows systems to segment complexity and create useful boundaries.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.gettyimages.com/2012/02/21/put-a-service-on-it-how-web-services-power-getty-images&#34;&gt;http://blog.gettyimages.com/2012/02/21/put-a-service-on-it-how-web-services-power-getty-images&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Agile: It’s a War on Dates</title>
          <link>http://blog.michaelhamrah.com/2012/01/agile-its-a-war-on-dates/</link>
          <pubDate>Sun, 15 Jan 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/01/agile-its-a-war-on-dates/</guid>
          <description>

&lt;p&gt;In a comment to my earlier article &lt;a href=&#34;http://wp.me/pnRto-a1&#34;&gt;Thoughts on Kanban&lt;/a&gt; someone brought up the subject of end dates. Businesses obsess about the &amp;#8220;When can we have it?&amp;#8221; question. Dates and deadlines trump all. Let me tell you a secret: dates are bullshit. It is a prohibitive mentality in today&amp;#8217;s world. Technology needs to reframe the question. Stakeholders need to change their engagement. No company ever succeeded because they made dates. Companies succeed when they continuously deliver innovation. It is not about the destination. It is about the journey and where you end up.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For agile to truly succeed the DNA of the company&amp;#8211;top to bottom&amp;#8211;must be continuous improvement through continuous delivery.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;i-want-everything-now:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;I Want Everything. Now.&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;A friend told me a story of a prioritization meeting he had with a stakeholder. After fleshing out seven distinct features with the dev team, the stakeholder was asked to prioritize. He walked up to the board, put a &amp;#8220;1&amp;#8221; next to everything, and walked out.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Really? Everything is a top priority? So you are saying you would rather have nothing than anything? Then what you want is worthless. You may think you need everything but you are showing your unwillingness to change or improve. People that cannot work through small changes definitely cannot deal with large ones.&lt;/p&gt;

&lt;p&gt;Yes, long-term vision is important. It is the goal. It ensures that everyone heads in the right direction every step of the way. It helps people make reasonable decisions. But it is still long-term; it is just a vision of the future negating the small details that allow the day-to-day. It is painful, ridiculous and unnecessary to wait for the future to just &amp;#8220;appear&amp;#8221;.&lt;/p&gt;

&lt;p&gt;Long term deliverables create impatient, anxious users. It creates large, unmanageable codebases, complex releases, excessive bugs. It disconnects original vision from delivered functionality. The cherry on the cake: it creates a confused user base making awkward and painful adjustments to radical new processes. There is no long-term goal that cannot be broken down and reached via small iterative releases. Baby steps. One at a time, together. It is a three-legged race for everyone.&lt;/p&gt;

&lt;h2 id=&#34;the-devil-is-in-the-details:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;The Devil Is In The Details.&lt;/h2&gt;

&lt;p&gt;This is the root cause of scope creep. &lt;em&gt;Okay, we&amp;#8217;re working on feature x, but can we do this? Can we do this? What about this?&lt;/em&gt; You tell me. Is it more important for you to do that or get what we have out? It is your call! When you look at the backlog is it more important to enhance the current feature or move to the next thing on the list?&lt;/p&gt;

&lt;p&gt;We have daily scrums to answer the improve or move question. Get involved in the day-to-day. Transparency is king. If you don&amp;#8217;t trust the person making the call then don&amp;#8217;t let them make the call. Empower key people: it&amp;#8217;s an organizational change which pushes agile forward.&lt;/p&gt;

&lt;p&gt;Active engagement between stakeholders and developers is agile development. The less barriers between the two the better. Getting a common sub-conscience understanding of &amp;#8220;what we are doing&amp;#8221; is the key to success. Small companies align on vision easily. Large companies need to break up into teams and align on goals.&lt;/p&gt;

&lt;h2 id=&#34;deadlines-get-things-done:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;Deadlines Get Things Done&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;You want to know if this really complex thing you are asking for can be done in eight weeks? Can you spell out every possible detail, define every wireframe, tell me how it should look on every device to every user in the world, outline every workflow, specify the amount of load it requires, explain how you will want to enhance it in the future, not bother us at all while we build it, let us decide any confusing or ambiguous detail, then maybe, maybe, I can do this thing that nobody has ever done before in eight weeks. If eight weeks later when you are unhappy with that one thing you did not explicitly specify (even though I totally asked you to specify everything) I will tell you &amp;#8220;It wasn&amp;#8217;t in the reqs&amp;#8221;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Is that how you want to work? Or would you rather tell me the gist of what we are doing, come up with a plan to get there, see what we can do first quickly, get it out, then take it from there. Is outlining every validation error on every page necessary to do now or can we start with the first page and take it from there?&lt;/p&gt;

&lt;p&gt;You may also use deadlines to motivate people. Vision, direction and importance also motivate people. It is pretty easy to get something done by saying &amp;#8220;This needs to happen by this date&amp;#8221;. But that shows you do not care what it does or how well it works. You are asking people to time box something because either the details are irrelevant or there is no trust in people to make the right decisions.&lt;/p&gt;

&lt;h2 id=&#34;always-be-releasing:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;Always Be Releasing&lt;/h2&gt;

&lt;p&gt;Releasing functionality does not mean users have to see it. Turning features on and off, experimenting with small audiences, refactoring one class rather than an entire stack; all these are powerful steps for modern companies to improve products. Constant feedback lets everyone know they are headed in the right direction. It lets dev teams know the health of their code base. What is better than actually integrating new code to production to test integration? What is better than knowing if a feature is worth investing in than testing it on a small set of production users?&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m sure you heard about the &lt;a href=&#34;http://en.wikipedia.org/wiki/Cone_of_Uncertainty&#34;&gt;cone of uncertainty&lt;/a&gt;. Small and explicit features with short estimations can be delivered accurately. Larger loosely defined features with long dates are difficult to predict. Break down large features into small, clear user stories. A big feature or a long date means you do not care about details.&lt;/p&gt;

&lt;p&gt;Short date ranges are okay and can help coordinate people. They work well when matched with story size. A few days, one week, one to three, and three to five are good ranges which require a decent discussion to work out details. Ranges allow for adjustment and can be refined as you move along the uncertainty cone. Ideally they are auto-calculated from story points. Anything +5 weeks requires a break down; there are too many variables. Don&amp;#8217;t think you can add up ranges either, that is not the way it works. It is about increasingly clarifying level of detail on what&amp;#8217;s ahead to maintain momentum. You can still, and should, deploy intermittently within the date range. Long dates don&amp;#8217;t provide detail. Without detail you do not care how features work. So you do not care what you get.&lt;/p&gt;

&lt;h2 id=&#34;you-work-for-a-tech-company:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;You Work For A Tech Company&lt;/h2&gt;

&lt;p&gt;Your type of business does not matter. Your size does not matter. In house, off shore, outsourced development does not matter. It&amp;#8217;s 2012. Your company uses technology to do business. You work for a tech company. As a tech person your job is to help your business realize this. As a stakeholder your job is to realize this and help your tech team help you do your job faster, better, easier.&lt;/p&gt;

&lt;h2 id=&#34;i-8217-ll-say-it-again-always-be-releasing:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;I&amp;#8217;ll Say It Again: Always Be Releasing.&lt;/h2&gt;

&lt;p&gt;Good companies consistently take their products to the next level. How? They build an incredible manufacturing pipeline. Why is Toyota&amp;#8217;s just-in-time practices so applicable to building software? Because development teams manufacture software. It&amp;#8217;s how the product is built. It&amp;#8217;s how it changes. It&amp;#8217;s how it&amp;#8217;s delivered. It&amp;#8217;s how it&amp;#8217;s fixed. Dev teams buy the land, construct the building, build the robots, define the pipeline, assemble the pieces, run quality control, load up the trucks, deliver and when all that is done they improve. Hopefully the new manufacturing plant allows for easy improvement. Otherwise somebody made a mistake.&lt;/p&gt;

&lt;h2 id=&#34;faster-better-stronger:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;Faster, Better, Stronger&lt;/h2&gt;

&lt;p&gt;It is everyone&amp;#8217;s responsibility to ensure that manufacturing pipeline delivers as efficiently as possible with no flaws. Continuous integration, unit tests, programming languages, server frameworks, agile development, clear vision, well written stories, cohesive vision, user feedback; it all goes into building a solid manufacturing process. You do not need to get it right the first time. You just need to change and improve when needed. Tech leaders ensure they are building the right process for the business. Stakeholders enable and leverage that pipeline effectively. Radical product changes, disconnected vision or tech decisions lead to numerous and slow manufacturing plants.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Dates are bullshit. It&amp;#8217;s about where you are, where you want to be, and what&amp;#8217;s next. That&amp;#8217;s the conversation.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Update: I missed a section of date ranges matching to story points relating to the cone of certainty. Short date ranges are a good tool for predicting near term deliverables and framing what&amp;#8217;s next. These ranges are most effective when your agile tool auto-calculates velocity from story points. Remember, the bigger the complexity, the greater the date range.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Building for the Web: Understanding The Network</title>
          <link>http://blog.michaelhamrah.com/2012/01/building-for-the-web-understanding-the-network/</link>
          <pubDate>Fri, 06 Jan 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/01/building-for-the-web-understanding-the-network/</guid>
          <description>

&lt;p&gt;My &lt;a href=&#34;http://wp.me/pnRto-aa&#34;&gt;first post on web technology&lt;/a&gt; talks about what we are trying to accomplish when building for the web. There are four ways we can break down the standard flow of &lt;em&gt;client action/server action/result&lt;/em&gt;: delivering, serving, rendering and developing. This post focuses on delivering content by understanding the network. Why use a &lt;a href=&#34;http://en.wikipedia.org/wiki/Content_delivery_network&#34;&gt;cdn&lt;/a&gt;? What&amp;#8217;s all the fuss about connections and compressed static assets? The network is often overlooked but understanding how it operates is essential for building high performing websites. A 50ms rendering time with a 50ms db query is meaningless if it takes three seconds to download a page.&lt;/p&gt;

&lt;h2 id=&#34;tcp-know-it:117df65302b8db2107451cddb1557896&#34;&gt;TCP: Know It.&lt;/h2&gt;

&lt;p&gt;Going from client to server and back again rests on the network and how well you use it. &lt;a href=&#34;http://en.wikipedia.org/wiki/Transmission_Control_Protocol&#34;&gt;TCP&lt;/a&gt; dominates communication on the web and is worth knowing well. In order to send data from one point to another a connection is established between two points via a back-and-forth handshake. Once established, data flows between the two in a series of packets. TCP offers reliability by acknowledging receipt of every packet sent by sending a second acknowledgement packet back to the server. The time it takes to go from one end to another and back is called latency or round-trip time. At any given time there are packets in flight waiting acknowledgement of receipt. TCP only allows a certain amount of unacknowledged packets in flight; this is called window size. Connections start with small window sizes but as more successful transfers occur the window size will increase (&lt;a href=&#34;http://en.wikipedia.org/wiki/Slow-start&#34;&gt;known as slow start&lt;/a&gt;). This effectively increases bandwidth because more data is sent at once. The longer the latency the slower a connection; if the window size is full then the server must wait for acknowledgements before sending more data. This is on top of the time it actually takes to send packets. The best case scenario is low latency with large, full windows.&lt;/p&gt;

&lt;h2 id=&#34;reliability:117df65302b8db2107451cddb1557896&#34;&gt;Reliability&lt;/h2&gt;

&lt;p&gt;Reliable connections are also important; if packets are lost they must be resent. Retransmissions slow down transfers because tcp guarantees in-order delivery of data to the application layer. If a packet is dropped and needs to be resent nothing will be delivered to the application until that one packet is received. UDP, an alternative to TCP, doesn&amp;#8217;t offer the same guarantees and assumes issues are dealt with at the application layer.&lt;/p&gt;

&lt;p&gt;There is a &lt;a href=&#34;http://coding.smashingmagazine.com/2011/11/14/analyzing-network-characteristics-using-javascript-and-the-dom-part-1/&#34;&gt;good article by Phillip Tellis on understanding the network with JS&lt;/a&gt; which talks about data transfer and TCP. &lt;a href=&#34;http://www.wireshark.org/&#34;&gt;Wireshark&lt;/a&gt; is another great tool for analyzing packets across a network. You can actually view individual packets as they come and go and see how window size is scaling, view retransmissions, measure bandwidth, and examine latency.&lt;/p&gt;

&lt;h2 id=&#34;the-importance-of-connections:117df65302b8db2107451cddb1557896&#34;&gt;The Importance of Connections&lt;/h2&gt;

&lt;p&gt;Establishing a connection takes time because of the handshake involved and latency considerations. A 100ms latency could mean more than 300ms before any data is even received on top of dealing with any dns lookups and os overhead. Keeping a connection alive avoids this creation overhead. Connection pooling, for example, is a popular technique to manage database connections. A web server will talk to a database frequently; if the connection is already established there is no overhead in executing a new query which can trim valuable time off of serving a request. &lt;a href=&#34;http://en.wikipedia.org/wiki/Ping&#34;&gt;Ping&lt;/a&gt; and &lt;a href=&#34;http://en.wikipedia.org/wiki/Traceroute&#34;&gt;traceroute&lt;/a&gt; are two worthwhile tools that examine latency and the &amp;#8220;hops&amp;#8221; packets take from one network to another as they travel from end to end.&lt;/p&gt;

&lt;p&gt;Connections take time to create, have a relatively limited availability and require overhead to manage. It may seem like a great idea to keep connections open for the long haul, but there is a limited number of connections a server can sustain. Concurrent connections is a popular benchmark which examines how many simultaneous connections can occur at any given time. If you hit that mark, new requests will have to wait until something frees up. The ideal situation is to pump as much as possible through a few connections so there are more available to others. If you can serve multiple files files at once great; but why keep six empty connections open between requests if you don&amp;#8217;t need too?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;On a side note, this is where server architecture comes into play. A server usually processes a request by building a web page from a framework. If this can be done asynchronously by the web server, or offloaded somewhere else, the web server can handle a higher number of sustained connections. The server can grab a new connection while it waits for data to send on an existing one. We&amp;#8217;ll talk more about this in another post. Fast server times also keep high concurrent connections from arising in the first place.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;optimizing-the-network:117df65302b8db2107451cddb1557896&#34;&gt;Optimizing the Network&lt;/h2&gt;

&lt;p&gt;Lowering latency and optimizing data throughput are what dominate delivery optimization. It is important to keep the data which flows between client and server to a minimum. Downloading a 100kb page is a lot faster than downloading a 1000kb page. Compressing static content like css and javascript greatly reduces payload which is why tools like &lt;a href=&#34;http://documentcloud.github.com/jammit/&#34;&gt;Jammit&lt;/a&gt; and &lt;a href=&#34;http://code.google.com/closure/&#34;&gt;Google Closure&lt;/a&gt; are so ubiquitous. These tools can also merge files; because of http chatter it is faster to download one larger file than several small files. Remember the importance of knowing http? Each http request requires reusing or establishing a connection, a header request sent, the server handling the request, and the response. Doing this once is better than twice. Most web servers can also dynamically compress http responses and should be used when possible.&lt;/p&gt;

&lt;p&gt;Fixing latency can be done by using a content delivery network like &lt;a href=&#34;http://aws.amazon.com/cloudfront/&#34;&gt;Amazon Cloudfront&lt;/a&gt; or &lt;a href=&#34;http://www.akamai.com&#34;&gt;Akamai&lt;/a&gt;. They shorten the distance between a request and response by taking content from your server and spreading it on their infrastructure all over the world. When a user requests a resource the cdn routes the request to the server with the lowest latency. A user in Japan can download a file from Japan a lot faster than he can from Europe. Shorter distance, fewer hops, fewer retransmissions. A good cdn strategy rests on how easy it is to push content to a cdn and how easy it is to refresh it. Both concerns should be well researched when leveraging a cdn. You don&amp;#8217;t want stale css files in the wild when you release a new version of your app.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&#34;http://en.wikipedia.org/wiki/WAN_optimization&#34;&gt;WAN accelerator&lt;/a&gt; is also a cool technique. Let&amp;#8217;s say you want to deliver a dynamic web page from the US to Tokyo. You could have that travel over the open internet with a high latency connection. Or you could route that request to a data center in Tokyo with an optimized connection to the US. The user gets a low-latency connection to the Tokyo datacenter which in turn gets a low-latency high bandwidth connection to the US. This can greatly simplify issues with running and keeping multiple data centers in sync.&lt;/p&gt;

&lt;h2 id=&#34;the-bottom-line:117df65302b8db2107451cddb1557896&#34;&gt;The Bottom Line&lt;/h2&gt;

&lt;p&gt;There&amp;#8217;s a lot of effort underway in making the web faster by changing how tcp connections are leveraged on the web. Http 1.0 requires a new connection for every request/response and browsers limit the number of parallel connections between client and server between two and six. Http keep-alive and &lt;a href=&#34;http://en.wikipedia.org/wiki/HTTP_pipelining&#34;&gt;http pipelining&lt;/a&gt; offer mechanisms to push more content through existing connections. Rails 3.1 introduced http streaming via chunked responses. Browsers can fetch assets in parallel with the main html response as soon as tags appear in the main response stream. &lt;a href=&#34;http://www.chromium.org/spdy/spdy-whitepaper&#34;&gt;Spdy&lt;/a&gt;, an effort by Google, is worth checking out: it proposes a multi-pronged attack to leverage as much flow as possible on a single connection. The docs also illustrate interesting pain points with the network on the web. The bottom line is simple: reduce the amount of data that needs to go from one place to another and make the travel time as fast as possible. Small amounts of data over existing parallel connections make a fast web.&lt;/p&gt;

&lt;dl&gt;
&lt;dt&gt;This approach shouldn&amp;#8217;t be limited to users and servers; optimizing network communication within your datacenter is extremely important. You have total control over your infrastructure and can tune your network accordingly. You can also choose your communication protocols: using something like &lt;a href=&#34;http://thrift.apache.org/&#34;&gt;thrift&lt;/a&gt; or &lt;a href=&#34;http://code.google.com/p/protobuf/&#34;&gt;protocol buffers&lt;/a&gt; can save a tremendous amount of bandwidth over xml-based web services on http.&lt;/p&gt;&lt;/dt&gt;
&lt;dt&gt;:&lt;a href=&#34;http://www.scala-lang.org/&#34;&gt;http://www.scala-lang.org/&lt;/a&gt;&lt;/p&gt;&lt;/dt&gt;
&lt;/dl&gt;

&lt;p&gt;:&lt;a href=&#34;http://python.org/&#34;&gt;http://python.org/&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Building for the Web: What Are We Trying to Accomplish?</title>
          <link>http://blog.michaelhamrah.com/2012/01/web-technology-what-are-we-trying-to-accomplish/</link>
          <pubDate>Wed, 04 Jan 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/01/web-technology-what-are-we-trying-to-accomplish/</guid>
          <description>

&lt;p&gt;The web technology landscape is huge and growing every day. There are hundreds of options from servers to languages to frameworks for building the next big thing. Is it &lt;a href=&#34;http://www.nginx.org/en/&#34;&gt;nginx&lt;/a&gt; + &lt;a href=&#34;http://unicorn.bogomips.org/&#34;&gt;unicorn&lt;/a&gt; + &lt;a href=&#34;http://rubini.us/&#34;&gt;rubinus&lt;/a&gt; or a &lt;a href=&#34;http://nodejs.org/&#34;&gt;node.js&lt;/a&gt; restful service on &lt;a href=&#34;http://cassandra.apache.org/&#34;&gt;cassandra&lt;/a&gt; running with &lt;a href=&#34;http://emberjs.com/&#34;&gt;ember.js&lt;/a&gt; and html5 on the front end? Should I learn or ? What&amp;#8217;s the best nosql database for a socially powered group buying predicative analysis real-time boutique mobile aggregator that scales to 100 million users and never fails?&lt;/p&gt;

&lt;p&gt;It is true there are many choices out there but web technology boils down to a very simple premise. You want to respond to a user action as quickly as possible, under any circumstance, while easily changing functionality. Everything from the technology behind this blog to what goes into facebook operates on that simple idea. The problem comes down to doing it at the scale and speed of the modern web. You have hundreds of thousands of users; you have hundreds of thousands of things you want them to see; you want them to buy, share, create, and/or change those things; you want to deliver a beautiful, customized experience; everything that happens needs to be instantaneous; it can never stop working; and the cherry on the cake is that everything is constantly changing; different features, different experiences, different content; different users. The simple &amp;#8220;hello world&amp;#8221; app is easy. But how do you automatically translate that into every language with a personal message and show a real-time graph with historical data of every user accessing the page &lt;em&gt;at scale&lt;/em&gt;? What if we wanted to have users leave messages on the same page? Hopefully by the end of this series you will get a sense of how all the pieces fit together and what&amp;#8217;s involved from going to 10 users to 10 thousand to 10 million.&lt;/p&gt;

&lt;h2 id=&#34;the-web-at-50-000-feet:ba587cddf41afc4433f829b6dc01b418&#34;&gt;The Web at 50,000 Feet&lt;/h2&gt;

&lt;p&gt;The web breaks down to three distinct areas: what happens with the client, what happens on the server, and what happens in between. Usually this is rendering a web page: a user clicks a link, a page delivered, the browser displays the content. It could also involve handling an ajax request, calling an API, posting a search form. Either way it boils down to &lt;em&gt;client action, server action, result&lt;/em&gt;. Doing this within a users attention span given any amount of meaningful content in a fail-safe way with even the smallest amount of variation is why we have all this technology. It all works together to combine flexibility and speed with power and simplicity. The trick is using the right tool in the swiss-army knife of tech to get the job done.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s break all this down a bit. Handling a user action well comes down to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Having the server-side handle the request quickly (Serving)&lt;/li&gt;
&lt;li&gt;A quick travel time between the client and server (Delivering)&lt;/li&gt;
&lt;li&gt;Quickly displaying the result (Rendering)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And developing and managing all this successfully comes down to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Changing any aspect of what is going on quickly and easily (Developing)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As sites grow and come under heavier load doing any one of these things becomes increasingly difficult. More features, more users, more servers, more code. There is only so much one server can do. There is also only so much a server &lt;em&gt;needs&lt;/em&gt; to do. Why hit the database if you can cache the result? Why render a page if you don&amp;#8217;t need to? Why download a one meg html page when it&amp;#8217;s only 100kb compressed? Why download a page if you don&amp;#8217;t have to? How do you do all this and keep your code simple? How do you ensure everything still works even if your &lt;a href=&#34;http://aws.amazon.com/message/65648/&#34;&gt;data center goes down&lt;/a&gt;?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Http plays an important role in all of this. I didn&amp;#8217;t truly appreciate http until I read &lt;a href=&#34;http://www.amazon.com/Restful-Web-Services-Leonard-Richardson/dp/0596529260&#34;&gt;Restful Web Services&lt;/a&gt; by Sam Ruby and Leonard Richardson. Http as an application protocol offers an elegant, scalable mechanism for transferring data and defining intent. Understanding http verbs, various http headers and how http sits on top of tcp/ip can go along way in mastering the web.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;making-it-all-work-together:ba587cddf41afc4433f829b6dc01b418&#34;&gt;Making it all work together&lt;/h2&gt;

&lt;p&gt;So how do you choose and use all the tools out there to serve, deliver, render and develop for the web? What does &lt;em&gt;client action/server action/result&lt;/em&gt; have to do with &lt;a href=&#34;http://rack.rubyforge.org/&#34;&gt;rack&lt;/a&gt; and &lt;a href=&#34;http://en.wikipedia.org/wiki/Web_Server_Gateway_Interface&#34;&gt;wsgi&lt;/a&gt;? I naïvely thought I could write everything I wanted to in a single post: from using &lt;a href=&#34;http://sass-lang.com/&#34;&gt;sass&lt;/a&gt; for compressed, minimized css to sharding databases for horizontal scalability. It will be easier to spread it out a bit so stay tuned. But remember: any language, framework or tool out there is really about improving &lt;em&gt;client action/server action/result&lt;/em&gt;. Even something like &lt;a href=&#34;http://en.wikipedia.org/wiki/WebSocket&#34;&gt;websockets&lt;/a&gt;. Websockets eliminates the client request completely. Why wait for a user to tell you something when you can push them content? Knowing your problem domain, your bottlenecks, and your available options will help you choose the right tool and make the right time/cost/benefit decision.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ll dig into the constraints and various techniques to effectively &lt;a href=&#34;http://wp.me/pnRto-af&#34;&gt;deliver&lt;/a&gt;, &lt;a href=&#34;http://wp.me/pnRto-al&#34;&gt;serve, develop&lt;/a&gt; and &lt;a href=&#34;http://wp.me/pnRto-at&#34;&gt;render&lt;/a&gt; for the web in upcoming posts.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Thoughts on Kanban</title>
          <link>http://blog.michaelhamrah.com/2011/12/thoughts-on-kanban/</link>
          <pubDate>Thu, 15 Dec 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/12/thoughts-on-kanban/</guid>
          <description>

&lt;p&gt;One of my favorite achievements in the agile/lean world has been the progression from standard Scrum practices to a &lt;a href=&#34;http://en.wikipedia.org/wiki/Kanban_(development)&#34;&gt;Kanban&lt;/a&gt; approach of software development. In fact, Kanban, in my opinion, is such an ideal approach to software development I cannot imagine approaching team-based development any other way.&lt;/p&gt;

&lt;h2 id=&#34;what-8217-s-wrong-with-scrum:d7ec680e7904e258131968165612fab2&#34;&gt;What&amp;#8217;s Wrong With Scrum?&lt;/h2&gt;

&lt;p&gt;Before answering this, I want to mention Kanban came only after altering, tweaking, and refining the Scrum process as much as possible. If anything, Kanban represents a &lt;em&gt;graduation&lt;/em&gt; from Scrum. Scrum worked, and worked well, but it was time to take the approach to the next level. Why? The Scrum process was failing. It became too constrained, too limiting. As I mentioned in my &lt;a href=&#34;http://www.michaelhamrah.com/blog/2008/12/adventures-in-agile-practical-scrum-intro/&#34;&gt;three-year old (as of this writing) post on Scrum&lt;/a&gt; one needs to constantly iterate in refining the practice. Pick one thing that isn&amp;#8217;t working, fix it, and move one. Quite simply, there was nothing with Scrum left to refine except Scrum itself.&lt;/p&gt;

&lt;h3 id=&#34;why-scrum-was-failing:d7ec680e7904e258131968165612fab2&#34;&gt;Why Scrum Was Failing&lt;/h3&gt;

&lt;p&gt;The main issue was simply it was time to break free from the time-boxed approach of sprints. Too much effort went into putting stories into iterations. Too much effort went into managing the process. This process took away from releasing new functionality. &lt;em&gt;Nothing can be more important than releasing new functionality.&lt;/em&gt; Tweaking iteration length did not help; one week caused too many meetings to happen too frequently. Two weeks and the early sprint planning effort was lost on stories which would not occur until the second week. Too much time went into making stories &amp;#8220;the right size&amp;#8221;. Some where too small; not worth discussing in a group. Some were too big but they did not make sense to break down to fit into the iteration. Worse, valuable contributions in meetings only occurred with a few people. This had nothing to do with the quality of dev talent; some really good developers did not jive with the story time/sprint review/retrospective/group think model. Why would they? Who really likes meetings?&lt;/p&gt;

&lt;h3 id=&#34;rethinking-constraints:d7ec680e7904e258131968165612fab2&#34;&gt;Rethinking Constraints&lt;/h3&gt;

&lt;p&gt;Scrum has a specific approach to constraints: limit by time. Focus on what can be accomplished in X timeframe (sprints). Add those sprints into releases. Wash, rinse, repeat. Kanban, however, rethinks constraints. Time is irrelevant; the constraint is how much work can occur at any one time. This is, essentially, your work in progress. Limit your work in work in progress (WIP) to work you can be actively doing at any one time. In order to do new work, old work must be done.&lt;/p&gt;

&lt;h3 id=&#34;always-be-releasing:d7ec680e7904e258131968165612fab2&#34;&gt;Always Be Releasing&lt;/h3&gt;

&lt;p&gt;The beauty of this approach is that it lends itself well to a continuous deployment approach. If you work on something, and work on it until it is done, when it is done, it can be released. So release it. Why wait until an arbitrary date? The development pipeline in Kanban is similar to Scrum. Stories are prioritized, they are sized, they are ready for work, they are developed, they are tested, they are released. The main difference is instead of doing these at set times, they are done &lt;em&gt;just-in-time&lt;/em&gt;. In order to move from one stage of the process (analysis, development, testing, etc) there must be an open &amp;#8220;slot&amp;#8221; in the next stage. This is your WIP limit. If there isn&amp;#8217;t an open slot, it cannot move, and stays as is. People can be focused on moving stories through the pipeline rather than meeting arbitrary deadlines, no matter how those deadlines came to be. Even blocking items can have WIP limits. The idea is simple: you have X resources. Map those resources directly to work items as soon as they are available, and see them through to the end. Then start again.&lt;/p&gt;

&lt;h3 id=&#34;everything-is-just-in-time:d7ec680e7904e258131968165612fab2&#34;&gt;Everything is Just In Time&lt;/h3&gt;

&lt;p&gt;All of the benefits of Scrum are apparent in Kanban. Transparency into what is being worked on and the state of stories. Velocity can still be measured; stories are sized and can be timed through the pipeline. Averages can be calculated over time for approximate release dates. The business can prioritize what is next up to the point of development. Bugs can be weaved into the pipeline as necessary, without having to detract from sprints. With the right build and deploy setup releases can occur as soon as code is merged into the master branch. Standup meetings are still important.&lt;/p&gt;

&lt;h3 id=&#34;the-goal:d7ec680e7904e258131968165612fab2&#34;&gt;The Goal&lt;/h3&gt;

&lt;p&gt;The theory of constraints is nothing new. My first encounter was with &lt;a href=&#34;http://www.amazon.com/Goal-Process-Ongoing-Improvement/dp/0884270610&#34;&gt;The Goal by Eliyahu Goldratt&lt;/a&gt;. The goal, in this case, is to release new functionality as efficiently (not quickly, not regularly; efficiently) as possible. There is a process to this: an idea happens, a request comes in. It is evaluated, it is fleshed out, given a cost. It is planned, implemented, and tested. It is released. Some are small, some are big. Some can be broken down. But in teams large and small, they go from inception to implementation to release. Value must be delivered efficiently. It can happen quickly, but it does not need to be arbitrarily time-boxed.&lt;/p&gt;

&lt;p&gt;Scrum is a great and effective approach to software development. It helps focus the business and dev teams on thinking about what is next. It is a great way to get teams on board with a goal and working, in sync, together. It follows a predictable pattern to what will happen when. It offers the constraint of time. Kanban offers the constraint of capacity. For software development this is a far more effective constraint to managing work. You still need solid, manageable stories. You just don&amp;#8217;t have to fit a square peg in a round hole. Kanban streamlines the development process so resources, which always have a fixed limit, are the real limit you are dealing with. They are matched directly to the current state of work so a continuous stream of value can be delivered without the stop-and-go Scrum approach.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>My post about image delivery on the Getty Images Blog</title>
          <link>http://blog.michaelhamrah.com/2011/12/my-post-about-image-delivery-on-the-getty-images-blog/</link>
          <pubDate>Thu, 08 Dec 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/12/my-post-about-image-delivery-on-the-getty-images-blog/</guid>
          <description>&lt;p&gt;I wrote an article covering how we move images to our customers on the new &lt;a href=&#34;http://blog.gettyimages.com/2011/12/06/from-camera-to-customer-faster-than-ever-before/&#34;&gt;Getty Images blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The system is a suite of .NET applications which handle various steps in our workflow. It features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A custom C# module which sits with IIS FTP to alert when a new image arrives&lt;/li&gt;
&lt;li&gt;Services built with the &lt;a href=&#34;https://github.com/Topshelf/Topshelf&#34;&gt;Topshelf framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WCF services which wrap a rules engine featuring dynamic code generation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Getty Images blog be covering more insight into the system as well as other technology developed at Getty Images. So check out the new blog and subscribe to the RSS feed!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Solr: Improving Performance and Other Considerations</title>
          <link>http://blog.michaelhamrah.com/2011/11/solr-improving-performance-and-other-considerations/</link>
          <pubDate>Tue, 29 Nov 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/11/solr-improving-performance-and-other-considerations/</guid>
          <description>

&lt;p&gt;We use &lt;a href=&#34;http://lucene.apache.org/solr/&#34;&gt;Solr&lt;/a&gt; as our search engine for one of our internal systems. It has been awesome; before, we had to deal with very messy sql statements to support many search criteria. Solr allows us to stick our denormalized data into an index and search on an arbitrary number of fields via an elegant, RESTful interface. It&amp;#8217;s extremely fast, easy to use, and easy to scale. I wanted to share some lessons learned from our experience with Solr.&lt;/p&gt;

&lt;h2 id=&#34;know-your-use-cases:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Know Your Use Cases&lt;/h2&gt;

&lt;p&gt;There are two worlds of Solr: writing data (committing) and reading data (querying). Solr should not be treated like a database or some nosql solution; it is a search indexer built on top of Lucene. Treat it like a search indexer and not a permanent data store; it doesn&amp;#8217;t behave like a database. There are plenty of tools to keep data in database in sync with Solr; the worst case scenario is you have to sync it yourself. You should know how heavy you will query it, how much you&amp;#8217;ll write to it, and have a rough idea what your schema will be (but it doesn&amp;#8217;t have to be 100%). Knowing your use cases will allow you to configure your instance and define your schema appropriately.&lt;/p&gt;

&lt;p&gt;Solr offers a variety of ways to index and parse data; when you&amp;#8217;re starting out, you don&amp;#8217;t need to pick one. Solr has a great copyField feature that allows you to index the same data in multiple ways. This can be great for trying out new things or doing A/B comparisons. Once your patterns are well defined, you can tune your index and configuration as needed.&lt;/p&gt;

&lt;p&gt;Our use cases are pretty straight forward; we simply need to search many different fields and aggregate results. We don&amp;#8217;t need to deal with lexical analyzation or sorting on score. Our biggest issue was actually commits because we didn&amp;#8217;t thoroughly vet our update patterns. Remember, Solr is about commits as much as it is about querying. You need to realize there will be some lag between when you update Solr and when you see the results. There are a large number of factors that go into how long that delay will be (it could be very quick), but it will be there, and you should design your system in knowing there will be a delay rather than trying to avoid it. The commit section covers why you shouldn&amp;#8217;t try and commit on every update, even under moderate load.&lt;/p&gt;

&lt;h2 id=&#34;know-configuration-options:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Know Configuration Options&lt;/h2&gt;

&lt;p&gt;Go through the solrconfig.xml and schema.xml files. It&amp;#8217;s well documented and there are lots of good bits in there (solrconfig.xml is often missed!). The caches are what matter most, and explained in later sections. If you know your usage patterns you can get a good sense of how you can tune your caches for optimal results. Autowarming is also important; it allows Solr to reuse caches from previous indexes when things change.&lt;/p&gt;

&lt;p&gt;Don&amp;#8217;t forget that Solr sits on top of Java, so you should also tune the JVM as appropriate. This probably will revolve around how much memory to allocate to the JVM. Be sure to give as much as possible, especially in production.&lt;/p&gt;

&lt;h2 id=&#34;understand-commits:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Understand Commits&lt;/h2&gt;

&lt;p&gt;You should control the number of commits being made to Solr. Load testing is important; you need to know how often and what happens when Solr will rebuilds an index. You shouldn&amp;#8217;t commit on every update; you will surely hit memory and performance issues. When a commit occurs, an index and search warmer need to be built. A search warmer is a view onto an index. Caches may need to be pre-populated. Locking occurs. You don&amp;#8217;t want to have that overhead if you don&amp;#8217;t need it. If you have any post commit listeners those will also run. Finally, updating without forcing a commit is a lot faster than forcing a commit on update. The downside is simply that data will not be immediately available.&lt;/p&gt;

&lt;p&gt;This is where autocommit comes into play. We use an autocommit every 5 seconds or 5000 docs. We never hit 5000 commits in less than 5 seconds; we just don&amp;#8217;t want data to be too stale. 5000 docs allows us to re-index in production if we need to without killing the system. This ratio provides a good enough index time for searches to work appropriately without causing too many commits from choking the system. Again, know your usage patterns and you can get this number right.&lt;/p&gt;

&lt;h2 id=&#34;search-warmers-and-cold-searches:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Search Warmers and Cold Searches&lt;/h2&gt;

&lt;p&gt;Solr caching works by creating a view on an index called a searcher. A commit will create a warming search to prep the index and the cache. How long this takes is tricky to say, but the more rows, indexable fields, and the more parsing that is done the longer it takes. The default is to only allow two warming searches at once, and depending on how you’re doing commits, you can easily surpass that limit. If you read the solrconfig.xml file you&amp;#8217;ll see that 1-2 is useful for read-only slaves. So you&amp;#8217;re going to want to increase this number on your main instance; but be aware, you can kill your available memory if you&amp;#8217;re committing so much you have a high number of warmers.&lt;/p&gt;

&lt;p&gt;By default Solr will block if a search warmer isn&amp;#8217;t available. Depending on how and when you&amp;#8217;re committing, you may not want this. For instance, if the first search is warming an index, it could be a while before it returns. Be sure to reuse old warmers and see if you can live with a semi-built index. This is all handled in the solrconfig.xml file. Read it!&lt;/p&gt;

&lt;h2 id=&#34;increase-cache-sizes:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Increase Cache Sizes&lt;/h2&gt;

&lt;p&gt;Don&amp;#8217;t forget out-of-the-box mode is not production mode. We&amp;#8217;ve touched on a committing and search warmers. Cache sizes are another important aspect and should be as big as possible. This allows more warmers to be reused and offers a greater opportunity to search against cached search results (fq parameters) versus new query results (q parameters). The more we can cache the better; it also allows Solr to carry over search warmers when rebuilding indices which is very helpful.&lt;/p&gt;

&lt;h2 id=&#34;lock-types:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Lock Types&lt;/h2&gt;

&lt;p&gt;Luckily the default lock type is now &amp;#8220;Native&amp;#8221; which means Solr uses OS level locking. Previously it was single and this killed the system in concurrent update scenarios. Go native.&lt;/p&gt;

&lt;h2 id=&#34;understand-and-leverage-q-and-fq-parameters:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Understand and Leverage Q and FQ parameters&lt;/h2&gt;

&lt;p&gt;Q is the original query, fq is a filter query.  For larger sets this is important. If the original query is cached an fq query will just search the cached original query, rather than the entire index.  So if you have an index with one million records, and a query returns 100k results, a q/fq combination will only search the 100k cached records. This is a big performance win. Ensure your cache settings are big enough for your usage patterns to create more cache hits.&lt;/p&gt;

&lt;h2 id=&#34;minimize-use-of-facets:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Minimize Use of Facets&lt;/h2&gt;

&lt;p&gt;Calculating facets is time consuming and can easily increase a search 2-5x than normal.  This is the slowest bottleneck we have with Solr (but still, it’s minimal compared to sql).  If you can avoid facets than do so.  If you can’t, only calculate them once on initial load, and design a UI that doesn’t need to refresh them (i.e. paging via ajax, etc).  When searching from a facet, use the fq parameter to minimize the set you&amp;#8217;re searching on from your q query. This also reduces the required number of entries that are calculated for a facet and greatly increases performance.&lt;/p&gt;

&lt;h2 id=&#34;avoid-dynamic-fields-in-solr:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Avoid Dynamic Fields in Solr&lt;/h2&gt;

&lt;p&gt;This is more of an application architectural decision rather than anything else, and probably somewhat controversial. I feel you should avoid the use of dynamic fields and focus on defining your schema. I feel you can easily lose control over your schema if your model changes often as you have no base to work from. That can have unintended consequences depending on how you wrap your Solr instance and how you serialize and deserialize Solr data. It’s not too much up front work to define your schema during development that would call for the use of dynamic fields in production, unless of course your app necessitates using dynamic fields for one reason or another.&lt;/p&gt;

&lt;p&gt;The other, more valid argument is that on a per-field level you can specify multi-values, required, and indexable fields. Solr handles multi valued and indexable fields differently on commits. If you are using dynamic fields and are indexing each one, and are not actually searching nor returning these fields, you have a really high and unnecessary commit cost. At the very least, consider turning off indexing for dynamic fields if you don&amp;#8217;t need it.&lt;/p&gt;

&lt;h2 id=&#34;use-field-lists:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Use Field Lists&lt;/h2&gt;

&lt;p&gt;You should always specify what data you want returned from the query with fl (field list). This is extremely important!  Depending on how you’ve set up your schema, you probably have a ton of fields you don’t actually need returned to the UI.  This is common when you are indexing the same field with different parsers via the copyField functionality. Use fl to get back only the data you need- this will greatly reduce the amount data (and network traffic) returned, and speed up the query because Solr will not have to fetch unnecessary fields from its internal storage. In a high-read environment, you can greatly reduce both memory and network load by trimming the fat from your dataset.&lt;/p&gt;

&lt;h2 id=&#34;have-a-reindex-strategy:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Have a Reindex strategy&lt;/h2&gt;

&lt;p&gt;There will come a time when you need to reindex your Solr instance. Most likely this will be when you&amp;#8217;re releasing a new feature. It&amp;#8217;s important to have a reindexing strategy ready to go. Let&amp;#8217;s say you add a new field to your UI which you want to search on. You release your code, but that field is not in Solr yet so you get no results. Or, you get a doc back from Solr, you deserialize it to your object model, and get an error because you expect the field to be there and it&amp;#8217;s not. You must prepare for that. You could change your schema file, reindex in a background process, and then release code when ready. In this scenario make sure you can reindex without killing the system. It&amp;#8217;s also important to know how long it will take. Having to reindex like this may not be practical if takes a couple of days. You could also reindex to a second, unused Solr instance, and when you deploy you cut over to the new instance. By looking at your db update timestamps you can sync any missed data. (Remember how I said Solr is not a data store? This is a reason.)&lt;/p&gt;

&lt;h2 id=&#34;final-thoughts:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;Remember that data in Solr needs to be stored, indexed and returned. If you are only using dynamic fields, indexing all of them, defining copyField settings left and right and returning all that data because you are not using field lists (and potentially calculating facets on everything), you are generating a lot of unnecessary overhead. Keep it small and keep it slim. You&amp;#8217;ll lower your storage needs, your memory requirements, and your result set. You&amp;#8217;ll speed up commits as well.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Data Modeling at Scale:  MongoDb &#43; Mongoid, Callbacks, and Denormalizing Data for Efficiency</title>
          <link>http://blog.michaelhamrah.com/2011/08/data-modeling-at-scale-mongodb-mongoid-callbacks-and-denormalizing-data-for-efficiency/</link>
          <pubDate>Fri, 12 Aug 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/08/data-modeling-at-scale-mongodb-mongoid-callbacks-and-denormalizing-data-for-efficiency/</guid>
          <description>

&lt;p&gt;I found myself confronted with a MongoDb data modeling problem. I have your vanilla User model which has many Items. The exact nature of an Item is irrelevant, but let us say a User can have lots of Items. I struggled with trying to figure out how to model this data in a flexible way while still leveraging the documented-orientated nature of MongoDb. The answer may seem obvious to some but it is interesting to weigh the options available.&lt;/p&gt;

&lt;h3 id=&#34;to-embed-or-not-to-embed:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;To Embed or Not to Embed&lt;/h3&gt;

&lt;p&gt;The main choice was to embed Items in a User or have that as a separate collection. I do not think it makes sense to go vice versa, as Users are unique and clearly a top level entity. It would not make sense to have thousands of the same User in an Items collection. So the choice was between having Items in its own collection or embedding it in Users. A couple of factors came into play: How can I access, sort, or page through Item results if it is embedded in a User? What happens if I had so many Items in a User class I hit the MongoDb 4mb document size limit? (Unlikely: 4mb is a lot of data, but I would certainly not want to have to refactor that logic later on!) What would sharding look like with a large number of very large User documents? Most importantly, at what point would the number of Items be problematic with this approach? A hundred? A thousand? A hundred thousand?&lt;/p&gt;

&lt;h3 id=&#34;when-to-embed:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;When to Embed&lt;/h3&gt;

&lt;p&gt;I think embedded documents are an awesome feature of MongoDb, and the general approach, as recommended on the docs, is to say &amp;#8220;Why wouldn&amp;#8217;t I put this in an embedded document?&amp;#8221;. I would say if the number of Items a User would have is relatively small (say, enough that you would not need to page them on a UI, or if it would not create large network io by just accessing that field) then it can be an embedded document. The decision is a lot simpler if it is a 1..1 relationship as the potential size is clearly defined. 1..N relationships break down with embedded relations when N becomes so large that accessing it as a whole is impractical. As far as I know there does not seem to be a way to page or sort through an embedded array directly within MongoDb: you need to pull the entire field out of the database with field selection and then page on the client. Note MongoDb offers numerous ways to find data within a document no matter how it is stored within the document (see the &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Dot+Notation+%28Reaching+into+Objects%29&#34;&gt;docs on dot notation&lt;/a&gt; for more). You can even query on the position of elements in an array, which is helpful with sorted embedded lists (find me all Users who have Item Z as the first element). But sadly you cannot say &amp;#8220;give me the first to the Nth element in an embedded array&amp;#8221;. It is all or nothing.&lt;/p&gt;

&lt;p&gt;Now Mongoid does offer the ability to page through an embedded association using a gem (seems like people use &lt;a href=&#34;https://github.com/amatsuda/kaminari&#34;&gt;Kaminari&lt;/a&gt; as will_paginate was removed from Mongoid some time ago). However, this paging is done within the ruby object for embedded relations. More importantly, it is only done on a per-document basis. Under the hood you need to grab the entire embedded relation &lt;em&gt;embedded within its root document&lt;/em&gt; (think an array of Users containing an array of Items, not a plain array of Items). This means you cannot grab a collection of embedded documents which span multiple root documents. You cannot say &amp;#8220;give me all Items of type &amp;#8216;X&amp;#8217;. You need to say &amp;#8220;give me all Users and its Items containing Items of type &amp;#8216;X&amp;rsquo;&amp;#8221;. If you ever ran into the &amp;#8220;Access to the collection for XXX is not allowed since it is an embedded document, please access a collection from the root document&amp;#8221; error you are probably trying to issue an unsupported Mongoid query by bypassing a root document. You think you can treat embedded relations like normal collections, but you can&amp;#8217;t.&lt;/p&gt;

&lt;h3 id=&#34;when-to-have-separate-collections:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;When to Have Separate Collections&lt;/h3&gt;

&lt;p&gt;So where does that leave us: If the relation is small enough, than an embedded relation is fine: we just need to realize that we can never really treat elements in that collection across its top level document and that getting those elements is an all-or-nothing decision for each parent document. For the sake of argument, let us say a User can have thousands of Items, and we wanted the ability to list Items across Users in a single view. That would be too much to manage as its own field as an embedded document, and we could not aggregate Items across Users easily. So it needs to be in its own collection. This now gives us numerous sorting options and paging features like skip and limit to reduce network traffic. If we have Items as its own collection then we can create a DBRef between the two. This is a classical relational breakdown. The thing that smells with this approach, specifically when using MongoDb, is that if I were viewing a list of Items, and wanted to show the username associated with them, I would either have to use a DBRef command to pull user information or make two queries. Less than ideal. A JOIN would certainly be easier (albeit at scale, impractical, but probably for the DbRef approach too).&lt;/p&gt;

&lt;h3 id=&#34;the-solution:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;The Solution&lt;/h3&gt;

&lt;p&gt;So what I&amp;#8217;m really looking for is the ability to show the username with a list of Items when each has its own collection. The trick is I do not need to aggregate this data when I am pulling it out of the database. Instead I can assemble it before I put in the database and it will all be there when I take it out. Classic denormalization. With Mongoid and &lt;a href=&#34;http://mongoid.org/docs/callbacks.html&#34;&gt;Callbacks&lt;/a&gt; this becomes extremely easy.&lt;/p&gt;

&lt;p&gt;On my Items class I add a _:belongs&lt;em&gt;to :user&lt;/em&gt; property along with a &lt;em&gt;:username&lt;/em&gt; property. I want to ensure that a &lt;em&gt;:user&lt;/em&gt; always exists, so I add a &lt;em&gt;validates_presence_of :user&lt;/em&gt; validation. I do not need to add &lt;em&gt;:username&lt;/em&gt; to this validation as we will see below. Then I leverage callbacks like so:&lt;/p&gt;

&lt;pre class=&#34;syntax ruby&#34;&gt;before_save :add_username

protected
def add_username
  if user_id_changed?
    self.username = user.username
  end
end&lt;/pre&gt;

&lt;p&gt;What will happen is if the User property changed Mongoid will set the current Item&amp;#8217;s username value to the user.username property value. The username field is now stored within the Item document, and I can query on this field as easily as any other Item property (including the user_id relation on the Item document). More importantly, it is already available in a query result so there is no need to make an additional query on User.username for display. Any time the user changes (if Items can switch Users) the username will be updated automatically before the save to maintain consistency. Because the :user object is required, there is no need to also make :username required. Username will read from the required User property before each save. There is a slight catch with this approach: callbacks will only be run on document which received the save call, so be careful with cascading updates. As always a great test suite will always ensure the behavior you want is enforced.&lt;/p&gt;

&lt;h3 id=&#34;sharding:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;Sharding&lt;/h3&gt;

&lt;p&gt;The other point about the user relation, whether it is via the username field or on user_id, is that it makes a good shard key. If we shard off of this field (probably in conjunction of another key) you can control things like write scaling while keeping relevant data close together for querying. For instance, sharding only on username will put all data in the same server to make querying a user&amp;#8217;s items extremely efficient. Sharding on username and something else will get writes distributed across servers at the expense of having to gather elements across servers when returning results. The bottom line is know your use case: are faster writes more important than faster reads? Which one are you doing more of?&lt;/p&gt;

&lt;h3 id=&#34;in-conclusion:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;In Conclusion&lt;/h3&gt;

&lt;p&gt;I think there are two important things to realize when it comes to modeling with not just Mongoid but with any type of data store, sql or nosql. First when you are dealing with scale you want to put your data in the same way you want to get it out. Know your data access patterns. Sql allows a tremendous amount of flexibility, but joining numerous tables across millions of rows is extremely inefficient. More importantly, if you model your data in NoSql incorrectly, you could end up with similar performance problems. In the case of the data denormalization exercise above, adding a username field to the Items collections saves us from a DbRef later. Plus, with the use of callbacks, getting our data into Mongoid in a denormalized way is easy. We could easily apply the same principle to a sql-based solution: add a username column to a Item table or create a materialized view/indexed view on the Users/Items data. If you are debating a no-sql solution over a sql one, take a look at the cost/benefit of one approach over another in terms of how easy it is to model your data around data access. I think MongoDb gives a good amount of flexibility, especially with querying and indices, while still promoting some of the NoSql goodies like easy sharding for scalability and easy replication for reliability and read scaling.&lt;/p&gt;

&lt;p&gt;Secondly, it is extremely important to know your toolset. With MongoDb, you get a tremendous amount of querying power: filtering on any field, no matter the nesting, even if it&amp;#8217;s an array; creating indices on said fields; map/reduce views; only retrieving specific fields from a document; the list is nearly endless. ORM features are important too: How does Mongoid map its API to MongoDB commands? How does it deal with dirty tracking? What callbacks are available? The coolest thing on the &lt;a href=&#34;http://www.mongoid.org&#34;&gt;Mongoid&lt;/a&gt; website is the statement &lt;em&gt;This is why the documentation provides the exact queries that Mongoid is executing against the database when you call a persistence operation. If we took the time to tell you, you should listen.&lt;/em&gt; VERY TRUE! I like that. The point being, there should be a purpose why you are choosing a NoSql solution: so know what it is and leverage it. It will mean the difference of succeeding at scale or failing at launch.&lt;/p&gt;

&lt;h3 id=&#34;cassandra:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;Cassandra&lt;/h3&gt;

&lt;p&gt;As an interesting footnote, I think Cassandra exemplifies the query-first approach to data modeling (I mean, it states so on its wiki!). Cassandra&amp;#8217;s uniqueness is in its masterless approach as a key/value store. It comes with some interesting features: the choice of using a secondary index vs. columnfamily as index, numerous comparison operators on columnfamily names, super columns vs. columns for storing data, replication and write consistency options across multiple data centers. This leads to plenty of benefits but with a certain cost. As for the know your tools/know your data philosophy, an example is the typical choice of &amp;#8220;Do you create a row and use its respected columns as an index, choosing an appropriate column comparison type, or do you treat your data as a key/value store and use a secondary index for queries?&amp;#8221; One the one hand, you have a pre-sorted list that queries from one machine and with one call with slices for paging; on the other, you may need to farm out to a lot of machines to get the data you want. Knowing your options is important, and knowing what you have to do to implement your choice is nearly as important. Even with the best Cassandra ORMs you still need to do a lot of prep to get your data into and out of Cassandra in a meaningful way.&lt;/p&gt;

&lt;h3 id=&#34;final-thought:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;Final Thought&lt;/h3&gt;

&lt;p&gt;In a bit of contradictory advice, I&amp;#8217;d say don&amp;#8217;t sweat it too much. Do some preliminary research, go with your hunch and trust your ability to refactor when needed. If you wait to figure out the perfect solution, you won&amp;#8217;t build anything!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Nina, My New Favorite Web (Micro)Framework</title>
          <link>http://blog.michaelhamrah.com/2011/05/nina-my-new-favorite-web-microframework/</link>
          <pubDate>Tue, 10 May 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/05/nina-my-new-favorite-web-microframework/</guid>
          <description>

&lt;p&gt;One of the things I&amp;#8217;m excited to see is the huge increase in Open Source projects in the .NET world. NuGet has certainly helped the recent explosion, but even before that there have been numerous projects gaining legs in the .NET community. Even better, the movement has been learning from other programming ecosystems to bring some great functionality into all kinds of .NET based systems.&lt;/p&gt;

&lt;p&gt;One of my favorite projects on the scene is &lt;a href=&#34;http://jondot.github.com/nina/&#34;&gt;Nina, a Web Micro Framework&lt;/a&gt; by &lt;a href=&#34;http://twitter.com/#!/jondot&#34;&gt;jondot&lt;/a&gt;. What exactly is a web micro framework? Quite simply it easily allows you to go from an HTTP request to a server side method call with little friction. The project is inspired by &lt;a href=&#34;http://www.sinatrarb.com/&#34;&gt;Sinatra&lt;/a&gt; a very popular ruby framework for server-side interaction which doesn&amp;#8217;t involve all the overhead of a convention based framework like Ruby on Rails.&lt;/p&gt;

&lt;h2 id=&#34;wait-isn-8217-t-this-mvc:340cb49b2e29e9bef84588c65c69d675&#34;&gt;Wait, Isn&amp;#8217;t This .MVC?&lt;/h2&gt;

&lt;p&gt;Sort of- but the two frameworks take very different approaches in how they map an HTTP request to a function call. .MVC is a huge improvement over &amp;#8220;that which must not be named&amp;#8221; but still abstracts the underlying HTTP request/response: controllers and actions to handle logic, models to represent data, views to render results, and routing to figure out what to do. This is usually a good thing as you can easily get fully formed objects into and out of the server in an organized way and has incredible benefits over WebForms. But sometimes that is too much for what you want or need. In our ajax driven world we simply want to do something&amp;#8211;GET or POST some data&amp;#8211;as quickly and easily as possible. We don&amp;#8217;t want to set up a routing for new controller, create a model or view model, invoke an action, return a view, and all that other stuff; we just want to look at the request and do something. That&amp;#8217;s where Nina comes in- it elegantly lets you &amp;#8220;think&amp;#8221; in HTTP by providing an API to do something based on a given HTTP request. It&amp;#8217;s extremely lightweight and extremely fast. It&amp;#8217;s the bare essentials of MVC by providing a minimalist view of functionality in a well defined DSL. On the plus side, the MVC framework and Nina can complement each other quite well (Nina can also stand on its own, too!). Let&amp;#8217;s take a look.&lt;/p&gt;

&lt;h2 id=&#34;how-it-works:340cb49b2e29e9bef84588c65c69d675&#34;&gt;How It Works&lt;/h2&gt;

&lt;p&gt;Nina is essentially functionality added to a web project in the same way the MVC bits are added to a web project. It&amp;#8217;s not an entirely new HTTP server implementation. It&amp;#8217;s powered off of the standard .NET HttpApplication class and unlike the various &lt;a href=&#34;http://owin.org/&#34;&gt;OWIN&lt;/a&gt; toolkits Nina doesn&amp;#8217;t try and rewrite the underlying HttpContext or IIS server stack. To start things off Nina is powered by creating a class that handles all requests to a given url, referred to as an endpoint. This class inherits from &lt;em&gt;Nina.Application&lt;/em&gt; and handles all requests to that endpoint- no matter what the rest of the url is. This is done by &amp;#8220;mounting&amp;#8221; the class to an endpoint in your Global.asax file. It&amp;#8217;s not too different than setting up a routing for MVC. However, instead of MVC, you&amp;#8217;re not routing directly to specific actions or a pattern of actions, but &amp;#8220;gobbing&amp;#8221; up all requests to that url endpoint. Below is an example of a global.asax file from the Nina demo project. There are two Nina applications- the Contacts class gets mounted to the contacts endpoint and Posts gets mounted to the blog endpoint.&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;private static void RegisterRoutes()
    {
            RouteTable.Routes.Add(new MountingPoint(&#34;contacts&#34;));
            RouteTable.Routes.Add(new MountingPoint(&#34;blog&#34;));
    }&lt;/pre&gt;

&lt;p&gt;When you&amp;#8217;re mounting an endpoint any request to that endpoint will go to that class- and that class will handle everything else. So anything with a url of /contacts, /contacts/123, /contacts/some/long/path/with/file.html?x=1&amp;amp;y=1 will go to the Contacts class. There&amp;#8217;s no automatic mapping of url parts to action names, or auto filling of parameters. That&amp;#8217;s all handled by the class you specify which inherits from Nina.Application. Routing to individual methods is handled within these classes by leveraging the Nina DSL. I like this approach, as it keeps routing logic tied to specific endpoints rather than requiring you to centrally locate everything or to dictate globally how routing should work via conventions. Of course, there are pros and cons in either case. In very complex systems the Global.asax can get quite large; you can certainly refactor routing logic into helper functions as necessary, but moving routing definitions closer to the logic has its benefits. I&amp;#8217;m also not too big of a fan when it comes to attribute based programming so not having to pepper your action methods with specific filters- whether for a Uri template in the case of WCF or Http Verbs for .MVC- is a big plus.&lt;/p&gt;

&lt;h2 id=&#34;handling-requests:340cb49b2e29e9bef84588c65c69d675&#34;&gt;Handling Requests&lt;/h2&gt;

&lt;p&gt;This is where the beauty of Nina comes in. Once we&amp;#8217;ve mounted an application to an endpoint we can handle what to do based on two variables: the HTTP method and the path of the request. This is done via four function calls which are part of the Nina.Application class and map to the four HTTP verbs: Get(), Put(), Post() and Delete(). Each function takes in two parameters: the first is a Uri template which determines when this method gets invoked. The second is a lambda with a signature of Func&lt;NameValueCollection, HttpContext, ResourceResult&gt;. This lambda is what gets invoked when the current requests matches the Uri template. The first parameter are the template parts (explained later), the second parameter is the underlying HttpContext object, and the Function returns a Nina.ResourceResult class. For all intents and purposes a ResourceResult is similar to an ActionResult in .MVC. Nina provides quite a number of ResourceResults, from Html views to various serialization objects to binary data.&lt;/p&gt;

&lt;p&gt;This setup is powered by an extremely nice DSL for handling function invocation from HTTP requests and yields a very nice description of your endpoint. You specify the HTTP verb required to invoke the function. You specify the Uri template to when that match should occur&amp;#8211;very similar to setting up routes&amp;#8211;and your handler is actually a parameter, which you can specify inline or elsewhere if needed. The Uri templating is pretty slick, as it allows any level of fuzzy matching. Because the template is automatically parsed and passed as a variable to your handler, you can easily get out elements of the Uri using the template tokens in your Uri. Let&amp;#8217;s take a look at a simple example:&lt;/p&gt;

&lt;p&gt;Take a look at the example application below.&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;public class Contacts : Nina.Application
    {
        public Contacts()
        {
            Get(&#34;&#34;, (m, c) =&amp;gt;
                        {
                            //Returns anything at the root endpoint, i.e. /contacts
                            var data = SomeRepository.GetAll();
                            return Json(data);
                        });
            Get(&#34;Detail/{id}&#34;, (m, c) =&amp;gt;
                                   {
                                       //Returns /contacts/detail/XYZ

                                       //m is the bound parameters in the template
                                       //this will be a collection with m[&#34;ID&#34;] returning XYZ
                                       var id = m[&#34;ID&#34;]; //returns XYZ

                                       var data = SomeRepository.GetDetail(id);
                                       return View(&#34;viewname&#34;, data); //Nina has configurable ViewEngines!
                                   });

            Post(&#34;&#34;, (m,c) =&amp;gt;
                         {
                             //A post request to the root endpoint.

                             return Nothing(); 
                         });
         }
}&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;re exposing three operations: two GET calls and one POST. We&amp;#8217;re handling a GET and POST operation at the endpoint root. In our global.asax we&amp;#8217;ve mounted this application at /contacts, so everything here is relative to /contacts. A template of &amp;#8220;&amp;#8221; will simply match a Uri of /contacts. If we wrote  &lt;code class=&#34;syntax c#&#34;&gt;RouteTable.Routes.Add(new MountingPoint(&amp;ldquo;contacts&amp;rdquo;));&lt;/code&gt; in our Global.asax than this class would be at the root of our application, i.e. &amp;#8220;&lt;a href=&#34;http://localhost/&amp;amp;#8221&#34;&gt;http://localhost/&amp;amp;#8221&lt;/a&gt;;. Finally, we have another GET call at /detail/{id}. This is actually a URI template, similar to a Route, so anything which matches that template will be handled by that function. In this case /detail/123 or /detail/xyz would match. The template variables are passed as a key/value array in the &amp;#8220;m&amp;#8221; parameter of the lambda and can easily be pulled out. These are your template parts that are automatically parsed out for you.&lt;/p&gt;

&lt;p&gt;Using this DSL we can create any number of handlers for any GET, POST, PUT or even DELETE request. We can easily access HTTP Headers, Form variables, or the Request/Response objects from the HttpContext class. Most importantly we can easily view how a request will get handled by our system. The abstraction that MVC brings via Routes, Controllers and Actions is helpful; but not always necessary. Nina provides a different way of describing what you want done that serves a variety of purposes.&lt;/p&gt;

&lt;h2 id=&#34;returning-results:340cb49b2e29e9bef84588c65c69d675&#34;&gt;Returning Results&lt;/h2&gt;

&lt;p&gt;So far we&amp;#8217;ve focused on the Request side of Nina and haven&amp;#8217;t delved too much in the Response side. Nina&amp;#8217;s response system is very similar to .MVC&amp;#8217;s ActionResult infrastructure. Nina has a suite of classes which inherit from ResourceResult that allows you to output a response in a variety of ways. You can serialize an object into Json or Xml, render straight text, return a file, return only a status code, or even return a view. Nina supports numerous View engines&amp;#8211;including Razor but also NHaml, NDjango and Spark&amp;#8211;that&amp;#8217;s beyond the scope of this blog but worth checking out. I&amp;#8217;m a big fan of Haml. Results are returned using one of the method calls provided through the Nina.Application class and should serve all your needs. The best thing to do is explore the Nina.Application class itself and find out which methods return ResourceResults objects.&lt;/p&gt;

&lt;h2 id=&#34;this-is-cool-but-why-use-it:340cb49b2e29e9bef84588c65c69d675&#34;&gt;This is cool, but why use it?&lt;/h2&gt;

&lt;p&gt;The great part about Nina is that even though it can stand alone as an application, it can just as easily augment an existing WebForms (Blah!) or MVC application via mounting endpoints using the Routing engine. There are times when you want speed and simplicity for your web app rather than a fully-fledged framework. MVC is great, but requires quite a few moving parts and abstracts away underlying HTTP. The new Restful Web API&amp;#8217;s Microsoft is rolling out for WCF are also nice, but I&amp;#8217;ve never been a fan of attribute based programming and the WCF endpoints are service specific. Nina offers much more flexibility. Nina strikes the right balance by honoring existing HTTP conventions while providing flexibility of output. Sinatra, Nina&amp;#8217;s inspiration, came about by those who didn&amp;#8217;t want to follow the Rails bandwagon and the MVC convention it implemented. They wanted an easier, lightweight way of parsing and handling HTTP requests, and that&amp;#8217;s exactly what Nina does.&lt;/p&gt;

&lt;p&gt;Here are some use cases where Nina works well:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Json powered services. Even though MVC has JsonResult, Nina provides a low friction way of issuing a get request to return Json data, useful for Autosuggest lists or other Json powered services. JQuery thinks in terms of get/post commands so mapping these directly to mounted endpoints becomes much more fluid. One of my more popular articles is the &lt;a href=&#34;http://www.michaelhamrah.com/blog/2010/08/the-new-webapp-architecture-asp-net-mvc-3-jquery-templating-with-pure-and-the-json-value-provider/&#34;&gt;New Web App Architecture&lt;/a&gt;. Nina provides a nice alternative to Json powered services that can augment one of the newer javascript frameworks like &lt;a href=&#34;knockoutjs.com&#34;&gt;Knockout&lt;/a&gt; or &lt;a href=&#34;http://documentcloud.github.com/backbone/&#34;&gt;Backbone&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Better file delivery. HttpHandlers work well, but exist entirely outside the domain of your app. Powering file delivery through Nina- either because the info is in a data store or required specific authentication, works well.&lt;/li&gt;
&lt;li&gt;Conventions aren&amp;#8217;t required. Setting up routings, organizing views, and implementing action methods all require work and coding. Most of the time, you just want to render something or save something. Posting a search form, save a record via ajax, polling for alerts are all things that could be done with the conventions of MVC but aren&amp;#8217;t necessarily needed. Try the lightweight approach of Nina and you&amp;#8217;ll be glad you did. With support for View engines you may even want to come up with your own conventions for organizing content.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When the time it takes to do something simple simply becomes too great you&amp;#8217;re using the wrong tool. I strongly encourage you to play around with Nina&amp;#8211; you&amp;#8217;ll soon learn to love the raw power of HTTP and the simplicity of the API. It will augment your existing tool belt quite well and you&amp;#8217;ll find how much you can do when when you can express yourself in different ways.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Updated MVC3 Html5 Boilerplate Template: Now with Twitter and Facebook</title>
          <link>http://blog.michaelhamrah.com/2011/03/updated-mvc3-html5-boilerplate-template-now-with-twitter-and-facebook/</link>
          <pubDate>Mon, 21 Mar 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/03/updated-mvc3-html5-boilerplate-template-now-with-twitter-and-facebook/</guid>
          <description>&lt;p&gt;I pushed a major update to the MVC3/Html5 Boilerplate Template found on the &lt;a href=&#34;https://github.com/mhamrah/Html5OpenIdTemplate&#34;&gt;github&lt;/a&gt; page. The new update includes the latest boilerplate code and uses the DotnetOpenAuth CTP for logging in via Twitter and Facebook. Thanks to &lt;a href=&#34;http://www.twitter.com/jacob4u2&#34;&gt;@jacob4u2&lt;/a&gt; for making some necessary web.config changes (he has an alternate template on his &lt;a href=&#34;https://bitbucket.org/jacob4u2/mothereffin-html5-site&#34;&gt;bitbucket&lt;/a&gt; site you should also check out.&lt;/p&gt;

&lt;p&gt;Your best option is to &lt;code class=&#34;syntax bash&#34;&gt;git clone git@github.com:mhamrah/Html5OpenIdTemplate.git&lt;/code&gt; the template with your own app. That way you&amp;#8217;ll get the latest nu-get packages with the bundle. You can also use the template, but you&amp;#8217;ll need to manually pull &lt;a href=&#34;http://sourceforge.net/projects/dnoa/files/CTP/OAuth2/&#34;&gt;the latest CTP for DotNetOpenAuth&lt;/a&gt; to get the latest dlls.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Quicktip: Use Negative Margins with CSS Transforms to Fix Clipping</title>
          <link>http://blog.michaelhamrah.com/2011/03/quicktip-use-negative-margins-with-css-transforms-to-fix-clipping/</link>
          <pubDate>Wed, 02 Mar 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/03/quicktip-use-negative-margins-with-css-transforms-to-fix-clipping/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve been playing around with CSS Transforms and had an annoying issue: when rotating divs at an angle, the edge of the div also rotated leaving a gap where I didn&amp;#8217;t want one. See the pic:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2011/03/rotated-div.png&#34;&gt;&lt;img src=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2011/03/rotated-div.png?resize=246%2C117&#34; alt=&#34;&#34; title=&#34;rotated div2&#34; class=&#34;aligncenter size-full wp-image-531&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this case, the div is actually a header tag I wanted to span the length of the page, like a ribbon stretching the width of browser. But I did not want that gap. So what to do? I thought about using Transforms to skew the header the angle required to maintain a vertical line, but that&amp;#8217;s annoying.&lt;/p&gt;

&lt;p&gt;Instead, I simply added a negative margin to the width to stretch the header enough to hide the gap. Here&amp;#8217;s the css and final result:&lt;/p&gt;

&lt;pre class=&#34;syntax css&#34;&gt;header 
{
  background-color: #191919;
  margin: 75px -20px;
  -webkit-transform: rotate(-10deg);
  -moz-transform: rotate(-10deg);
  tranform:rotate(-10deg);
}
&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://i0.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2011/03/rotated-div2.png&#34;&gt;&lt;img src=&#34;http://i0.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2011/03/rotated-div2.png?resize=246%2C117&#34; alt=&#34;&#34; title=&#34;rotated div2&#34; class=&#34;aligncenter size-full wp-image-531&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You may find rotate and skew is a better combination to achieve the desired result- however, if you have text in your containing element, that text will also be skewed if you use skew.&lt;/p&gt;

&lt;p&gt;To achieve the desired result within a page (where you don&amp;#8217;t have the benefit of viewport clipping) you can always put the rotated element within another element, use negative margins, and set &lt;code class=&#34;syntax css&#34;&gt;overflow:hidden&lt;/code&gt; to achieve the desired result.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Loving Vim</title>
          <link>http://blog.michaelhamrah.com/2011/02/loving-vim/</link>
          <pubDate>Sun, 06 Feb 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/02/loving-vim/</guid>
          <description>

&lt;p&gt;Vim has quickly become my go-to editor of choice for Windows, Mac and Linux. So far I&amp;#8217;ve had about three months of serious Vim usage and I&amp;#8217;m just starting to hit that vim-as-second-nature experience where the power really starts to shine. I&amp;#8217;m shocked I&amp;#8217;ve waited this long to put in the time to seriously learn it. Now that I&amp;#8217;m past the beginner hump I wish I learned Vim long ago- when I tried Vim in the past, I just never got over that WTF-is-going-on-here frustration! Better late than never I suppose!&lt;/p&gt;

&lt;h3 id=&#34;why-i-like-vim-8211-mac:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Why I Like Vim &amp;#8211; Mac&lt;/h3&gt;

&lt;p&gt;Coming from Visual Studio, I longed for a VS like experience for programming on my Mac, whether it&amp;#8217;s html/css/js or for my recent focus on Ruby and Rails. I checked out both &lt;a href=&#34;http://www.aptana.com&#34;&gt;Aptana&lt;/a&gt; and &lt;a href=&#34;http://www.eclipse.org&#34;&gt;Eclipse&lt;/a&gt; but quickly became frustrated- it was kind of like VS but not really and it was just too weird going back and forth. Plus, my biggest pet peave with development started to emerge: I wasn&amp;#8217;t learning a language, I was learning a tool that abstracted the language away. There&amp;#8217;s nothing that could be worse- once your tool hides the benefits of the underlying infrastructure, you&amp;#8217;re missing the point, and you&amp;#8217;ll usually be behind the curve because the language always moves faster than the support.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://macromates.com&#34;&gt;TextMate&lt;/a&gt; then became the go-to: it&amp;#8217;s widely used, powerful, and there&amp;#8217;s a lot of resources for learning. The simplified environment mixed with the command line really created a higher degree of fluidity, and I realized how nice it can be to develop outside an integrated environment. Textmate has its features- Command-T is slick, the project drawer is helpful, and the Rails support is great along with the other available bundles. But it lacked split windows which drove me crazy. There&amp;#8217;s nothing more essential than split windows: I want to see my specs and code side-by-side. I want to see my html and js side-by-side. And you can&amp;#8217;t do that with the Textmate. So I turned to &lt;a href=&#34;http://code.google.com/p/macvim&#34;&gt;MacVim&lt;/a&gt; and haven&amp;#8217;t looked back.&lt;/p&gt;

&lt;h3 id=&#34;why-i-like-vim-8211-windows:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Why I Like Vim &amp;#8211; Windows&lt;/h3&gt;

&lt;p&gt;Don&amp;#8217;t get me wrong: I love me some Visual Studio. Visual Studio was my first IDE when programming professionally and my thought was &amp;#8220;wow, I can really focus on building stuff rather than pulling my hair out with every build, every exception and every bug&amp;#8221;. It was so much better than the emacs days of college. It&amp;#8217;s my go-to for anything .NET, as it should be. But there are some text-editor needs that aren&amp;#8217;t related to coding or .NET, and VS is too much of a beast to deal with for those things. First, for html/js/css editing that&amp;#8217;s not part of a Visual Studio project, VS not great to work with. It&amp;#8217;s annoying to be forced to create a Visual Studio project to house related content, especially when it&amp;#8217;s already grouped together in the file system. Quickly checking out an html template or a js code samples becomes tedious when you just want to look around. The VS File Explorer is a step in the right direction, but it&amp;#8217;s not there yet; I know there&amp;#8217;s shell plugins for a &amp;#8220;VS Project Here&amp;#8221; shortcut but really? Is that necessary?&lt;/p&gt;

&lt;p&gt;Then there&amp;#8217;s the notepad issue. Notepad is barely an acceptable editor for checking out the occasional config file or random text file. Everyone knows how incredibly limited it can be, not to mention how much it sucks for large files. Pretty much everything about it sucks, actually, and everyone knows it. &lt;a href=&#34;http://notepad-plus-plus.org/&#34;&gt;Notepad++&lt;/a&gt; is a nice alternative, but it&amp;#8217;s no Vim. So after I got modestly comfortable with MacVim, I thought, why not do this on Windows too? So I started using &lt;a href=&#34;http://www.vim.org/download.php&#34;&gt;gVim&lt;/a&gt; and haven&amp;#8217;t looked back.&lt;/p&gt;

&lt;h3 id=&#34;why-i-like-vim-8211-linux:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Why I Like Vim &amp;#8211; Linux&lt;/h3&gt;

&lt;p&gt;This blog runs WordPress on a Linux box hosted by Rackspace. Occasionally, I need to pop in via ssh, edit a config file, push some stuff, tweak some settings, etc. &lt;a href=&#34;http://www.nano-editor.org/&#34;&gt;Nano&lt;/a&gt; was the lightweight go-to editor of choice, and works well. There are many differences between nano and Vim, the biggest being nano is a &amp;#8220;modeless&amp;#8221; editor while Vim&amp;#8217;s power comes from the Normal, Insert, and Visual modes. Like always, stick with what works. But once you&amp;#8217;re hooked on Vim, I&amp;#8217;d be surprised how often you go back to Nano, especially when you get your configuration files synced up with source control, and Vim&amp;#8217;s ubiquity on Linux.&lt;/p&gt;

&lt;h3 id=&#34;why-i-like-vim:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Why I Like Vim&lt;/h3&gt;

&lt;p&gt;There&amp;#8217;s plenty of resources out there about Vim and what makes it great- a quick &lt;a href=&#34;http://www.google.com/search?q=why+i+love+vim&#34;&gt;google search&lt;/a&gt; &lt;a href=&#34;http://www.google.com/search?q=coming+home+to+vim&#34;&gt;will give you some great resources&lt;/a&gt;. But I really appreciated Vim when I became comfortable with the following features:&lt;/p&gt;

&lt;h4 id=&#34;splits-and-buffers:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Splits and Buffers&lt;/h4&gt;

&lt;p&gt;Splits are one of my favorite features- it allows you to view more than one file at once on the same screen. The power of splits allows you to have multiple vertically and horizontally split windows so you can see anything you want. There&amp;#8217;s also tab support, but I find using splits and managing buffers a better way to cycle through files. Buffers allow you to keep files open and active but in the background, so you can quickly swap them into the current window in a variety of ways. It&amp;#8217;s like having a stack of papers on a table while easily going to any page instantaneously. This is different than having files within a project, which would be like having those papers in a folder- buffers provide another level of abstraction allowing you to manipulate a set of content together. In summary, you have a set of papers in a folder (the current directory) and put them on the desk to work on (using buffers) and arrange them in front of you (using splits).&lt;/p&gt;

&lt;h4 id=&#34;modes:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Modes&lt;/h4&gt;

&lt;p&gt;A major feature of Vim, which really sets it apart from other editors, is how it separates behavior into different modes- specifically, normal and insert modes. Insert mode is simple: it allows you to write text. But normal mode is all about navigation and manipulation: finding text, cutting lines, moving stuff around, substituting words, running commands, etc. It offers a whole new level of functionality including shell commands interaction for doing anything you would on the command line. With the plethora of plugins around you can pretty much do anything within Vim you could imagine- from simple editing, to testing, to source control management, to deployments. Modes break you free from a whole slew of Ctrl + whatever commands required in other editors, allowing for precision movement with a minimal set of keystrokes. The best analogy is you can &amp;#8220;program your editing&amp;#8221; in a way unmatched from any other editor.&lt;/p&gt;

&lt;h4 id=&#34;search-and-replace-aka-substitute:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Search and Replace (aka Substitute)&lt;/h4&gt;

&lt;p&gt;Vim&amp;#8217;s Search and Substitute features make any old find and replace dialog box seem stupid. I&amp;#8217;ve barely started unlocking the power of search and replace, but already, I wish every application behaved this way. In normal mode you can easily create everything from simple string searches to complex regex to find what your looking for in a couple of keystrokes. On top of that, it&amp;#8217;s only a few more keystrokes to replace text. Because it&amp;#8217;s all driven by key commands, you can easily change or alter what you&amp;#8217;re doing without having to start an entire search and you never have that &amp;#8220;context switch&amp;#8221; of filling out a form in a dialog box. It&amp;#8217;s all right in front of you. Highlighting allows you to see matches and you can lock in a search to easily jump between results. Viewing and editing configuration files is the real win for me over other editors: whatever the size, I can open a file and type a few keystrokes to go to exactly where I need to be, even if I&amp;#8217;m not sure where to go. This is so much better than using notepad or even Visual Studio.&lt;/p&gt;

&lt;h4 id=&#34;source-controlled-configuration:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Source controlled configuration&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/mhamrah/vimfiles&#34;&gt;I put my vimfiles&lt;/a&gt; on &lt;a href=&#34;https://github.com/mhamrah/&#34;&gt;github&lt;/a&gt; so I can synchronize them across platforms. This offers an unparalleled level of uniformity across environments with minimal effort. A lot of people do this, and it&amp;#8217;s helpful to see how others have configured their environment. You&amp;#8217;ll pick up a lot of neat tidbits by reading people&amp;#8217;s Vimfiles!&lt;/p&gt;

&lt;h3 id=&#34;vim-across-platforms:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Vim Across Platforms&lt;/h3&gt;

&lt;p&gt;Vim can run in either a gui window (like &lt;a href=&#34;http://code.google.com/p/macvim&#34;&gt;MacVim&lt;/a&gt; and &lt;a href=&#34;http://www.vim.org/download.php&#34;&gt;gVim&lt;/a&gt;) or from a command line. These are two different executables with a slightly different feature set. Usually, you get a little more with a gui vim, especially around OS integration (like cutting and pasting text) while running shell commands are easier with command line vim. Gui Vim also offers better color support for syntax highlighting.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://code.google.com/p/macvim&#34;&gt;MacVim&lt;/a&gt; is the Vim app for the Mac. It has a really nice full screen mode and native Mac commands alongside the Vim ones. I love the &lt;a href=&#34;http://peepcode.com/products/peepopen&#34;&gt;Peepopen&lt;/a&gt; search plugin which is only available on the Mac (and can be used with Textmate). It&amp;#8217;s a slick approach which is better than Textmate&amp;#8217;s Command-T. I like running MacVim in full screen mode with the toolbar off to get the most screen real estate.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.vim.org/download.php&#34;&gt;gVim&lt;/a&gt; is the Windows gui version of Vim, and I find it preferable over the command line Vim via cygwin. gVim has a shell extension which lets you open any file in gVim- set it as the default to avoid notepad. Note that Vim on Windows reads configuration files from the _vimrc, _gvimrc, and _vimfiles directory, which is different than the normal .vimrc and .vim location on other platforms. That hung me up when I was trying to sync configuration via git.&lt;/p&gt;

&lt;p&gt;As for command line Vim, that&amp;#8217;s probably what you&amp;#8217;ll be using on Linux. In &lt;a href=&#34;http://peepcode.com/products/play-by-play-bernhardt&#34;&gt;Gary Bernhardt&amp;#8217;s Play-by-Play Peepcode&lt;/a&gt; video I learned a cool trick of running Vim via the command line, then using jobs to suspend (Ctrl-Z) to return to the command line, than going back to Vim via foreground (fg).&lt;/p&gt;

&lt;h3 id=&#34;learning-vim:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Learning Vim&lt;/h3&gt;

&lt;p&gt;There are plenty of resources on the web for getting started with Vim. Steve Losh&amp;#8217;s &lt;a href=&#34;http://stevelosh.com/blog/2010/09/coming-home-to-vim/&#34;&gt;Coming Home To Vim&lt;/a&gt; is a great overview that points you to a lot of other helpful posts. I bought a subscription to &lt;a href=&#34;http://www.peepcode.com&#34;&gt;Peepcode&lt;/a&gt; and picked up the &lt;a href=&#34;http://peepcode.com/products/smash-into-vim-ii&#34;&gt;Smash Into Part II&lt;/a&gt; episode. I didn&amp;#8217;t watch Part I because I felt like it was too basic, but Part II has a lot of substantial content. Peepcode offers a high quality product so you probably can&amp;#8217;t go wrong with getting both if you&amp;#8217;d rather be safe than sorry. I also watched the &lt;a href=&#34;http://peepcode.com/products/play-by-play-bernhardt&#34;&gt;Gary Bernhardt Play by Play&lt;/a&gt; which focuses a lot on using Vim with Rspec and Ruby, and use the &lt;a href=&#34;http://peepcode.com/products/peepopen&#34;&gt;Peepopen&lt;/a&gt; plugin for file search with Vim on my Mac.&lt;/p&gt;

&lt;p&gt;Here are some tips to avoid the beginner frustration:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Take it slow. There&amp;#8217;s a learning curve, but it&amp;#8217;s worth it.&lt;/li&gt;
&lt;li&gt;Don&amp;#8217;t sweat plugins when you&amp;#8217;re starting out. Yes, everyone says &amp;#8220;use Pathogen, use Rails.vim, use xyz&amp;#8221; and it&amp;#8217;s absolutely correct. But it&amp;#8217;s not essential when you&amp;#8217;re starting out.&lt;/li&gt;
&lt;li&gt;Take it one step at a time. Learn about Vim via tutorials and blogs so you know what&amp;#8217;s there, but don&amp;#8217;t try and do everything at once. Keep that knowledge in the back of your head, get comfortable with one thing, then move onto the next. You&amp;#8217;ll just end up confusing yourself if you do everything at once.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Focusing on items in the following order will allow you to build on your knowledge:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;editing text and searching, as that&amp;#8217;s what you&amp;#8217;ll be doing most&lt;/li&gt;
&lt;li&gt;file management, like opening, saving, and navigating to files&lt;/li&gt;
&lt;li&gt;other navigation, like jumping to lines or words and the power of hjkl (don&amp;#8217;t use arrow keys!)&lt;/li&gt;
&lt;li&gt;manipulation like replacing text, cutting and pasting&lt;/li&gt;
&lt;li&gt;window and buffer management, including splits&lt;/li&gt;
&lt;li&gt;start using and learning plugins to see where you can eliminate friction&lt;/li&gt;
&lt;li&gt;start customizing your vimrc file to make the vim experience more comfortable now that you know the basics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Good luck!&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
