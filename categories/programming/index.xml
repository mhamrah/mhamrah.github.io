<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> </title>
    <link>http://blog.michaelhamrah.com/categories/programming/</link>
    <language>en-us</language>
    <author>Michael Hamrah</author>
    <rights>(C) 2015</rights>
    <updated>2015-06-27 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Fleet Unit Files for Kubernetes on CoreOS</title>
          <link>http://blog.michaelhamrah.com/2015/06/fleet-unit-files-for-kubernetes-on-coreos/</link>
          <pubDate>Sat, 27 Jun 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/06/fleet-unit-files-for-kubernetes-on-coreos/</guid>
          <description>&lt;p&gt;As I&amp;#8217;ve been leveraging CoreOS more and more for running docker containers, the limitations of Fleet have become apparent.&lt;/p&gt;

&lt;p&gt;Despite the benefits of &lt;a href=&#34;http://blog.michaelhamrah.com/2015/04/easy-scaling-with-fleet-and-coreos/&#34;&gt;dynamic unit files via the Fleet API&lt;/a&gt; there is still a need for fine-grained scheduling, discovery, and more complex dependencies across containers. Thus I&amp;#8217;ve been exploring Kubernetes. Although young, it shows promise for practical usage. The &lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes&#34;&gt;Kubernetes repository&lt;/a&gt; has a plethora of &amp;#8220;getting started&amp;#8221; examples across a variety of environments. There are a few CoreOS related already, but they embed the kubernetes units in a cloud-config file, which may not be what you want.&lt;/p&gt;

&lt;p&gt;My preference is to separate the CoreOS cluster setup from the Kubernetes installation. Keeping your CoreOS cloud-config minimal has many benefits, especially on AWS where you can&amp;#8217;t easily update your cloud-config. Plus, you may already have a CoreOS cluster and you just want to deploy Kubernetes on top of it.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/mhamrah/kubernetes-coreos-units&#34;&gt;Fleet Unit Files for Kubernetes on CoreOS are on GitHub&lt;/a&gt;. The unit files makes a few assumptions, mainly you are running with a &lt;a href=&#34;https://coreos.com/docs/cluster-management/setup/cluster-architectures/#production-cluster-with-central-services&#34;&gt;production setup using central services&lt;/a&gt;. For AWS &lt;a href=&#34;http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/&#34;&gt;you can use Cloudformation to manage multiple sets of CoreOS roles as distinct stacks, and join them together via input parameters&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Networking Basics: Understanding CIDR notation and Subnets: what’s up with /16 and /24?</title>
          <link>http://blog.michaelhamrah.com/2015/05/networking-basics-understanding-cidr-notation-and-subnets-whats-up-with-16-and-24/</link>
          <pubDate>Tue, 12 May 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/05/networking-basics-understanding-cidr-notation-and-subnets-whats-up-with-16-and-24/</guid>
          <description>&lt;p&gt;An IP address (specifically, an IPv4 address), like 192.168.1.51, is really just 32 bits of data. 32 ordinary bits, like a 32 bit integer, but represented a little differently than a normal int32. These 32 bits are split up into 4 8-bit blocks, with each block represented as a number with a dot in between. This is called dotted-decimal notation: 192.168.1.51. Curious why an IP address never has a number in it greater than 255? That&amp;#8217;s the maximum value for 8 bits of data: you can only represent 0 to 255 with 8 bits.&lt;/p&gt;

&lt;p&gt;The IP address 192.168.1.51 in binary is 11000000 10101000 00000001 00110011, or just 11000000101010000000000100110011. If this were an int32 it would be 3232235827. Why not just use this number as an address? By breaking up the address into blocks we can logically group related ip addresses together. This is helpful for routing and subnets (as in sub-networks) which is where we usually see &lt;a href=&#34;http://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing&#34;&gt;CIDR notation&lt;/a&gt;. It lets us specify a common range of IP address by saying some bits in the IP address are fixed, while others can change. When we add a slash and a number between 0 and 32 after an IP, like 192.168.1.&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;24&lt;/sub&gt;, we specify which bits of the address are fixed and which can be changed.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s take a look at Docker networking. Docker creates a subnet for all containers on a machine. It uses CIDR notation to specify a range of IP address for containers, usually 172.17.42.&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;16&lt;/sub&gt;. The /16 tells us the first sixteen bits in the dotted-decimal ip address are fixed. So any ip address, in binary, must begin with 1010110000010001. We are &amp;#8220;masking&amp;#8221; the first 16 bits of the address. Docker could assign a container of 172.23.42.1 but could not assign it 172.32.42.1. That&amp;#8217;s because the first four bits in 23 are the same as 17 (0001) but the first four bits 32 are different (0010).&lt;/p&gt;

&lt;p&gt;Another way to specify a range of IP addresses is with a &amp;#8220;subnet mask&amp;#8221;. Instead of simply saying 172.17.42.&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;16&lt;/sub&gt;, we could give the ip address of 172.17.42.1 and a subnet mask of 255.255.0.0. We often see this with tools like &lt;code&gt;ifconfig&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;255.255.0.0 in binary is 11111111 11111111 00000000 00000000. The first sixteen bits are 1 telling us which bits in the corresponding IP address are fixed. Sometimes you see a subnet mask of 255.240.0.0: that&amp;#8217;s the same as an ip range of /12. The number 240 is 11110000 in binary, masking the first 12 bits in the subnet mask 255.240.0.0 (8 1&amp;#8217;s for 255, and 4 1&amp;#8217;s for 240). You could never have a subnet of 239, because there is a 0 in the middle of the binary number 239 (11101111), defeating the purpose of a mask.&lt;/p&gt;

&lt;p&gt;A /16 range gives us 2 8 bit blocks to play with, or 65535 combinations: 2^16, or 256*256. But the 0 and 255 numbers are reserved in the IP address space, so we really only get 254 * 254 combinations, or 64516 addresses. This is important when setting up networks: if you want to make sure you have enough IP addresses for all the things in your network. A software defined network like &lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;Flannel&lt;/a&gt;, which allows you to create a private subnet for docker containers across hosts, uses a /16 subnet for a cluster and /24 range per node. So an entire cluster can only have 64516 ip address. More specifically, it can only have 254 nodes running 254 containers. Large networks, like those at most companies, use the reserved address space 10.0.0.0/8. The first 8 bits are fixed giving us 24 bits to play with: or 16,387,064 possible addresses (because we can&amp;#8217;t use 0 or 255). These are usually broken up into several subnet works, like 10.252.0.0/16 and 10.12.0.0/16, carving up smaller subnets from the larger address space.&lt;/p&gt;

&lt;p&gt;Subnet masks and CIDR notation play prominent roles in a variety of areas beyond specifying subnets. They are heavily used in routing tables to specify where traffic for a particular IP should go. They are used extensively in AWS and other cloud providers to specify firewall rules for security groups as well as their VPC product. Understanding CIDR notion and subnet masks make other aspects of networking: interfaces, gateways, routing tables much easier to understand.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Easy Scaling with Fleet and CoreOS</title>
          <link>http://blog.michaelhamrah.com/2015/04/easy-scaling-with-fleet-and-coreos/</link>
          <pubDate>Fri, 10 Apr 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/04/easy-scaling-with-fleet-and-coreos/</guid>
          <description>&lt;p&gt;One element of a successful production deployment is the ability to easily scale the number of instances your process is running. Many cloud providers, both on the PaaS and IaaS front, offer such functionality: AWS Auto Scaling Groups, Heroku&amp;#8217;s process size, Marathon&amp;#8217;s instance count. I was hoping for something similar in the CoreOS world. &lt;a href=&#34;http://deis.io&#34;&gt;Deis&lt;/a&gt;, the PaaS-on-CoreOS service, offers Heroku-like scaling, but I don&amp;#8217;t want to commit to the Deis layer nor its build pack approach (for no other reason than personal preference). Fleet, CoreOS&amp;#8217;s distributed systemd service, offers service templating, but you cannot say &amp;#8220;now run three instances of service x&amp;#8221;. Being programmers we can do whatever we want, and luckily, we&amp;#8217;re only a little bash script away from replicating the &amp;#8220;scale to x instances&amp;#8221; functionality of popular providers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.michaelhamrah.com/2015/03/deploying-docker-containers-on-coreos-with-the-fleet-api/&#34;&gt;You&amp;#8217;ll want to enable the Fleet HTTP Api&lt;/a&gt; for this script to work. You can easily port this to the Fleet CLI, but I much prefer the http api because it doesn&amp;#8217;t involve ssh, and provides more versatility into how and where you run the script.&lt;/p&gt;

&lt;p&gt;Conceptually the flow is straightforward:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Given a process we want to set the number of running instances to some &lt;code&gt;desired_count&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;desired_count&lt;/code&gt; is less than &lt;code&gt;current_count&lt;/code&gt;, scale down.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;desired_count&lt;/code&gt; is more than &lt;code&gt;current_count&lt;/code&gt;, scale up.&lt;/li&gt;
&lt;li&gt;If they are the same, do nothing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fleet offers service templating so you can have a service unit named &lt;code&gt;my_awesome_app@.service&lt;/code&gt; with specific copies named &lt;code&gt;my_awesome_app@1, my_awesome_app@2, my_awesome_app@N&lt;/code&gt; representing specific running instances. Currently Fleet doesn&amp;#8217;t offer a way to group these related services together but we can easily pattern match on the service name to control specific running instances. The steps are straightforward:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Query the Fleet API for all instances&lt;/li&gt;
&lt;li&gt;Filter by all services matching the specified name&lt;/li&gt;
&lt;li&gt;See how many instances we have running for the given service&lt;/li&gt;
&lt;li&gt;Destroy or create instances using specific service names until we match the &lt;code&gt;desired_size&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these steps are easily achievable with Fleet&amp;#8217;s HTTP Api (or fleetctl) and a little bash. To give our script some context, let&amp;#8217;s start with how we want to use the script. Ideally it will look like this:&lt;/p&gt;

&lt;pre class=&#34;toolbar-overlay:false syntax bash&#34;&gt;./scale-fleet my_awesome_app 5
&lt;/pre&gt;

&lt;p&gt;First, let&amp;#8217;s set up our script &lt;code&gt;scale-fleet&lt;/code&gt; and set the command line arguments:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

FLEET_HOST=&amp;lt;YOUR FLEET API HOST&gt;

# You may want to consider cli flags 
SERVICE_NAME=$1
DESIRED_SIZE=$2
&lt;/pre&gt;

&lt;p&gt;Next we want to query the Fleet API and filter on all units with a prefix of &lt;code&gt;SERVICE_NAME&lt;/code&gt; which have a process number. This will give us an array of units matching &lt;code&gt;my_awesome_app@1.service&lt;/code&gt;, not the base template of &lt;code&gt;my_awesome_app@.service&lt;/code&gt;. These are the units we will either add to or destroy as appropriate. The latest 1.5 version of jq supports regex expressions, but as of this writing 1.4 is the common release version, so we&amp;#8217;ll parse the json response with jq, and then filter with grep. Finally some bash trickery will parse the result into an array we can loop through later.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Curls the API and filter on a specific pattern, storing results in an array
INSTANCES=($(curl -s $FLEET_HOST/fleet/v1/units | jq &#34;.units[].name | select(startswith(\&#34;$SERVICE@\&#34;))&#34; | grep &#39;\w@\d\.service&#39;))

# A bash trick to get size of array
CURRENT_SIZE=${#INSTANCES[@]}
echo &#34;Current instance count for $SERVICE is: $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;Next let&amp;#8217;s scaffold the various scenarios for matching &lt;code&gt;CURRENT_SIZE&lt;/code&gt; with &lt;code&gt;DESIRED_SIZE&lt;/code&gt;, which boils down to some if statements.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;if [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; then
  echo &#34;doing nothing, current size is equal desired size&#34;
elif [[ $DESIRED_SIZE &amp;lt; $CURRENT_SIZE ]]; then
  echo &#34;going to scale down instance $CURRENT_SIZE&#34;
  # More stuff here
else 
  echo &#34;going to scale up to $DESIRED_SIZE&#34;
  # More stuff here
fi
&lt;/pre&gt;

&lt;p&gt;When the desired size equals the current size we don&amp;rsquo;t need to do anything. Scaling down is easy, we simply loop, deleting the specific instance, until the desired and current states match. You can drop in the following snippet for scaling down:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;until [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; do
    curl -X DELETE $FLEET_HOST/fleet/v1/units/${SERVICE}@${CURRENT_SIZE}.service

    let CURRENT_SIZE = CURRENT_SIZE-1
  done
  echo &#34;new instance count is $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;Scaling up is a bit trickier. Unfortunately you can&amp;rsquo;t simply create a new unit from a template like you can with the fleetctl CLI. But you can do exactly what the fleetctl does: copy the body from the base template and create a new one with the specific full unit name. With the body we can loop, creating instances, until our current size matches the desired size. Let&amp;rsquo;s walk it through step-by-step:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;echo &#34;going to scale up to $desired_size&#34;
 # Get payload by parsing the options field from the base template
 # And build our new payload for PUTing later
 payload=`curl -s $FLEET_HOST/fleet/v1/units/${SERVICE}@.service | jq &#39;. | { &#34;desiredState&#34;:&#34;launched&#34;, &#34;options&#34;: .options }&#39;`

 #Loop, PUTing our new template with the appropriate name
 until [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; do
   let current_size=current_size+1

   curl -X PUT -d &#34;${payload}&#34; -H &#39;Content-Type: application/json&#39; $FLEET_HOST/fleet/v1/units/${SERVICE}@${CURRENT_SIZE}.service 
 done
 echo &#34;new instance count is $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;With our script in place we can scale away:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Scale up to 5 instances
$ ./scale-fleet my_awesome_app 5

# Scale down
$ ./scale-fleet my_awesome_app 3
&lt;/pre&gt;

&lt;p&gt;Because this all comes down to a simple bash script you can easily run it from a variety of places. It can be part of a parameterized Jambi job to scale manually with a UI, part of an &lt;a href=&#34;http://github.com/hashicorp/envconsul&#34;&gt;envconsul&lt;/a&gt; setup with a key set in &lt;a href=&#34;consul.io&#34;&gt;Consul&lt;/a&gt;, or it can fit into a larger script that reads performance characteristics from some monitoring tool and reacts accordingly. You can also combine this with AWS Cloudformation or another cloud provider: if you&amp;rsquo;re CPU&amp;rsquo;s hit a certain threshold, you can scale the specific worker role running your instances, and have your &lt;code&gt;desired_size&lt;/code&gt; be some factor of that number.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been on a bash kick lately. It&amp;rsquo;s a versatile scripting language that easily portable. The syntax can be somewhat mystic, but as long as you have a shell, you have all you need to run your script.&lt;/p&gt;

&lt;p&gt;The final, complete script is here:&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Managing CoreOS Clusters on AWS with CloudFormation</title>
          <link>http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/</link>
          <pubDate>Wed, 25 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/</guid>
          <description>&lt;p&gt;Personally, I find CloudFormation a somewhat annoying tool, yet I haven&amp;#8217;t replaced it with anything else. Those json files can get so ugly and unwieldy. Alternatives exist; you can try an abstraction like &lt;a href=&#34;https://github.com/cloudtools/troposphere&#34;&gt;troposphere&lt;/a&gt; or &lt;a href=&#34;https://jclouds.apache.org&#34;&gt;jclouds&lt;/a&gt;, or ditch cfn completely with something like &lt;a href=&#34;https://www.terraform.io&#34;&gt;terraform&lt;/a&gt;. These are interesting tools but somehow I find myself sticking with the straight-up json approach, the aws cli, and some bash scripting: the pieces are already there, they just need to be strung together. In the end it&amp;#8217;s not that bad, and there are some tools and techniques I&amp;#8217;ve picked up which really help out. I recently applied these to managing CoreOS clusters with CFN, and wanted to share a simplified version of the approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://coreos.com/docs/running-coreos/cloud-providers/ec2/&#34;&gt;CoreOS provides a default CloudFormation template&lt;/a&gt; which is a great start for cluster experimentation. But scaling out, where nodes are coming and going, can be disastrous for etcd&amp;#8217;s quorum consensus if you&amp;#8217;re not careful. You just don&amp;#8217;t want to remove nodes from a formed etcd cluster. &lt;a href=&#34;https://coreos.com/docs/cluster-management/setup/cluster-architectures/&#34;&gt;CoreOS&amp;#8217;s cluster documentation&lt;/a&gt; has a section on production configuration: you want a core set of nodes for running central services, with various worker nodes for specific purposes. We can elaborate this with a short-list of requirements:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;You want to tag sets of instances with specific roles so you can group dependencies and isolate apps when needed.&lt;/strong&gt;&lt;/em&gt; Although possible, it&amp;#8217;s unrealistic to actually run any app on any node. More likely you want to group apps into front-facing and back-facing and treat those nodes differently. For instance, you could map the IP&amp;#8217;s of front-facing nodes to a Route53 endpoint.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;You want a cluster of heterogeneous instances for different workloads&lt;/strong&gt;&lt;/em&gt; Certain apps require certain characteristics. Even though you&amp;#8217;re running everything in docker containers, you still want to have c4&amp;#8217;s for compute-intensive loads, r3&amp;#8217;s for memory-intensive loads, etc. Look at your applications and map them to a system topology. You can also scale these groups of instances differently, but you want to see your entire system as a whole: not as independent, discrete parts.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;At some point, you&amp;#8217;ll need to update the configuration of your instances. You want to do this surgically, without accidentally destroying your cluster&lt;/strong&gt;&lt;/em&gt;. You may be one bad cfn update from relaunching an auto scaling group or misconfiguring an instance which causes a replacement. Just like normal instances you want to apply updates and reconfiguration of nodes in a sane, logical way. If you only had one cfn template for your entire cluster, it&amp;#8217;s all or nothing. That&amp;#8217;s not a choice we want to make.&lt;/p&gt;

&lt;p&gt;CoreOS won&amp;#8217;t let you forget about the underlying nodes; it just adds a little abstraction so you don&amp;#8217;t need to deal with specific nodes as much.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m assuming you&amp;#8217;re familiar with CloudFormation and the basics of a template. For our setup we&amp;#8217;ll start with the &lt;a href=&#34;https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template&#34;&gt;us-east-1 hvm CoreOS template&lt;/a&gt; and modify it along the way. This template create a straight-up CoreOS cluster launched in an Auto Scaling Group, uses a LaunchConfig&amp;#8217;s UserData to set some Cloud-Config settings. Like most templates you need a few parameters to launch. The non-default ones are your keypair and the etcd Discovery Url for forming the cluster. We are going to launch this stack with the CLI (who needs user interfaces?)&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s create a bash script, &lt;code&gt;coreos-cfn.sh&lt;/code&gt;, to call our create stack (don&amp;#8217;t forget to chmod +x). We need a DiscoveryUrl so we&amp;#8217;ll get a new one in our script and pass it as a parameter to CFN.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash 

DISCOVERY_URL=`curl -s -w &#34;\n&#34; https://discovery.etcd.io/new`
#Check to make sure the above command worked, or exit
[[ $? -ne 0 ]] &amp;#038;&amp;#038; echo &#34;Could not generate discovery url.&#34; &amp;#038;&amp;#038; exit 1

if [ -z &#34;$COREOS_KEYPAIR&#34; ]; then
  KEYPAIR=yourkey.pem
fi

# Create the CloudFormation stack
aws cloudformation create-stack \
    --stack-name coreos-test \
    --template-body file://coreos-stable-hvm.template \
    --capabilities CAPABILITY_IAM \
    --tags Key=Name,Value=CoreOS \
    --parameters \
        ParameterKey=DiscoveryURL,ParameterValue=${DISCOVERY_URL} \
        ParameterKey=KeyPair,ParameterValue=${KEYPAIR}
&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-z $KEYPAIR&lt;/code&gt; tests to see if there&amp;#8217;s a keypair set as an environment variable; if not, it uses the specified one. If you run &lt;code&gt;coreos-cfn.sh&lt;/code&gt; you should see the CLI spit out the ARN for the stack. Before we do that, let&amp;#8217;s make two minor tweaks.&lt;/p&gt;

&lt;p&gt;There are two key pieces of information we want to remember from this cluster: The DiscoveryUrl, so can access cluster state, and the AutoScalingGroup, so we can easily inspect instances in the future. Because the DiscoveryUrl is a parameter the aws cli will remember it for you. We need to add the auto scaling group as an output:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;&#34;Outputs&#34;: {
    &#34;AutoScalingGroup&#34; : {
      &#34;Value&#34;: { &#34;Ref&#34;: &#34;CoreOSServerAutoScale&#34; }
    }
  }
&lt;/pre&gt;

&lt;p&gt;After launching the cluster we can use the CLI and some jq to get back these parameters. It&amp;#8217;s a simple built-in storage mechanism of AWS, and all you need is the original stack name:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Get back the DiscoveryURL: Describe the stack, select the parameter list
DISCOVERY_URL=`aws cloudformation describe-stacks --stack-name coreos-test | \
  jq -r &#39;[.Stacks[].Parameters[]][] | select (.ParameterKey == &#34;DiscoveryURL&#34;) | .ParameterValue&#39;`

# Get back the auto-scaling-group-id
LEADER_ASG=`aws cloudformation describe-stacks --stack-name coreos-test | \
  jq -r &#39;[.Stacks[].Outputs[]][] | select (.OutputKey == &#34;AutoScalingGroup&#34;) | .OutputValue&#39;`

echo &#34;Discovery Url is $DISCOVERY_URL and Leader ASG is $LEADER_ASG&#34;
&lt;/pre&gt;

&lt;p&gt;Why is this important? Because now we can either inspect the state of the cluster via the disovery url service, or query the ASG to inspect running nodes directly:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Query AWS for Leader Nodes
$aws ec2 describe-instances --filters Name=tag-value,Values=$LEADER_ASG | \
  jq &#39;.Reservations[].Instances[].NetworkInterfaces[].PrivateIpAddress&#39;

# Inspect the Discovery Url for nodes, trimming port. 
$ `curl -s $DISCOVERY_URL | jq &#39;.node.nodes[].value[0:-5]&#39;

# Taking the latter one step further, we can build an Etcd Peers string using Jq, xargs and tr
$ ETCD_PEERS=`curl -s $DISCOVERY_URL | jq &#39;.node.nodes[].value[0:-5]&#39; | xargs -I{}  echo &#34;{}:4001&#34; | tr &#34;\\n&#34; &#34;,&#34;`
# Drop the last ,
$ ETCD_PEERS=${ETCD_PEERS%?}
&lt;/pre&gt;

&lt;p&gt;Armed with this information we are now able to spin up new CoreOS nodes and have it use our CoreOS leader cluster for management. The &lt;a href=&#34;https://coreos.com/docs/cluster-management/setup/cluster-architectures/&#34;&gt;CoreOS Cluster Architecture page&lt;/a&gt; has the specific &lt;code&gt;cloud-config&lt;/code&gt; settings which amount to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Disable etcd, we don&amp;#8217;t need it&lt;/li&gt;
&lt;li&gt;Set etcd peer settings to a comma delimited list of nodes for Fleet, Locksmith&lt;/li&gt;
&lt;li&gt;Set environment variables for fleet and etcd in start scripts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;#8217;ll make the etcd peer list a parameter for our template. We can duplicate our leader template, replace the &lt;code&gt;UserData&lt;/code&gt; portion of the &lt;code&gt;LaunchConfig&lt;/code&gt; with the updated settings from the link above, and add &lt;code&gt;{ Ref: }&lt;/code&gt; parameters where appropriate. Let&amp;#8217;s also add a metadata parameter as well:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;&#34;Parameters&#34;: {
    &#34;EtcdPeers&#34; : {
      &#34;Description&#34; : &#34;A comma delimited list of etcd endpoints to use for state management.&#34;,
      &#34;Type&#34; : &#34;String&#34;
    },
    &#34;FleetMetadata&#34; : {
      &#34;Description&#34; : &#34;A comma delimited list of key=value attributes to apply for fleet&#34;,
      &#34;Type&#34; : &#34;String&#34;
    }
  }
&lt;/pre&gt;

&lt;p&gt;We can use the &lt;code&gt;Ref&lt;/code&gt; functionality to pass these to our &lt;code&gt;UserData&lt;/code&gt; script of the &lt;code&gt;LaunchConfig&lt;/code&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;//other config above
  &#34;UserData&#34; : { &#34;Fn::Base64&#34;:
          { &#34;Fn::Join&#34;: [ &#34;&#34;, [
            &#34;#cloud-config\n\n&#34;,
            &#34;coreos:\n&#34;,
            &#34;  fleet:\n&#34;,
            &#34;    metadata: &#34;, { &#34;Ref&#34;: &#34;FleetMetadata&#34; }, &#34;\n&#34;,
            &#34;    etcd_servers: $&#34;, { &#34;Ref&#34;: &#34;EtcdPeers&#34; }, &#34;\n&#34;,
            &#34;  locksmith:\n&#34;,
            &#34;    endpoint: &#34;, { &#34;Ref&#34;: &#34;EtcdPeers&#34; }, &#34;\n&#34;
            ] ]
          }

// Other config below
&lt;/pre&gt;

&lt;p&gt;Finally we need a bash script which lets us inspect the existing stack information to pass as parameters to this new template. I also appreciate a CLI tool with a sane set of explicit flags. When I launch a secondary set of CoreOS nodes, I&amp;#8217;d like something simple to set the name, type, metadata and where I want to join to:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ launch-worker-group.sh -n r3-workers -t r3.large -j coreos-test -m &#34;instancetype=r3,role=worker&#34;
&lt;/pre&gt;

&lt;p&gt;Bash has a flag-parsing abilities in its &lt;code&gt;getopts&lt;/code&gt; function which we&amp;#8217;ll simply use to set variables:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

while getopts n:j:m:s: FLAG; do
  case $FLAG in
    n)  STACK_NAME=${OPTARG};;
    j)  JOIN=${OPTARG};;
    m)  METADATA=${OPTARG};;
    t)  INSTANCE_TYPE =${OPTARG};;
    [?])
      print &gt;&amp;#038;2 &#34;Usage: $0 [ -n stack-name ] [ -j join to leader] [ -m fleet-metadata ] [ -t instance-type ]&#34;
      exit 1;;
  esac
done

shift $((OPTIND-1))

# You can set defaults, too:
if [ -z $INSTANCE_TYPE ]; then 
  INSTANCE_TYPE =&#34;m3.medium&#34;
fi
&lt;/pre&gt;

&lt;p&gt;With this in place it&amp;#8217;s just a matter of calling the AWS CLI with our new template and updated parameters. The only thing we&amp;#8217;re doing differently than the original script is using CloudFormation&amp;#8217;s json parameter functionality. This allows for more structured data in variables. Otherwise the comma-delimited list for etcd peers will throw off the CLI call.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;DISCOVERY_URL=`aws cloudformation describe-stacks --stack-name $JOIN | \
  jq -r &#39;[.Stacks[].Parameters[]][] | select (.ParameterKey == &#34;DiscoveryURL&#34;) | .ParameterValue&#39;`
# Taking the latter one step further, we can build an Etcd 
# Peers string using jq, xargs and tr to flatten
ETCD_PEERS=`curl -s $DISCOVERY_URL | jq &#39;.node.nodes[].value[0:-5]&#39; | \
  xargs -I{}  echo &#34;{}:4001&#34; | tr &#34;\\n&#34; &#34;,&#34;`

# Drop the last ,
ETCD_PEERS=${ETCD_PEERS%?}

 # Create the CloudFormation stack
 aws cloudformation create-stack \
    --stack-name STACK_NAME \
    --template-body file://coreos-worker-hvm.template \
    --capabilities CAPABILITY_IAM \
    --tags Key=Name,Value=CoreOS Key=Role,Value=Worker \
    --parameters &#34;[
      { \&#34;ParameterKey\&#34;:\&#34;FleetMetadata\&#34;,\&#34;ParameterValue\&#34;:\&#34;${METADATA}\&#34; },
      { \&#34;ParameterKey\&#34;:\&#34;InstanceType\&#34;,\&#34;ParameterValue\&#34;:\&#34;${INSTANCE_TYPE}\&#34; },
      { \&#34;ParameterKey\&#34;:\&#34;EtcdPeers\&#34;,\&#34;ParameterValue\&#34;:\&#34;${ETCD_PEERS%?}\&#34; },
      { \&#34;ParameterKey\&#34;:\&#34;KeyPair\&#34;,\&#34;ParameterValue\&#34;:\&#34;${KEYPAIR}\&#34; }
    ]&#34;
&lt;/pre&gt;

&lt;p&gt;And launch it! This will create a new stack for your worker nodes with whatever metadata you want, with whatever instance type you want.&lt;/p&gt;

&lt;p&gt;There are a few ways to extend this. For one, we haven&amp;#8217;t dealt with updating or destroying the stack. You can create separate shell scripts or combine them together with flags for determining which action to take. I prefer the latter as it keeps all related scripts in one file, but you can break out accordingly. You can use the AWS CLI and the Stack Name to query for private ip&amp;#8217;s and update Route 53 accordingly, bypassing the need for an ELB.&lt;/p&gt;

&lt;p&gt;You can do a lot with bash and other CLI tools like jq. You don&amp;#8217;t need to scour GitHub for open source tools, or frameworks that have bells and whistles. The core components are there, you just need to glue them together. Yes, your scripts may get out of hand, but at that point it&amp;#8217;s worth looking for alternatives because there&amp;#8217;s probably a specific problem you need to solve. Remember, be opinionated and let those choices guide you. At some point in the future I may be raving about Terraform; friends say it&amp;#8217;s a great tool, but it&amp;#8217;s just not one that I need-or particularly want-to use now.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Slimming down Dockerfiles: Decrease the size of Gitlab’s CI runner from 900 to 420 mb</title>
          <link>http://blog.michaelhamrah.com/2015/03/slimming-down-dockerfiles-decrease-the-size-of-gitlabs-ci-runner-from-900-to-420-mb/</link>
          <pubDate>Sun, 22 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/slimming-down-dockerfiles-decrease-the-size-of-gitlabs-ci-runner-from-900-to-420-mb/</guid>
          <description>

&lt;p&gt;I&amp;#8217;ve been leveraging Gitlab CI for our continuous integration needs, running both the CI site and CI runners on our CoreOS cluster in docker containers. It&amp;#8217;s working well. On the runner side, after cloning the ci-runner repositroy and running a &lt;code&gt;docker build -t base-runner .&lt;/code&gt; , I was a little disappointed with the size of the runner. It weighed in at 900MB, a fairly hefty size for something that should be a lightweight process. I&amp;#8217;ve built the &lt;a href=&#34;https://github.com/gitlabhq/gitlab-ci-runner/blob/master/Dockerfile&#34;&gt;ci-runner dockerfile&lt;/a&gt; with the name &amp;#8220;base-runner&amp;#8221;:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;base-runner      latest      aaf8a1c6a6b8    2 weeks ago    901.1 MB
&lt;/pre&gt;

&lt;p&gt;The dockerfile is well documented and organized, but I immediately noticed some things which cause dockerfile bloat. There are some great resources on slimming down docker files, including &lt;a href=&#34;http://www.centurylinklabs.com/optimizing-docker-images/&#34;&gt;optimizing docker images&lt;/a&gt; and the &lt;a href=&#34;https://github.com/gliderlabs/docker-alpine&#34;&gt;docker-alpine&lt;/a&gt; project. The advice comes down to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use the smallest possible base layer (usually Ubuntu is not needed)&lt;/li&gt;
&lt;li&gt;Eliminate, or at least reduce, layers&lt;/li&gt;
&lt;li&gt;Avoid extraneous cruft, usually due to excessive packages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#8217;s make some minor changes to see if we can slim down this image. At the top of the dockerfile, we see the usual apt-get commands:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;# Update your packages and install the ones that are needed to compile Ruby
RUN apt-get update -y
RUN apt-get upgrade -y
RUN apt-get install -y curl libxml2-dev libxslt-dev libcurl4-openssl-dev libreadline6-dev libssl-dev patch build-essential zlib1g-dev openssh-server libyaml-dev libicu-dev

# Download Ruby and compile it
RUN mkdir /tmp/ruby
RUN cd /tmp/ruby &amp;#038;&amp;#038; curl --silent ftp://ftp.ruby-lang.org/pub/ruby/2.0/ruby-2.0.0-p481.tar.gz | tar xz
RUN cd /tmp/ruby/ruby-2.0.0-p481 &amp;#038;&amp;#038; ./configure --disable-install-rdoc &amp;#038;&amp;#038; make install
&lt;/pre&gt;

&lt;p&gt;Each &lt;code&gt;RUN&lt;/code&gt; command creates a separate layer, and nothing is cleaned up. These artifacts will stay with the container unnecessarily. Running another &lt;code&gt;RUN rm -rf /tmp&lt;/code&gt; won&amp;#8217;t help, because the history is still there. We need things gone and without a trace. We can &amp;#8220;flatten&amp;#8221; these commands and add some cleanup commands while preserving readability:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Update your packages and install the ones that are needed to compile Ruby
# Download Ruby and compile it
RUN apt-get update -y &amp;#038;&amp;#038; 
    apt-get upgrade -y &amp;#038;&amp;#038; 
    apt-get install -y curl libxml2-dev libxslt-dev libcurl4-openssl-dev libreadline6-dev libssl-dev patch build-essential zlib1g-dev openssh-server libyaml-dev libicu-dev &amp;#038;&amp;#038; 
    mkdir /tmp/ruby &amp;#038;&amp;#038; 
    cd /tmp/ruby &amp;#038;&amp;#038; curl --silent ftp://ftp.ruby-lang.org/pub/ruby/2.0/ruby-2.0.0-p481.tar.gz | tar xz &amp;#038;&amp;#038; 
    cd /tmp/ruby/ruby-2.0.0-p481 &amp;#038;&amp;#038; ./configure --disable-install-rdoc &amp;#038;&amp;#038; make install &amp;#038;&amp;#038; 
    apt-get clean &amp;#038;&amp;#038; 
    rm -rf /var/lib/apt/lists/* /tmp/*
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s only one run command, and the last two lines cleanup the apt-get downloads and &lt;code&gt;tmp&lt;/code&gt; space. Let&amp;#8217;s see how we well we do:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;$ docker images | grep base-runner
base-runner      latest      2a454f84e4e8      About a minute ago   566.9 MB
&lt;/pre&gt;

&lt;p&gt;Not bad; with one simple modification we went from 902mb to 566mb. This change comes at the cost of build speed. Because there&amp;#8217;s no previously cached layer, we always start from the beginning. When creating docker files, I usually start with multiple run commands so history is preserved while I&amp;#8217;m working on the file, but then concatenate everything at the end to minimize cruft.&lt;/p&gt;

&lt;p&gt;566mb is a good start, but can we do better? The goal of this build is to install the ci-runner. This requires Ruby and some dependencies, all documented on the ci-runner&amp;#8217;s readme. As long as we&amp;#8217;re meeting those requirements, we&amp;#8217;re good to go. Let&amp;#8217;s switch to debian:wheezy. We&amp;#8217;ll also need to tweak the locale setting for debian. Our updated dockerfile starts with this:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;# gitlab-ci-runner

FROM debian:wheezy
MAINTAINER Michael Hamrah &amp;lt;m@hamrah.com&gt;

# Get rid of the debconf messages
ENV DEBIAN_FRONTEND noninteractive

# Update your packages and install the ones that are needed to compile Ruby
# Download Ruby and compile it
RUN apt-get update -y &amp;#038;&amp;#038; 
    apt-get upgrade -y &amp;#038;&amp;#038; 
    apt-get install -y locales curl libxml2-dev libxslt-dev libcurl4-openssl-dev libreadline6-dev libssl-dev patch build-essential zlib1g-dev openssh-server libyaml-dev libicu-dev &amp;#038;&amp;#038; 
    mkdir /tmp/ruby &amp;#038;&amp;#038; 
    cd /tmp/ruby &amp;#038;&amp;#038; curl --silent ftp://ftp.ruby-lang.org/pub/ruby/2.0/ruby-2.0.0-p481.tar.gz | tar xz &amp;#038;&amp;#038; 
    cd /tmp/ruby/ruby-2.0.0-p481 &amp;#038;&amp;#038; ./configure --disable-install-rdoc &amp;#038;&amp;#038; make install &amp;#038;&amp;#038; 
    apt-get clean &amp;#038;&amp;#038; 
    rm -rf /var/lib/apt/lists/* /tmp/*
&lt;/pre&gt;

&lt;p&gt;Let&amp;#8217;s check this switch:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;base-runner      latest      40a1465ebaed      3 minutes ago      490.3 MB
&lt;/pre&gt;

&lt;p&gt;Better. A slight modification can slim this down some more; the dockerfile builds ruby from source. Not only does this take longer, it&amp;#8217;s not needed: we can just include the &lt;code&gt;ruby&lt;/code&gt; and &lt;code&gt;ruby-dev&lt;/code&gt; packages; on debian:wheezy these are good enough for running the ci-runner. By removing the install-from-source commands we can get the image down to:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;base-runner      latest      bb4e6306811d      About a minute ago   423.6 MB
&lt;/pre&gt;

&lt;p&gt;This now more than 50% less then the original, with a minimal amount of tweaking.&lt;/p&gt;

&lt;h1 id=&#34;pushing-even-further:ef76312dca5f3c1992b296f85b019da1&#34;&gt;Pushing Even Further&lt;/h1&gt;

&lt;p&gt;Normally I&amp;#8217;m not looking for an absolute minimal container. I just want to avoid excessive bloat, and some simple commands can usually go a long way. I also find it best to avoid packages in favor of pre-built binaries. As an example I do a lot of work with Scala, and have an sbt container for builds. If I were to install the SBT package from debian I&amp;#8217;d get a container weighing in at a few hundred megabytes. That&amp;#8217;s because the SBT package pulls in a lot of dependencies: java, for one. But if I already have a jre, all I really need is the sbt jar file and a bash script to launch. That considerably shrinks down the dockerfile size.&lt;/p&gt;

&lt;p&gt;When selecting a base image, it&amp;#8217;s important to realize what you&amp;#8217;re getting. A linux distribution is simply the linux kernel and an opinionated configuration of packages, tools and binaries. Ubuntu uses aptitude for package management, Fedora uses Yum. Centos 6 uses a specific version of the kernel, while version 7 uses another. You get one set of packages with Debian, another with Ubuntu. That&amp;#8217;s the power of specific communities: how frequently things are updated, how well they&amp;#8217;re maintained, and what you get out-of-box. A docker container jails a process to only see a specific part of the filesystem; specifically, the container&amp;#8217;s file system. Using a distribution ensures that the required libraries and support binaries are there, in place, where they should be. But major distributions aren&amp;#8217;t designed to run specific applications; their general-purposes servers that are designed to run a variety of apps and processes. If you want to run a single process there&amp;#8217;s a lot that comes with an OS you don&amp;#8217;t need.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s a re-emergence of lightweight linux distributions in the docker world first popularized with embedded systems. You&amp;#8217;re probably familiar with &lt;a href=&#34;https://registry.hub.docker.com/_/busybox/&#34;&gt;busybox&lt;/a&gt; useful for running one-off bash commands. Because of busybox&amp;#8217;s embedded roots, it&amp;#8217;s not quite intended for packages. &lt;a href=&#34;https://www.alpinelinux.org&#34;&gt;Alpine Linux&lt;/a&gt; is another alternative which features its own &lt;a href=&#34;https://registry.hub.docker.com/_/alpine/&#34;&gt;official registry&lt;/a&gt;. It&amp;#8217;s still very small, based on busybox, and has its own package system. I tried getting gitlab&amp;#8217;s ci-runner working with alpine, but unfortunately some of the ruby gems used by ci-runner require GNU packages which aren&amp;#8217;t available, and I didn&amp;#8217;t want to compile them manually. In terms of time/benefit, I can live with 400mb and move on to something else. For most things you can probably do a lot with Alpine and keep your containers really small: great for doing continuous deploys to a bunch of servers.&lt;/p&gt;

&lt;p&gt;The bottom line is know what you need. If you want a minimal container, build up, rather than slim down. You usually need the runtime for your application (if any), your app, and supporting dependencies. Know those dependencies, and avoid cruft when you can.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Book Review: Go Programming Blueprints, and the beauty of a language.</title>
          <link>http://blog.michaelhamrah.com/2015/03/book-review-go-programming-blueprints-and-the-beauty-of-a-language/</link>
          <pubDate>Fri, 20 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/book-review-go-programming-blueprints-and-the-beauty-of-a-language/</guid>
          <description>&lt;p&gt;Just over two years ago my wife and I [traveled around Asia for several months)[thegreatbigadventure.tumblr.com]. I didn’t do any programming while I was gone &lt;a href=&#34;http://blog.michaelhamrah.com/2013/04/six-months-of-computer-science-without-computers/&#34;&gt;but I did a great deal of reading&lt;/a&gt;, gaining a new-found appreciation for programming and technology. I became deeply interested in Scala and Go for their respective approachs to statically typed languages. Scala for its functional programming aspects and Go for its refreshing and intentionally succinct approach to interfaces, types and its anti-inheritance. The criticism I most often here with Scala; that’s it too open, too free-for-fall in its paradigms is in stark contrast to the main criticisms I hear of Go: it’s too limiting, too constrained.&lt;/p&gt;

&lt;p&gt;Since returning a majority of my time is focused on Scala, yet I still keep a hand in the Go cookie jar. Both languages are incredibly productive, and I appreciate FP the more I use it and understand it. Scala’s criticism is legitimate; it can be a chaotic language. However, my personal opinion is the language shouldn’t constrain you: it’s the discipline of the programmer to write code well, not the language. A bad programmer is going to destroy any language; a good programmer can make any code beautiful. More importantly, no language is magical. A language is a tool, and it’s up to the programmer to use it effectively.&lt;/p&gt;

&lt;p&gt;Learning a language is more than just knowing how to write a class or function. Learning a language is about composing these together effectively and using the ecosystem around the language. Scala’s benefit is the ecosystem around the JVM; idiomatic Scala is contentious debate, as you have the functional programmers on one side and the more lenient anti-javaists on the other (Martin Odersky’s talk &lt;a href=&#34;https://www.youtube.com/watch?v=ecekSCX3B4Q&#34;&gt;Scala: The Simple Parts&lt;/a&gt; is a great overview of where Scala shines). Go, on the other hand, is truly effective when you embrace its opinions and leverage its ecosystem: understanding imports and go get, writing small, independent modules, reusing these modules, embracing interfaces, and understanding the power of goroutines.&lt;/p&gt;

&lt;p&gt;Last summer I had the great pleasure of being a technical reviewer for Mat Ryer’s &lt;a href=&#34;http://bit.ly/GoBb&#34;&gt;Go Programming Blueprints&lt;/a&gt;. I’ve read a great deal of programming books in my career and appreciated Mat’s approach to showcasing the power and simplicity of Go. It’s not for beginners programmers, but if you have some experience, not even with Go, you can kick-start a working knowledge easily with Mat’s book. My favorite aspect is it explains how to write idiomatic Go to build applications. One example application composes discrete services and links them with bitly’s NSQ library, another uses a routing library on top of Go’s httpRequest handler. The book isn’t just isolated to web programs, there’s a section on writing CLI apps which link together with standard in and standard out. For those criticizing Go’s terseness Mat’s book exemplifies what you can do with those terse systems: write scalable, composable apps that are also maintainable and readable. The books shows why so many exciting new tools are written in Go: you can do a lot with little, and they compile to statically linked, minimal binaries.&lt;/p&gt;

&lt;p&gt;As you develop your craft of writing code, you develop certain opinions on the way code should work. When your language is inline with your opinions, or you develop opinions based on the language, you are effectively using that language. If you are learning a new language, like Go, but still applying your existing opinions on how to develop applications (say, by wishing the language had Generics), you struggle. Worse, you are attempting to shape a new language to the one you know, effectively programming in the old language. You should embrace what the language offers, and honor its design decisions. Mat’s book shows how to apply Go’s design decisions effectively. The language itself will evolve and grow, but it will do it in a way that enhances and honors its design decisions. And if you still don’t like it, or Scala, well there’s always Rust.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Deploying Docker Containers on CoreOS with the Fleet API</title>
          <link>http://blog.michaelhamrah.com/2015/03/deploying-docker-containers-on-coreos-with-the-fleet-api/</link>
          <pubDate>Tue, 17 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/deploying-docker-containers-on-coreos-with-the-fleet-api/</guid>
          <description>

&lt;p&gt;I&amp;#8217;ve been spending a lot more time with CoreOS in search of a docker-filled utopian PaaS dreams. I haven&amp;#8217;t found quite what I&amp;#8217;m looking for, but with some bash scripting, a little http, good tools and a lotta love I&amp;#8217;m coming close. There are no shortage of solutions to this problem, and honestly, nobody&amp;#8217;s really figured this out yet in an easy, fluid, turn-key type of way. You&amp;#8217;ve probably read about CoreOS, Mesos, Marathon, Kubernetes&amp;#8230; maybe even dug into Deis, Flynn, Shipyard. You&amp;#8217;ve spun up a cluster, and are like&amp;#8230; This is great, now what.&lt;/p&gt;

&lt;p&gt;What I want is to go from an app on my laptop to running in a production environment with minimal fuss. I don&amp;#8217;t want to re-invent the wheel; there are too many people solving this problem in a similar way. I like CoreOS because it provides a bare-bones docker runtime with a solid set of low-level tools. Plus, a lot of people I&amp;#8217;m close with have been using it, so the cross-pollination of ideas helps overcome some hurdles.&lt;/p&gt;

&lt;p&gt;One of these hurdles is how you launch containers on a cluster. I really like &lt;a href=&#34;https://github.com/mesosphere/marathon&#34;&gt;Marathon&amp;#8217;s&lt;/a&gt; http api for Mesos, but I also like the simplicity of CoreOS as a platform. CoreOS&amp;#8217;s distributed init system is &lt;a href=&#34;https://github.com/coreos/fleet&#34;&gt;Fleet&lt;/a&gt;, which leverages systemd for running a process on a CoreOS node (it doesn&amp;#8217;t have to be a container). It has some nice features, but having to constantly write similar systemd files and run fleetctl to manage containers is somewhat annoying.&lt;/p&gt;

&lt;p&gt;Turns out, &lt;a href=&#34;https://github.com/coreos/fleet/blob/master/Documentation/api-v1.md&#34;&gt;Fleet has an http API&lt;/a&gt;. It&amp;#8217;s not quite as nice as Marathon&amp;#8217;s; you can&amp;#8217;t easily scale to N number of instances, but it does come close. There are a few examples of using the API to launch containers, but I wanted a more end-to-end solution that eliminated boilerplate.&lt;/p&gt;

&lt;h2 id=&#34;activate-the-fleet-api:f1075c253a77011bd480830af8403bf8&#34;&gt;Activate the Fleet API&lt;/h2&gt;

&lt;p&gt;The Fleet API isn&amp;#8217;t enabled out-of-the-box. That makes sense as the API is currently unsecured, so you shouldn&amp;#8217;t enable it unless you have the proper VPC set up. &lt;a href=&#34;https://coreos.com/docs/launching-containers/config/fleet-deployment-and-configuration/&#34;&gt;CoreOS has good documentation on getting the API running&lt;/a&gt;. For a quick start you can drop the following yaml snippet into your cloudconfig&amp;#8217;s units section:&lt;/p&gt;

&lt;pre class=&#34;syntax yaml&#34;&gt;- name: fleet.socket
  drop-ins:
    - name: 30-ListenStream.conf
      content: |
        [Socket]
        ListenStream=8080
        Service=fleet.service
        [Install]
        WantedBy=sockets.target
&lt;/pre&gt;

&lt;h2 id=&#34;exploring-the-api:f1075c253a77011bd480830af8403bf8&#34;&gt;Exploring the API&lt;/h2&gt;

&lt;p&gt;With the API enabled, it&amp;#8217;s time to get to work. The &lt;a href=&#34;https://github.com/coreos/fleet/blob/master/Documentation/api-v1.md&#34;&gt;API has some simple documentation&lt;/a&gt; but offers enough to get started. I personally like the minimal approach, although I wish it was more feature-rich (it is v1, and better than nothing).&lt;/p&gt;

&lt;p&gt;You can do a lot with curl, bash and jq. First, let&amp;#8217;s see what&amp;#8217;s running. All these examples assume you have a FLEET_ENDPOINT environment variable set with the host and port:&lt;/p&gt;

&lt;p&gt;On a side note, environment variables are key to reuse the same functionality across environments. In my opinion, they aren&amp;#8217;t used nearly enough. Check out the &lt;a href=&#34;http://12factor.net/config&#34;&gt;twelve-factor app&amp;#8217;s config section&lt;/a&gt; to understand the importance of environment variables.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -s $FLEET_ENDPOINT/fleet/v1/units | jq &#39;.units[] | { name: .name, currentState: .currentState}&#39;
&lt;/pre&gt;

&lt;p&gt;Sure, you can get the same data by running &lt;code&gt;fleetctl list-units&lt;/code&gt;, but the http command doesn&amp;#8217;t involve ssh, which can be a plus if you have a protected network, are are running from an application or CI server.&lt;/p&gt;

&lt;h2 id=&#34;creating-containers:f1075c253a77011bd480830af8403bf8&#34;&gt;Creating Containers&lt;/h2&gt;

&lt;p&gt;Instead of crafting a fleet template and running &lt;code&gt;fleetctl start sometemplate&lt;/code&gt; , we want to launch new units via http. This involves PUTting a resource to the /units/ endpoint under the name of your unit (it&amp;#8217;s actually /fleet/v1/units, it took me forever to find the path prefix). The Fleet API will build a corresponding systemd unit from the json payload, and the content closely corresponds to what you can do with &lt;a href=&#34;https://coreos.com/docs/launching-containers/launching/fleet-unit-files/&#34;&gt;a fleet unit file&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The schema takes in a &lt;code&gt;desiredState&lt;/code&gt; and an array of &lt;code&gt;options&lt;/code&gt; which specify the &lt;code&gt;section&lt;/code&gt;, &lt;code&gt;name&lt;/code&gt;, and &lt;code&gt;value&lt;/code&gt; for each line. Most Fleet templates follow a similar pattern as exemplified with &lt;a href=&#34;https://coreos.com/docs/launching-containers/launching/launching-containers-fleet/&#34;&gt;the launching containers with Fleet guide&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Cleanup potentially running containers&lt;/li&gt;
&lt;li&gt;Pull the container&lt;/li&gt;
&lt;li&gt;Run the container&lt;/li&gt;
&lt;li&gt;Define X-Fleet parameters, like conflicts.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Again we&amp;#8217;ll use curl, but writing json on the command line is really annoying. So let&amp;#8217;s create a &lt;code&gt;unit.json&lt;/code&gt; for our payload defining the tasks for CoreOS&amp;#8217;s apache container:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;{
  &#34;desiredState&#34;: &#34;launched&#34;,
  &#34;options&#34;: [
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/usr/bin/docker kill %p-i%&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/usr/bin/docker rm %p-%i&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker pull coreos/%p&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStart&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker run --rm --name %pi-%i -p 80 coreos/%p /usr/sbin/apache2ctl -D FOREGROUND&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStop&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker stop %p-%i&#34;
    },
    {
      &#34;section&#34;: &#34;X-Fleet&#34;,
      &#34;name&#34;: &#34;Conflicts&#34;,
      &#34;value&#34;: &#34;%p@*.service&#34;
    }
  ]
}
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a couple of things of note in this snippet:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We&amp;#8217;re adding a &amp;#8220;-&amp;#8221; in front of the docker kill and docker rm commands of the ExecStartPre tasks. This tells to Fleet to continue if there&amp;#8217;s an error; these tasks are precautionary to remove an existing phantom container if it will conflict with the newly launched one.&lt;/li&gt;
&lt;li&gt;We&amp;#8217;re using Fleet&amp;#8217;s systemd placeholders %p and %i to replace actual values in our template with values from the template name. This provides a level of agnosticism in our template; we can easily reuse this template to launch different containers by changing the name. Unfortunately this doesn&amp;#8217;t quite work in our example because it&amp;#8217;s apache specific, but if you were running a container with an entry point command specified, it would work fine. You&amp;#8217;ll also want to manage containers under your own namespace, either in a private or public registry.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can launch this file with curl:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -d @unit.json -w &#34;%{http_code}&#34; -H &#39;Content-Type: application/json&#39; $FLEETCTL_ENDPOINT/fleet/v1/units/apache@1.service
&lt;/pre&gt;

&lt;p&gt;If all goes well you&amp;#8217;ll get back a &lt;code&gt;201 Created&lt;/code&gt; response. Try running the &lt;code&gt;list units&lt;/code&gt; curl command to see your container task.&lt;/p&gt;

&lt;p&gt;We can run &lt;code&gt;fleetctl cat apache@1&lt;/code&gt; to view the generated systemd unit:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;[Service]
ExecStartPre=-/usr/bin/docker kill %p-%I
ExecStartPre=-/usr/bin/docker rm %p-%i
ExecStartPre=/usr/bin/docker pull coreos/%p
ExecStart=/usr/bin/docker run --rm --name %pi-%i -p 80 coreos/%p /usr/sbin/apache2ctl -D FOREGROUND
ExecStop=/usr/bin/docker stop %p-%i

[X-Fleet]
Conflicts=%p@*.service
&lt;/pre&gt;

&lt;p&gt;Want to launch a second task? Just post again, but change the instance number from 1 to 2:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -d @unit.json -w &#34;%{http_code}&#34; -H &#39;Content-Type: application/json&#39; $FLEETCTL_ENDPOINT/fleet/v1/units/apache@2.service
&lt;/pre&gt;

&lt;p&gt;When you&amp;#8217;re done with your container, you can simple issue a delete command to tear it down:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X DELETE -w &#34;%{http_code}&#34; $FLEET_ENDPOINT/fleet/v1/units/apache@1.service
&lt;/pre&gt;

&lt;h2 id=&#34;deploying-new-versions:f1075c253a77011bd480830af8403bf8&#34;&gt;Deploying New Versions&lt;/h2&gt;

&lt;p&gt;Launching individual containers is great, but for continuous delivery, you need deploy new versions with no downtime. The example above used systemd&amp;#8217;s placeholders for providing the name of the container, but left the apache commands in place. Let&amp;#8217;s use another CoreOS example container from the &lt;a href=&#34;https://coreos.com/blog/zero-downtime-frontend-deploys-vulcand/&#34;&gt;zero downtime frontend deploys&lt;/a&gt; blog post. This &lt;code&gt;coreos/example&lt;/code&gt; container uses an entrypoint and tagged docker versions to go from a v1 to a v2 version of the app. Instead of creating multiple, similar, fleet unit files like that blog post, can we make an agnostic http call that works across versions? Yes we can.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s conceptually figure out how this would work. We don&amp;#8217;t want to change the json payload across versions, so the body must be static. We could use some form of templating or find-and-replace, but let&amp;#8217;s try and avoid that complexity for now. Can we make due with the options provided us? We know that the %p parameter lets us pass in the template name to our body. So if we can specify the name and version of the container we want to launch in the name of the unit file we PUT, we&amp;#8217;re good to go.&lt;/p&gt;

&lt;p&gt;So we want to:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -d @unit.json -w &#34;%{http_code&#34;} -H &#39;Content-Type: application/json&#39; $FLEETCTL_ENDPOINT/fleet/v1/units/example:1.0.0@1.service
&lt;/pre&gt;

&lt;p&gt;I tried this with the above snippet, but replaced the pull and run commands above with the following:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;{
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStart&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker run --rm --name %p-%i -p 80 coreos/%p&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStop&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker stop %p-%i&#34;
    },
&lt;/pre&gt;

&lt;p&gt;Unfortunately, this didn&amp;#8217;t work because the colon, :, in example:1.0.0 make the name invalid for a container. I could forego the name, but then I wouldn&amp;#8217;t be able to easily stop, kill or rm the container. So we need to massage the %p parameter a little bit. Luckily, bash to the rescue.&lt;/p&gt;

&lt;p&gt;Unfortunately, systemd is a little wonky when it comes to scripting in a unit file. It&amp;#8217;s relatively hard to create and access environment variables, you need fully-qualified paths, and multiple lines for arbitrary scripts are discouraged. After googling how exactly to do bash scripting in a systemd file, or why an environment variable wasn&amp;#8217;t being set, I began to understand the frustration in the community on popular distros switching to systemd. But we can still make do with what we have by launching a &lt;code&gt;/bin/bash&lt;/code&gt; command instead of the vanilla &lt;code&gt;/usr/bin/docker&lt;/code&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;{
  &#34;desiredState&#34;: &#34;launched&#34;,
  &#34;options&#34;: [
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker kill $APP-%i\&#34;&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker rm $APP-%i\&#34;&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker pull coreos/%p&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStart&#34;,
      &#34;value&#34;: &#34;/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker run --name $APP-%i -h $APP-%i -p 80 --rm coreos/%p\&#34;&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStop&#34;,
      &#34;value&#34;: &#34;/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker stop $APP-%i&#34;
    },
    {
      &#34;section&#34;: &#34;X-Fleet&#34;,
      &#34;name&#34;: &#34;Conflicts&#34;,
      &#34;value&#34;: &#34;%p@*.service&#34;
    }
  ]
}
&lt;/pre&gt;

&lt;p&gt;and we can submit with:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X PUT -d @unit.json -H &#39;Content-Type: application/json&#39;  $FLEET_ENDPOINT/fleet/v1/units/example:1.0.0@1.service
&lt;/pre&gt;

&lt;p&gt;More importantly, we can easily launch multiple containers of version two simultaneously:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X PUT -d @unit.json -H &#39;Content-Type: application/json&#39;  $FLEET_ENDPOINT/fleet/v1/units/example:2.0.0@1.service
curl -X PUT -d @unit.json -H &#39;Content-Type: application/json&#39;  $FLEET_ENDPOINT/fleet/v1/units/example:2.0.0@2.service
&lt;/pre&gt;

&lt;p&gt;and then destroy version one:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X DELETE -w &#34;%{http_code}&#34; $FLEET_ENDPOINT/fleet/v1/units/example:1.0.0@1.service
&lt;/pre&gt;

&lt;h2 id=&#34;more-jq-and-bash-fun:f1075c253a77011bd480830af8403bf8&#34;&gt;More jq and bash fun&lt;/h2&gt;

&lt;p&gt;Let&amp;#8217;s say you do start multiple containers, and you want to cycle them out and delete them. In our above example, we&amp;#8217;ve started two containers. How will we easily go from v2 to v3, and remove the v3 nodes? The marathon API has a simple &amp;#8220;scale&amp;#8221; button which does just that. Can we do the same for CoreOS? Yes we can.&lt;/p&gt;

&lt;p&gt;Conceptually, let&amp;#8217;s think about what we want. We want to select all containers running a specific version, grab the full unit file name, and then curl a DELETE operation to that endpoint. We can use the Fleet API to get our information, jq to parse the response, and the bash pipe operator with xargs to call our curl command.&lt;/p&gt;

&lt;p&gt;Stringing this together like so:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -s $FLEET_ENDPOINT/fleet/v1/units | jq &#39;.units[] | .name | select(startswith(&#34;example:1.0.0&#34;))&#39; | xargs -t -I{} curl -s -X DELETE $FLEET_ENDPOINT/fleet/v1/units/{}
&lt;/pre&gt;

&lt;p&gt;jq provides some very powerful json processing. We are pulling out the name field, and only selecting elements which start with our specific app and version, and then piping that to xargs. The -I{} flag for xargs is a substitution trick I learned. This allows you to do string placements rather than pass the field as an argument.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:f1075c253a77011bd480830af8403bf8&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I can pretty much guarantee no matter what you pick to run your Docker PaaS, it won&amp;#8217;t do exactly what you want. I can also guarantee that there will be a lot to learn: new apis, new commands, new tools. It&amp;#8217;s going to feel like pushing a round peg in a square hole. But that&amp;#8217;s okay; part of the experience is formulating opinions on how you want things to work. It&amp;#8217;s a blend of learning the patterns and practices of a tool versus configuring it to work the way you want. Always remember a few things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Keep It Simple&lt;/li&gt;
&lt;li&gt;Think about how it should work conceptually&lt;/li&gt;
&lt;li&gt;You can do a lot with command line.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With an API-enabled CoreOS cluster, you can easily plug deployment of containers to whatever build flow you use: your laptop, a github web hook, jenkins, or whatever flow you wish. Because all the above commands are bash, you can replace any part with a bash variable and execute appropriately. This makes parameterizing these commands into functions easy.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Adding Http Server-Side Events to Akka-Streams</title>
          <link>http://blog.michaelhamrah.com/2015/01/adding-http-server-side-events-to-akka-streams/</link>
          <pubDate>Sun, 18 Jan 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/01/adding-http-server-side-events-to-akka-streams/</guid>
          <description>

&lt;p&gt;In my last blog post we pushed messages from RabbitMq to the console using Akka-Streams. We used the &lt;a href=&#34;https://github.com/ScalaConsultants/reactive-rabbit&#34;&gt;reactive-rabbit&lt;/a&gt; library to create an Akka-Streams &lt;code&gt;Source&lt;/code&gt; for our &lt;em&gt;streams-playground&lt;/em&gt; queue and mapped the stream to a &lt;code&gt;println&lt;/code&gt; statement before dropping it into an empty &lt;code&gt;Sink&lt;/code&gt;. All Akka-Streams need both a &lt;code&gt;Source&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; to be runnable; we created a complete stream blueprint to be run later.&lt;/p&gt;

&lt;p&gt;Printing to the console is somewhat boring, so let&amp;#8217;s take it up a notch. The excellent &lt;a href=&#34;spray.io&#34;&gt;Spray Web Service&lt;/a&gt; library is being merged into Akka as &lt;a href=&#34;http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M2/scala/http/index.html&#34;&gt;Akka-Http&lt;/a&gt;. It&amp;#8217;s essentially Spray built with Akka-Streams in mind. The routing dsl, immutable request/response model, and high-performance http server are all there; think of it as Spray vNext. Check out Mathias Doenitz&amp;#8217;s &lt;a href=&#34;http://spray.io/scaladays2014/#/&#34;&gt;excellent slide deck on kaka-http from Scala days&lt;/a&gt; to learn more on this evolution of Spray; it also highlights the back-pressure functionality Akka-Streams will give you for Http.&lt;/p&gt;

&lt;p&gt;Everyone&amp;#8217;s familiar with the Request/Response model of Http, but to show the power of Akka-Streams we&amp;#8217;ll add Heiko Seeberger&amp;#8217;s &lt;a href=&#34;https://github.com/hseeberger/akka-sse&#34;&gt;Akka-SSE&lt;/a&gt; library which brings Server-Side Events to Akka-Http. &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/eventsource/basics/&#34;&gt;Server-Side Events&lt;/a&gt; are a more efficient form of long-polling that&amp;#8217;s a lighter protocol to the bi-directional WebSocket API. It allows the client to easily register a handler which the server can then push events to. Akka-SSE adds an SSE-enabled completion marshaller to Akka-Http so your response can be SSE-aware. Instead of printing messages to the console, we&amp;#8217;ll push those messages to the browser with SSE. This shows one of my favorite features of stream-based programming: we simply connect the specific pipes to create more complex flows, without worrying about the how; the framework handles that for us.&lt;/p&gt;

&lt;h2 id=&#34;changing-the-original-example:a9237168f3920272915cff712cbdae5e&#34;&gt;Changing the Original Example&lt;/h2&gt;

&lt;p&gt;If you&amp;#8217;re interested in the code, simply &lt;a href=&#34;https://github.com/mhamrah/streams-playground&#34;&gt;clone the original repo&lt;/a&gt; with &lt;code&gt;git clone https://github.com/mhamrah/streams-playground.git&lt;/code&gt; and then &lt;code&gt;git checkout adding-sse&lt;/code&gt; to get to this step in the repo.&lt;/p&gt;

&lt;p&gt;To modify the original example we&amp;#8217;re going to remove the &lt;code&gt;println&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; calls from &lt;code&gt;RabbitMqConsumer&lt;/code&gt; so we can plug in our enhanced &lt;code&gt;Source&lt;/code&gt; to the Akka-Http sink.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;def consume() = {
    Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
  }
&lt;/pre&gt;

&lt;p&gt;This is now a partial flow: we build up the original RabbitMq &lt;code&gt;Source&lt;/code&gt; with our map function to get the message body. Now the &amp;#8220;other end&amp;#8221; of the stream needs to be connected, which we defer until later. This is the essence of stream composition. There are multiple ways we can cut this up: our &lt;code&gt;map&lt;/code&gt; call could be the only thing in this function, with our &lt;code&gt;RabbitMq&lt;/code&gt; source defined elsewhere.&lt;/p&gt;

&lt;h2 id=&#34;adding-akka-http:a9237168f3920272915cff712cbdae5e&#34;&gt;Adding Akka-Http&lt;/h2&gt;

&lt;p&gt;If you&amp;#8217;re familiar with Spray, Akka-Http won&amp;#8217;t look that much different. We want to create an &lt;code&gt;Actor&lt;/code&gt; for our http service. There are just a few different traits we extend our original Actor from, and a different way plug our routing functions into the Akka-Streams pipeline.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;class HttpService
  extends Actor
  with Directives
  with ImplicitFlowMaterializer
  with SseMarshalling {
  // implementation
  // ...
}
&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;Directives&lt;/code&gt; gives us the routing dsl, similar to &lt;a href=&#34;http://spray.io/documentation/1.2.2/spray-routing/&#34;&gt;spray-routing&lt;/a&gt; (the functions are pretty much the same). Because Akka-Http uses Akka-Streams, we need an implicit &lt;code&gt;FlowMaterializer&lt;/code&gt; in scope to run the stream. &lt;code&gt;ImplicitFlowMaterializer&lt;/code&gt; provides a default. Finally, the &lt;code&gt;SseMarshalling&lt;/code&gt; trait from Heiko Seeberger&amp;#8217;s library provides the SSE functionality we want for our app. &lt;em&gt;If you&amp;#8217;re interested in a robust Akka-Streams sample, Heiko&amp;#8217;s &lt;a href=&#34;https://github.com/hseeberger/reactive-flows&#34;&gt;Reactive-Flows&lt;/a&gt; is worth checking out.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;##Binding to Http&lt;/p&gt;

&lt;p&gt;Within our actor body we&amp;#8217;ll create our http stream by binding a routing function to an http port. This is a little different than Spray; there&amp;#8217;s just some syntactical sugar so we can plug our routing function directly into the http pipeline:&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;//need an ExecutionContext for Futures
    import context.dispatcher

    //There&#39;s no receive needed, this is implicit
    //by our routing dsl.
    override def receive: Receive = Actor.emptyBehavior

    //We bind to an interface and create a 
    //Flow with our routing function
    Http()(context.system)
      .bind(Config.interface, Config.port)
      .startHandlingWith(route)

    //Simple composition of basic routes
    private def route: Route = sse ~ assets

    //Defined later
    private def see: Route = ???
    private def assets: Route = ???
&lt;/pre&gt;

&lt;p&gt;If we weren&amp;#8217;t using the Routing DSL we&amp;#8217;d need to explicitly handling HttpRequest messages in our receive partial function. But the &lt;code&gt;startHandlingWith&lt;/code&gt; call will do this for us; like spray-routing it takes in a routing function, and will call the appropriate route handler. New http requests will be pumped into the route handler and completed with the completion function at the end of the route.&lt;/p&gt;

&lt;p&gt;##Adding SSE&lt;/p&gt;

&lt;p&gt;The last piece of the puzzle is adding a specific route for SSE. We need two pieces for SSE support: first, an implicit function which converts the type produced from our &lt;code&gt;Source&lt;/code&gt; to an &lt;code&gt;Sse.Message&lt;/code&gt;; in this case, we need to go from a &lt;code&gt;String&lt;/code&gt; to an &lt;code&gt;Sse.Message&lt;/code&gt;. Secondly we need a route where a client can subscribe to the stream of server-side events.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;//Convert a String (our RabbitMq output) to an SSE Message
 implicit def stringToSseMessage(event: String): Sse.Message = {
      Sse.Message(event, Some(&#34;published&#34;))
    }

 //add a route for our sse endpoint.
 private def sse: Route = {
      path(&#34;messages&#34;) {
        get {
          complete {
            RabbitMqConsumer.consume
          }
        }
      }
    }
&lt;/pre&gt;

&lt;p&gt;In order for SSE to work in the browser we need to produce a stream of SSE messages with a specific content-type: &lt;code&gt;Content-Type: text/event-stream&lt;/code&gt;. That&amp;#8217;s what Akka-SSE provides: the SSE Message case classes and serialization to &lt;code&gt;text/event-stream&lt;/code&gt;. Our implicit function &lt;code&gt;stringToSseMessage&lt;/code&gt; allows the Scala types to align so the &amp;#8220;stream pipes&amp;#8221; can be attached together. In our case, we produce a stream of &lt;code&gt;String&lt;/code&gt;s, our RabbitMq message body. We need to produce a stream of &lt;code&gt;SSE.Messages&lt;/code&gt; so we add a simple conversion function. When a new client connects, they&amp;#8217;ll attach themselves to the consuming RabbitMq &lt;code&gt;Source&lt;/code&gt;. Akka-Http lets you natively complete a route with a &lt;code&gt;Flow&lt;/code&gt;; Akka-Sse simply completes that &lt;code&gt;Flow&lt;/code&gt; with the proper Http response for SSE.&lt;/p&gt;

&lt;h2 id=&#34;trying-it-out:a9237168f3920272915cff712cbdae5e&#34;&gt;Trying It Out&lt;/h2&gt;

&lt;p&gt;Fire up SBT and run &lt;code&gt;~reStart&lt;/code&gt;, ensuring you have RabbitMq running and set up a queue named &lt;code&gt;streams-playground&lt;/code&gt; (&lt;a href=&#34;https://github.com/mhamrah/streams-playground/blob/master/README.md&#34;&gt;see the README&lt;/a&gt;). In your console, try a simple &lt;code&gt;curl&lt;/code&gt; command:&lt;/p&gt;

&lt;pre class=&#34;bash&#34;&gt;$ curl http://localhost:8080/messages
&lt;/pre&gt;

&lt;p&gt;The curl command won&amp;#8217;t return. Start sending messages via the RabbitMq Admin console and you&amp;#8217;ll see the SSE output in action:&lt;/p&gt;

&lt;pre class=&#34;bash&#34;&gt;$ curl localhost:8080/messages
event:published
data:woot!

event:published
data:another message!
&lt;/pre&gt;

&lt;p&gt;Close the curl command, and fire up your browser at &lt;code&gt;http://localhost:8080&lt;/code&gt; you&amp;#8217;ll see a simple web page (served from the &lt;code&gt;assets&lt;/code&gt; route). Continue sending messages via RabbitMq, and those messages will be added to the dom. Most modern browsers natively support SSE with the &lt;code&gt;EventSource&lt;/code&gt; object. The following gist creates an event listener on the &lt;code&gt;&#39;published&#39;&lt;/code&gt; event, which is produced from our &lt;code&gt;implicit string =&amp;gt; sse&lt;/code&gt; function above:&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s also handlers for opening the initial sse connection and any errors produced. You could also add more events; our simple conversion only goes from a &lt;code&gt;String&lt;/code&gt; to one specific SSE of type &lt;code&gt;published&lt;/code&gt;. You could map a set of case classes&amp;#8211;preferably an algebraic data type&amp;#8211;to a set of events for the client. Most modern browsers support &lt;code&gt;EventStream&lt;/code&gt;; there&amp;#8217;s no need a for an additional framework or library. The gist above includes a test I copied from the &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/eventsource/basics/&#34;&gt;html5 rocks page on SSE&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;a-naive-implementation:a9237168f3920272915cff712cbdae5e&#34;&gt;A Naive Implementation&lt;/h2&gt;

&lt;p&gt;If you open up multiple browsers to localhost, or &lt;code&gt;curl http://localhost:8080/messages&lt;/code&gt; a few times, you&amp;#8217;ll notice that a published message only goes to one client. This is because our initial RabbitMq &lt;code&gt;Source&lt;/code&gt; only consumes one message from a queue, and passes that down the stream pipeline. That single message will only go to one of the connected clients; there&amp;#8217;s no fanout or broadcasting. You can do that with either RabbitMq or Akka-Streams, try experimenting for yourself!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>A Gentle Introduction To Akka Streams</title>
          <link>http://blog.michaelhamrah.com/2015/01/a-gentle-introduction-to-akka-streams/</link>
          <pubDate>Tue, 13 Jan 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/01/a-gentle-introduction-to-akka-streams/</guid>
          <description>

&lt;p&gt;I&amp;#8217;m happy to see stream-based programming emerge as a paradigm in many languages. Streams have been around for a while: take a look at the good &amp;#8216;ol | operator in Unix. Streams offer an interesting conceptual model to processing pipelines that is very functional: you have an input, you produce an output. You string these little functions together to build bigger, more complex pipelines. Most of the time you can make these functions asynchronous and parallelize them over input data to maximize throughput and scale. With a Stream, handling data is almost hidden behind the scenes: it just &lt;em&gt;flows&lt;/em&gt; through &lt;em&gt;functions&lt;/em&gt;, producing a new output from some input. In the case of an Http server, the Request-Response model across all clients is a Stream-based process: You map a Request to a Response, passing it through various functions which act on an input. Forget about MVC, it&amp;#8217;s all middleware. No need to set variables, iterate over collections, orchestrate function calls. Just concatenate stream-enabled functions together, and run your code. Streams offer a succinct programming model for a process. The fact it also scales is a nice bonus.&lt;/p&gt;

&lt;p&gt;Stream based programming is possible in a variety of languages, and I encourage you to explore this space. There&amp;#8217;s an excellent &lt;a href=&#34;https://github.com/substack/stream-handbook&#34;&gt;stream handbook for Node&lt;/a&gt;, &lt;a href=&#34;https://github.com/matz/streem&#34;&gt;an exploratory stream language from Yukihiro &amp;#8220;Matz&amp;#8221; Matsumoto of Ruby fame&lt;/a&gt;, &lt;a href=&#34;https://spark.apache.org/streaming/&#34;&gt;Spark Streaming&lt;/a&gt; and of course &lt;a href=&#34;http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M2/index.html&#34;&gt;Akka-Streams&lt;/a&gt; which joins the existing &lt;a href=&#34;https://github.com/scalaz/scalaz-stream&#34;&gt;scalaz-stream&lt;/a&gt; library for Scala. Even Go&amp;#8217;s &lt;a href=&#34;http://golang.org/pkg/net/http/#HandleFunc&#34;&gt;HttpHandler function&lt;/a&gt; is Stream-esque: you can easily wrap one function around another, building up a flow, and manipulate the Response stream accordingly.&lt;/p&gt;

&lt;h2 id=&#34;why-akka-streams:9c0e63de68271e30d1a6e002245492be&#34;&gt;Why Akka-Streams?&lt;/h2&gt;

&lt;p&gt;Akka-Streams provide a higher-level abstraction over Akka&amp;#8217;s existing actor model. The Actor model provides an excellent primitive for writing concurrent, scalable software, but it still is a primitive; it&amp;#8217;s not hard to find a few critiques of the model. So is it possible to have your cake and eat it too? Can we abstract the functionality we want to achieve with Actors into a set of function calls? Can we treat Actor Messages as Inputs and Outputs to Functions, with type safety? Hello, Akka-Streams.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s an excellent &lt;a href=&#34;http://www.typesafe.com/activator/template/akka-stream-scala&#34;&gt;activator template for Akka-Streams&lt;/a&gt; offering an in-depth tutorial on several aspects of Akka-Streams. For a more a gentler introduction, read on.&lt;/p&gt;

&lt;h2 id=&#34;the-recipe:9c0e63de68271e30d1a6e002245492be&#34;&gt;The Recipe&lt;/h2&gt;

&lt;p&gt;To cook up a reasonable dish, we are going to consume messages from &lt;a href=&#34;https://www.rabbitmq.com&#34;&gt;RabbitMq&lt;/a&gt; with the &lt;a href=&#34;https://github.com/ScalaConsultants/reactive-rabbit&#34;&gt;reactive-rabbit&lt;/a&gt; library and output them to the console. The code is on &lt;a href=&#34;https://github.com/mhamrah/streams-playground&#34;&gt;GitHub&lt;/a&gt;. If you&amp;#8217;d like to follow along, &lt;code&gt;git clone&lt;/code&gt; and then &lt;code&gt;git checkout intro&lt;/code&gt;; hopefully I&amp;#8217;ll build up more functionality in later posts so the master branch may differ.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s start with a code snippet:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;object RabbitMqConsumer {
 def consume(implicit flowMaterializer: FlowMaterializer) = {
    Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
      .foreach(println(_))
  }
}
&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;We use a RabbitMq connection to consume messages off of a queue named &lt;code&gt;streams-playground&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;For each message, we pull out the message and decode the bytes as a UTF-8 string&lt;/li&gt;
&lt;li&gt;We print it to the console&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-ingredients:9c0e63de68271e30d1a6e002245492be&#34;&gt;The Ingredients&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;Source&lt;/code&gt; is something which produces exactly one output. If you need something that generates data, you need a &lt;code&gt;Source&lt;/code&gt;. Our source above is produced from the &lt;code&gt;connection.consume&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;Sink&lt;/code&gt; is something with exactly one input. A &lt;code&gt;Sink&lt;/code&gt; is the final stage of a Stream process. The &lt;code&gt;.foreach&lt;/code&gt; call is a Sink which writes the input (_) to the console via &lt;code&gt;println&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;Flow&lt;/code&gt; is something with exactly one input and one output. It allows data to flow through a function: like calling &lt;code&gt;map&lt;/code&gt; which also returns an element on a collection. The &lt;code&gt;map&lt;/code&gt; call above is a &lt;code&gt;Flow&lt;/code&gt;: it consumes a &lt;code&gt;Delivery&lt;/code&gt; message and outputs a &lt;code&gt;String&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to actually run something using Akka-Streams you must have both a &lt;code&gt;Source&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; attached to the same pipeline. This allows you to create a &lt;code&gt;RunnableFlow&lt;/code&gt; and begin processing the stream. Just as you can compose functions and classes, you can compose streams to build up richer functionality. It&amp;#8217;s a powerful abstraction allowing you to build your processing logic independently of its execution. Think of stream libraries where you &amp;#8220;plug in&amp;#8221; parts of streams together and customize accordingly.&lt;/p&gt;

&lt;h2 id=&#34;a-simple-flow:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Simple Flow&lt;/h2&gt;

&lt;p&gt;You&amp;#8217;ll notice the above snippet requires an &lt;code&gt;implicit flowMaterializer: FlowMaterializer&lt;/code&gt;. A &lt;code&gt;FlowMaterializer&lt;/code&gt; is required to actually run a &lt;code&gt;Flow&lt;/code&gt;. In the snippet above &lt;code&gt;foreach&lt;/code&gt; acts as both a &lt;code&gt;Sink&lt;/code&gt; and a &lt;code&gt;run()&lt;/code&gt; call to run the flow. If you look at the Main.scala file you&amp;#8217;ll see I start the stream easily in one call:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;implicit val flowMaterializer = FlowMaterializer()
  RabbitMqConsumer.consume
&lt;/pre&gt;

&lt;p&gt;Create a queue named &lt;code&gt;streams-playground&lt;/code&gt; via the RabbitMq Admin UI and run the application. You can use publish messages in the RabbitMq Admin UI and they will appear in the console. Try some UTF-8 characters, like åßç∂!&lt;/p&gt;

&lt;h2 id=&#34;a-variation:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Variation&lt;/h2&gt;

&lt;p&gt;The original snippet is nice, but it does require the implicit FlowMaterializer to build and run the stream in &lt;code&gt;consume&lt;/code&gt;. If you remove it, you&amp;#8217;ll get a compile error. Is there a way to separate the definition of the stream with the running of the stream? Yes, by simply removing the &lt;code&gt;foreach&lt;/code&gt; call. &lt;code&gt;foreach&lt;/code&gt; is just syntactical sugar for a &lt;code&gt;map&lt;/code&gt; with a &lt;code&gt;run()&lt;/code&gt; call. By explicitly setting a &lt;code&gt;Sink&lt;/code&gt; without a call to &lt;code&gt;run()&lt;/code&gt; we can construct our stream blueprint producing a new object of type &lt;code&gt;RunnableFlow&lt;/code&gt;. Intuitively, it&amp;#8217;s a &lt;code&gt;Flow&lt;/code&gt; which can be &lt;code&gt;run()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the variation:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;def consume() = {
     Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
      .map(println(_))
      .to(Sink.ignore) //won&#39;t start consuming until run() is called!
  }
&lt;/pre&gt;

&lt;p&gt;We got rid of our &lt;code&gt;flowMaterializer&lt;/code&gt; implicit by terminating our Stream with a &lt;code&gt;to()&lt;/code&gt; call and a simple Sink.ignore which discards messages. This stream will not be run when called. Instead we must call it explicitly in Main.scala:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;implicit val flowMaterializer = FlowMaterializer()
  RabbitMqConsumer.consume().run()
&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;ve separated out the entire pipeline into two stages: the build stage, via the &lt;code&gt;consume&lt;/code&gt; call, and the run stage, with &lt;code&gt;run()&lt;/code&gt;. Ideally you&amp;#8217;d want to compose your stream processing as you wire up the app, with each component, like RabbitMqConsumer, providing part of the overall stream process.&lt;/p&gt;

&lt;h2 id=&#34;a-counter-example:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Counter Example&lt;/h2&gt;

&lt;p&gt;As an alternative, explore the &lt;a href=&#34;http://www.rabbitmq.com/tutorials/tutorial-one-java.html&#34;&gt;rabbitmq tutorials&lt;/a&gt; for Java examples. Here&amp;#8217;s a snippet from the site:&lt;/p&gt;

&lt;pre class=&#34;lang:java&#34;&gt;QueueingConsumer consumer = new QueueingConsumer(channel);
    channel.basicConsume(QUEUE_NAME, true, consumer);

    while (true) {
      QueueingConsumer.Delivery delivery = consumer.nextDelivery();
      String message = new String(delivery.getBody());
      System.out.println(&#34; [x] Received &#39;&#34; + message + &#34;&#39;&#34;);
    }
&lt;/pre&gt;

&lt;p&gt;This is typical of an imperative style. Our flow is controlled by the while loop, we have to explicitly manage variables, and there&amp;#8217;s no flow control. We could separate out the body from the while loop, but we&amp;#8217;d have a crazy function signature. Alternatively on the Akka side there&amp;#8217;s the solid &lt;a href=&#34;https://github.com/sstone/amqp-client&#34;&gt;amqp-client library&lt;/a&gt; which provides an Actor based model over RabbitMq:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;// create an actor that will receive AMQP deliveries
  val listener = system.actorOf(Props(new Actor {
    def receive = {
      case Delivery(consumerTag, envelope, properties, body) =&gt; {
        println(&#34;got a message: &#34; + new String(body))
        sender ! Ack(envelope.getDeliveryTag)
      }
    }
  }))

  // create a consumer that will route incoming AMQP messages to our listener
  // it starts with an empty list of queues to consume from
  val consumer = ConnectionOwner.createChildActor(conn, Consumer.props(listener, channelParams = None, autoack = false))
&lt;/pre&gt;

&lt;p&gt;You get the concurrency primitives via configuration over the actor system, but we still enter imperative-programming land in the Actor&amp;#8217;s &lt;code&gt;receive&lt;/code&gt; blog (sure, this can be refactored to some degree). In general, if we can model our process as a set of streams, we achieve the same benefits we get with functional programming: clear composition on what is happening, not how it&amp;#8217;s doing it.&lt;/p&gt;

&lt;p&gt;Streams can be applied in a variety of contexts. I&amp;#8217;m happy to see the amazing and powerful &lt;a href=&#34;http://spray.io&#34;&gt;spray.io&lt;/a&gt; library for Restful web services will be merged into Akka as a stream enabled http toolkit. It&amp;#8217;s also not hard to find out what&amp;#8217;s been done with &lt;a href=&#34;https://github.com/scalaz/scalaz-stream#projects-using-scalaz-stream&#34;&gt;scalaz-streams&lt;/a&gt; or the plethora of tooling already available in other languages.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Don’t Sweat Choice in Tech: Be Opinionated</title>
          <link>http://blog.michaelhamrah.com/2015/01/dont-sweat-choice-in-tech-be-opinionated/</link>
          <pubDate>Mon, 12 Jan 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/01/dont-sweat-choice-in-tech-be-opinionated/</guid>
          <description>&lt;p&gt;Recently I jumped back into some front-end development. My focus is primarily on backend systems and APIs so I welcomed the opportunity to hack on a UI. I keep tabs on the front-end world and a new project is a good opportunity for a test-drive or to level-up on an existing toolkit. The caveat, however, is the dreaded &lt;a href=&#34;http://techcrunch.com/2014/10/18/you-too-may-be-a-victim-of-developaralysis/&#34;&gt;developaralysis&lt;/a&gt;. We have so many choices that discerning the difference, picking the &amp;#8220;right one&amp;#8221;, and learning it becomes an overwhelming endeavor. Should I try out &lt;a href=&#34;http://gulpjs.com&#34;&gt;gulp&lt;/a&gt;? What about test-driving &lt;a href=&#34;http://facebook.github.io/react/&#34;&gt;react&lt;/a&gt;? Or should I go with the usual bootstrap/angular combo I&amp;#8217;ve come to know well?&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s hard to balance the cost of time in the present for the potential&amp;#8211;and I emphasize potential&amp;#8211;benefit of speed and simplicity later when choosing something new. Time is limited; do I need an exploration of browserify, amd, and umd when all I really need is a simple &lt;code&gt;script&lt;/code&gt; tag? Browserify looks cool, but what&amp;#8217;s the return on that investment? The flood of options occurs at every level of experience; it&amp;#8217;s endearing to overhear a debate amongst new developers on whether to learn rails or node first. It&amp;#8217;s definitely not helpful when &lt;a href=&#34;http://mashable.com/2014/01/21/learn-programming-languages/&#34;&gt;sites offer laundry lists of languages you should learn&lt;/a&gt;. C# and Java, really? I&amp;#8217;m surprised assembly wasn&amp;#8217;t on the list. Judging the nuances of NoSQL options is just as entertaining.&lt;/p&gt;

&lt;p&gt;My programming career, now inching the 15-year mark, has seen its fair share of languages and frameworks. Happily I no longer think about ASP.NET view state or the server-control lifecycle. These were instrumental at one time, and even though they are long gone, those experiences helped shape my current opinions on how I want to develop (or, in this particular case, not to develop) software. I didn&amp;#8217;t realize I had a choice back then on how I develop: ASP.NET seemed a given. That horrible windows-on-web paradigm pushed me to an intense focus on MVC, now a staple of many web frameworks. In turn, with the advent of APIs and more complex, task-focused UX, I am keenly interested in stream-based programming emerging in &lt;a href=&#34;http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M2/scala/stream-index.html&#34;&gt;Scala&lt;/a&gt; and &lt;a href=&#34;https://github.com/substack/stream-handbook&#34;&gt;Node&lt;/a&gt;. What I&amp;#8217;ve come to realize in the tumultuous world of programming, and with constantly needing to level-up, is that frameworks and languages are only part of the equation. The most important part is me, and you: the developer. Steve Ballmer got it right: &lt;a href=&#34;http://vimeo.com/6668315&#34;&gt;it&amp;#8217;s about developers&lt;/a&gt;. Languages and frameworks help us do things but our opinions on how we want to do them is what moves us forward. When a framework matches your opinions, getting stuff done is simple and intuitive. When you feel like your jumping through hoops it&amp;#8217;s time to try something different.&lt;/p&gt;

&lt;p&gt;My small foray back into the front-end world was met with the usual whirlwind of information. Not to mention the usual upgrade of tools, so any answers I find on google will be outdated:&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;
  &lt;p&gt;
    &amp;#8220;What&amp;#8217;s bower?&amp;#8221; &amp;#8220;A package manager, install it with npm.&amp;#8221; &amp;#8220;What&amp;#8217;s npm?&amp;#8221; &amp;#8220;A package manager, you can install it with brew&amp;#8221; &amp;#8220;What&amp;#8217;s brew?&amp;#8221; &amp;#8230;
  &lt;/p&gt;
  
  &lt;p&gt;
    — Stefan Baumgartner (@ddprrt) &lt;a href=&#34;https://twitter.com/ddprrt/status/529909875347030016&#34;&gt;November 5, 2014&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I just want to throw together a web site. Grunt vs. Gulp? Wait, there&amp;#8217;s this thing called &lt;a href=&#34;https://github.com/broccolijs/broccoli&#34;&gt;Broccoli&lt;/a&gt;? Is there something different than &lt;a href=&#34;http://getbootstrap.com&#34;&gt;Bootstrap&lt;/a&gt; that&amp;#8217;s less bootstrappy? Are people still using &lt;a href=&#34;http://html5boilerplate.com&#34;&gt;h5bp&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;It seemed even the simple go-to of &lt;a href=&#34;http://yeoman.io&#34;&gt;&lt;code&gt;yo angular&lt;/code&gt;&lt;/a&gt; was fraught with peril: what are all these questions I have to answer? A massive amount of files were generated. Yes, all important and I know all required for various things, but it&amp;#8217;s information overload. Why is &lt;code&gt;unresolved&lt;/code&gt; added to the body tag? What happens if I hack the meta-viewport settings? What is &lt;code&gt;build:js&lt;/code&gt; doing? Should I put on the blinders and ignore? Maybe I should try the possible simplicity of &lt;a href=&#34;http://blog.keithcirkel.co.uk/why-we-should-stop-using-grunt/&#34;&gt;just using npm&lt;/a&gt;. I was in it: developaralysis.&lt;/p&gt;

&lt;p&gt;Stop. Relax. Breathe. I already knew the simple answer to navigating the awesome amount of choice: opinion. Forget about existing, pre-conceived notions of software. You need to do something: how would you do it? Chances are someone&amp;#8217;s had a similar idea and &lt;a href=&#34;https://github.com/explore&#34;&gt;wrote some software&lt;/a&gt;. Don&amp;#8217;t even know what you need? Then start with something that&amp;#8217;s easy to learn. If it doesn&amp;#8217;t work out, you&amp;#8217;ll have formed an opinion on what you wanted to happen. This is learning by fire.&lt;/p&gt;

&lt;p&gt;Opinions have given birth to some of the most widely used software in the world. &lt;a href=&#34;http://en.wikipedia.org/wiki/Ruby_(programming_language)#Early_concept&#34;&gt;Yukihiro Matsumoto created Ruby from his dissatisfaction with other OOP languages&lt;/a&gt;. &lt;a href=&#34;http://www.aosabook.org/en/nginx.html&#34;&gt;Nginx was spawned by dissatisfaction in threaded web-servers&lt;/a&gt;. You may not be ready to write a new language, framework, or web server, but your opinions can still shape what you learn and where you invest your time.&lt;/p&gt;

&lt;p&gt;Nobody asked me a decade ago how I wanted to write web software. If they did I doubt I would have come up with anything similar to ASP.NET webforms, if I could even have put together a semi-coherent answer. Yet I was a full-time ASP.NET webforms developer, and that&amp;#8217;s how I was writing software. Eventually my teammates and I realized this way of programming was utter crap. We asked ourselves that simple question, which we should have asked a lot earlier: How do we want to do this?&lt;/p&gt;

&lt;p&gt;At any level it&amp;#8217;s important to develop opinions on how you want to achieve goals. Beginners may seem they have a difficult spot because there&amp;#8217;s so little grounding to formulate opinions: any answer may appear &amp;#8220;too simple&amp;#8221;. But the spectrum is the same for experience developers as well. There&amp;#8217;s always &amp;#8220;something else&amp;#8221; to know and factor in behind the curtain. You&amp;#8217;re constantly peeling layers off of the onion.&lt;/p&gt;

&lt;p&gt;Don&amp;#8217;t sweat the plethora of choice which exist. Nothing is perfect, and stagnation is the worst option. Take a moment and develop an opinion on how you want to solve a particular problem. Poke holes in your solution. See if somebody else has a similar idea, or a similar experience. Try something out: don&amp;#8217;t like how it happened or the result? Did you leverage the tool correctly? Okay, great, now you have the basis for something better. Develop your suite of go-to tooling. You can keep tabs on the eco-system, and cross-pollinate ideas across similar veins. Choice is a good thing: like a breadth-first search, letting you still run forward if you want.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Go Style Directory Layout for Scala with SBT</title>
          <link>http://blog.michaelhamrah.com/2014/12/go-style-directory-layout-for-scala-with-sbt/</link>
          <pubDate>Sun, 07 Dec 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/12/go-style-directory-layout-for-scala-with-sbt/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve come to appreciate Go&amp;#8217;s directory layout where &lt;em&gt;test&lt;/em&gt; and &lt;em&gt;build&lt;/em&gt; files are located side-by-side. This promotes a conscience testing priority. It also enables easy navigation to usage of a particular class/trait/object along with the implementation. After reading through the &lt;a href=&#34;http://www.scala-sbt.org/0.13.2/docs/Howto/defaultpaths.html&#34;&gt;getting-better-every-day sbt documentation&lt;/a&gt; I noticed you can easily change default directories for sources, alleviating the folder craziness of default projects. Simply add a few lines to your build.sbt:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;//Why do I need a Scala folder? I don&#39;t!
//Set the folder for Scala sources to the root &#34;src&#34; folder
scalaSource in Compile := baseDirectory.value / &#34;src&#34;

//Do the same for the test configuration. 
scalaSource in Test := baseDirectory.value / &#34;src&#34;

//We&#39;ll suffix our test files with _test, so we can exclude
//then from the main build, and keep the HiddenFileFilter
excludeFilter in (Compile, unmanagedSources) := HiddenFileFilter || &#34;*_test.scala&#34;

//And we need to re-include them for Tests 
excludeFilter in (Test, unmanagedSources) := HiddenFileFilter
&lt;/pre&gt;

&lt;p&gt;Although breaking from the norm of java-build tools may cause confusion, if you like the way something works, go for it; don&amp;#8217;t chain yourself to past practices. I never understood the class-to-file relationship of java sources, and I absolutely &lt;em&gt;hate&lt;/em&gt; navigating one-item folders. Thankfully Scala improved the situation, but the sbt maven-like defaults are still folder-heavy. IDEs make the situation easier, but I prefer simple text editors; and to paraphrase Dan North, &amp;#8220;Your fancy IDE is a painkiller for your shitty language&amp;#8221;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Running Consul on CoreOS</title>
          <link>http://blog.michaelhamrah.com/2014/11/running-consul-on-coreos/</link>
          <pubDate>Sat, 29 Nov 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/11/running-consul-on-coreos/</guid>
          <description>&lt;p&gt;I&amp;#8217;m a big fan of &lt;a href=&#34;consul.io&#34;&gt;Consul&lt;/a&gt;, Hashicorp&amp;#8217;s service discovery tool. I&amp;#8217;ve also become a fan of CoreOS, the cluster framework for running docker containers. Even though CoreOS comes with etcd for service discovery I find the feature set of Consul more compelling. And as a programmer I know I can have my cake and eat it too.&lt;/p&gt;

&lt;p&gt;My first take was to modify my &lt;a href=&#34;github.com/mhamrah/ansible-consul&#34;&gt;ansible-consul&lt;/a&gt; fork to run consul natively on CoreOS. Although this could work I find it defeats CoreOS&amp;#8217;s container-first approach with &lt;a href=&#34;https://github.com/coreos/fleet&#34;&gt;fleet&lt;/a&gt;. Jeff Lindsay created &lt;a href=&#34;https://github.com/progrium/docker-consul&#34;&gt;a consul docker container&lt;/a&gt; which does the job well. I created two fleet service files: one for launching the consul container and another for service discovery. At first the service discovery aspect seemed weird; I tried to pass ip addresses via the &amp;#8211;join parameter or use &lt;code&gt;ExecStartPost&lt;/code&gt; for running the join command. However I took a cue from the CoreOS cluster setup: sometimes you need a third party to get stuff done. In this case we the built in etcd server to manage the join ip address to kickstart the consul cluster.&lt;/p&gt;

&lt;p&gt;The second fleet service file acts as a sidekick:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For every running consul service there&amp;#8217;s a sidekick process&lt;/li&gt;
&lt;li&gt;The sidekick process writes the current IP to a key only if that key doesn&amp;#8217;t exist&lt;/li&gt;
&lt;li&gt;The sidekick process uses the value of that key to join the cluster with &lt;code&gt;docker exec&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The sidekick process removes the key if the consul service dies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two service files are below, but you should tweak for your needs.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You only need a 3 or 5 node server cluster. If your CoreOS deployment is large, use some form of restriction for the server nodes. You can do the same for the client nodes.&lt;/li&gt;
&lt;li&gt;The discovery script could be optimized. It will try and join whatever ip address is listed in the key. This avoids a few split brain scenarios, but needs to be tested.&lt;/li&gt;
&lt;li&gt;If you want DNS to work properly you need to set some Docker daemon options. Read the docker-consul README.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Clustering Akka Applications with Docker — Version 3</title>
          <link>http://blog.michaelhamrah.com/2014/11/clustering-akka-applications-with-docker-version-3/</link>
          <pubDate>Thu, 27 Nov 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/11/clustering-akka-applications-with-docker-version-3/</guid>
          <description>&lt;p&gt;The SBT Native Packager plugin now offers first-class Docker support for building Scala based applications. My last post involved combining SBT Native Packager, SBT Docker, and a custom start script to launch our application. We can simplify the process in two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Although the SBT Docker plugin allows for better customization of Dockerfiles it&amp;#8217;s unnecessary for our use case. SBT Native Packager is enough.&lt;/li&gt;
&lt;li&gt;A separate start script was required for IP address inspection so TCP traffic can be routed to the actor system. I recently contributed an update for &lt;a href=&#34;https://github.com/sbt/sbt-native-packager/pull/411&#34;&gt;better ENTRYPOINT support within SBT Native Packager&lt;/a&gt; which gives us options for launching our app in a container.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With this PR we can now add our IP address inspection snippet to our build removing the need for extraneous files. We could have added this snippet to &lt;code&gt;bashScriptExtraDefines&lt;/code&gt; but that is a global change, requiring &lt;code&gt;/sbin/ifconfig eth0&lt;/code&gt; to be available wherever the application is run. This is definitely infrastructure bleed-out and must be avoided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;The new code, on GitHub,&lt;/a&gt; uses a shell with ENTRYPOINT exec mode to set our environment variable before launching the application:&lt;/p&gt;

&lt;pre class=&#34;code scala&#34;&gt;dockerExposedPorts in Docker := Seq(1600)

dockerEntrypoint in Docker := Seq(&#34;sh&#34;, &#34;-c&#34;, &#34;CLUSTER_IP=`/sbin/ifconfig eth0 | grep &#39;inet addr:&#39; | cut -d: -f2 | awk &#39;{ print $1 }&#39;` bin/clustering $*&#34;)
&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;$*&lt;/code&gt; allows for command-line parameters to be honored when launching the container. Because the app leverages the Typesafe Config library we can also set via Java system properties:&lt;/p&gt;

&lt;pre class=&#34;code bash&#34;&gt;docker run --rm -i -t --name seed mhamrah/clustering:0.3 -Dclustering.cluster.name=example-cluster
&lt;/pre&gt;

&lt;p&gt;Launching the cluster is exactly as before:&lt;/p&gt;

&lt;pre class=&#34;code bash&#34;&gt;docker run --rm -d --name seed mhamrah/clustering:0.3
docker run --rm -d --name member1 --link seed:seed mhamrah/clustering:0.3
&lt;/pre&gt;

&lt;p&gt;For complex scripts it may be too messy to overload the ENTRYPOINT sequence. For those cases simply bake your own docker container as a base and use the ENTRYPOINT approach to call out to your script. SBT Native Packager will still upload all your dependencies and its bash script to &lt;code&gt;/opt/docker/bin/&amp;lt;your app&amp;gt;&lt;/code&gt;. The Docker &lt;code&gt;WORKDIR&lt;/code&gt; is set to &lt;code&gt;/opt/docker&lt;/code&gt; so you can drop the &lt;code&gt;/opt/docker&lt;/code&gt; as above.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Accelerate Team Development with your own SBT Plugin Defaults</title>
          <link>http://blog.michaelhamrah.com/2014/10/accelerate-team-development-with-your-own-sbt-plugin-defaults/</link>
          <pubDate>Mon, 13 Oct 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/10/accelerate-team-development-with-your-own-sbt-plugin-defaults/</guid>
          <description>

&lt;p&gt;My team manages several Scala services built with SBT. The setup of these projects are very similar, from included plugins, dependencies, and build-and-deploy configurations. At first we simply copied and paste these settings across projects but as the number of services increased the hunt-and-change strategy became laborious. Time to optimize.&lt;/p&gt;

&lt;p&gt;I heard of a few teams that created their own sbt plugins for default settings but couldn&amp;#8217;t find information on how this looked. The recent change to &lt;a href=&#34;http://www.scala-sbt.org/0.13/docs/Plugins.html&#34;&gt;AutoPlugins&lt;/a&gt; also didn&amp;#8217;t help existing documentation. I found Will Sargent&amp;#8217;s excellent post on &lt;a href=&#34;tersesystems.com/2014/06/24/writing-an-sbt-plugin&#34;&gt;writing an sbt plugin&lt;/a&gt; helpful but it wasn&amp;#8217;t what I was looking for. I want a plugin which included other plugins and set defaults for those plugins. The goal is to &amp;#8220;drop in&amp;#8221; this plugin and automatically have a set of defaults: using &lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;sbt-native-packager&lt;/a&gt;, a configured &lt;a href=&#34;https://github.com/sbt/sbt-release&#34;&gt;sbt-release&lt;/a&gt; and our nexus artifact server good-to-go.&lt;/p&gt;

&lt;h2 id=&#34;file-locations:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;File Locations&lt;/h2&gt;

&lt;p&gt;As an sbt refresher anything in the &lt;code&gt;project/&lt;/code&gt; folder relates to the build. If you want to develop your own plugin just for the current project you can simply add your .scala files to &lt;code&gt;project/&lt;/code&gt;. If you want to develop your own plugin as a standalone project you put those files in the &lt;code&gt;src/&lt;/code&gt; directory as usual. I mistakenly thought an sbt plugin project only required files in the &lt;code&gt;project/&lt;/code&gt; folder. Silly me.&lt;/p&gt;

&lt;h2 id=&#34;sbt-builds:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;SBT Builds&lt;/h2&gt;

&lt;p&gt;It&amp;#8217;s important to note that the project folder&amp;#8211;and the build itself&amp;#8211;is separate from how your source code is built. SBT uses Scala 2.10, so anything in the &lt;code&gt;project/&lt;/code&gt; folder will be built against 2.10 even if your project is set to 2.11. Thus when developing your plugin use Scala 2.10 to match sbt.&lt;/p&gt;

&lt;h2 id=&#34;dependencies:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;Dependencies&lt;/h2&gt;

&lt;p&gt;Usually when you include a plugin you specify it in the &lt;code&gt;project/plugins.sbt&lt;/code&gt;, right? But what if you&amp;#8217;re developing a plugin that uses other plugins? Your code is in &lt;code&gt;src/&lt;/code&gt; so it won&amp;#8217;t pick up anything in &lt;code&gt;project/&lt;/code&gt; as that only relates to your build. So you need to add whatever plugin you want as a &lt;code&gt;dependency&lt;/code&gt; in your build so its available in within your project, just like any other dependency. But there&amp;#8217;s a trick with sbt plugins. Originally I had the usual in &lt;code&gt;build.sbt&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;libraryDependencies += &amp;quot;com.typesafe.sbt&amp;quot; % &amp;quot;sbt-native-packager&amp;quot; % &amp;quot;0.8.0-M2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but kept getting unresolved dependency errors. This made no sense to me as the plugin is clearly available. It turns out if you want to include an sbt plugin as a project dependency you need to specify it in a special way, explicitly setting the sbt and scala version you want:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;libraryDependencies += sbtPluginExtra(&amp;quot;com.typesafe.sbt&amp;quot; % &amp;quot;sbt-native-packager&amp;quot; % &amp;quot;0.8.0-M2&amp;quot;, sbtV = &amp;quot;0.13&amp;quot;, scalaV = &amp;quot;2.10&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With that, your dependency will resolve and you can use include anything under sbt-native-packager when developing your plugin.&lt;/p&gt;

&lt;h2 id=&#34;specifying-your-plugin-defaults:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;Specifying your Plugin Defaults&lt;/h2&gt;

&lt;p&gt;With your separate project and dependencies satisfied you can now create your plugin which uses other plugins and defaults settings specific to you. This part is easy and follows the usual documentation. Declare an object which extends AutoPlugin and override &lt;code&gt;projectSettings&lt;/code&gt; or &lt;code&gt;buildSettings&lt;/code&gt;. This class looks exactly like it would if you were setting things manually in your build.&lt;/p&gt;

&lt;p&gt;For instance, here&amp;#8217;s how we&amp;#8217;d set the &lt;code&gt;java_server&lt;/code&gt; archetype as the default in our plugin:&lt;/p&gt;

&lt;pre class=&#34;code scala&#34;&gt;package com.hamrah.plugindefaults

import sbt._
import Keys._
import com.typesafe.sbt.SbtNativePackager._

object PluginDefaults extends AutoPlugin {
 override lazy val projectSettings = packageArchetype.java_server
}
&lt;/pre&gt;

&lt;p&gt;You can concatenate any other settings you want to project settings, like scalaVersion, scalacOptions, etc.&lt;/p&gt;

&lt;h2 id=&#34;using-the-plugin:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;Using the Plugin&lt;/h2&gt;

&lt;p&gt;You can build and publish your plugin to a repo and include it like you would any other plugin. Or you can include it locally for testing by putting this in your sbt file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lazy val root = project.in( file(&amp;quot;.&amp;quot;) ).dependsOn( defaultPluginSettings )
lazy val defaultPluginSettings = uri(&amp;quot;file:///&amp;lt;full path to your plugin directory&amp;gt;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your default settings can be explicitly added to your project if not automatically imported with a simple:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PluginDefaults.projectSettings
//or
settings = PluginDefaults.projectSettings // in a .scala file
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;in-closing:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;In Closing&lt;/h2&gt;

&lt;p&gt;As an FYI there could be better ways to do this. A lot of the above was trial and error, but works. If you have feedback or better suggestions please leave a comment!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Accessing the Docker Host Server Within a Container</title>
          <link>http://blog.michaelhamrah.com/2014/06/accessing-the-docker-host-server-within-a-container/</link>
          <pubDate>Sun, 29 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/accessing-the-docker-host-server-within-a-container/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/userguide/dockerlinks/#working-with-links-names&#34;&gt;Docker links&lt;/a&gt; are a great way to link two containers together but sometimes you want to know more about the host and network from within a container. You have a couple of options:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can access the Docker host by the container&amp;#8217;s gateway.&lt;/li&gt;
&lt;li&gt;You can access the Docker host by its ip address from within a container.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-gateway-approach:d6e669ce9206f072ff43b8fdf5b03a0c&#34;&gt;The Gateway Approach&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dotcloud/docker/issues/1143&#34;&gt;This GitHub Issue&lt;/a&gt; outlines the solution. Essentially you&amp;#8217;re using netstat to parse the gateway the docker container uses to access the outside world. This is the docker0 bridge on the host.&lt;/p&gt;

&lt;p&gt;As an example, we&amp;#8217;ll run a simple docker container which returns the hostname of the container on port 8080:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run -d -p 8080:8080 mhamrah/mesos-sample
&lt;/pre&gt;

&lt;p&gt;Next we&amp;#8217;ll run /bin/bash in another container to do some discovery:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run -i -t ubuntu /bin/bash
#once in, install curl:
apt-get update
apt-get install -y curl
&lt;/pre&gt;

&lt;p&gt;We can use the following command to pull out the gateway from netstat:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;netstat -nr | grep &#39;^0\.0\.0\.0&#39; | awk &#39;{print $2}&#39;
#returns 172.17.42.1 for me.
&lt;/pre&gt;

&lt;p&gt;We can then curl our other docker container, and we should get that docker container&amp;#8217;s hostname:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl 172.17.42.1:8080
# returns 00b019ce188c
&lt;/pre&gt;

&lt;p&gt;Nothing exciting, but you get the picture: it doesn&amp;#8217;t matter that the service is inside another container, we&amp;#8217;re accessing it via the host, and we didn&amp;#8217;t need to use links. We just needed to know the port the other service was listening on. If you had a service running on some other port&amp;#8211;say Postgres on 5432&amp;#8211;not running in a Docker container&amp;#8211;you can access it via &lt;code&gt;172.17.42.1:5432&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have docker installed in your container you can also query the docker host:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# In a container with docker installed list other containers running on the host for other containers:
docker -H tcp://172.17.42.1:2375 ps
CONTAINER ID        IMAGE                         COMMAND                CREATED              STATUS              PORTS                     NAMES
09d035054988        ubuntu:14.04                  /bin/bash              About a minute ago   Up About a minute   0.0.0.0:49153-&gt;8080/tcp   angry_bardeen
00b019ce188c        mhamrah/mesos-sample:latest   /opt/delivery/bin/de   8 minutes ago        Up 8 minutes        0.0.0.0:8080-&gt;8080/tcp    suspicious_colden
&lt;/pre&gt;

&lt;p&gt;You can use this for some hakky service-discovery.&lt;/p&gt;

&lt;h2 id=&#34;the-ip-approach:d6e669ce9206f072ff43b8fdf5b03a0c&#34;&gt;The IP Approach&lt;/h2&gt;

&lt;p&gt;The gateway approach is great because you can figure out a way to access a host from entirely within a container. You also have the same access via the host&amp;#8217;s ip address. I&amp;#8217;m using boot2docker, and the boot2docker ip address is &lt;code&gt;192.168.59.103&lt;/code&gt; and I can accomplish the same tasks as the gateway approach:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Docker processes, via ip:
docker -H tcp://192.168.59.103:2375 ps
# Other docker containers, via ip:
curl 192.168.59.103:8080
&lt;/pre&gt;

&lt;p&gt;Although there&amp;#8217;s no way to introspect the host&amp;#8217;s ip address (AFAIK) you can pass this in via an environment variable:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker@boot2docker:~$  docker run -i -t -e DOCKER_HOST=192.168.59.103 ubuntu /bin/bash
root@07561b0607f4:/# env
HOSTNAME=07561b0607f4
DOCKER_HOST=192.168.59.103
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PWD=/
&lt;/pre&gt;

&lt;p&gt;If the container knows the ip address of its host, you can broadcast this out to other services via the container&amp;#8217;s application. Useful for service discovery tools run from within a container where you want to tell others the host IP so others can find you.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Service Discovery Options with Marathon and Deimos</title>
          <link>http://blog.michaelhamrah.com/2014/06/service-discovery-options-with-marathon-and-deimos/</link>
          <pubDate>Sun, 29 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/service-discovery-options-with-marathon-and-deimos/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve become a fan of Mesos and Marathon: combined with &lt;a href=&#34;https://github.com/mesosphere/deimos&#34;&gt;Deimos&lt;/a&gt; you can create a DIY PaaS for launching and scaling Docker containers across a number of nodes. Marathon supports a bare-bones service-discovery mechanism through its task API, but it would be nice for containers to register themselves with some service discovery tool themselves. In order to achieve this containers need to know their host ip address and the port Marathon assigned them so they could tell other interested services where they can be found.&lt;/p&gt;

&lt;p&gt;Deimos allows default parameters to be passed in when executing &lt;code&gt;docker run&lt;/code&gt; and Marathon adds assigned ports to a container&amp;#8217;s environment variables. If a container has this information it can register it with a service discovery tool.&lt;/p&gt;

&lt;p&gt;Here we assign the host&amp;#8217;s IP address as a default run option in our &lt;a href=&#34;https://github.com/mesosphere/deimos/#configuration&#34;&gt;Deimos config file&lt;/a&gt;.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#/etc/deimos.cfg
[containers.options]
append: [&#34;-e&#34;, &#34;HOST_IP=192.168.33.12&#34;]
&lt;/pre&gt;

&lt;p&gt;Now let&amp;#8217;s launch our mesos-sample container to our Mesos cluster via Marathon:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;// Post to http://192.168.33.12/v2/apps
{
  &#34;container&#34;: {
    &#34;image&#34;: &#34;docker:///mhamrah/mesos-sample&#34;
  },
  &#34;cpus&#34;: &#34;1&#34;,
  &#34;id&#34;: &#34;www&#34;,
  &#34;instances&#34;: 1,
  &#34;mem&#34;: 512,
  &#34;ports&#34;: [0],
  &#34;uris&#34;: [],
  &#34;cmd&#34;: &#34;&#34;
}
&lt;/pre&gt;

&lt;p&gt;Once our app is launch, we can inspect all the environment variables in our container with the &lt;code&gt;/env&lt;/code&gt; endpoint from &lt;code&gt;mhamrah/mesos-sample&lt;/code&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;curl http://192.168.33.12:31894/env
[ {
  &#34;HOSTNAME&#34; : &#34;a4305981619d&#34;
}, {
  &#34;PORT0&#34; : &#34;31894&#34;
}, {
  &#34;PATH&#34; : &#34;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&#34;
}, {
  &#34;PWD&#34; : &#34;/tmp/mesos-sandbox&#34;
}, {
  &#34;PORTS&#34; : &#34;31894&#34;
}, {
  &#34;HOST_IP&#34; : &#34;192.168.33.12&#34;
}, {
  &#34;PORT&#34; : &#34;31894&#34;
}]
&lt;/pre&gt;

&lt;p&gt;With this information some startup script could use the &lt;code&gt;PORT&lt;/code&gt; (or &lt;code&gt;PORT0&lt;/code&gt;) and &lt;code&gt;HOST_IP&lt;/code&gt; to register itself for direct point-to-point communication in a cluster.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Setting up a Multi-Node Mesos Cluster running Docker, HAProxy and Marathon with Ansible</title>
          <link>http://blog.michaelhamrah.com/2014/06/setting-up-a-multi-node-mesos-cluster-running-docker-haproxy-and-marathon-with-ansible/</link>
          <pubDate>Thu, 26 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/setting-up-a-multi-node-mesos-cluster-running-docker-haproxy-and-marathon-with-ansible/</guid>
          <description>

&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;With Mesos 0.20 Docker support is now native, and Deimos has been deprecated. The ansible-mesos-playbook has been updated appropriately, and most of this blog post still holds true. There are slight variations with how you post to Marathon.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf&#34;&gt;Google Omega Paper&lt;/a&gt; has given birth to cloud vNext: cluster schedulers managing containers. You can make a bunch of nodes appear as one big computer and deploy anything to your own private cloud; just like Docker, but across any number of nodes. &lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes&#34;&gt;Google&amp;#8217;s Kubernetes&lt;/a&gt;, &lt;a href=&#34;flynn.io&#34;&gt;Flynn&lt;/a&gt;, &lt;a href=&#34;https://github.com/coreos/fleet&#34;&gt;Fleet&lt;/a&gt; and &lt;a href=&#34;http://mesos.apache.org/&#34;&gt;Apache Mesos&lt;/a&gt;, originally from Twitter, are implementations of Omega with the goal of abstracting away discrete nodes and optimizing compute resources. Each implementation has its own tweak, but they all follow the same basic setup: leaders, for coordination and scheduling; some service discovery component; some underlying cluster tool (like Zookeeper); followers, for processing.&lt;/p&gt;

&lt;p&gt;In this post we&amp;#8217;ll use &lt;a href=&#34;http://www.ansible.com/home&#34;&gt;Ansible&lt;/a&gt; to install a multi-node Mesos cluster using packages from &lt;a href=&#34;http://mesosphere.io/&#34;&gt;Mesosphere&lt;/a&gt;. Mesos, as a cluster framework, allows you to run a variety of cluster-enabled software, including &lt;a href=&#34;http://spark.apache.org/&#34;&gt;Spark&lt;/a&gt;, &lt;a href=&#34;https://github.com/mesosphere/storm-mesos&#34;&gt;Storm&lt;/a&gt; and &lt;a href=&#34;https://github.com/mesos/hadoop&#34;&gt;Hadoop&lt;/a&gt;. You can also run &lt;a href=&#34;https://github.com/jenkinsci/mesos-plugin&#34;&gt;Jenkins&lt;/a&gt;, schedule tasks with &lt;a href=&#34;https://github.com/airbnb/chronos&#34;&gt;Chronos&lt;/a&gt;, even run &lt;a href=&#34;https://github.com/mesosphere/elasticsearch-mesos&#34;&gt;ElasticSearch&lt;/a&gt; and &lt;a href=&#34;https://github.com/mesosphere/cassandra-mesos&#34;&gt;Cassandra&lt;/a&gt; without having to double to specific servers. We&amp;#8217;ll also set up &lt;a href=&#34;https://github.com/mesosphere/marathon&#34;&gt;Marathon&lt;/a&gt; for running services with &lt;a href=&#34;https://github.com/mesosphere/deimos&#34;&gt;Deimos&lt;/a&gt; support for Docker containers.&lt;/p&gt;

&lt;p&gt;Mesos, even with Marathon, doesn&amp;#8217;t offer the holistic integration of some other tools, namely Kubernetes, but at this point it&amp;#8217;s easier to set up on your own set of servers. Although young Mesos is one of the oldest projects of the group and allows more of a DIY approach on service composition.&lt;/p&gt;

&lt;h3 id=&#34;tl-dr:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/mhamrah/ansible-mesos-playbook&#34;&gt;The playbook is on github, just follow the readme!&lt;/a&gt;&lt;/em&gt;. If you want to simply try out Mesos, Marathon, and Docker &lt;a href=&#34;http://mesosphere.io/learn/run-docker-on-mesosphere/&#34;&gt;mesosphere has an excellent tutorial to get you started on a single node&lt;/a&gt;. This tutorial outlines the creation of a more complex multi-node setup.&lt;/p&gt;

&lt;h3 id=&#34;system-setup:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;System Setup&lt;/h3&gt;

&lt;p&gt;The system is divided into two parts: a set of masters, which handle scheduling and task distribution, with a set of slaves providing compute power. Mesos uses Zookeeper for cluster coordination and leader election. A key component is service discovery: you don&amp;#8217;t know which host or port will be assigned to a task, which makes, say, accessing a website running on a slave difficult. The Marathon API allows you to query task information, and we use this feature to configure HAProxy frontend/backend resources.&lt;/p&gt;

&lt;p&gt;Our masters run:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Zookeeper&lt;/li&gt;
&lt;li&gt;Mesos-Master&lt;/li&gt;
&lt;li&gt;HAProxy&lt;/li&gt;
&lt;li&gt;Marathon&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and our slaves run:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mesos-Slave&lt;/li&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;li&gt;Deimos, the Mesos -&amp;gt; Docker bridge&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ansible:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Ansible&lt;/h3&gt;

&lt;p&gt;Ansible works by running a playbook, composed of roles, against a set of hosts, organized into groups. My &lt;a href=&#34;https://github.com/mhamrah/ansible-mesos-playbook&#34;&gt;Ansible-Mesos-Playbook&lt;/a&gt; on GitHub has an example hosts file with some EC2 instances listed. You should be able to replace these with your own EC2 instances running Ubuntu 14.04, our your own private instances running Ubuntu 14.04. Ansible allows us to pass node information around so we can configure multiple servers to properly set up our masters, zookeeper set, point slaves to masters, and configure Marathon for high availability.&lt;/p&gt;

&lt;p&gt;We want at least three servers in our master group for a proper zookeeper quorum. We use host variables to specify the zookeeper id for each node.&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;[mesos_masters]
ec2-54-204-214-172.compute-1.amazonaws.com zoo_id=1
ec2-54-235-59-210.compute-1.amazonaws.com zoo_id=2
ec2-54-83-161-83.compute-1.amazonaws.com zoo_id=3
&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/mhamrah/ansible-mesos&#34;&gt;mesos-ansible&lt;/a&gt; playbook will use nodes in the &lt;code&gt;mesos_masters&lt;/code&gt; for a variety of configuration options. First, the &lt;code&gt;/etc/zookeeper/conf/zoo.cfg&lt;/code&gt; will list all master nodes, with &lt;code&gt;/etc/zookeeper/conf/myid&lt;/code&gt; being set appropriately. It will also set up upstart scripts in &lt;code&gt;/etc/init/mesos-master.conf&lt;/code&gt;, &lt;code&gt;/etc/init/mesos-slave.conf&lt;/code&gt; with default configuration files in &lt;code&gt;/etc/defaults/mesos.conf&lt;/code&gt;. Mesos 0.19 supports external executors, so we use Deimos to run docker containers. This is only required on slaves, but the configuration options are set in the shared &lt;code&gt;/etc/defaults/mesos.conf&lt;/code&gt; file.&lt;/p&gt;

&lt;h3 id=&#34;marathon-and-haproxy:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Marathon and HAProxy&lt;/h3&gt;

&lt;p&gt;The playbook leverages an &lt;code&gt;ansible-marathon&lt;/code&gt; role to install a custom build of marathon with Deimos support. If Mesos is the OS for the data center, Marathon is the init system. Marathoin allows us to &lt;code&gt;http post&lt;/code&gt; new tasks, containing docker container configurations, which will run on Mesos slaves. With HAProxy we can use the masters as a load balancing proxy server routing traffic from known hosts (the masters) to whatever node/port is running the marathon task. HAProxy is configured via a cron job running &lt;a href=&#34;https://github.com/mhamrah/ansible-marathon/blob/master/files/haproxy_dns_cfg&#34;&gt;a custom bash script&lt;/a&gt;. The script queries the marathon API and will route to the appropriate backend by matching a host header prefix to the marathon job name.&lt;/p&gt;

&lt;h3 id=&#34;mesos-followers-slaves:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Mesos Followers (Slaves)&lt;/h3&gt;

&lt;p&gt;The slaves are pretty straightforward. We don&amp;#8217;t need any host variables, so we just list whatever slave nodes you&amp;#8217;d like to configure:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;[mesos_slaves]
ec2-54-91-78-105.compute-1.amazonaws.com
ec2-54-82-227-223.compute-1.amazonaws.com 
&lt;/pre&gt;

&lt;p&gt;Mesos-Slave will be configured with Deimos support.&lt;/p&gt;

&lt;h3 id=&#34;the-result:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;The Result&lt;/h3&gt;

&lt;p&gt;With all this set up you can set up a wildcard domain name, say &lt;code&gt;*.example.com&lt;/code&gt;, to point to all of your master node ip addresses. If you launch a task like &amp;#8220;www&amp;#8221; you can visit www.example.com and you&amp;#8217;ll hit whatever server is running your application. Let&amp;#8217;s try launching a simple web server which returns the docker container&amp;#8217;s hostname:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;POST to one of our masters:

POST /v2/apps

{
  &#34;container&#34;: {
    &#34;image&#34;: &#34;docker:///mhamrah/mesos-sample&#34;
  },
  &#34;cpus&#34;: &#34;.25&#34;,
  &#34;id&#34;: &#34;www&#34;,
  &#34;instances&#34;: 4,
  &#34;mem&#34;: 512,
  &#34;ports&#34;: [0],
  &#34;uris&#34;: []
}
&lt;/pre&gt;

&lt;p&gt;We run four instances allocating 25% of a cpu with an application name of &lt;code&gt;www&lt;/code&gt;. If we hit &lt;code&gt;www.example.com&lt;/code&gt;, we&amp;#8217;ll get the hostname of the docker container running on whatever slave node is hosting the task. Deimos will inspect whatever ports are &lt;code&gt;EXPOSE&lt;/code&gt;d in the docker container and assign a port for Mesos to use. Even though the config script only works on port 80 you can easily reconfigure for your own needs.&lt;/p&gt;

&lt;p&gt;To view marathon tasks, simply go to one of your master hosts on port 8080. Marathon will proxy to the correct master. To view mesos tasks, navigate to port 5050 and you&amp;#8217;ll be redirected to the appropriate master. You can also inspect the STDOUT and STDERR of Mesos tasks.&lt;/p&gt;

&lt;h3 id=&#34;notes:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Notes&lt;/h3&gt;

&lt;p&gt;In my testing I noticed, on rare occasion, the cluster didn&amp;#8217;t have a leader or marathon wasn&amp;#8217;t running. You can simply restart zookeeper, mesos, or marathon via ansible:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#Restart Zookeeper
ansible mesos_masters -a &#34;sudo service zookeeper restart&#34;
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a high probability something won&amp;#8217;t work. Check the logs, it took me a while to get things working: grepping &lt;code&gt;/var/log/syslog&lt;/code&gt; will help, along with &lt;code&gt;/var/log/upstart/mesos-master.conf&lt;/code&gt;, &lt;code&gt;mesos-slave.conf&lt;/code&gt; and &lt;code&gt;marathon.conf&lt;/code&gt;, along with the &lt;code&gt;/var/log/mesos/&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;what-8217-s-next:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;What&amp;#8217;s Next&lt;/h3&gt;

&lt;p&gt;Cluster schedulers are an exciting tool for running production applications. It&amp;#8217;s never been easier to build, package and deploy services on public, private clouds or bare metal servers. Mesos, with Marathon, offers a cool combination for running docker containers&amp;#8211;and other mesos-based services&amp;#8211;in production. &lt;a href=&#34;https://engineering.twitter.com/university/videos/docker-mesos&#34;&gt;This Twitter U video highlights how OpenTable uses Mesos for production&lt;/a&gt;. The HAProxy approach, albeit simple, offers a way to route traffic to the correct container. HAProxy will detect failures and reroute traffic accordingly.&lt;/p&gt;

&lt;p&gt;I didn&amp;#8217;t cover inter-container communication (say, a website requiring a database) but you can use your service-discovery tool of choice to solve the problem. The Mesos-Master nodes provide good &amp;#8220;anchor points&amp;#8221; for known locations to look up stuff; you can always query the marathon api for service discovery. Ansible provides a way to automate the install and configuration of mesos-related tools across multiple nodes so you can have a serious mesos-based platform for testing or production use.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Akka Clustering with SBT-Docker and SBT-Native-Packager</title>
          <link>http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/</link>
          <pubDate>Thu, 19 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/</guid>
          <description>

&lt;p&gt;Since my last post on &lt;a href=&#34;http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/&#34;&gt;akka clustering with docker containers&lt;/a&gt; a new plugin, &lt;a href=&#34;https://github.com/marcuslonnberg/sbt-docker&#34;&gt;SBT-Docker&lt;/a&gt;, has emerged which allows you to build docker containers directly from SBT. I&amp;#8217;ve updated my &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;akka-docker-cluster-example&lt;/a&gt; to leverage these two plugins for a smoother docker build experience.&lt;/p&gt;

&lt;h2 id=&#34;one-step-build:9dc58615474f52923afa41a9d5040e47&#34;&gt;One Step Build&lt;/h2&gt;

&lt;p&gt;The approach is basically the same as the previous example: we use SBT Native Packager to gather up the appropriate dependencies, upload them to the docker container, and create the entrypoint. I decided to keep the start script approach to &amp;#8220;prep&amp;#8221; any environment variables required before launching. With SBT Docker linked to Native Packager all you need to do is fire&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;docker
&lt;/pre&gt;

&lt;p&gt;from sbt and you have a docker container ready to launch or push.&lt;/p&gt;

&lt;h2 id=&#34;understanding-the-build:9dc58615474f52923afa41a9d5040e47&#34;&gt;Understanding the Build&lt;/h2&gt;

&lt;p&gt;SBT Docker requires a dockerfile defined in your build. I want to pass in artifacts from native packager to docker. This allows native packager to focus on application needs while docker is focused on docker. Docker turns into just another type of package for your app.&lt;/p&gt;

&lt;p&gt;We can pass in arguments by mapping the appropriate parameters to a function which returns the Dockerfile. In build.spt:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;// Define a dockerfile, using parameters from native-packager
dockerfile in docker &amp;lt;&amp;lt;= (name, stagingDirectory in Universal) map {
  case(appName, stageDir) =&gt;
    val workingDir = s&#34;/opt/${appName}&#34;
    new Dockerfile {
      //use java8 base box
      from(&#34;relateiq/oracle-java8&#34;)
      maintainer(&#34;Michael Hamrah&#34;)
      //expose our akka port
      expose(1600)
      //upload native-packager staging directory files
      add(stageDir, workingDir)
      //make files executable
      run(&#34;chmod&#34;, &#34;+x&#34;, s&#34;/opt/${appName}/bin/${appName}&#34;)
      run(&#34;chmod&#34;, &#34;+x&#34;, s&#34;/opt/${appName}/bin/start&#34;)
      //set working directory
      workDir(workingDir)
      //entrypoint into our start script
      entryPointShell(s&#34;bin/start&#34;, appName, &#34;$@&#34;)
    }
}
&lt;/pre&gt;

&lt;h3 id=&#34;linking-sbt-docker-to-sbt-native-packager:9dc58615474f52923afa41a9d5040e47&#34;&gt;Linking SBT Docker to SBT Native Packager&lt;/h3&gt;

&lt;p&gt;Because we&amp;#8217;re relying on Native Packager to assemble our runtime dependencies we need to ensure the native packager files are &amp;#8220;staged&amp;#8221; before docker tries to upload them. Luckily it&amp;#8217;s easy to create dependencies with SBT. We simply have docker depend on the native packager&amp;#8217;s stage task:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;docker &amp;lt;&amp;lt;= docker.dependsOn(com.typesafe.sbt.packager.universal.Keys.stage.in(Compile))
&lt;/pre&gt;

&lt;h3 id=&#34;adding-additional-files:9dc58615474f52923afa41a9d5040e47&#34;&gt;Adding Additional Files&lt;/h3&gt;

&lt;p&gt;The last step is to add our start script to the native packager build. Native packager has a &lt;code&gt;mappings&lt;/code&gt; key where we can add files to our package. I kept the start script in the docker folder and I want it in the bin directory within the docker container. Here&amp;rsquo;s the mapping:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;mappings in Universal += baseDirectory.value / &#34;docker&#34; / &#34;start&#34; -&gt; &#34;bin/start&#34;
&lt;/pre&gt;

&lt;p&gt;With this setting everything will be assembled as needed and we can package to any type we want. Setting up a cluster with docker is &lt;a href=&#34;http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/&#34;&gt;the same as before&lt;/a&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run --name seed -i -t clustering
docker run --name c1 -link seed:seed -i -t clustering
&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s interesting to note SBT Native Packager also has docker support, but it&amp;rsquo;s undocumented and doesn&amp;rsquo;t allow granular control over the Dockerfile output. Until SBT Native Packager fully supports docker output the SBT Docker plugin is a nice tool to package your sbt-based apps.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Spray Directives: Creating Your Own, Simple Directive</title>
          <link>http://blog.michaelhamrah.com/2014/05/spray-directives-creating-your-own-simple-directive/</link>
          <pubDate>Sat, 24 May 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/05/spray-directives-creating-your-own-simple-directive/</guid>
          <description>&lt;p&gt;The &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/&#34;&gt;spray-routing&lt;/a&gt; package provides an excellent dsl for creating restful api&amp;#8217;s with Scala and Akka. This dsl is powered by &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/key-concepts/directives/&#34;&gt;directives&lt;/a&gt;, small building blocks you compose to filter, process and compose requests and responses for your API. Building your own directives lets you create reusable components for your application and better organize your application.&lt;/p&gt;

&lt;p&gt;I recently refactored some code in a Spray API to leverage custom directives. The &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/advanced-topics/custom-directives/&#34;&gt;Spray documentation provides a good reference on custom directives&lt;/a&gt; but I found myself getting hung up in a few places.&lt;/p&gt;

&lt;p&gt;As an example we&amp;#8217;re going to write a custom directive which produces a UUID for each request. Here&amp;#8217;s how I want to use this custom directive:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;generateUUID { uuid =&gt;
  path(&#34;foo&#34;) {
   get {
     //log the uuid, pass it to your app, or maybe just return it
     complete { uuid.toString }
   }
  }
}
&lt;/pre&gt;

&lt;p&gt;Usually you leverage existing directives to build custom directives. I (incorrectly) started with the &lt;code&gt;provide&lt;/code&gt; directive to provide a value to an inner route:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;import spray.routing._
import java.util.UUID
import Directives._

trait UuidDirectives {
  def generateUuid: Directive1[UUID] = {
    provide(UUID.randomUUID)
  }
}
&lt;/pre&gt;

&lt;p&gt;Before I explain what&amp;#8217;s wrong, let&amp;#8217;s dig into the code. First, generateUuid is a function which returns a Directive1 wrapping a UUID value. Directive1 is just a type alias for &lt;code&gt;Directive[UUID :: HNil]&lt;/code&gt;. Directives are centered around a feature of the shapeless library called heterogeneous lists, or HLists. An &lt;code&gt;HList&lt;/code&gt; is simply a list, but each element in the list can be a different, specific type. Instead of a generic &lt;code&gt;List[Any]&lt;/code&gt;, your list can be composed of specific types of list of String, Int, String, UUID. The first element of this list is a String, not an Any, and the second is an Int, with all the features of an Int. In the directive above I just have an &lt;code&gt;HList&lt;/code&gt; with one element: &lt;code&gt;UUID&lt;/code&gt;. If I write &lt;code&gt;Directive[UUID :: String :: HNil]&lt;/code&gt; I have a two element list of &lt;code&gt;UUID&lt;/code&gt; and String, and the compiler will throw an error if I try to use this directive with anything other a &lt;code&gt;UUID&lt;/code&gt; and a String. HLists sound like a lightweight case class, but with an &lt;code&gt;HList&lt;/code&gt;, you get a lot of list-like features. HLists allow the compiler to do the heavy lifting of type safety, so you can have strongly-typed functions to compose together.&lt;/p&gt;

&lt;p&gt;Provide is a directive which (surprise surprise) will provide a value to an inner route. I thought this would be perfect for my directive, and the corresponding test ensures it works:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;import org.scalatest._
import org.scalatest.matchers._
import spray.testkit.ScalatestRouteTest
import spray.http._
import spray.routing.Directives._

class UuidDirectivesSpec
  extends FreeSpec
  with Matchers
  with UuidDirectives
  with ScalatestRouteTest {

  &#34;The UUID Directive&#34; - {
    &#34;can generate a UUID&#34; in {
      Get() ~&gt; generateUuid { uuid =&gt; complete(uuid.toString) } ~&gt; check  {
        responseAs[String].size shouldBe 36
      }
    }
  }
}
&lt;/pre&gt;

&lt;p&gt;But there&amp;#8217;s an issue! Spray directives are classes are composed when instantiated via an apply() function. The &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/advanced-topics/understanding-dsl-structure/&#34;&gt;Spray docs on understanding the dsl structure&lt;/a&gt; explains it best, but in summary, generateUuid will only be called once when the routing tree is built, not on every request.&lt;/p&gt;

&lt;p&gt;A better unit test shows the issue:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;will generate different UUID per request&#34; in {
      //like the runtime, instantiate route once
      val uuidRoute =  generateUuid { uuid =&gt; complete(uuid.toString) }

      var uuid1: String = &#34;&#34;
      var uuid2: String = &#34;&#34;
      Get() ~&gt; uuidRoute ~&gt; check  {
        responseAs[String].size shouldBe 36
        uuid1 = responseAs[String]
      }
      Get() ~&gt; uuidRoute ~&gt; check  {
        responseAs[String].size shouldBe 36
        uuid2 = responseAs[String]
      }
      //fails!
      uuid1 shouldNot equal (uuid2)
    }
  }
&lt;/pre&gt;

&lt;p&gt;The fix is simple: we need to use the &lt;code&gt;extract&lt;/code&gt; directive which applies the current RequestContext to our route so it&amp;#8217;s called on every request. For our UUID directive we don&amp;#8217;t need anything from the request, just the function which is run for every request:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;trait UuidDirectives {
  def generateUuid: Directive[UUID :: HNil] = {
    extract(ctx =&gt;
        UUID.randomUUID)
  }
}
&lt;/pre&gt;

&lt;p&gt;With our randomUUID call wrapped in an extract directive we have a unique call per request, and our tests pass!&lt;/p&gt;

&lt;p&gt;In a following post we&amp;#8217;ll add some more complexity to our custom directive, stay tuned!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Spray Directives: Custom Directives, Part Two: flatMap</title>
          <link>http://blog.michaelhamrah.com/2014/05/spray-directives-custom-directives-part-two-flatmap/</link>
          <pubDate>Sat, 24 May 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/05/spray-directives-custom-directives-part-two-flatmap/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;http://blog.michaelhamrah.com/2014/05/spray-directives-creating-your-own-simple-directive/&#34;&gt;Our last post covered custom Spray Directives&lt;/a&gt;. We&amp;#8217;re going to expand our UUID directive a little further. Generating a unique ID per request is great, but what if we want the client to pass in an existing unique identifier to act as a correlation id between systems?&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll modify our existing directive by checking to see if the client supplied a correlation-id request-header using the existing &lt;code&gt;optionalHeaderValueByName&lt;/code&gt; directive:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;def generateUuid: Directive[UUID :: HNil] = {
    optionalHeaderValueByName(&#34;correlation-id&#34;) {
      case Some(value) =&gt; provide(UUID.fromString(value))
      case None =&gt; provide(UUID.randomUUID)
    }
  }
&lt;/pre&gt;

&lt;p&gt;Unfortunately this code doesn&amp;#8217;t compile! We get an error because Spray is looking for a Route, which is a function of RequestContext =&amp;gt; Unit:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;[error]  found   : spray.routing.Directive1
[error]     (which expands to)  spray.routing.Directive[shapeless.::]
[error]  required: spray.routing.RequestContext =&gt; Unit
[error]       case Some(value) =&gt; provide(UUID.fromString(value))
&lt;/pre&gt;

&lt;p&gt;What do we do? &lt;code&gt;flatMap&lt;/code&gt; comes to the rescue. Here&amp;#8217;s the deal: we need to transform one directive (&lt;code&gt;optionalHeaderValueByName&lt;/code&gt;) into another directive (one that provides a UUID). We do this by using flatMap to focus on the value in the first directive (the option returned from &lt;code&gt;optionalHeaderValueByName&lt;/code&gt;) and return another value (the UUID). With &lt;code&gt;flatMap&lt;/code&gt; we are basically &amp;#8220;repackaging&amp;#8221; one value into another package.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the updated code which properly compiles:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;def generateUuid: Directive[UUID :: HNil] = {
    //use flatMap to match on the Option returned and provide
    //a new value
    optionalHeaderValueByName(&#34;correlation-id&#34;).flatMap {
      case Some(value) =&gt; provide(UUID.fromString(value))
      case None =&gt; provide(UUID.randomUUID)
    }
  }
&lt;/pre&gt;

&lt;p&gt;and the test:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;can extract a uuid value from the header&#34; in {
      val uuid = java.util.UUID.randomUUID.toString

      Get() ~&gt; addHeader(&#34;correlation-id&#34;, uuid) ~&gt; uuidRoute ~&gt; check {
        responseAs[String] shouldEqual uuid
      }
    }
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a small tweak we&amp;#8217;ll make to our UUID directive to show another example of directive composition. If the client doesn&amp;#8217;t supply a UUID, and we call generateUUID multiple times, we&amp;#8217;ll get different uuids for the same request. This defeats the purpose of a single correlation id, and prevents us from extracting a uuid multiple times per request. A failing test shows the issue:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;can extract the same uuid twice per request&#34; in {
      var uuid1: String =&#34;&#34;
      var uuid2: String = &#34;&#34;
      Get() ~&gt; generateUuid { uuid =&gt;
        {
          uuid1 = uuid.toString
          generateUuid { another =&gt;
            uuid2 = another.toString
            complete(&#34;&#34;)
          }
        }
      } ~&gt; check {
        //fails
        uuid1 shouldEqual uuid2
      }
    }
&lt;/pre&gt;

&lt;p&gt;To fix the issue, if we generate a UUID, we will add it to the request header as if the client supplied it. We&amp;#8217;ll use the mapRequest directive to add the generated UUID to the header.&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;def generateUuid: Directive[UUID :: HNil] = {
    optionalHeaderValueByName(&#34;correlation-id&#34;).flatMap {
      case Some(value) =&gt; provide(UUID.fromString(value))
      case None =&gt;
        val id = UUID.randomUUID
        mapRequest(r =&gt; r.withHeaders(r.headers :+ RawHeader(&#34;correlation-id&#34;, id.toString))) &amp;#038; provide(id)
    }
  }
&lt;/pre&gt;

&lt;p&gt;In my first version I had the mapRequest call and the provide call on separate lines (there was no &amp;amp;). mapRequest was never being called, and it was because mapRequest was not being returned as a value- only the provide directive is returned. We need to &amp;#8220;merge&amp;#8221; these two directives with the &amp;amp; operator. &lt;code&gt;mapRequest&lt;/code&gt; is a no-op returning a Directive0 (a Directive with a Nil HList) so combining it with provide yields a Directive1[UUID], which is exactly what we want.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Running an Akka Cluster with Docker Containers</title>
          <link>http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/</link>
          <pubDate>Sun, 23 Mar 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/</guid>
          <description>

&lt;p&gt;&lt;strong&gt;Update! You can now use SBT-Docker with SBT-Native Packager for a better sbt/docker experience. &lt;a href=&#34;http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/&#34;&gt;Here&amp;#8217;s the new approach&lt;/a&gt; with an &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;updated GitHub repo&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We recently upgraded our vagrant environments to use &lt;a href=&#34;http://docker.io&#34;&gt;docker&lt;/a&gt;. One of our projects relies on &lt;a href=&#34;http://doc.akka.io/docs/akka/2.3.0/common/cluster.html&#34;&gt;akka&amp;#8217;s cluster functionality&lt;/a&gt;. I wanted to easily run an akka cluster locally using docker as sbt can be somewhat tedious. &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;The example project is on github&lt;/a&gt; and the solution is described below.&lt;/p&gt;

&lt;p&gt;The solution relies on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;Sbt Native Packager&lt;/a&gt; to package dependencies and create a startup file.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library for configuring the app&amp;#8217;s ip address and seed nodes. We setup cascading configurations that will look for docker link environment variables if present.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example/blob/master/bin/dockerize&#34;&gt;A simple bash script&lt;/a&gt; to package the app and build the docker container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;#8217;m a big fan of &lt;a href=&#34;http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library and the environment variable overrides come in handy for providing sensible defaults with optional overrides. It&amp;#8217;s the preferred way we configure our applications in upper environments.&lt;/p&gt;

&lt;p&gt;The tricky part of running an akka cluster with docker is knowing the ip address each remote node needs to listen on. An akka cluster relies on each node listening on a specific port and hostname or ip. It also needs to know the port and hostname/ip of a seed node the cluster. As there&amp;#8217;s no catch-all binding we need specific ip settings for our cluster.&lt;/p&gt;

&lt;p&gt;A simple bash script within the container will figure out the current IP for our cluster configuration and &lt;a href=&#34;http://docs.docker.io/en/latest/use/working_with_links_names/&#34;&gt;docker links&lt;/a&gt; pass seed node information to newly launched nodes.&lt;/p&gt;

&lt;h2 id=&#34;first-step-setup-application-configuration:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;First Step: Setup Application Configuration&lt;/h2&gt;

&lt;p&gt;The configuration is the same as that of a normal cluster, but I&amp;#8217;m using substitution to configure the ip address, port and seed nodes for the application. For simplicity I setup a &lt;code&gt;clustering&lt;/code&gt; block with defaults for running normally and environment variable overrides:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;clustering {
 ip = &#34;127.0.0.1&#34;
 ip = ${?CLUSTER_IP}
 port = 1600
 port = ${?CLUSTER_PORT}
 seed-ip = &#34;127.0.0.1&#34;
 seed-ip = ${?CLUSTER_IP}
 seed-ip = ${?SEED_PORT_1600_TCP_ADDR}
 seed-port = 1600
 seed-port = ${?SEED_PORT_1600_TCP_PORT}
 cluster.name = clustering-cluster
}

akka.remote {
    log-remote-lifecycle-events = on
    netty.tcp {
      hostname = ${clustering.ip}
      port = ${clustering.port}
    }
  }
  cluster {
    seed-nodes = [
       &#34;akka.tcp://&#34;${clustering.cluster.name}&#34;@&#34;${clustering.seed-ip}&#34;:&#34;${clustering.seed-port}
    ]
    auto-down-unreachable-after = 10s
  }
}
&lt;/pre&gt;

&lt;p&gt;As an example the &lt;code&gt;clustering.seed-ip&lt;/code&gt; setting will use &lt;em&gt;127.0.0.1&lt;/em&gt; as the default. If it can find a _CLUSTER&lt;em&gt;IP&lt;/em&gt; or a &lt;em&gt;SEED_PORT_1600_TCP_ADDR&lt;/em&gt; override it will use that instead. You&amp;#8217;ll notice the latter override is using docker&amp;#8217;s environment variable pattern for linking: that&amp;#8217;s how we set the cluster&amp;#8217;s seed node when using docker. You don&amp;#8217;t need the _CLUSTER&lt;em&gt;IP&lt;/em&gt; in this example but that&amp;#8217;s the environment variable we use in upper environments and I didn&amp;#8217;t want to change our infrastructure to conform to docker&amp;#8217;s pattern. The cascading settings are helpful if you&amp;#8217;re forced to follow one pattern depending on the environment. We do the same thing for the ip and port of the current node when launched.&lt;/p&gt;

&lt;p&gt;With this override in place we can use substitution to set the seed nodes in the akka cluster configuration block. The expression &lt;code&gt;&amp;quot;akka.tcp://&amp;quot;${clustering.cluster.name}&amp;quot;@&amp;quot;${clustering.seed-ip}&amp;quot;:&amp;quot;${clustering.seed-port}&lt;/code&gt; builds the proper akka URI so the current node can find the seed node in the cluster. Seed nodes avoid potential split-brain issues during network partitions. You&amp;#8217;ll want to run more than one in production but for local testing one is fine. On a final note the cluster-name setting is arbitrary. Because the name of the actor system and the uri must match I prefer not to hard code values in multiple places.&lt;/p&gt;

&lt;p&gt;I put these settings in resources/reference.conf. We could have named this file application.conf, but I prefer bundling configurations as reference.conf and reserving application.conf for external configuration files. A setting in application.conf will override a corresponding reference.conf setting and you probably want to manage application.conf files outside of the project&amp;#8217;s jar file.&lt;/p&gt;

&lt;h2 id=&#34;second-sbt-native-packager:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Second: SBT Native Packager&lt;/h2&gt;

&lt;p&gt;We use the native packager plugin to build a runnable script for our applications. For docker we just need to run &lt;code&gt;universal:stage&lt;/code&gt;, creating a folder with all dependencies in the &lt;code&gt;target/&lt;/code&gt; folder of our project. We&amp;#8217;ll move this into a staging directory for uploading to the docker container.&lt;/p&gt;

&lt;h2 id=&#34;third-the-dockerfile-and-start-script:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Third: The Dockerfile and Start script&lt;/h2&gt;

&lt;p&gt;The dockerfile is pretty simple:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;FROM dockerfile/java

MAINTAINER Michael Hamrah m@hamrah.com

ADD tmp/ /opt/app/
ADD start /opt/start
RUN chmod +x /opt/start

EXPOSE 1600

ENTRYPOINT [ &#34;/opt/start&#34; ]
&lt;/pre&gt;

&lt;p&gt;We start with Dockerfile&amp;#8217;s java base image. We then upload our staging &lt;code&gt;tmp/&lt;/code&gt; folder which has our application from sbt&amp;#8217;s native packager output and a corresponding executable start script described below. I opted for &lt;code&gt;ENTRYPOINT&lt;/code&gt; instead of &lt;code&gt;CMD&lt;/code&gt; so the container is treated like an executable. This makes it easier to pass in command line arguments into the sbt native packager script in case you want to set java system properties or override configuration settings via command line arguments.&lt;/p&gt;

&lt;p&gt;The start script is how we tee up the container&amp;#8217;s IP address for our cluster application:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

CLUSTER_IP=&lt;code&gt;/sbin/ifconfig eth0 | grep &#39;inet addr:&#39; | cut -d: -f2 | awk &#39;{ print $1}&#39;&lt;/code&gt; /opt/app/bin/clustering $@
&lt;/pre&gt;

&lt;p&gt;The script sets an inline environment variable by parsing &lt;code&gt;ifconfig&lt;/code&gt; output to get the container&amp;#8217;s ip. We then run the &lt;em&gt;clustering&lt;/em&gt; start script produced from sbt native packager. The &lt;code&gt;$@&lt;/code&gt; lets us pass along any command line settings set when launching the container into the sbt native packager script.&lt;/p&gt;

&lt;h2 id=&#34;fourth-putting-it-together:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Fourth: Putting It Together&lt;/h2&gt;

&lt;p&gt;The last part is a simple bash script named &lt;code&gt;dockerize&lt;/code&gt; to orchestrate each step. By running this script we run sbt native packager, move files to a staging directory, and build the container:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

echo &#34;Build docker container&#34;

#run sbt native packager
sbt universal:stage

#cleanup stage directory
rm -rf docker/tmp/

#copy output into staging area
cp -r target/universal/stage/ docker/tmp/

#build the container, remove intermediate nodes
docker build -rm -t clustering docker/

#remove staged files
rm -rf docker/tmp/
&lt;/pre&gt;

&lt;p&gt;With this in place we simply run&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;bin/dockerize
&lt;/pre&gt;

&lt;p&gt;to create our docker container named clustering.&lt;/p&gt;

&lt;h2 id=&#34;running-the-application-within-docker:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Running the Application within Docker&lt;/h2&gt;

&lt;p&gt;With our clustering container built we fire up our first instance. This will be our seed node for other containers:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -i -t -name seed clustering
2014-03-23 00:20:39,918 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:20:40,392 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:20:40,403 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:20:40,418 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:20:41,404 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
&lt;/pre&gt;

&lt;p&gt;Next we fire up a second node. Because of our reference.conf defaults all we need to do is link this container with the name &lt;em&gt;seed&lt;/em&gt;. Docker will set the environment variables we are looking for in the bundled reference.conf:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -name c1 -link seed:seed -i -t clustering
2014-03-23 00:22:49,332 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:22:49,788 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:22:49,797 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:22:50,238 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
2014-03-23 00:22:50,249 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:22:50,803 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.3:1600
&lt;/pre&gt;

&lt;p&gt;You&amp;#8217;ll see the current leader discovering new nodes and the appropriate broadcast messages sent out. We can even do this a third time and all nodes will react:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -name c2 -link seed:seed -i -t clustering
2014-03-23 00:24:52,768 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:24:53,224 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:24:53,235 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:24:53,470 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
2014-03-23 00:24:53,472 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.3:1600
2014-03-23 00:24:53,478 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:24:55,401 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.4:1600
&lt;/pre&gt;

&lt;p&gt;Try killing a node and see what happens!&lt;/p&gt;

&lt;h2 id=&#34;modifying-the-docker-start-script:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Modifying the Docker Start Script&lt;/h2&gt;

&lt;p&gt;There&amp;#8217;s another reason for the docker start script: it opens the door for different seed discovery options. Container linking works well if everything is running on the same host but not when running on multiple hosts. Also setting multiple seed nodes via docker links will get tedious via environment variables; it&amp;#8217;s doable but we&amp;#8217;re getting into coding-cruft territory. It would be better to discover seed nodes and set that configuration via command line parameters when launching the app.&lt;/p&gt;

&lt;p&gt;The start script gives us control over how we discover information. We could use &lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;etcd&lt;/a&gt;, &lt;a href=&#34;http://www.serfdom.io/&#34;&gt;serf&lt;/a&gt; or even zookeeper to manage how seed nodes are set and discovered, passing this to our application via environment variables or additional command line parameters. Seed nodes can easily be set via system properties set via the command line:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;-Dakka.cluster.seed-nodes.0=akka.tcp://ClusterSystem@host1:2552
-Dakka.cluster.seed-nodes.1=akka.tcp://ClusterSystem@host2:2552
&lt;/pre&gt;

&lt;p&gt;The start script can probably be configured via sbt native packager but I haven&amp;#8217;t looked into that option. Regardless this approach is (relatively) straight forward to run akka clusters with docker. The &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;full project is on github&lt;/a&gt;. If there&amp;#8217;s a better approach I&amp;#8217;d love to know!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Testing Akka’s FSM: Using setState for unit testing</title>
          <link>http://blog.michaelhamrah.com/2014/03/testing-akkas-fsm-using-setstate-for-unit-testing/</link>
          <pubDate>Fri, 21 Mar 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/03/testing-akkas-fsm-using-setstate-for-unit-testing/</guid>
          <description>&lt;p&gt;I wrote &lt;a href=&#34;http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/&#34;&gt;about Akka&amp;#8217;s Finite State Machine&lt;/a&gt; as a way to model a process. One thing I didn&amp;#8217;t discuss was testing an FSM. Akka has &lt;a href=&#34;http://doc.akka.io/docs/akka/snapshot/scala/testing.html&#34;&gt;great testing support&lt;/a&gt; and FSM&amp;#8217;s can easily be tested using the &lt;a href=&#34;http://doc.akka.io/api/akka/snapshot/index.html#akka.testkit.TestFSMRef&#34;&gt;&lt;code&gt;TestFSMRef&lt;/code&gt;&lt;/a&gt; class.&lt;/p&gt;

&lt;p&gt;An FSM is defined by its states and the data stored between those states. For each state in the machine you can match on both an incoming message and current state data. Our previous example modeled a process to check data integrity across two systems. We&amp;#8217;ll continue that example by adding tests to ensure the FSM is working correctly. &lt;a href=&#34;http://blog.michaelhamrah.com/2013/02/embracing-test-driven-development-for-speed/&#34;&gt;These should have been before we developed the FSM&lt;/a&gt; but late tests are (arguably) better than no tests.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s important to test combinations of messages against various states and data. You don&amp;#8217;t want to be in a position to run through a state machine to the state you want for every test. Luckily, there&amp;#8217;s a handy &lt;code&gt;setState&lt;/code&gt; method to explicitly set the current state and data of the FSM. This lets you &amp;#8220;fast forward&amp;#8221; the FSM to the exact state you want to test against.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s say we want to test a &lt;code&gt;DataRetrieved&lt;/code&gt; message in the &lt;code&gt;PendingComparison&lt;/code&gt; state. We also want to test this message against various &lt;code&gt;Data&lt;/code&gt; combinations. We can set this state explicitly:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;The ComparisonEngine&#34; - {
  &#34;in the PendingComparison state&#34; - {
    &#34;when a DataRetrieved message arrives from the old system&#34; - {
      &#34;stays in PendingComparison with updated data when no other data is present&#34; in {
        val fsm = TestFSMRef(new ComparisonEngine())
        
        //set our initial state with setState
        fsm.setState(PendingComparison, ComparisonStatus(&#34;someid&#34;, None, None))

        fsm ! DataRetrieved(&#34;oldSystem&#34;, &#34;oldData&#34;)

        fsm.stateName should be (PendingComparison)
        fsm.stateData should be (ComparisonStatus(&#34;someid&#34;, Some(&#34;oldData&#34;), None))
      }
    }
  }
}
&lt;/pre&gt;

&lt;p&gt;It may be tempting to send more messages to continue verifying the FSM is working correctly. This will yield a large, unwieldy and brittle test. It will make refactoring difficult and make it harder to understand what &lt;em&gt;should&lt;/em&gt; be happening.&lt;/p&gt;

&lt;p&gt;Instead, to further test the FSM, be explicit about the current state and what should happen next. Add a test for it:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;when a DataRetrieved message arrives from the old system&#34; - {
  &#34;moves to AllRetrieved when data from the new system is already present&#34; in {
    val fsm = TestFSMRef(new ComparisonEngine())
        
    //set our initial state with setState
    fsm.setState(PendingComparison, ComparisonStatus(&#34;someid&#34;, None, Some(&#34;newData&#34;)))

    fsm ! DataRetrieved(&#34;oldSystem&#34;, &#34;oldData&#34;)
    fsm.stateName should be (AllRetrieved)
    fsm.stateData should be (ComparisonStatus(&#34;someid&#34;, Some(&#34;oldData&#34;), Some(&#34;newData&#34;)))
  }
}
&lt;/pre&gt;

&lt;p&gt;By mixing in ImplicitSender or using TestProbes we can also verify messages the FSM should be sending in response to incoming messages.&lt;/p&gt;

&lt;p&gt;Testing is an essential part of developing applications. Unit tests should be explicit and granular. For higher level orchestration integration tests, taking a black-box approach, provide ways to oversee entire processes. Don&amp;#8217;t let your code become too unwieldy to manage: use the tools at your disposal and good coding practices to stay lean. Akka&amp;#8217;s FSM provides ways of programming transitional behavior over time and Akka&amp;#8217;s FSM Testkit support provides a way of ensuring that code works over time.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Using Typesafe’s Config for Scala (and Java) for Application Configuration</title>
          <link>http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/</link>
          <pubDate>Sun, 23 Feb 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/</guid>
          <description>

&lt;p&gt;I recently leveraged &lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library to refactor configuration settings for a project. I was very pleased with the API and functionality of the library.&lt;/p&gt;

&lt;p&gt;The documentation is pretty solid so there&amp;#8217;s no need to go over basics. One feature I like is the clear hierarchy when specifying configuration values. I find it helpful to put as much as possible in a reference.conf file in the /resources directory for an application or library. These can get overridden in a variety of ways, primarily by adding an application.conf file to the bundled output&amp;#8217;s classpath. The &lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;sbt native packager&lt;/a&gt;, helpful for deploying applications, makes it easy to attach a configuration file to an output. This is helpful if you have settings which you normally wouldn&amp;#8217;t want to use during development, say using remote actors with akka. I find placing a reasonable set of defaults in a reference.conf file allows you to easily transport a configuration around while still overriding it as necessary. Otherwise you can get into copy and paste hell by duplicating configurations across multiple files for multiple environments.&lt;/p&gt;

&lt;h2 id=&#34;alternative-overrides:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Alternative Overrides&lt;/h2&gt;

&lt;p&gt;There are two other interesting ways you can override configuration settings: using environment variables or java system properties. The environment variable approach comes in very handy when pushing to cloud environments where you don&amp;#8217;t know what a configuration is beforehand. Using the ${?VALUE} pattern a property will only be set if a value exists. This allows you to provide an option for overriding a value without actually having to specify one.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s an example in a conf file using substitution leveraging this technique:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http {
 port = 8080
 port = ${?HTTP_PORT}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;re setting a default port of 8080. If the configuration can find a valid substitute it will replace the port value with the substitute; otherwise, it will keep it at 8080. The configuration library will look up its hierarchy for an HTTP_PORT value, checking other configuration files, Java system properties, and finally environment variables. Environment variables aren&amp;#8217;t perfect, but they&amp;#8217;re easy to set and leveraged in a lot of places. If you leave out the ? and just have ${HTTP_PORT} then the application will throw an exception if it can&amp;#8217;t find a value. But by using the ? you can override as many times as you want. This can be helpful when running apps on Heroku where environment variables are set for third party services.&lt;/p&gt;

&lt;h3 id=&#34;using-java-system-properties:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Using Java System Properties&lt;/h3&gt;

&lt;p&gt;Java system properties provide another option for setting config values. The shell script created by sbt-native-packager supports java system properties, so you can also set the http port via the command line using the -D flag:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/bash_script_from_native_packager -Dhttp.port=8081
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be helpful if you want to run an akka based application with a different log level to see what&amp;#8217;s going on in production:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/some_akka_app_script -Dakka.loglevel=debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately sbt run doesn&amp;#8217;t support java system properties so you can&amp;#8217;t tweak settings with the command line when running sbt. The &lt;a href=&#34;https://github.com/spray/sbt-revolver&#34;&gt;sbt-revolver&lt;/a&gt; plugin, which allows you to run your app in a forked JVM, does allow you to pass java arguments using the command line. Once you&amp;#8217;re set up with this plugin you can change settings by adding your Java overrides after &lt;code&gt;---&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;re-start --- -Dhttp.port=8081
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;with-c3p0:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;With c3p0&lt;/h3&gt;

&lt;p&gt;I was really excited to see that the &lt;a href=&#34;http://www.mchange.com/projects/c3p0/#c3p0_conf&#34;&gt;c3p0 connection pool library also supports Typesafe Config&lt;/a&gt;. So you can avoid those annoying xml-based files and merge your c3p0 settings directly with your regular configuration files. I&amp;#8217;ve migrated an application to a &lt;a href=&#34;docker.io&#34;&gt;docker&lt;/a&gt; based development environment and used this c3p0 feature with &lt;a href=&#34;http://docs.docker.io/en/latest/use/working_with_links_names/&#34;&gt;docker links&lt;/a&gt; to set mysql settings:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app {
 db {
  host = localhost
  host = ${?DB_PORT_3306_TCP_ADDR}
  port = &amp;quot;3306&amp;quot;
  port = ${?DB_PORT_3306_TCP_PORT}
 }
}

c3p0 {
 named-configs {
  myapp {
      jdbcUrl = &amp;quot;jdbc:mysql://&amp;quot;${app.db.host}&amp;quot;:&amp;quot;${app.db.port}&amp;quot;/MyDatabase&amp;quot;
  }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When I link a mysql container to my app container with &lt;code&gt;--link mysql:db&lt;/code&gt; Docker will inject the DB_PORT_3306_TCP_* environment variables which are pulled by the above settings.&lt;/p&gt;

&lt;h3 id=&#34;accessing-values-from-code:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Accessing Values From Code&lt;/h3&gt;

&lt;p&gt;One other practice I like is having a single &amp;#8220;Config&amp;#8221; class for an application. It can be very tempting to load a configuration node from anywhere in your app but that can get messy fast. Instead, create a config class and access everything you need through that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;object MyAppConfig {
  private val config =  ConfigFactory.load()

  private lazy val root = config.getConfig(&amp;quot;my_app&amp;quot;)

  object HttpConfig {
    private val httpConfig = config.getConfig(&amp;quot;http&amp;quot;)

    lazy val interface = httpConfig.getString(&amp;quot;interface&amp;quot;)
    lazy val port = httpConfig.getInt(&amp;quot;port&amp;quot;)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Type safety, Single Responsibility, and no strings all over the place.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;When dealing with configuration think about what environments you have and what the actual differences are between those environments. Usually this is a small set of differing values for only a few properties. Make it easy to change just those settings without changing&amp;#8211;or duplicating&amp;#8211;anything else. This could done via environment variables, command line flags, even loading configuration files from a url. Definitely avoid copying the same value across multiple configurations: just distill that value down to a lower setting in a hierarchy. By minimizing configuration files you&amp;#8217;ll be making your life a lot easier.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re developing an app for distribution, or writing a library, providing a well-documented configuration file (&lt;a href=&#34;https://github.com/spray/spray/blob/master/spray-can/src/main/resources/reference.conf&#34;&gt;spray&amp;#8217;s spray-can reference.conf is an excellent example&lt;/a&gt;) you can allow users to override defaults easily in a manner that is suitable for them and their runtimes.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Using Akka’s ClusterClient</title>
          <link>http://blog.michaelhamrah.com/2014/02/using-akkas-clusterclient/</link>
          <pubDate>Wed, 05 Feb 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/02/using-akkas-clusterclient/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve been spending some time implementing a feature which leverages Akka&amp;#8217;s &lt;a href=&#34;http://doc.akka.io/docs/akka/2.2.3/contrib/cluster-client.html&#34;&gt;ClusterClient&lt;/a&gt; (&lt;a href=&#34;http://doc.akka.io/api/akka/2.2.3/index.html#akka.contrib.pattern.ClusterClient&#34;&gt;api docs&lt;/a&gt;). A ClusterClient can be useful if:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You are running a service which needs to talk to another service in a cluster, but you don&amp;#8217;t that service to be in the cluster (cluster roles are another option for interconnecting services where separate hosts are necessary but I&amp;#8217;m not sold on them just yet).&lt;/li&gt;
&lt;li&gt;You don&amp;#8217;t want the overhead of running an http client/server interaction model between these services, but you&amp;#8217;d like similar semantics. Spray is a great akka framework for api services but you may not want to write a Spray API or use an http client library.&lt;/li&gt;
&lt;li&gt;You want to use the same transparency of local-to-remote actors but don&amp;#8217;t want to deal with remote actorref configurations to specific hosts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The documentation was a little thin on some specifics so getting started wasn&amp;#8217;t as smooth sailing as I&amp;#8217;d like. Here are some gotchas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You only need &lt;code&gt;akka.extensions = [&amp;quot;akka.contrib.pattern.ClusterReceptionistExtension&amp;quot;]&lt;/code&gt; on the Host (Server) Cluster. (If your client isn&amp;#8217;t a cluster, you&amp;#8217;ll get a runtime exception).&lt;/li&gt;
&lt;li&gt;&amp;#8220;Receptionist&amp;#8221; is the default name for the Host Cluster actor managing ClusterClient connections. Your ClusterClient connects first to the receptionist (via the set of initial contacts) then can start sending messages to actors in the Host Cluster. The name is configurable.&lt;/li&gt;
&lt;li&gt;The client actor system using the ClusterClient needs to have a Netty port open. You must use either actor.cluster.ClusterActorRefProvider or actor.remote.RemoteActorRefProvider. Otherwise the Host Cluster and ClusterClient can&amp;#8217;t establish proper communication. You can use the ClusterActorRefProvider on the client even you&amp;#8217;re not running a cluster.&lt;/li&gt;
&lt;li&gt;As a ClusterClient you wrap messages with a ClusterClient.send (or sendAll) message first. (I was sending vanilla messages and they weren&amp;#8217;t going through, but this is in the docs).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ClusterClients are worth checking out if you want to create physically separate yet interconnected systems but don&amp;#8217;t want to go through the whole load-balancer or http-layer setup. Just another tool in the Akka toolbelt!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>First Class Function Example in Scala and Go</title>
          <link>http://blog.michaelhamrah.com/2014/01/first-class-function-example-in-scala-and-go/</link>
          <pubDate>Mon, 20 Jan 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/01/first-class-function-example-in-scala-and-go/</guid>
          <description>&lt;p&gt;Go and Scala both make functions first-class citizens of their language. I recently had to recurse a directory tree in Go and came across the &lt;a href=&#34;http://golang.org/pkg/path/filepath/#Walk&#34;&gt;Walk&lt;/a&gt; function which exemplifies first-class functions. The Walk function talks a path to start a directory traversal and calls a function WalkFunc for everything it finds in the sub-tree:&lt;/p&gt;

&lt;pre class=&#34;syntax go&#34;&gt;func Walk(root string, walkFn WalkFunc) error &lt;/pre&gt;

&lt;p&gt;If you&amp;#8217;re coming from the &lt;a href=&#34;http://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html&#34;&gt;Kingdom of Nouns&lt;/a&gt; you may assume WalkFunc is a class or interface with a method for Walk to call. But that cruft is gone; WalkFunc is just a regular function with a defined signature given its own type, WalkFunc:&lt;/p&gt;

&lt;pre class=&#34;syntax go&#34;&gt;type WalkFunc func(path string, info os.FileInfo, err error) error
&lt;/pre&gt;

&lt;p&gt;Why is this interesting? I wasn&amp;#8217;t surprised Go would have a built-in method for crawling a directory tree. It&amp;#8217;s a pretty common scenario, and I&amp;#8217;ve written similar code many times before. What&amp;#8217;s uncommon about directory crawling is what you want to do with those files: open them up, move them around, inspect them. Separating the common from the uncommon is where first-class functions come into play. How much code have you had to write to just write the code you want?&lt;/p&gt;

&lt;p&gt;Scala hides the OOP-ness of its underlying runtime by compile-time tricks, putting a first-class function like:&lt;/p&gt;

&lt;pre class=&#34;syntax go&#34;&gt;val walkFunc = (file: java.io.File) =&gt; { /* do something with the file */ }
&lt;/pre&gt;

&lt;p&gt;into a class of &lt;a href=&#34;http://www.scala-lang.org/api/current/index.html#scala.Function1&#34;&gt;Function1&lt;/a&gt;. C# does something similar with its various &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/bb534960(v=vs.110).aspx&#34;&gt;function classes&lt;/a&gt; and delegate constructs. Go makes the interesting design decision of forcing function declarations outside of structs, putting an emphasis on stand-alone functions and struct extensibility. There are no classes in Go to encapsulate functions.&lt;/p&gt;

&lt;p&gt;We can write a walk method for our walkFunc in Scala by creating a method which takes a function as a parameter (methods and functions have nuanced differences in Scala, but don&amp;#8217;t worry about it):&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;object FileUtil {
  def walk(file: File, depth: Int, walkFunc: (File, Int) =&gt; Unit): Unit = {
    walkFunc(file, depth)
    Option(file.listFiles).map(_.map(walk(_, depth + 1, walkFunc)))
  }
}
&lt;/pre&gt;

&lt;p&gt;In our Scala walk function we added a depth parameter which tracks how deep you are in the stack. We&amp;#8217;re also wrapping the listFiles method in an Option to avoid a possible null pointer exception.&lt;/p&gt;

&lt;p&gt;We can tweak our walkFunc and use our Scala walk function:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;import FileUtil._
val walkFunc = (path: File, depth: Int) =&gt; { println(s&#34;$depth, ${path}&#34;) }
walk(new File(&#34;/path/to/dir&#34;), 0, walkFunc)
&lt;/pre&gt;

&lt;p&gt;Because typing (File, Int) =&amp;gt; Unit is somewhat obscure, type aliases come in handy. We can refactor this with a type alias:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;type WalkFunc = (File, Int) =&gt; Unit
&lt;/pre&gt;

&lt;p&gt;And update our walk method accordingly:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;def walk(file: File, depth: Int, walkFunc: WalkFunc): Unit = { ... }
&lt;/pre&gt;

&lt;p&gt;First class functions are powerful constructs making code flexible and succinct. If all you need is to call a function than pass that function as a parameter to your method. Just as classes have the &lt;a href=&#34;http://en.wikipedia.org/wiki/Single_responsibility_principle&#34;&gt;single responsibility principle&lt;/a&gt; functions can have them too; avoid doing too much at once like file crawling and file processing. Instead pass a file processor call to your file crawling function.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Programming Akka’s Finite State Machines in Scala</title>
          <link>http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/</link>
          <pubDate>Thu, 16 Jan 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/</guid>
          <description>&lt;p&gt;Over the past few months my team has been building a new suite of services using Scala and Akka. An interesting aspect of Akka&lt;/p&gt;

&lt;p&gt;we leverage is its &lt;a href=&#34;http://doc.akka.io/docs/akka/2.2.3/scala/fsm.html&#34;&gt;Finite State Machine&lt;/a&gt; support. Finite State Machines&lt;/p&gt;

&lt;p&gt;are a staple of computer programming although not often used in practice. A conceptual process can usually be represented with a finite state machine: there are a defined number of states with explicit transitions between states. If we have a vocabulary&lt;/p&gt;

&lt;p&gt;around these states and transitions we can program the state machine.&lt;/p&gt;

&lt;p&gt;A traditional implementation of an FSM is to check and maintain state explicitly via if/else conditions, use the state design pattern, or implement some other construct. Using Akka&amp;#8217;s FSM support, which explicitly defines states and offers transition hooks, allows us to easily implement our conceptual model of a process. FSM is built on top of Akka&amp;#8217;s actor model giving excellent concurrency controls so we can run many of these state machines simultaneously. You can implement your own FSM with Akka&amp;#8217;s normal actor behavior with the &lt;em&gt;become&lt;/em&gt; method to change the partial function handling messages. However FSM offers some nice hooks plus data management in addition to just changing behavior.&lt;/p&gt;

&lt;p&gt;As an example we will use Akka&amp;#8217;s FSM support to check data in two systems. Our initial process is fairly simplistic but provides a good overview of leveraging Finite State Machines. Say we are rolling out a new system and we want to ensure data flows to both the old and new system. We need a process which waits a certain&lt;/p&gt;

&lt;p&gt;amount of time for data to appear in both places. If data is found in both systems we will check the data for consistency,&lt;/p&gt;

&lt;p&gt;if data is never found after a threshold we will alert data is out of sync.&lt;/p&gt;

&lt;p&gt;Based on our description we have four states. We define our states using a sealed trait:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;sealed trait ComparisonStates
case object AwaitingComparison extends ComparisonStates
case object PendingComparison extends ComparisonStates
case object AllRetrieved extends ComparisonStates
case object DataUnavailable extends ComparisonStates
&lt;/pre&gt;

&lt;p&gt;Next we define the data we manage between state transitions. We need to manage an identifier with data from&lt;/p&gt;

&lt;p&gt;the old and new system. Again we use a sealed trait:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;sealed trait Data
case object Uninitialized extends Data
case class ComparisonStatus(id: String, oldSystem: Option[SomeData] = None, newSystem: Option[SomeData] = None) extends Data
&lt;/pre&gt;

&lt;p&gt;A state machine is just a normal actor with the FSM trait mixed in. We declare our&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;ComparisonEngine&lt;/pre&gt;

&lt;p&gt;actor with FSM support,&lt;/p&gt;

&lt;p&gt;specifying our applicable state and data types:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;class ComparisonEngine extends Actor with FSM[ComparisonStates, Data] {
}
&lt;/pre&gt;

&lt;p&gt;Instead of handling messages directly in a receive method FSM support creates an additional layer of messaging handling.&lt;/p&gt;

&lt;p&gt;When using FSM you match on both message and current state. Our FSM only handles two messages: &lt;em&gt;Compare(id: Int)&lt;/em&gt; and&lt;/p&gt;

&lt;p&gt;&lt;em&gt;DataRetrieved(system: String, someData: SomeData)&lt;/em&gt;. You can construct your data types and messages any way&lt;/p&gt;

&lt;p&gt;you please. I like to keep states abstract as we can generalize on message handling. This&lt;/p&gt;

&lt;p&gt;prevents us from dealing with too many states and state transitions.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s start implementing the body of our &lt;em&gt;ComparisonEngine&lt;/em&gt;. We will start with our initial state:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;startWith(AwaitingComparison, Uninitialized)

when(AwaitingComparison) {
  case Event(Compare(id), Uninitialized) =&gt;
    goto(PendingComparison) using ComparisonStatus(id)
}
&lt;/pre&gt;

&lt;p&gt;We simply declare our initial state is AwaitingComparison, and the only message we are willing to process is a Compare.&lt;/p&gt;

&lt;p&gt;When we receive this message we go to a new state&amp;#8211;PendingComparison&amp;#8211;and set some data. Notice how we aren&amp;#8217;t actually doing anything else?&lt;/p&gt;

&lt;p&gt;A great aspect of FSM is the ability to listen on state transitions. This allows us to separate state transition logic from state transition&lt;/p&gt;

&lt;p&gt;actions. When we transition from an initial state to a PendingComparison state we want to ask our two systems for data. We simply match&lt;/p&gt;

&lt;p&gt;on state transitions and add our applicable logic:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;onTransition {
    case AwaitingComparison -&gt; PendingComparison =&gt;
      nextStateData match {
        case ComparisonStatus(id, old, new) =&gt; {
          oldSystemChecker ! VerifyData(id)
          newSystemChecker ! VerifyData(id)
        }
      }
    }
&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;oldSystemChecker&lt;/em&gt; and &lt;em&gt;newSystemChecker&lt;/em&gt; are actors responsible for verifying data in their respective systems. These can be passed in to the FSM as constructor arguments, or you can have the FSM create the actors and supervise their lifecycle.&lt;/p&gt;

&lt;p&gt;These two actors will send a DataRetrieved message back to our FSM when data is present. Because we are now in the &lt;em&gt;PendingComparison&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;state we specify our new state transition actions against a set of possible scenarios:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;when(PendingComparison, stateTimeout = 15 minutes) {
  case Event(DataRetrieved(&#34;old&#34;, old), ComparisonStatus(id, _, None)) =&gt; {
    stay using ComparisonStatus(id, Some(old), None)
  }
  case Event(DataRetrieved(&#34;new&#34;, new), ComparisonStatus(id, None, _)) =&gt; {
    stay using ComparisonStatus(id, None, Some(new))
  }
  case Event(StateTimeout, c: ComparisonStatus) =&gt; {
    goto(IdUnavailable) using c
  }
  case Event(DataRetrieved(system, data), cs @ ComparisonStatus(_, _, _)) =&gt; {
    system match {
      case &#34;old&#34; =&gt; goto(AllRetrieved) using cs.copy(old = Some(data))
      case &#34;new&#34; =&gt; goto(AllRetrieved) using cs.copy(new = Some(data))
    }
  }
}
&lt;/pre&gt;

&lt;p&gt;Our snippet says we will wait 15 minutes for our systemChecker actors to return with data, otherwise, we&amp;#8217;ll timeout and go to the unavailable state. Either the old&lt;/p&gt;

&lt;p&gt;system or new system will return first, in which case, one set of data in our ComparisonStatus will be None. So we stay in the PendingComparison state until&lt;/p&gt;

&lt;p&gt;the other system returns. If our previous pattern matches do not match, we know the current message we are processing is the final message. Notice how we don&amp;#8217;t care how these actors are getting their data. That&amp;#8217;s the responsibility of the child actors.&lt;/p&gt;

&lt;p&gt;Once we have all our data,&lt;/p&gt;

&lt;p&gt;so we go to the AllRetrieved state with the data from the final message.&lt;/p&gt;

&lt;p&gt;There are a couple of ways we could have defined our states. We could have a state for the oldSystem returned or newSystem returned. I find it easier to&lt;/p&gt;

&lt;p&gt;create a generic &lt;em&gt;PendingComparison&lt;/em&gt; state to keep our pattern matching for pending comparisons consolidated in a single partial function.&lt;/p&gt;

&lt;p&gt;Our final states are pretty simple: we just stop our state machine!&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;when(IdUnavailable) {
  case Event(_, _) =&gt; {
    stop
  }
}
when(AllRetrieved) {
  case Event(_, _) =&gt; {
    stop
  }
}
&lt;/pre&gt;

&lt;p&gt;Our last step is to add some more onTransition checks to handle our final states:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;case PendingComparison -&gt; AllRetrieved =&gt;
    nextStateData match {
      case ComparisonStatus(id, old, new) =&gt; {
        //Verification logic
     }
   }
 case _ -&gt; IdUnavailable =&gt;
   nextStateData match {
     case ComparisonStatus(id, old, new) =&gt; {
      //Handle timeout
      }
   }
&lt;/pre&gt;

&lt;p&gt;We don&amp;#8217;t care how we got to the &lt;em&gt;AllRetrieved&lt;/em&gt; state; we just know we are there and we have the data we need. We can offload our verification logic&lt;/p&gt;

&lt;p&gt;to another actor or inline it within our FSM as necessary.&lt;/p&gt;

&lt;p&gt;Implementing processing workflows can be tricky involving a lot of boilerplate code. Conditions must be checked, timeouts handled, error handling implemented.&lt;/p&gt;

&lt;p&gt;The Akka FSM approach provides a foundation for implementing workflow based processes on top of Akka&amp;#8217;s great supervision support. We create a ComparisonEngine&lt;/p&gt;

&lt;p&gt;for every piece of data we need to check. If an engine dies we can supervise and restart. My favorite feature is the separation of what causes a state transition&lt;/p&gt;

&lt;p&gt;with what happens during a state transition. Combined with isolated behavior amongst actors this creates a cleaner, isolated and composable application to manage.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Overview on Web Performance and Scalability</title>
          <link>http://blog.michaelhamrah.com/2013/10/overview-on-web-performance-and-scalability/</link>
          <pubDate>Sat, 12 Oct 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/10/overview-on-web-performance-and-scalability/</guid>
          <description>&lt;p&gt;I recently gave a talk to some junior developers on performance and scalability. The talk is relatively high-level, providing an overview of non-programming topics which are important for performance and scalability. The &lt;a href=&#34;http://michaelhamrah.com/perf/#/&#34;&gt;original deck is here&lt;/a&gt; and on &lt;a href=&#34;https://speakerdeck.com/mhamrah/things-to-know-about-web-performance&#34;&gt;speaker deck&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A few months ago I also &lt;a href=&#34;http://michaelhamrah.com/spdy/&#34;&gt;gave a talk on spdy&lt;/a&gt; which is also on &lt;a href=&#34;https://speakerdeck.com/mhamrah/intro-to-spdy&#34;&gt;speaker deck&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Spray API Development: Getting Started with a Spray Web Service Using JSON</title>
          <link>http://blog.michaelhamrah.com/2013/06/scala-web-apis-up-and-running-with-spray-and-akka/</link>
          <pubDate>Sat, 22 Jun 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/06/scala-web-apis-up-and-running-with-spray-and-akka/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;spray.io&#34;&gt;Spray&lt;/a&gt; is a great library for building http api&amp;#8217;s with Scala. Just like &lt;a href=&#34;playframework.com&#34;&gt;Play!&lt;/a&gt; it&amp;#8217;s built with &lt;a href=&#34;akka.io&#34;&gt;Akka&lt;/a&gt; and provides numerous low and high level tools for http servers and clients. It puts Akka and Scala&amp;#8217;s asynchronous programming model first for high performance, composable application development.&lt;/p&gt;

&lt;p&gt;I wanted to highlight the &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-routing/&#34;&gt;spray-routing&lt;/a&gt; library which provides a nice DSL for defining web services. The routing library can be used with the standalone &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-can/#spray-can&#34;&gt;spray-can&lt;/a&gt; http server or in any servlet container.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll highlight a simple entity endpoint, unmarshalling Json data into an object and deferring actual process to another Akka actor. To get started with your own spray-routing project, I created a &lt;a href=&#34;https://github.com/n8han/giter8&#34;&gt;giter8&lt;/a&gt; template to bootstrap your app:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$g8 mhamrah/sbt -b spray&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://spray.io/documentation/&#34;&gt;The documentation&lt;/a&gt; is quite good and &lt;a href=&#34;https://github.com/spray/spray&#34;&gt;the source code is worth browsing&lt;/a&gt;. For a richer routing example check out &lt;a href=&#34;https://github.com/spray/spray/tree/release/1.1/examples/spray-routing/on-spray-can&#34;&gt;Spray&amp;#8217;s own routing project&lt;/a&gt; which shows off http-streaming and a few other goodies.&lt;/p&gt;

&lt;h2 id=&#34;creating-a-server:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Creating a Server&lt;/h2&gt;

&lt;p&gt;We are going to create three main structures: An actor which contains our Http Service, a trait which contains our route definition, and a Worker actor that will do the work of the request.&lt;/p&gt;

&lt;p&gt;The service actor is launched in your application&amp;#8217;s main method. Here we are using Scala&amp;#8217;s App class to launch our server feeding in values from &lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;typesafe config&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
val service= system.actorOf(Props[SpraySampleActor], &amp;quot;spray-sample-service&amp;quot;)
IO(Http) ! Http.Bind(service, system.settings.config.getString(&amp;quot;app.interface&amp;quot;), system.settings.config.getInt(&amp;quot;app.port&amp;quot;))

println(&amp;quot;Hit any key to exit.&amp;quot;)
val result = readLine()
system.shutdown()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because Spray is based on Akka, we are just creating a standard actor system and passing our service to &lt;a href=&#34;http://doc.akka.io/docs/akka/snapshot/scala/io.html&#34;&gt;Akka&amp;#8217;s new IO library&lt;/a&gt;. This is the high performance foundation for our service built on the spray-can server.&lt;/p&gt;

&lt;h2 id=&#34;the-service-actor:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;The Service Actor&lt;/h2&gt;

&lt;p&gt;Our service actor is pretty lightweight, as the functionality is deferred to our route definition in the HttpService trait. We only need to set the actorRefFactory and call runRoutes from our trait. You could simply set routes directly in this class, but the separation has its benefits, primarily for testing.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
class SpraySampleActor extends Actor with SpraySampleService with SprayActorLogging {
  def actorRefFactory = context
  def receive = runRoute(spraysampleRoute)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-service-trait-8211-spray-8217-s-routing-dsl:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;The Service Trait &amp;#8211; Spray&amp;#8217;s Routing DSL&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-routing/key-concepts/routes/&#34;&gt;Spray&amp;#8217;s Routing DSL&lt;/a&gt; is where Spray really shines. It is similar to Sinatra inspired web frameworks like Scalatra, but relies on composable function elements so requests pass through a series of actions similar to &lt;a href=&#34;http://unfiltered.databinder.net/&#34;&gt;Unfiltered&lt;/a&gt;. The result is an easy to read syntax for routing and the Dont-Repeat-Yourself of composable functions.&lt;/p&gt;

&lt;p&gt;To start things off, we&amp;#8217;ll create a simple get/post operation at the /entity path:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
trait SpraySampleService extends HttpService {
  val spraysampleRoute = {
    path(&amp;quot;entity&amp;quot;) {
      get { 
        complete(&amp;quot;list&amp;quot;)
      } ~
      post {
        complete(&amp;quot;create&amp;quot;)
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The path, get and complete operations are &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-routing/key-concepts/directives/#directives&#34;&gt;Directives&lt;/a&gt;, the building blocks of Spray routing. Directives take the current http request and process a particular action against it. The above snippet doesn&amp;#8217;t much except filter the request on the current path and the http action. The path directive also lets you pull out path elements:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
path (&amp;quot;entity&amp;quot; / Segment) { id =&amp;gt;
    get {
      complete(s&amp;quot;detail ${id}&amp;quot;)
    } ~
    post {
      complete(s&amp;quot;update ${id}&amp;quot;)
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a number ways to pull out elements from a path. Spray&amp;#8217;s unit tests are the best way to explore the possibilities.&lt;/p&gt;

&lt;p&gt;You can use curl to test the service so far:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!bash
curl -v http://localhost:8080/entity
curl -v http://localhost:8080/entity/1234
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;unmarshalling:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Unmarshalling&lt;/h2&gt;

&lt;p&gt;One of the nice things about Spray&amp;#8217;s DSL is how function composition allows you to build up request handling. In this snippet we use json4s support to unmarshall the http request into a JObject:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
/* We need an implicit formatter to be mixed in to our trait */
object Json4sProtocol extends Json4sSupport {
  implicit def json4sFormats: Formats = DefaultFormats
}

trait SpraySampleService extends HttpService {
  import Json4sProtocol._

  val spraysampleRoute = {
    path(&amp;quot;entity&amp;quot;) {
      /* ... */
      post {
        entity(as[JObject]) { someObject =&amp;gt;
          doCreate(someObject)
        }
      } 
     /* ... */
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We use the Entity to directive to unmarshall the request, which finds the implicit json4s serializer we specified earlier. SomeObject is set to the JObject produced, which is passed to our yet-to-be-built doCreate method. If Spray can&amp;#8217;t unmarshall the entity an error is returned to the client.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s a curl command that sets the http method to POST and applies the appropriate header and json body:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!bash
curl -v -X POST http://localhost:8080/entity -H &amp;quot;Content-Type: application/json&amp;quot; -d &amp;quot;{ \&amp;quot;property\&amp;quot; : \&amp;quot;value\&amp;quot; }&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;leveraging-akka-and-futures:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Leveraging Akka and Futures&lt;/h2&gt;

&lt;p&gt;We want to keep our route structure clean, so we defer actual work to another Akka worker. Because Spray is built with Akka this is pretty seamless. We need to create our ActorRef to send a message. We&amp;#8217;ll also implement our doCreate function called within the earlier POST /entity directive:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
//Our worker Actor handles the work of the request.
val worker = actorRefFactory.actorOf(Props[WorkerActor], &amp;quot;worker&amp;quot;)

def doCreate[T](json: JObject) = {
  //all logic must be in the complete directive
  //otherwise it will be run only once on launch
  complete {
    //We use the Ask pattern to return
    //a future from our worker Actor,
    //which then gets passed to the complete
    //directive to finish the request.
    (worker ? Create(json))
                .mapTo[Ok]
                .map(result =&amp;gt; s&amp;quot;I got a response: ${result}&amp;quot;)
                .recover { case _ =&amp;gt; &amp;quot;error&amp;quot; }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a couple of things going on here. Our worker class is looking for a Create message, which we send to the actor with the ask (?) pattern. The ask pattern lets us know the task completed so we call then tell the client. When we get the Ok message we simply return the result; in the case of an error we return a short message. The response future returned is passed to Spray&amp;#8217;s complete directive, which will then complete the request to the client. There&amp;#8217;s no blocking occurring in this snippet: we are just wiring up futures and functions.&lt;/p&gt;

&lt;p&gt;Our worker doesn&amp;#8217;t do much but out the message contents and return a random number:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
class WorkerActor extends Actor with ActorLogging {
import WorkerActor._

def receive = {
  case Create(json) =&amp;gt; {
    log.info(s&amp;quot;Create ${json}&amp;quot;)
    sender ! Ok(util.Random.nextInt(10000))
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can view how the entire request is handled &lt;a href=&#34;https://github.com/mhamrah/spray-sample/blob/master/src/main/scala/Boot.scala&#34;&gt;by viewing the source file&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;wrapping-up:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Wrapping Up&lt;/h2&gt;

&lt;p&gt;Reading the documentation and exploring the unit tests are the best way to understand the power of Spray&amp;#8217;s routing DSL. The performance of the standalone &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-can/&#34;&gt;spray-can&lt;/a&gt; service is outstanding, and the Akka platform adds resiliency through its lifecycle management tools. Akka&amp;#8217;s remoting feature allows systems to build out their app tiers. A project I&amp;#8217;m working on is using Spray and Akka to publish messages to a pub/sub system for downstream request handling. It&amp;#8217;s an excellent platform for high performance API development. &lt;a href=&#34;https://github.com/mhamrah/spray-sample&#34;&gt;Full spray-sample is on GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Updating Flickr Photos with Gpx Data using Scala: Getting Started</title>
          <link>http://blog.michaelhamrah.com/2013/05/updating-flickr-photos-with-gpx-data-using-scala-getting-started/</link>
          <pubDate>Sun, 05 May 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/05/updating-flickr-photos-with-gpx-data-using-scala-getting-started/</guid>
          <description>

&lt;p&gt;If you read this blog you know I&amp;#8217;ve just returned from six months of travels around Asia, documented on our tumblr, &lt;a href=&#34;http://thegreatbigadventure.tumblr.com&#34;&gt;The Great Big Adventure&lt;/a&gt; with photos on &lt;a href=&#34;http://flickr.com/hamrah&#34;&gt;Flickr&lt;/a&gt;. Even though my camera doesn&amp;#8217;t have a GPS, I realized toward the second half of the trip I could mark GPS waypoints and write a program to link that data later. I decided to write this little app in Scala, a language I&amp;#8217;ve been learning since my return. The app is still a work in progress, but instead of one long post I&amp;#8217;ll spread it out as I go along.&lt;/p&gt;

&lt;h2 id=&#34;the-workflow:57d2935fd637ebe85b97477296b70272&#34;&gt;The Workflow&lt;/h2&gt;

&lt;p&gt;When I took a photo I usually marked the location with a waypoint in my GPS. I accumulated a set of around 1000 of these points spread out over three gpx (xml) files. My plan is to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read in the three gpx files and combine them into a distinct list.&lt;/li&gt;
&lt;li&gt;For each day I have at least one gpx point, get all of my flickr images for that data.&lt;/li&gt;
&lt;li&gt;For each image, find the waypoint timestamp with the least difference in time.&lt;/li&gt;
&lt;li&gt;Update that image with the waypoint data on Flickr.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;getting-started:57d2935fd637ebe85b97477296b70272&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;If you&amp;#8217;re going to be doing anything with Scala, learning &lt;a href=&#34;http://scala-sbt.org&#34;&gt;sbt&lt;/a&gt; is essential. Luckily, it&amp;#8217;s pretty straightforward, but the documentation across the internet is somewhat inconsistent. As of this writing, &lt;a href=&#34;http://twitter.github.io/scala_school/sbt.html&#34;&gt;Twitter&amp;#8217;s Scala School SBT Documentation&lt;/a&gt;, which I used as a reference to get started, incorrectly states that SBT creates a template for you. It no longer does, with the preferred approach to use &lt;a href=&#34;https://github.com/n8han/giter8&#34;&gt;giter8&lt;/a&gt;, an excellent templating tool. I created &lt;a href=&#34;https://github.com/mhamrah/sbt.g8&#34;&gt;my own simplified version&lt;/a&gt; which is based off of the excellently documented &lt;a href=&#34;https://github.com/ymasory/sbt.g8&#34;&gt;template by Yuvi Masory&lt;/a&gt;. Some of the versions in build.sbt are a outdated, but it&amp;#8217;s worthwhile reading through the code to get a feel for the Scala and SBT ecosystem. The g8 project also contains a good working example of custom sbt commands (like g8-test). One gotcha with SBT: if you change your build.sbt file, you must call &lt;em&gt;reload&lt;/em&gt; in the sbt console. Otherwise, your new dependencies will not be picked up. For rubyists this is similar to running &lt;em&gt;bundle update&lt;/em&gt; after changing your gemfile.&lt;/p&gt;

&lt;h2 id=&#34;testing:57d2935fd637ebe85b97477296b70272&#34;&gt;Testing&lt;/h2&gt;

&lt;p&gt;I&amp;#8217;m a big fan of TDD, and strive for a test-first approach. It&amp;#8217;s easy to get a feel for the small stuff in the scala repl, but orchestration is what programming is all about, and TDD allows you to design and throughly test functionality in a repeatable way. The two main libraries are &lt;a href=&#34;https://code.google.com/p/specs/&#34;&gt;specs&lt;/a&gt; (actually, it&amp;#8217;s now &lt;a href=&#34;http://etorreborre.github.io/specs2/&#34;&gt;specs2&lt;/a&gt;) and &lt;a href=&#34;http://www.scalatest.org/&#34;&gt;ScalaTest&lt;/a&gt;. I originally went with specs2. It was fine, but I wasn&amp;#8217;t too impressed with the output and not thrilled with the matchers. I believe these are all customizable, but to get a better feel for the ecosystem I switched to ScalaTest. I like ScalaTest&amp;#8217;s default output better and the flexible composition of testing styles (I&amp;#8217;m using FreeSpec) and matchers (ShouldMatchers) provide a great platform for testing. Luckily, both specs2 and scalatest integrate with SBT which provides continuous testing and growl support, so you don&amp;#8217;t need to fully commit to either one too early.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Six Months of Computer Science Without Computers</title>
          <link>http://blog.michaelhamrah.com/2013/04/six-months-of-computer-science-without-computers/</link>
          <pubDate>Mon, 22 Apr 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/04/six-months-of-computer-science-without-computers/</guid>
          <description>

&lt;p&gt;A few weeks ago I returned from &lt;a href=&#34;http://thegreatbigadventure.tumblr.com&#34;&gt;a six month trip around Asia&lt;/a&gt;. I didn&amp;#8217;t have a computer while abroad, but I was able to catch up on several tech books I never had time for previously. Reading about programming without actually programming was an interesting and rewarding circumstance. It provided a unique mental model: it was no longer about &amp;#8220;how you do this&amp;#8221; but about &amp;#8220;why would you do this&amp;#8221;. Accomplishment of a task via implementation was not an end goal. The end goal was simply absorbing information; once read, it didn&amp;#8217;t need to be applied. It only needed to be reasoned about and hypothetically applied under a specific situation (which I usually did on a trek or on a beach). Before I would have been eager to try it out, hacking away, but without a computer, I couldn&amp;#8217;t. It was liberating. Given a problem, and a set of constraints, what&amp;#8217;s the ideal solution? I realize this is somewhat of an ivory-tower mentality, however, I also realized some of the best software has emerged from an idealism to solve problems in an opinionated way. Sometimes we are too consumed by the here-and-now we fail to step back for the bigger picture. Conversely, we hold onto our ideals and fail to adapt to changing circumstances.&lt;/p&gt;

&lt;p&gt;My favorite aspect of learning technology while traveling abroad did not come from any book or video. A large part of computer science is about optimizing systems under the pressure of constraints. Efficient algorithms, clean code, improving performance. The world is full of sub-optimal processes. Burmese hotels, the Lao transportation system, and Nepalese immigration to name a few. On a larger scale sun-optimal problems are created by geographic, socio-economic, or political constraints. People try the best they can to improve their way of life, and unfortunately, the processes are often &amp;#8220;implemented&amp;#8221; with a &amp;#8220;naïve&amp;#8221; solution. Some are also inspiring. It was powerful to see these systems up close, with cultural and historical factors so foreign. One thing is certain: when you optimize for efficiency, everyone wins.&lt;/p&gt;

&lt;p&gt;Below are a selection of books and resources I found particularly interesting. I encourage you to check them out, hopefully away from a computer in a foreign land:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://programmer.97things.oreilly.com/wiki/index.php/97_Things_Every_Programmer_Should_Know&#34;&gt;97 Things Every Programmer Should Know&lt;/a&gt; : A great selection of tidbits from a variety of sources. Nothing new for the experienced programmer, but reading through the sections is a great refresher to keep core principles fresh. Worthwhile to randomly select a chapter now and again for those &amp;#8220;oh yeah&amp;#8221; moments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://shop.oreilly.com/product/9780596510046.do&#34;&gt;Beautiful Code&lt;/a&gt; by Andy Oram and Greg Wilson: My favorite book. Not so much about code, but the insight about solving problems makes it a great read. I appreciate the intelligent thought process which went into some of the chapters. Python&amp;#8217;s hashtable implementation and debugging prioritization in the Linux kernel are two highlights.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://shop.oreilly.com/product/0636920022626.do&#34;&gt;Exploring Everyday Things with R and Ruby&lt;/a&gt; by Sau Sheong Chang: This is a short book with great content. You only need an elementary knowledge of programming and mathematics to appreciate the concepts. It&amp;#8217;s also a great way to get a taste of R. The book covers a variety of topics from statistics, machine learning, and simulations. My favorite aspect is how to use modeling to verify a hypothesis or create a simulation. The chapters involving emergent behavior are particularly interesting.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://shop.oreilly.com/product/0636920018483.do&#34;&gt;Machine Learning for Hackers&lt;/a&gt; by Drew Conway and John Myles White: I&amp;#8217;ve been interested in machine learning for a while, and I was very happy with this read. Far more technical and mathematical than &lt;em&gt;Exploring Everyday Things&lt;/em&gt;, this book digs into supervised and unsupervised learning and several aspects of statistics. If you&amp;#8217;re interested in data science and are comfortable with programming, this book is for you.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.manning.com/raychaudhuri/&#34;&gt;Scala in Action&lt;/a&gt; by Nilanjan Raychaudhuri: Scala and Go have been on my radar for a while as new languages to learn. It&amp;#8217;s funny to learn a new programming language without being able to test-drive it, but I appreciated the separation. My career has largely been focused on OOP: leveraging design patterns, class composition, SOLID principles, enterprise architecture. After reading this book I realize I was missing out on great functional programming paradigms I was only unconsciously using. Languages like Clojure and Haskell are gaining steam for a radically different approach to OOP, and Scala provides a nice balance between the two. It&amp;#8217;s also wonderfully expressive: traits, the type system, and for-comprehension are beautiful building blocks to managing complex behavior. Since returning I&amp;#8217;ve been doing Scala full-time and couldn&amp;#8217;t be happier. It&amp;#8217;s everything you need with a statically typed language with everything you want from a dynamic one (well, there&amp;#8217;s still no method-missing, at least not yet). I looked at a few Scala books and this is easily on the top of the list. Nilanjan does an excellent job balancing language fundamentals with applied patterns.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://shop.oreilly.com/product/0636920014348.do&#34;&gt;HBase: The Definitive Guide&lt;/a&gt; by Lars George: I&amp;#8217;ve been deeply interested in distributed databases and performance for some time. I purchased this book a few years ago when first exploring NoSQL databases. Since then, Cassandra has eclipsed the distributed hashtable family of databases (Riak, Hbase, Voldemort) but I found this book a great read. No matter what implementation you go with, this book will help you think in a column-orientated way, offering great tidbits into architectural tradeoffs which went into HBase&amp;#8217;s design. At the very least, this book will give you a solid foundation to compare against other BigTable/Dynamo clones.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.aosabook.org/en/index.html&#34;&gt;The Architecture of Open Source Applications&lt;/a&gt;: I was excited when I stumbled upon this website. It offers a plethora of information from elite contributors. The applied-practices and deep architectural insight are valuable lessons to learn from. &lt;a href=&#34;http://www.aosabook.org/en/nginx.html&#34;&gt;Andrew Alexeev on Nginx&lt;/a&gt;, &lt;a href=&#34;http://www.aosabook.org/en/distsys.html&#34;&gt;Kate Matsudaira on Scalable Web Architecture&lt;/a&gt; and &lt;a href=&#34;http://www.aosabook.org/en/zeromq.html&#34;&gt;Martin Sústrik on ZeroMQ&lt;/a&gt; are highlights.&lt;/p&gt;

&lt;h2 id=&#34;itunes-u:c1a63adcb7740f27e4178f1e17a10120&#34;&gt;iTunes U&lt;/h2&gt;

&lt;p&gt;I was also able to check out some courses on iTunes U while traveling. &lt;a href=&#34;http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2010/index.htm&#34;&gt;The MIT OCW Performance Engineering of Software Systems&lt;/a&gt; was my favorite. Prof. Saman Amarasinghe and Prof. Charles Leiserson were both entertaining lecturers, and the course provided great insight into memory management, parallel programming, hardware architecture, and bit hacking. I also watched several lectures on algorithms giving me a new found appreciation for Big-O notation (I wish I remembered more while on the job interview circuit). I&amp;#8217;ve been gradually neglecting the importance of algorithmic design since graduating ten years ago, but found revisiting sorting algorithms, dynamic programming, and graph algorithms refreshing. Focusing on how well code runs is as important as how well it&amp;#8217;s written. Like most things, there&amp;#8217;s a naïve brute-force solution and an elegant, efficient other solution. You may not know what the other solution is, but knowing there&amp;#8217;s one lurking behind the curtain will make you a better engineer.&lt;/p&gt;

&lt;p&gt;So, if you can (and you definitely can!) take a break, grab a book, read it distraction free, gaze out in space and think. You&amp;#8217;ll like what you&amp;#8217;ll find!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>SPDY Slide Deck</title>
          <link>http://blog.michaelhamrah.com/2013/04/spdy-slide-deck/</link>
          <pubDate>Sun, 14 Apr 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/04/spdy-slide-deck/</guid>
          <description>&lt;p&gt;I recently gave a talk on &lt;a href=&#34;http://www.chromium.org/spdy&#34;&gt;SPDY&lt;/a&gt;, the new protocol which will serve as the foundation for HTTP 2.0. SPDY introduces some interesting features to solve current limitations with how HTTP 1.1 sits on top of TCP. &lt;a href=&#34;http://www.michaelhamrah.com/spdy/&#34;&gt;Check out the deck for a high-level overview, with links.&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Choosing a Technology: You’re Asking the Wrong Question</title>
          <link>http://blog.michaelhamrah.com/2013/03/choosing-a-technology-youre-asking-the-wrong-question/</link>
          <pubDate>Wed, 27 Mar 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/03/choosing-a-technology-youre-asking-the-wrong-question/</guid>
          <description>&lt;p&gt;When making a choice in the tech world there are two wide-spread approaches: &amp;#8220;What&amp;#8217;s better, X or Y?&amp;#8221; and &amp;#8220;Should I use xyz?&amp;#8221;. The &amp;#8220;or&amp;#8221; debate is always an entertaining topic usually ending in an absurdly hilarious flame war. The &amp;#8220;Should I use xyz?&amp;#8221; is a subtler, more prevalent question in the tech community leading to an extensive amount of discourse. Fairly rational, usually with some good insight, but still a time consuming task. I&amp;#8217;ve fallen victim to both approaches when exploring a technology decision. What I realized is I&amp;#8217;m asking the wrong question. There are only two things I should ask:&lt;/p&gt;

&lt;p&gt;1) What problem do I need to a solve?&lt;/p&gt;

&lt;p&gt;2) How do I want to solve it?&lt;/p&gt;

&lt;p&gt;Once I take this approach I have an opinionated basis for decision-making and I have a clear direction in how to make that decision. Frameworks&amp;#8211;web or javascript&amp;#8211;are excellent examples on taking this approach. Most of these frameworks were born on the simple premise of solving a problem in an opinionated way. Backbone takes a bare-bones approach to a front-end, event-driven structure; Ember offers a robust, &amp;#8220;things just happen&amp;#8221; framework. Sinatra and co. offers an http-first approach to development. Rails and variants are opinionated in web application structure. Do you agree with that approach? Yes, excellent! No? Find something else or roll your own.&lt;/p&gt;

&lt;p&gt;Don&amp;#8217;t know the answer? That&amp;#8217;s okay too. Most beginners want to make the &amp;#8220;right&amp;#8221; choice on what to learn. But the thing is there is no &amp;#8220;right&amp;#8221; answer. For a beginner choosing python vs. ruby vs. php vs scala wastes effort. Just build something using something: you&amp;#8217;ll soon develop your own opinions, with &amp;#8220;how easy is this to learn&amp;#8221; probably the first. Next, when your rails codebase is out of control and you&amp;#8217;re drowning in method_missing issues maybe you&amp;#8217;ll want a more granular, service-orientated approach and the type-safety of Scala. Maybe not&amp;#8230; But you&amp;#8217;ll have a valid problem to solve and a reasonable opinion to go with it.&lt;/p&gt;

&lt;p&gt;I suggest reading &lt;a href=&#34;http://www.aosabook.org/en/nginx.html&#34;&gt;Andrew Alexeev&amp;#8217;s reason on why he built NGINX&lt;/a&gt; and &lt;a href=&#34;https://www.varnish-cache.org/trac/wiki/ArchitectNotes&#34;&gt;Poul-Henning Kamp&amp;#8217;s rationale on how you write a modern application&lt;/a&gt;. Like so many others these incredible open-source systems were born from a problem and the way someone wanted it solved. But those systems didn&amp;#8217;t happen overnight and the authors didn&amp;#8217;t start from scratch. They spent years encountering, learning, and dealing with problems in their respective spaces. They knew the problem domain well, they knew how they wanted the problem solved, and they solved it.&lt;/p&gt;

&lt;p&gt;So put your choice in a context and don&amp;#8217;t sweat the details which are irrelevant to the task at hand. When you need to know those details you&amp;#8217;ll know them, and when you hit problems you&amp;#8217;ll know how you want them solved.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Markdown Powered Resume with CSS Print Styles</title>
          <link>http://blog.michaelhamrah.com/2013/03/markdown-powered-resume-with-css-print-styles/</link>
          <pubDate>Sat, 23 Mar 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/03/markdown-powered-resume-with-css-print-styles/</guid>
          <description>&lt;p&gt;As much as I wish a LinkedIn profile could be a substitute for a resume, it&amp;#8217;s not, and I needed an updated resume. My previous resume was done some time ago with InDesign when I was on a design-tools kick. It worked well, but InDesign isn&amp;#8217;t the best choice for a straight forward approach to a resume and I was not interested in going back to word. So in honor of my friend Karthik&amp;#8217;s &lt;a href=&#34;http://kufli.blogspot.com/2013/02/evolution-of-my-resume-karthik.html&#34;&gt;programming themed resume&lt;/a&gt; I had an idea: program my resume. My requirements were simple:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Easy to edit: I should be able to update and output with minimal effort.&lt;/li&gt;
&lt;li&gt;Easy to design: Something simple, but not boilerplate.&lt;/li&gt;
&lt;li&gt;Export to Html and PDF: For easy distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;#8217;m a big fan of &lt;a href=&#34;http://daringfireball.net/projects/markdown/syntax&#34;&gt;Markdown&lt;/a&gt; and happy to see the prevalence of Markdown across the web, however fragmented. I use Markdown to publish this blog and felt it would work well for writing a resume. The only problem is layout: you have minimal control over structural html elements which can make aspects of design difficult. For writing articles this isn&amp;#8217;t a problem but when you need structural markup for CSS it can be limiting. Luckily I found &lt;a href=&#34;https://github.com/bhollis/maruku&#34;&gt;Maruku&lt;/a&gt;, a ruby-based markdown interpreter which supports &lt;a href=&#34;http://michelf.ca/projects/php-markdown/extra/&#34;&gt;PHP Markdown Extra&lt;/a&gt; and a &lt;a href=&#34;http://maruku.rubyforge.org/proposal.html&#34;&gt;new meta-data syntax&lt;/a&gt; for adding id, css, and div elements to a page. It does take away from Markdown&amp;#8217;s simplicity but adds enough structure for design. Combined with CSS I had everything I needed to fulfill my requirements.&lt;/p&gt;

&lt;p&gt;My &lt;a href=&#34;https://github.com/mhamrah/mlh.com/blob/master/michael-hamrah-resume.md&#34;&gt;markdown resume&lt;/a&gt; is on GitHub. I was surprised it rendered well with GitHub-Flavored Markdown despite the extraneous Maruku elements. I knew I was on the right track. Maruku lets you add your own stylesheets to the html output which I used for &lt;a href=&#34;http://www.michaelhamrah.com/michael-hamrah-resume.html&#34;&gt;posting online&lt;/a&gt;. One simple command gets me from markdown to ready-to-publish html. Exactly what I wanted.&lt;/p&gt;

&lt;p&gt;Markulu supports pdf output as well, but requires a heavy LaTex install which I wasn&amp;#8217;t happy with. I also wasn&amp;#8217;t impressed with the LaTex PDF output. Luckily there&amp;#8217;s an easy alternative: printing to PDF. I used some &lt;a href=&#34;https://github.com/mhamrah/mlh.com/blob/master/scss/resume.scss&#34;&gt;SASS media query overrides&lt;/a&gt; on top of Html 5 Boilerplate&amp;#8217;s default styles to control the print layout in the way I wanted. You can even specify page breaks and print margins via CSS. I favored Safari&amp;#8217;s pdf output over Chrome&amp;#8217;s for the sole reason Safari automatically embedded custom fonts in the final PDF.&lt;/p&gt;

&lt;p&gt;At the end of the day I realized I probably didn&amp;#8217;t need to add explicit divs to Markdown; I could have gotten the layout I wanted with just vanilla Markdown and CSS3 queries. I also could have a semantically better markup if I used HAML to add &lt;section&gt; tags instead of divs where appropriate, but HAML would have added a considerable amount of extraneous information to the markup. I&amp;#8217;m also not sure editing the raw HAML text would have been as easy as Markdown.&lt;/p&gt;

&lt;p&gt;At the end of the day, it&amp;#8217;s all a tradeoff. GitHub flavored markdown, Markdown Here and other interpreters support fenced code blocks; I like the idea of adding fenced blocks to get &lt;section&gt; elements to get semantic correctness and layout elements in the html output. Unfortunately there&amp;#8217;s no official Markdown spec and support is somewhat fragmented across various implementations, but &lt;a href=&#34;http://www.codinghorror.com/blog/2012/10/the-future-of-markdown.html&#34;&gt;hopefully it will come together soon&lt;/a&gt;. Until then, if you need it, you can always fork. Luckily I didn&amp;#8217;t have to take it that far.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Scalability comparison of WordPress with NGINX/PHP-FCM and Apache on an ec2-micro instance.</title>
          <link>http://blog.michaelhamrah.com/2013/03/scalability-comparison-of-wordpress-with-nginxphp-fcm-and-apache-on-an-ec2-micro-instance/</link>
          <pubDate>Sun, 17 Mar 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/03/scalability-comparison-of-wordpress-with-nginxphp-fcm-and-apache-on-an-ec2-micro-instance/</guid>
          <description>&lt;p&gt;For the past few years this blog ran apache + mod_php on an ec2-micro instance. It was time for a change; I&amp;#8217;ve enjoyed using nginx in other projects and thought I could get more out of my micro server. I went with a php-fpm/nginx combo and am very surprised with the results. The performance charts are below; for php the response times varied little under minimal load, but nginx handled heavy load far better than apache. Overall throughput with nginx was phenomenal from this tiny server. The result for static content was even more impressive: apache effectively died after ~2000 concurrent connections and 35k total pages killing the server; nginx handled the load to 10,000 very well and delivered 160k successful responses.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the &lt;a href=&#34;http://loader.io&#34;&gt;loader.io&lt;/a&gt; results from static content from &lt;a href=&#34;http://www.michaelhamrah.com&#34;&gt;http://www.michaelhamrah.com&lt;/a&gt;, comparing apache with nginx. I suggest clicking through and exploring the charts:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/f1c357b13b1f554eef534b79866eb5ce&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Apache only handled 33.5k successful responses up to about 1,300 concurrent connections, and died pretty quickly. Nginx did far better:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/9430bdfcab50f31dc66f3ea3014beb84&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;160k successful response with a 22% error rate and avg. response time of 142ms. Not too shabby. The apache run effectively killed the server and required a full reboot as ssh was unresponsive. Nginx barely hiccuped.&lt;/p&gt;

&lt;p&gt;The results of my wordpress/php performance is also interesting. I only did 1000 concurrent users hitting blog.michaelhamrah.com. Here&amp;#8217;s the apache result:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/210867953c97cdd2dd4308dce17bcae3&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;There was a 21% error rate with 13.7k request served and a 237ms average response time (I believe the lower average is due to errors). Overall not too bad for an ec2-micro instance, but the error rate was quite high and nginx again did far better:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/631e11ff9206c6c7a3820c891380c9a3&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;A total of 19k successes with a 0% error rate. The average response time was a little higher than apache, but nginx did serve far more responses. I also get a kick out of the response time line between the two charts. Apache is fairly choppy as it scales up, while nginx increases smoothly and evens out when the concurrent connections plateaus. That&amp;#8217;s what scalability should look like!&lt;/p&gt;

&lt;p&gt;There are plenty of guides online showing how to get set up with nginx/php-fpm. &lt;a href=&#34;http://codex.wordpress.org/Nginx&#34;&gt;The Nginx guide on WordPress Codex&lt;/a&gt; is the most thorough, but there&amp;#8217;s a &lt;a href=&#34;http://todsul.com/install-configure-php-fpm&#34;&gt;straightforward nginx/php guide on Tod Sul&lt;/a&gt;. I also relied on an &lt;a href=&#34;http://dak1n1.com/blog/12-nginx-performance-tuning&#34;&gt;nginx tuning guide from Dakini&lt;/a&gt; and &lt;a href=&#34;http://calendar.perfplanet.com/2012/using-nginx-php-fpmapc-and-varnish-to-make-wordpress-websites-fly/&#34;&gt;this nginx/wordpress tuning guide from perfplanet&lt;/a&gt;. They both have excellent information. I also think you should check out the &lt;a href=&#34;https://github.com/h5bp/server-configs/blob/master/nginx/nginx.conf&#34;&gt;html5 boilerplate nginx conf files&lt;/a&gt; which have great bits of information.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re setting this up yourself, start simple and work your way up. The guides above have varying degrees of information and various configuration options which may conflict with each other. Here&amp;#8217;s some tips:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Decide if you&amp;#8217;re going with a socket or tcp/ip connection between nginx + php-fcm. A socket connection is slightly faster and local to the system, but a tcp/ip is (marginally) easier to set up and good if you are spanning multiple nodes (you could create a php app farm to compliment an nginx front-facing web farm).&lt;/p&gt;

&lt;p&gt;I chose to go with the socket approach between nginx/php-fpm. It was relatively painless, but I did hit a snag passing nginx requests to php. I kept getting a &amp;#8220;no input file specified&amp;#8221; error. It turns out it was a simple permissions issue: the default php-fpm user was different the nginx user the webserver runs under. Which leads me to:&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Plan your users. Security issues are annoying, so make sure file and app permissions are all in sync.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check your settings! Read through default configuration options so you know what&amp;#8217;s going on. For instance you may end up running more worker processes in your nginx instance than available cpu&amp;#8217;s killing performance. Well documented configuration files are essential to tuning.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Plan for access and error logging. If things go wrong during the setup, you&amp;#8217;ll want to know what&amp;#8217;s going on and if your server is getting requests. You can turn access logs of later.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get your app running, test, and tune. If you do too many configuration settings at once you&amp;#8217;ll most likely hit a snag. I only did a moderate amount of tuning; nginx configuration files vary considerably, so again it&amp;#8217;s a good idea to read through the options and make your own call. Ditto for php-fcm.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I am really happy with the idea of running php as a separate process. Running php as a daemon has many benefits: you have a dedicate process you can monitor and recycle for php without effecting your web server. Pooling apps allows you to tune them individually. You&amp;#8217;re also not tying yourself to a particular web server; php-fpm can run fine with apache. In TCP mode you can even offload your web server to separate node. At the very least, you can distinguish php usage against web server usage.&lt;/p&gt;

&lt;p&gt;So my only question is why would anyone still use apache?&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to Handle a Super Bowl Size Spike in Web Traffic</title>
          <link>http://blog.michaelhamrah.com/2013/02/how-to-handle-a-super-bowl-size-spike-in-web-traffic/</link>
          <pubDate>Wed, 06 Feb 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/02/how-to-handle-a-super-bowl-size-spike-in-web-traffic/</guid>
          <description>

&lt;p&gt;I was shocked to learn the number of &lt;a href=&#34;http://www.yottaa.com/blog/bid/265815/Coke-SodaStream-the-13-Websites-That-Crashed-During-Super-Bowl-2013&#34;&gt;sites which failed to handle the spike in web traffic during the Super Bowl&lt;/a&gt;. Most of these sites served static content and should have scaled easily with the use of CDNs. Scaling sites, even dynamic ones, are achievable with well known tools and techniques.&lt;/p&gt;

&lt;h2 id=&#34;the-problem-is-simple:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;The Problem is Simple&lt;/h2&gt;

&lt;p&gt;At a basic level accessing a web page is when one computer, the client, connects to a server and downloads some content. A problem occurs when the number of people requesting content exceeds the ability to deliver content. It&amp;#8217;s just like a restaurant. When there are too many customers people must wait to be served. Staff becomes stressed and strained. Computers are the same. Excessive load causes things to break down.&lt;/p&gt;

&lt;h2 id=&#34;optimization-comes-in-three-forms:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Optimization Comes in Three Forms&lt;/h2&gt;

&lt;p&gt;To handle more requests there are three things you can do: produce (render) content faster, deliver (download) content faster and add more servers to handle more connections. Each of these solutions has a limit. Designing for these limits is architecting for scale.&lt;/p&gt;

&lt;p&gt;A page is composed of different types of content: html, css and js. This content is either dynamic (changes frequently) or static (changes infrequently). Static content is easier to scale because you create it once and deliver it repeatedly. The work of rendering is eliminated. Static content can be pushed out to CDNs or cached locally to avoid redownloading. Requests to origin servers are reduced or eliminated. You can also download content faster with small payload sizes. There is less to deliver if there is less markup and the content is compressed. Less to deliver means faster download.&lt;/p&gt;

&lt;p&gt;Dynamic content is trickier to cache because it is always changing. Reuse is difficult because pages must be regenerated for specific users at specific times. Scaling dynamic content involves database tuning, server side caching, and code optimization. If you can render a page quickly you can deliver more pages because the server can move on to new requests. Most often, at scale, you want to treat treat dynamic content like static content as best you can.&lt;/p&gt;

&lt;p&gt;Adding more servers is usually the easiest way to scale but breaks down quickly. The more servers you have the more you need to keep in sync and manage. You may be able to add more web servers, but those web servers must connect to database servers. Even powerful database servers can only handle so many connections and adding multiple database servers is complicated. You may be able to add specific types of servers, like cache servers, to achieve the results you need without increasing your entire topology.&lt;/p&gt;

&lt;p&gt;The more servers you have the harder it is to keep content fresh. You may feel increasing your servers will increase your load. It will become expensive to both manage and run. You may be able to achieve a similar result if you cut your response times which also gives the end user a better experience. If you understand the knobs and dials of your system you can tune properly.&lt;/p&gt;

&lt;h2 id=&#34;make-assumptions:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Make Assumptions&lt;/h2&gt;

&lt;p&gt;Don&amp;#8217;t be afraid to make assumptions about your traffic patterns. This will help you optimize for your particular situation. For most publicly facing websites traffic is anonymous. This is particularly true during spikes like the Super Bowl. Because you can deliver the same page to every anonymous user you effectively have static content for those users. Cache controls determine how long content is valid and powers HTTP accelerators and CDNs for distribution. You don&amp;#8217;t need to optimize for everyone; split your user base into groups and optimize for the majority. Even laxing cache rules on pages to a minute can shift the burden away from your application servers freeing valuable resources. Anonymous users will get the benefit of cached content with a quick download, dynamic users will have fast servers.&lt;/p&gt;

&lt;p&gt;You can also create specific rendering pipelines for anonymous and known users for highly dynamic content. If you can identify anonymous users early you may be able to avoid costly database queries, external API calls or page renders.&lt;/p&gt;

&lt;h2 id=&#34;understand-http:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Understand HTTP&lt;/h2&gt;

&lt;p&gt;HTTP powers the web. The better you understand HTTP the better you can leverage tools for optimizing the web. Specifically look at &lt;a href=&#34;http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html&#34;&gt;http cache headers&lt;/a&gt; which allow you to use web accelerators like Varnish and CDNs. The vary header will allow you to split anonymous and known users giving you fine grained control on who gets what. Expiration headers determine content freshness. The worst thing you can do is set cache headers to private on static content preventing browsers from caching locally.&lt;/p&gt;

&lt;h2 id=&#34;try-varnish-and-esi:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Try Varnish and ESI&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.varnish-cache.org&#34;&gt;Varnish&lt;/a&gt; is an HTTP accelerator. It caches dynamic content produced from your website for efficient delivery. Web frameworks usually have their own features for caching content, but Varnish allows you to bypass your application stack completely for faster response times. You can deliver a pre-rendered dynamic page as if it were a static page sitting in memory for a greater number of connections.&lt;/p&gt;

&lt;p&gt;Edge Side Includes allow you to mix static and dynamic content together. If a page is 90% similar for everyone, you can cache the 90% in Varnish and have your application server deliver the other 10%. This greatly reduces the work your app server needs to do. ESI&amp;#8217;s are just emerging into web frameworks. It will play a more prominent role in Rails 4.&lt;/p&gt;

&lt;h2 id=&#34;use-a-cdn-and-multiple-data-centers:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Use a CDN and Multiple Data Centers&lt;/h2&gt;

&lt;p&gt;You don&amp;#8217;t need to add more servers to your own data center. You can leverage the web to fan work out across the Internet. I talk more about CDN&amp;#8217;s, the importance of edge locations and latency in my post &lt;a href=&#34;http://www.michaelhamrah.com/blog/2012/01/building-for-the-web-understanding-the-network/&#34;&gt;Building for the Web: Understanding the Network&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Your application servers should be reserved for doing application-specific work which is unique to every request. There are more efficient ways of delivering the same content to multiple people than processing a request top-to-bottom via a web framework. Remember &amp;#8220;the same&amp;#8221; doesn&amp;#8217;t mean the same indefinitely; it&amp;#8217;s the same for whatever timeframe you specify.&lt;/p&gt;

&lt;p&gt;If you run Varnish servers in multiple data centers you can effectively create your own CDN. Your database and content may be on the east coast but if you run a Varnish server on the west coast an anonymous user in San Fransisco will have the benefit of a fast response time and you&amp;#8217;ve saved a connection to your app server. Even if Varnish has to deliver 10% dynamic content via an ESI on the east coast it can leverage the fast connection between data centers. This is much better then the end user hoping coast-to-coast themselves for an entire page.&lt;/p&gt;

&lt;p&gt;Amazon&amp;#8217;s Route 53 offers the ability to route requests to an optimal location. There are other geo-aware DNS solutions. If you have a multi-region setup you are not only building for resiliency your are horizontally scaling your requests across data centers. At massive scale even load balancers may become overloaded so round-robin via DNS becomes essential. DNS may be a bottleneck as well. If your DNS provider can&amp;#8217;t handle the flood of requests trying to map your URL to your IP address nobody can even get to your data center!&lt;/p&gt;

&lt;h2 id=&#34;use-auto-scaling-groups-or-alerting:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Use Auto Scaling Groups or Alerting&lt;/h2&gt;

&lt;p&gt;If you can take an action when things get rough you can better handle spikes. Auto scaling groups are a great feature of AWS when some threshold is maxed. If you&amp;#8217;re not on AWS good monitoring tools will help you take action when things hit a danger zone. If you design your application with auto-scaling in mind, leveraging load balancers for internal communication and avoiding state, you are in a better position to deal with traffic growth. Scaling on demand saves money as you don&amp;#8217;t need to run all your servers all the time. Pinterest gave a talk explaining how it saves money by reducing its server farm at night when traffic is low.&lt;/p&gt;

&lt;h2 id=&#34;compress-and-serialized-data-across-the-wire:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Compress and Serialized Data Across the Wire&lt;/h2&gt;

&lt;p&gt;Page sizes can be greatly reduced if you enable compression. Web traffic is mostly text which is easily compressible. A 100kb page is a lot faster to download than a 1mb page. Don&amp;#8217;t forget about internal communication as well. In todays API driven world using efficient serialization protocols like protocol buffers can greatly reduce network traffic. Most RPC tools support some form of optimal serialization. SOAP was the rage in the early 2000s but XML is one of the worst ways to serialize data for speed. Compressed content allows you to store more in cache and reduces network I/O as well.&lt;/p&gt;

&lt;h2 id=&#34;shut-down-features:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Shut Down Features&lt;/h2&gt;

&lt;p&gt;A performance bottleneck may be caused by one particular feature. When developing new features, especially on a high traffic site, the ability to shut down a misbehaving feature could be the quick solution to a bad problem. Most high-traffic websites &amp;#8220;leak&amp;#8221; new features by deploying them to only 10% of their users to monitor behavior. Once everything is okay they activate the feature everywhere. Similar to determining page freshness for caches, determining available features under load can keep a site alive. What&amp;#8217;s more important: one specific feature or the entire system?&lt;/p&gt;

&lt;h2 id=&#34;non-blocking-i-o:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Non-Blocking I/O&lt;/h2&gt;

&lt;p&gt;Asynchronous programming is a challenge and probably a last-resort for scaling. Sometimes servers break down without any visible threshold. You may have seen a slow request but memory, cpu, and network levels are all okay. This scenario is usually caused by blocking threads waiting on some form of I/O. Blocked threads are plugs that clog your application. They do nothing and prevent other things from happening. If you call external web services, run long database queries or perform disk I/O beware of synchronous operations. They are bottlenecks. Asynchronous based frameworks like node.js put asynchronous programming at the forefront of development making them attractive for handling numerous concurrent connections. Asynchronous programming also paves the way for queue-based architectures. If every request is routed through a queue and processed by a worker the queue will help even out spikes in traffic. The queue size will also determine how many workers you need. It may be trickier to code but it&amp;#8217;s how things scale.&lt;/p&gt;

&lt;h2 id=&#34;think-at-scale:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Think at Scale&lt;/h2&gt;

&lt;p&gt;When dealing with a high-load environment nothing can be off the table. What works for a few thousand users will grow out of control for a few million. Even small issues will become exponentially problematic.&lt;/p&gt;

&lt;p&gt;Scaling isn&amp;#8217;t just about the tools to deal with load. It&amp;#8217;s about the decisions you make on how your application behaves. The most important thing is determining page freshness for users. The decisions for an up-to-the-second experience for every user are a lot different than an up-to-the-minute experience for anonymous users. When dealing with millions of concurrent requests one will involve a lot of engineering complexity and the other can be solved quickly.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Embracing Test Driven Development for Speed</title>
          <link>http://blog.michaelhamrah.com/2013/02/embracing-test-driven-development-for-speed/</link>
          <pubDate>Mon, 04 Feb 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/02/embracing-test-driven-development-for-speed/</guid>
          <description>

&lt;p&gt;A few months ago I helped a developer looking to better embrace test driven development. The session was worthwhile and made me reflect on my journey with TDD.&lt;/p&gt;

&lt;p&gt;Writing tests is one thing. Striving for full test coverage, writing tests first and leveraging integration and unit tests is another. Some people find writing tests cumbersome and slow. Others may ignore tests for difficult scenarios or code spikes. When first working with tests I felt the same way. Over time I worked through issues and my feeling towards TDD changed. The pain was gone and I worked more effectively.&lt;/p&gt;

&lt;p&gt;TDD is about speed. Speed of development and speed of maintenance. Once you leverage TDD as a way to better produce code you&amp;#8217;ve unlocked the promise of TDD: Code more, debug less.&lt;/p&gt;

&lt;h2 id=&#34;stay-in-your-editor:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Stay In Your Editor&lt;/h2&gt;

&lt;p&gt;How many times have you verified something works by firing up your browser in development? Too many times. You build, you wait for the app to start, you launch the browser, you click a link, you fill in forms, you hit submit. Maybe there&amp;#8217;s a breakpoint you step through or some trace statements you output. How much time have you wasted going from coding to verifying your code works? Too much time.&lt;/p&gt;

&lt;p&gt;Stay in your editor. It has everything you need to get stuff done. Avoid the context switch. Avoid repetitive typing. Have one window for your code and another for your tests. Even on small laptops you can split windows to have both open at once. Gary Bernhardt, in an excellent &lt;a href=&#34;https://peepcode.com/products/play-by-play-bernhardt&#34;&gt;Peepcode&lt;/a&gt;, shows how he runs specs from within vim. Ryan Bates, in his screencast &lt;a href=&#34;http://railscasts.com/episodes/275-how-i-test&#34;&gt;How I Test&lt;/a&gt;, only uses the browser for UI design. If you leave your editor you are wasting time and suffering a context switch.&lt;/p&gt;

&lt;p&gt;Every language has some sort of continuous testing runtime. Detect a file change, run applicable tests. Take a look at &lt;a href=&#34;https://github.com/guard/guard&#34;&gt;Guard&lt;/a&gt;. Selenium and company are excellent browser testing tools. Jasmine works great for Javascript. Rspec and Capybara are a solid combination. Growl works well for notifications. By staying in your editor you are coding all that manual verification away. Once coded you can repeat indefinitely.&lt;/p&gt;

&lt;h2 id=&#34;start-with-tests:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Start with Tests&lt;/h2&gt;

&lt;p&gt;Test driven doesn&amp;#8217;t mean test after. This may be the hardest rule for newcomers to follow. We&amp;#8217;ve been so engrained to write code, to design classes, to focus on OOP. We know what we need to do. We just need to do it. Once code works we&amp;#8217;ll then write tests to ensure it always works. I&amp;#8217;ve done this bad practice myself.&lt;/p&gt;

&lt;p&gt;When you test last you&amp;#8217;re missing the &lt;em&gt;why&lt;/em&gt;. &lt;em&gt;Customer gets welcome email after signing up&lt;/em&gt; means nothing without context. If you know &lt;em&gt;why&lt;/em&gt; this is needed you are in a better position to define your required tests and start shaping your code. The notification could be a simple acknowledgement or part of some intricate flow. If you know the &lt;em&gt;why&lt;/em&gt; you are not driving blind. The what you will build and the how you will build it will follow. If you code the other way around, testing later, you&amp;#8217;re molding the problem to your solution. Define the problem first, then solve succinctly.&lt;/p&gt;

&lt;h2 id=&#34;start-with-failing-tests:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Start with Failing Tests&lt;/h2&gt;

&lt;p&gt;One of my favorite newbie mistakes is when a developer writes some code, then writes a test, watches the test pass, then is surprised when the code fails in the browser. But the test passed!&lt;/p&gt;

&lt;p&gt;Anyone can write a green test. It is the action of going from red to green which gives the test meaning. Something needs to work, it doesn&amp;#8217;t. Red state. You change your code, you make it work. Green state. Without the red state first you have no idea how you got to a green state. Was it a bug in your test? Did you test the right thing? Did you forget to assert something? Who knows.&lt;/p&gt;

&lt;p&gt;Combined with the &lt;em&gt;why&lt;/em&gt; going from red to green gives the code shape. You don&amp;#8217;t need to over-think class design. The code you write has purpose: it implements a need to make something work that doesn&amp;#8217;t. As your functionality becomes more complex, your code becomes more nimble. You deal with dependencies, spawning new tests and classes when cohesion breaks down. You stay focused on your goal: make something work. Combined with git commits you have a powerful history to branch and backtrack if necessary. As always, don&amp;#8217;t be afraid to refactor.&lt;/p&gt;

&lt;h2 id=&#34;testing-first-safeguards-agile-development:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Testing First Safeguards Agile Development&lt;/h2&gt;

&lt;p&gt;Testing first also acts as a safeguard. Too often developers will pull work from a backlog prematurely. They&amp;#8217;ll make assumptions, code to those assumptions, and have to make too many changes before release. If the first thing you do after pulling a story is ask yourself &amp;#8220;how can I verify this works&amp;#8221; you&amp;#8217;re thinking in terms of your end-user. You&amp;#8217;re writing acceptance tests. You understand what you need to deliver. BDD tools like &lt;a href=&#34;http://cukes.info/&#34;&gt;Cucumber&lt;/a&gt; put this paradigm in the foreground. You can achieve the same effect with vanilla integration tests.&lt;/p&gt;

&lt;h2 id=&#34;always-test-difficult-code:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Always Test Difficult Code&lt;/h2&gt;

&lt;p&gt;Most of the time not testing comes down to two reasons. The code is too hard to test or the code is not worth testing. There are other reasons, but they are all poor excuses. If you want to test code you can test code.&lt;/p&gt;

&lt;p&gt;Code shouldn&amp;#8217;t be too hard to test. Testing distributed, asynchronous systems is hard but still testable. When code is too hard to test you have the wrong abstraction. You&amp;#8217;re API isn&amp;#8217;t working. You aren&amp;#8217;t adhering to SOLID principles. Your testing toolkit isn&amp;#8217;t sufficient.&lt;/p&gt;

&lt;p&gt;Static languages can rely on dependency injection to handle mocking, dynamic languages can intercept methods. Tools like &lt;a href=&#34;https://www.relishapp.com/vcr/vcr&#34;&gt;VCR&lt;/a&gt; and Cassette can fake http requests for external dependencies. Databases can be tested in isolation or &lt;a href=&#34;https://github.com/nulldb/nulldb&#34;&gt;faked&lt;/a&gt;. Asynchronous code can be tricky to test but becomes easier when separating pre and post conditions (you can also block in unit tests to handle synchronization).&lt;/p&gt;

&lt;p&gt;The code you don&amp;#8217;t test, especially difficult code, will always bite you. Taking the time to figure out how to test will clean up the code and will give you incredible insight into how your underlying framework works.&lt;/p&gt;

&lt;h2 id=&#34;always-test-your-code:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Always Test Your Code&lt;/h2&gt;

&lt;p&gt;I worked with a developer that didn&amp;#8217;t write tests because the requirements, and thus code, were changing too much and dealing with the failing tests was tedious. It actually signified a red flag exposing larger issues in the organization but the point is a common one. Some developers don&amp;#8217;t test because code may be thrown out or it&amp;#8217;s just a spike and not worth testing.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re not testing first because it&amp;#8217;s a faster way to develop, realize that there is no such thing as throw away code (on the other hand, &lt;a href=&#34;http://code.dblock.org/treat-every-line-of-code-as-if-its-going-to-be-thrown-away-one-day&#34;&gt;all code is throw away code&lt;/a&gt;). Mixing good, tested code with untested code creates technical debt. If you put a drop of sewer in a barrel of wine you will have a barrel of sewer. The code has no &lt;em&gt;why&lt;/em&gt;. It may be just a spike but it could also turn out to be the next best thing. Then you&amp;#8217;re left retrofitting unit tests, fitting a square peg in a round hole.&lt;/p&gt;

&lt;h2 id=&#34;balancing-integration-and-unit-tests:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Balancing Integration and Unit Tests&lt;/h2&gt;

&lt;p&gt;Once you start testing first a lot of pieces fall into place. The balance between integration and unit tests is an interesting topic when dealing with code coverage. There will be overlap in code coverage but not in terms of covered functionality.&lt;/p&gt;

&lt;p&gt;Unit tests are the distinct pieces of your code. Integration tests are how those pieces fit together. You have a customer class and a customer page. The unit tests are the rules around the customer model or the distinct actions around the customer controller. The integration tests are how the end user interacts with those models top to bottom. &lt;a href=&#34;http://pivotallabs.com/cucumber-step-definitions-are-not-methods/&#34;&gt;Pivotal Labs talks about changing state in cucumber steps&lt;/a&gt; showing how integration tests monitor the flow of events in an application. Unit tests are for the discrete methods and properties which drive those individual events.&lt;/p&gt;

&lt;h2 id=&#34;automate:8bd207e84bfe3e779bdf8e84fc299a65&#34;&gt;Automate&lt;/h2&gt;

&lt;p&gt;Developing applications is much more than coding. Focusing on tools and techniques at your disposal will help you write code more effectively. Your IDE, command line skills, testing frameworks, libraries and development paradigms are as important as the code you right. They are your tools and become more powerful when used correctly.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Bus travel tips in Turkey</title>
          <link>http://blog.michaelhamrah.com/2012/09/bus-travel-tips-in-turkey/</link>
          <pubDate>Thu, 13 Sep 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/09/bus-travel-tips-in-turkey/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2012/09/20120913-205957.jpg&#34;&gt;&lt;img src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2012/09/20120913-205957.jpg?w=660&#34; alt=&#34;20120913-205957.jpg&#34; class=&#34;alignnone size-full&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ve been travelling around turkey for the past three weeks and have relied heavily on bus travel to get around. Travel books have great info, but there are a couple of more things to think about when dealing with buses in Turkey.&lt;/p&gt;

&lt;p&gt;First, there are several companies that serve various routes. Pamukkale, KamilKoc and Metro are the big ones. There are several more depending on where you are and where you&amp;#8217;re going. If you don&amp;#8217;t see the bus time you want, or if a bus is full, check with another company. Prices are fairly set so I don&amp;#8217;t think it&amp;#8217;s worth negotiating down. If you are booking through a tour operator, hotel or another reseller they will most likely book through another company, most likely one of the above.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s important to ask what type of bus you&amp;#8217;ll be taking. There are big coach buses, older buses, and minibuses. Ideally you want to be on a big coach bus, often referred to as a big bus. There is usually wifi and tv (turkish only) on big buses, but I haven&amp;#8217;t been on one yet with power. One did have USB outlets but was unable to charge the iPad. Minibuses and older buses may not have air conditioning, so it&amp;#8217;s important to ask. Big buses have the smoothest ride and the most legroom. If you don&amp;#8217;t like the bus you&amp;#8217;re getting at the time you want, see if there&amp;#8217;s another time with a better bus or go to another company. Always get your ticket from someone behind a desk. There will be plenty of people trying to sherpa you here and there, but just go right to the desk. At some otogars there are valets to help you. They may appear to be trying to sell you something. Just ask the right questions and you&amp;#8217;ll be fine. Turkish people are very nice and very helpful.&lt;/p&gt;

&lt;p&gt;The bus may make a lot of stops. We were on a minibus from Denizli to Fethiye and the bus stopped for anybody along the road. It was nuts! People would be waiting on the road no more than 50 meters away from each other and the bus would pull up, slow down, see if anybody needed to get on. Also, some buses will stop at rest areas every 45 minutes to an hour for breaks. On our way to Selcuk we had to stop at a rest area for 15 minutes even though we were only five minutes out from our destination. Ask if it is a direct bus and how many stops it will make. Usually the big coaches are better than the minibuses in terms of stopping. If possible, just avoid minibuses. The ride will most likely be bumpy as well unless it is a newer minibus or a tourist minibus.&lt;/p&gt;

&lt;p&gt;Seats are assigned on buses, so ask for a seat up in the front. Some bus companies have seat maps so you can see where you&amp;#8217;ll be seated. You don&amp;#8217;t need to rush onto the bus, just put your bags on, get on, and find your seat. Everyone is very nice and will gladly help you out. You&amp;#8217;ll also get tea or coffee on the bus with a snack. If the bus is really bumpy don&amp;#8217;t get anything hot. You&amp;#8217;ll probably spill it, need to drink it really quickly, than have to go to the bathroom. Most buses don&amp;#8217;t have bathrooms (they do make a lot of stops, so don&amp;#8217;t worry, but some restrooms cost 1TL). Another funny thing is that on minibuses in small towns there will be guy walking up and down with lemon or rose oil for your hands. A nice little refresher!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2012/09/20120919-164620.jpg&#34;&gt;&lt;img src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2012/09/20120919-164620.jpg?w=660&#34; alt=&#34;20120919-164620.jpg&#34; class=&#34;alignnone size-full&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Dolmuses are fantastic. These are little vans that go around towns to pick people up and drop them off along the way. They are extremely cheap, extremely frequent and should be leveraged. They are just as good as taxis and cost a lot less. They are great within cities to get to more remote areas and to travel among smaller towns. They are great on the Turqouise coast to explore different beaches. Essentially, you just wait outside on the road in the direction you want and a van with people will pull up. Hotel, pensyon and guest house operators are very helpful with Dolmus transport. Depending on where you are on the Lycian way you could even send your bags ahead to be picked up by your next stop.&lt;/p&gt;

&lt;p&gt;If you stick with the big buses and know your options bus travel in Turkey is a great and economical way to get around. Always bring earplugs and an eyemask, especially on night buses. There will always be a crying baby and someone reading with the light on.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Effective Caching Strategies: Understanding HTTP, Fragment and Object Caching</title>
          <link>http://blog.michaelhamrah.com/2012/08/effective-caching-strategies-understanding-http-fragment-and-object-caching/</link>
          <pubDate>Sat, 18 Aug 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/08/effective-caching-strategies-understanding-http-fragment-and-object-caching/</guid>
          <description>

&lt;p&gt;Caching is one of the most effective techniques to speed up a website and has become a staple of modern web architecture. Effective caching strategies will allow you to get the most out of your website, ease pressure on your database and offer a better experience for users. Yet as the old &lt;a href=&#34;http://martinfowler.com/bliki/TwoHardThings.html&#34;&gt;adage says&lt;/a&gt; caching&amp;#8211;especially invalidation&amp;#8211;is tricky. How to deal with dynamic pages, deciding what to cache, per-user personalization and invalidation are some of the challenges which come along with caching.&lt;/p&gt;

&lt;h3 id=&#34;caching-levels:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Caching Levels&lt;/h3&gt;

&lt;p&gt;There a three broad levels of caching:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html&#34;&gt;HTTP Caching&lt;/a&gt;&lt;/em&gt; allows for full-page caching via HTTP headers on URIs. This must be enabled on all static content and should be added to dynamic content when possible. It is the best form of caching, especially for dynamic pages, as you are serving generated html content and your application can effectively leverage reverse-proxies like &lt;a href=&#34;http://www.squid-cache.org/&#34;&gt;Squid&lt;/a&gt; and &lt;a href=&#34;https://www.varnish-cache.org/&#34;&gt;Varnish&lt;/a&gt;. &lt;a href=&#34;http://www.mnot.net/cache_docs/&#34;&gt;Mark Nottingham&amp;#8217;s great overview on HTTP Caching is worth a read&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Fragment Caching&lt;/em&gt; allows you to cache page fragments or partial templates. When you cannot cache an entire http response, fragment caching is your next best bet. You can quickly assemble your pages from pre-generated html snippets. For a page involving disparate dynamic content you can build your result page from cached html fragments for each section. For listing pages, like search results, you can build the page from html fragments for each id and not regenerate markup. For detail pages you can separate less-volatile or common sections from high-volatile or per-user sections.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Object Caching&lt;/em&gt; allows you to cache a full object (as in a model or viewmodel). When you must generate html for each user/request, or when your objects are shared across various views, object caching can be extremely helpful. It allows you to better deal with expensive queries and lessen hits to your database.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The goal is to make your response times as fast as possible while lessening load. The more html (or data) you can push closer to the end-user the better. HTTP caching is better than fragment caching: you are ready to return the rendered page. When combined with a CDN even dynamic pages can be pushed to edge locations for faster response times. Fragment caching is better than object caching: you already have the rendered html to build the page. Object caching is better than a database call: you already have the cached query result or denormalized object for your view. The deeper you get in the stack (the closer to the datastore) the more options you have to vary the output. Consequently the more expensive and longer the operation will take.&lt;/p&gt;

&lt;h3 id=&#34;break-content-down-cache-for-views:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Break Content Down; Cache for Views&lt;/h3&gt;

&lt;p&gt;A cache strategy is dependent on breaking content down to store and reuse later. The more granular you can get the more options you have to serve cached content. There are two main dimensions: what to cache and whom to cache for. It is difficult to HTTP cache a page with a &amp;#8220;Hello, {{ username }}&amp;#8221; in the header for all users. However if you break your users down into logged-in users and anonymous users you can easily HTTP cache your homepage for just anonymous users using the &lt;em&gt;vary&lt;/em&gt; http-header and defer to fragment caching for logged-in users.&lt;/p&gt;

&lt;p&gt;Cache key naming strategies allow you to vary the &lt;em&gt;what&lt;/em&gt; with the &lt;em&gt;who for&lt;/em&gt; in a robust way by creating multiple versions of the same resource. A cache key could include the role of the user and the page, such as &lt;em&gt;role:page:fragement:id&lt;/em&gt;, as in _anon:widget&lt;em&gt;detail:widget:1234&lt;/em&gt; and serve the &lt;em&gt;widget detail&lt;/em&gt; html fragment to anonymous users. The same widget could be represented in a search detail list via _anon:widget&lt;em&gt;search:widget:1234&lt;/em&gt;. When widget 1234 updates both keys are invalidated. Most people opt for object caching for an easy win with dynamic pages, specifically by caching via a primary key or id. This can be helpful, but if you break down your content into the &lt;em&gt;what&lt;/em&gt; and &lt;em&gt;who for&lt;/em&gt; with a good key naming strategy you can leverage fragment caching and save on rendering time.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;vary&lt;/em&gt; http header is very helpful for dealing with HTTP caching and is not used widely enough. By varying URIs based on certain headers (like authorization or a cookie value) you can cache different representations for the same resource in a similar way to creating multiple keys. Think of the cache key as the URI plus whatever is set in the &lt;em&gt;vary&lt;/em&gt; header. This opens up the power of HTTP caching for dynamic or per-user content.&lt;/p&gt;

&lt;p&gt;You are ready to deliver content quickly when you think about your cache in terms of views and not data. Cache a denormalized object with child associations for easy rendering without extra lookups. Store rendered html fragments for sections of a page that are common to users on otherwise specific content. &amp;#8220;Popular&amp;#8221; and &amp;#8220;Recent&amp;#8221; may be expensive queries; storing rendered html saves on processing time and can be injected into the main page. You can even reuse fragments across pages. A good cache key naming strategy allows for different representations of the same data which can easily be invalidated.&lt;/p&gt;

&lt;h3 id=&#34;cache-invalidation:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Cache Invalidation&lt;/h3&gt;

&lt;p&gt;Nobody likes stale data. As you think about caching think about what circumstances to invalidate the cache. Time-based expirations are convenient but can usually be avoided by invalidating caches on create and update commands. A good cache key naming strategy helps. Web frameworks usually have a notion of &amp;#8220;callbacks&amp;#8221; to perform secondary actions when a primary action takes place. A set of fragment and object caches for a widget could be invalidated when a record is updated. If cache values are granular enough you could invalidate sections of a page, like blog comments, when a comment is added and not expire the entire blog post.&lt;/p&gt;

&lt;p&gt;HTTP Etags provide a great mechanism for dealing with stale HTTP requests. Etags allow a more invalidation options than the basic if-modified-since headers. When dealing with Etags the most important thing is to avoid processing the entire request simply to generate the Etag to validate against (this saves network bandwidth but does not save processing time). Caching Etag values against URIs are a good way to see if an Etag is still valid to send the proper 304 NOT MODIFIED response as quickly as possible in the request cycle. Depending on your needs you can also cache sets of Etag values against URIs to handle various representations.&lt;/p&gt;

&lt;p&gt;If you must rely on time-based expiration try to add expiration callbacks to keep the cache fresh, especially for expensive queries in high-load scenarios.&lt;/p&gt;

&lt;h3 id=&#34;edge-side-includes-fragment-caching-for-http:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Edge Side Includes: Fragment Caching for HTTP&lt;/h3&gt;

&lt;p&gt;Edge Side Includes are a great way of pushing more dynamic content closer to users. ESIs essentially give you the benefits of fragment caching with the performance of HTTP caching. If you are considering using a tool like &lt;a href=&#34;http://www.squid-cache.org/&#34;&gt;Squid&lt;/a&gt; or &lt;a href=&#34;https://www.varnish-cache.org/&#34;&gt;Varnish&lt;/a&gt; ESIs are essential and will allow you to add customized content to otherwise similar pages. The &lt;em&gt;user panel&lt;/em&gt; in the header of a page is a classic example of an ESI usage. If the user panel is the only variant of an otherwise common page for all users, the common elements could be pulled from the reverse-proxy within milliseconds and the &amp;#8220;Welcome, {{USER}}&amp;#8221; injected dynamically as a fragment from the application server before sending everything to the client. This bypasses the application stack lightening load and decreasing processing time.&lt;/p&gt;

&lt;h3 id=&#34;distributed-or-centralized-caches-are-better:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Distributed or Centralized Caches are Better&lt;/h3&gt;

&lt;p&gt;Distributed and/or centralized caches are better than in-memory application server cache stores. By using a distributed cache like &lt;a href=&#34;http://memcached.org/&#34;&gt;Memcache&lt;/a&gt;, or a centralized cache store like &lt;a href=&#34;http://redis.io&#34;&gt;Redis&lt;/a&gt;, you can drop duplicate data caches to make caching and invalidating objects easier. Even though caching objects in a web app&amp;#8217;s memory space is convenient and reduces network i/o, it soon becomes impractical in a web farm. You do not want to build up caches per-server or steal memory space away from the web server. Nor do you want to have to hunt and gather objects across a farm to invalidate caches. If you do not want to support your own cache farm, there are plenty of SaaS services to deal with caching.&lt;/p&gt;

&lt;h3 id=&#34;compress-when-possible:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Compress When Possible&lt;/h3&gt;

&lt;p&gt;Compressing content helps. Memory is a far more valuable resource for web apps than cpu cycles. When possible, compress your serialized cache content. This lowers the memory footprint so you can put more stuff in cache, and lightens the transfer load (and time) between your cache server and application server. For HTTP caching the helpful &lt;em&gt;vary&lt;/em&gt; http header can also be used to cache content for browsers supporting compression and those that don&amp;#8217;t. For object caching, only store what you need in the cache. Even though compression helps reduce the footprint, not storing extraneous data further reduces the footprint and saves serialization time.&lt;/p&gt;

&lt;h3 id=&#34;nosql-to-the-rescue:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;NoSQL to the Rescue&lt;/h3&gt;

&lt;p&gt;One of the interesting trends I am reading about is how certain NoSQL stores are eliminating the need for separate cache farms. NoSQL solutions are beneficial for a variety of reasons even though they create significant data-modeling challenges. What NoSQL solutions lack in the flexibility of representing and accessing data (i.e. no joins, minimal search) they can make up in their distributed nature, fault-tolerance, end access efficiency. When you model your data for your views, putting the burden on storing data in the same way you want to get it out, you&amp;#8217;re essentially replacing your denormalized memory-caching tier with a more durable solution. Cassandra and other Dynamo/Bigtable type stores are key-value stores, similar to cache stores, with the value part offering some sort of structured data type (in the case of Cassandra, sorted lists via column families). MongoDb and Redis, (not Dynamo inspired) offer similar advantages; Redis&amp;#8217; sorted sets/sorted lists offer a variety of solutions for listing problems, MongoDb allows you to query objects.&lt;/p&gt;

&lt;p&gt;If you are okay with storing (and updating) multiple-versions of your data (again, you are caching for views) you can cut the two-layer approach of separate cache and data stores. The trick is storing everything you need to render a view for a given key. Searches could be handled by a search-server like Solr or ElasticSearch; listing results could be handled by maintaining your own index via a sorted-list value via another key. When using Cassandra you&amp;#8217;d get fast, masterless, and scalable persistant storage. In general this approach is only worthwhile if your views are well-defined. The worst thing you want to do is refactor your entire data model when your views change!&lt;/p&gt;

&lt;h3 id=&#34;how-web-frameworks-help:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;How Web Frameworks Help&lt;/h3&gt;

&lt;p&gt;There is always debate on differences between frameworks and languages. One of the things I always look for is how easy it is to add caching to your application. Rails offers great support for caching, and the &lt;a href=&#34;http://guides.rubyonrails.org/caching_with_rails.html&#34;&gt;Caching with Rails&lt;/a&gt; guide is worth a read no matter what framework or language you use. It easily supports fragment caching in views via content blocks, behind-the-scene action caching support, has a pluggable cache framework to use different stores, and most importantly has an extremely flexible invalidation framework via model observers and cache sweepers. When choosing any type of framework, &amp;#8220;how to cache&amp;#8221; should be a bullet point at the top of the list.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Web Services @ Getty Images</title>
          <link>http://blog.michaelhamrah.com/2012/02/web-services-getty-images/</link>
          <pubDate>Tue, 21 Feb 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/02/web-services-getty-images/</guid>
          <description>&lt;p&gt;I wrote a post for the Getty Images Technology blog on how we use SOA at Getty Images. As systems become more and more complex with unique scalability needs, SOA allows systems to segment complexity and create useful boundaries.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.gettyimages.com/2012/02/21/put-a-service-on-it-how-web-services-power-getty-images&#34;&gt;http://blog.gettyimages.com/2012/02/21/put-a-service-on-it-how-web-services-power-getty-images&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Agile: It’s a War on Dates</title>
          <link>http://blog.michaelhamrah.com/2012/01/agile-its-a-war-on-dates/</link>
          <pubDate>Sun, 15 Jan 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/01/agile-its-a-war-on-dates/</guid>
          <description>

&lt;p&gt;In a comment to my earlier article &lt;a href=&#34;http://wp.me/pnRto-a1&#34;&gt;Thoughts on Kanban&lt;/a&gt; someone brought up the subject of end dates. Businesses obsess about the &amp;#8220;When can we have it?&amp;#8221; question. Dates and deadlines trump all. Let me tell you a secret: dates are bullshit. It is a prohibitive mentality in today&amp;#8217;s world. Technology needs to reframe the question. Stakeholders need to change their engagement. No company ever succeeded because they made dates. Companies succeed when they continuously deliver innovation. It is not about the destination. It is about the journey and where you end up.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For agile to truly succeed the DNA of the company&amp;#8211;top to bottom&amp;#8211;must be continuous improvement through continuous delivery.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;i-want-everything-now:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;I Want Everything. Now.&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;A friend told me a story of a prioritization meeting he had with a stakeholder. After fleshing out seven distinct features with the dev team, the stakeholder was asked to prioritize. He walked up to the board, put a &amp;#8220;1&amp;#8221; next to everything, and walked out.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Really? Everything is a top priority? So you are saying you would rather have nothing than anything? Then what you want is worthless. You may think you need everything but you are showing your unwillingness to change or improve. People that cannot work through small changes definitely cannot deal with large ones.&lt;/p&gt;

&lt;p&gt;Yes, long-term vision is important. It is the goal. It ensures that everyone heads in the right direction every step of the way. It helps people make reasonable decisions. But it is still long-term; it is just a vision of the future negating the small details that allow the day-to-day. It is painful, ridiculous and unnecessary to wait for the future to just &amp;#8220;appear&amp;#8221;.&lt;/p&gt;

&lt;p&gt;Long term deliverables create impatient, anxious users. It creates large, unmanageable codebases, complex releases, excessive bugs. It disconnects original vision from delivered functionality. The cherry on the cake: it creates a confused user base making awkward and painful adjustments to radical new processes. There is no long-term goal that cannot be broken down and reached via small iterative releases. Baby steps. One at a time, together. It is a three-legged race for everyone.&lt;/p&gt;

&lt;h2 id=&#34;the-devil-is-in-the-details:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;The Devil Is In The Details.&lt;/h2&gt;

&lt;p&gt;This is the root cause of scope creep. &lt;em&gt;Okay, we&amp;#8217;re working on feature x, but can we do this? Can we do this? What about this?&lt;/em&gt; You tell me. Is it more important for you to do that or get what we have out? It is your call! When you look at the backlog is it more important to enhance the current feature or move to the next thing on the list?&lt;/p&gt;

&lt;p&gt;We have daily scrums to answer the improve or move question. Get involved in the day-to-day. Transparency is king. If you don&amp;#8217;t trust the person making the call then don&amp;#8217;t let them make the call. Empower key people: it&amp;#8217;s an organizational change which pushes agile forward.&lt;/p&gt;

&lt;p&gt;Active engagement between stakeholders and developers is agile development. The less barriers between the two the better. Getting a common sub-conscience understanding of &amp;#8220;what we are doing&amp;#8221; is the key to success. Small companies align on vision easily. Large companies need to break up into teams and align on goals.&lt;/p&gt;

&lt;h2 id=&#34;deadlines-get-things-done:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;Deadlines Get Things Done&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;You want to know if this really complex thing you are asking for can be done in eight weeks? Can you spell out every possible detail, define every wireframe, tell me how it should look on every device to every user in the world, outline every workflow, specify the amount of load it requires, explain how you will want to enhance it in the future, not bother us at all while we build it, let us decide any confusing or ambiguous detail, then maybe, maybe, I can do this thing that nobody has ever done before in eight weeks. If eight weeks later when you are unhappy with that one thing you did not explicitly specify (even though I totally asked you to specify everything) I will tell you &amp;#8220;It wasn&amp;#8217;t in the reqs&amp;#8221;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Is that how you want to work? Or would you rather tell me the gist of what we are doing, come up with a plan to get there, see what we can do first quickly, get it out, then take it from there. Is outlining every validation error on every page necessary to do now or can we start with the first page and take it from there?&lt;/p&gt;

&lt;p&gt;You may also use deadlines to motivate people. Vision, direction and importance also motivate people. It is pretty easy to get something done by saying &amp;#8220;This needs to happen by this date&amp;#8221;. But that shows you do not care what it does or how well it works. You are asking people to time box something because either the details are irrelevant or there is no trust in people to make the right decisions.&lt;/p&gt;

&lt;h2 id=&#34;always-be-releasing:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;Always Be Releasing&lt;/h2&gt;

&lt;p&gt;Releasing functionality does not mean users have to see it. Turning features on and off, experimenting with small audiences, refactoring one class rather than an entire stack; all these are powerful steps for modern companies to improve products. Constant feedback lets everyone know they are headed in the right direction. It lets dev teams know the health of their code base. What is better than actually integrating new code to production to test integration? What is better than knowing if a feature is worth investing in than testing it on a small set of production users?&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m sure you heard about the &lt;a href=&#34;http://en.wikipedia.org/wiki/Cone_of_Uncertainty&#34;&gt;cone of uncertainty&lt;/a&gt;. Small and explicit features with short estimations can be delivered accurately. Larger loosely defined features with long dates are difficult to predict. Break down large features into small, clear user stories. A big feature or a long date means you do not care about details.&lt;/p&gt;

&lt;p&gt;Short date ranges are okay and can help coordinate people. They work well when matched with story size. A few days, one week, one to three, and three to five are good ranges which require a decent discussion to work out details. Ranges allow for adjustment and can be refined as you move along the uncertainty cone. Ideally they are auto-calculated from story points. Anything +5 weeks requires a break down; there are too many variables. Don&amp;#8217;t think you can add up ranges either, that is not the way it works. It is about increasingly clarifying level of detail on what&amp;#8217;s ahead to maintain momentum. You can still, and should, deploy intermittently within the date range. Long dates don&amp;#8217;t provide detail. Without detail you do not care how features work. So you do not care what you get.&lt;/p&gt;

&lt;h2 id=&#34;you-work-for-a-tech-company:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;You Work For A Tech Company&lt;/h2&gt;

&lt;p&gt;Your type of business does not matter. Your size does not matter. In house, off shore, outsourced development does not matter. It&amp;#8217;s 2012. Your company uses technology to do business. You work for a tech company. As a tech person your job is to help your business realize this. As a stakeholder your job is to realize this and help your tech team help you do your job faster, better, easier.&lt;/p&gt;

&lt;h2 id=&#34;i-8217-ll-say-it-again-always-be-releasing:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;I&amp;#8217;ll Say It Again: Always Be Releasing.&lt;/h2&gt;

&lt;p&gt;Good companies consistently take their products to the next level. How? They build an incredible manufacturing pipeline. Why is Toyota&amp;#8217;s just-in-time practices so applicable to building software? Because development teams manufacture software. It&amp;#8217;s how the product is built. It&amp;#8217;s how it changes. It&amp;#8217;s how it&amp;#8217;s delivered. It&amp;#8217;s how it&amp;#8217;s fixed. Dev teams buy the land, construct the building, build the robots, define the pipeline, assemble the pieces, run quality control, load up the trucks, deliver and when all that is done they improve. Hopefully the new manufacturing plant allows for easy improvement. Otherwise somebody made a mistake.&lt;/p&gt;

&lt;h2 id=&#34;faster-better-stronger:1219c6c1689c6c74fe0a969441c812fa&#34;&gt;Faster, Better, Stronger&lt;/h2&gt;

&lt;p&gt;It is everyone&amp;#8217;s responsibility to ensure that manufacturing pipeline delivers as efficiently as possible with no flaws. Continuous integration, unit tests, programming languages, server frameworks, agile development, clear vision, well written stories, cohesive vision, user feedback; it all goes into building a solid manufacturing process. You do not need to get it right the first time. You just need to change and improve when needed. Tech leaders ensure they are building the right process for the business. Stakeholders enable and leverage that pipeline effectively. Radical product changes, disconnected vision or tech decisions lead to numerous and slow manufacturing plants.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Dates are bullshit. It&amp;#8217;s about where you are, where you want to be, and what&amp;#8217;s next. That&amp;#8217;s the conversation.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Update: I missed a section of date ranges matching to story points relating to the cone of certainty. Short date ranges are a good tool for predicting near term deliverables and framing what&amp;#8217;s next. These ranges are most effective when your agile tool auto-calculates velocity from story points. Remember, the bigger the complexity, the greater the date range.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Building for the Web: Understanding The Network</title>
          <link>http://blog.michaelhamrah.com/2012/01/building-for-the-web-understanding-the-network/</link>
          <pubDate>Fri, 06 Jan 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/01/building-for-the-web-understanding-the-network/</guid>
          <description>

&lt;p&gt;My &lt;a href=&#34;http://wp.me/pnRto-aa&#34;&gt;first post on web technology&lt;/a&gt; talks about what we are trying to accomplish when building for the web. There are four ways we can break down the standard flow of &lt;em&gt;client action/server action/result&lt;/em&gt;: delivering, serving, rendering and developing. This post focuses on delivering content by understanding the network. Why use a &lt;a href=&#34;http://en.wikipedia.org/wiki/Content_delivery_network&#34;&gt;cdn&lt;/a&gt;? What&amp;#8217;s all the fuss about connections and compressed static assets? The network is often overlooked but understanding how it operates is essential for building high performing websites. A 50ms rendering time with a 50ms db query is meaningless if it takes three seconds to download a page.&lt;/p&gt;

&lt;h2 id=&#34;tcp-know-it:117df65302b8db2107451cddb1557896&#34;&gt;TCP: Know It.&lt;/h2&gt;

&lt;p&gt;Going from client to server and back again rests on the network and how well you use it. &lt;a href=&#34;http://en.wikipedia.org/wiki/Transmission_Control_Protocol&#34;&gt;TCP&lt;/a&gt; dominates communication on the web and is worth knowing well. In order to send data from one point to another a connection is established between two points via a back-and-forth handshake. Once established, data flows between the two in a series of packets. TCP offers reliability by acknowledging receipt of every packet sent by sending a second acknowledgement packet back to the server. The time it takes to go from one end to another and back is called latency or round-trip time. At any given time there are packets in flight waiting acknowledgement of receipt. TCP only allows a certain amount of unacknowledged packets in flight; this is called window size. Connections start with small window sizes but as more successful transfers occur the window size will increase (&lt;a href=&#34;http://en.wikipedia.org/wiki/Slow-start&#34;&gt;known as slow start&lt;/a&gt;). This effectively increases bandwidth because more data is sent at once. The longer the latency the slower a connection; if the window size is full then the server must wait for acknowledgements before sending more data. This is on top of the time it actually takes to send packets. The best case scenario is low latency with large, full windows.&lt;/p&gt;

&lt;h2 id=&#34;reliability:117df65302b8db2107451cddb1557896&#34;&gt;Reliability&lt;/h2&gt;

&lt;p&gt;Reliable connections are also important; if packets are lost they must be resent. Retransmissions slow down transfers because tcp guarantees in-order delivery of data to the application layer. If a packet is dropped and needs to be resent nothing will be delivered to the application until that one packet is received. UDP, an alternative to TCP, doesn&amp;#8217;t offer the same guarantees and assumes issues are dealt with at the application layer.&lt;/p&gt;

&lt;p&gt;There is a &lt;a href=&#34;http://coding.smashingmagazine.com/2011/11/14/analyzing-network-characteristics-using-javascript-and-the-dom-part-1/&#34;&gt;good article by Phillip Tellis on understanding the network with JS&lt;/a&gt; which talks about data transfer and TCP. &lt;a href=&#34;http://www.wireshark.org/&#34;&gt;Wireshark&lt;/a&gt; is another great tool for analyzing packets across a network. You can actually view individual packets as they come and go and see how window size is scaling, view retransmissions, measure bandwidth, and examine latency.&lt;/p&gt;

&lt;h2 id=&#34;the-importance-of-connections:117df65302b8db2107451cddb1557896&#34;&gt;The Importance of Connections&lt;/h2&gt;

&lt;p&gt;Establishing a connection takes time because of the handshake involved and latency considerations. A 100ms latency could mean more than 300ms before any data is even received on top of dealing with any dns lookups and os overhead. Keeping a connection alive avoids this creation overhead. Connection pooling, for example, is a popular technique to manage database connections. A web server will talk to a database frequently; if the connection is already established there is no overhead in executing a new query which can trim valuable time off of serving a request. &lt;a href=&#34;http://en.wikipedia.org/wiki/Ping&#34;&gt;Ping&lt;/a&gt; and &lt;a href=&#34;http://en.wikipedia.org/wiki/Traceroute&#34;&gt;traceroute&lt;/a&gt; are two worthwhile tools that examine latency and the &amp;#8220;hops&amp;#8221; packets take from one network to another as they travel from end to end.&lt;/p&gt;

&lt;p&gt;Connections take time to create, have a relatively limited availability and require overhead to manage. It may seem like a great idea to keep connections open for the long haul, but there is a limited number of connections a server can sustain. Concurrent connections is a popular benchmark which examines how many simultaneous connections can occur at any given time. If you hit that mark, new requests will have to wait until something frees up. The ideal situation is to pump as much as possible through a few connections so there are more available to others. If you can serve multiple files files at once great; but why keep six empty connections open between requests if you don&amp;#8217;t need too?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;On a side note, this is where server architecture comes into play. A server usually processes a request by building a web page from a framework. If this can be done asynchronously by the web server, or offloaded somewhere else, the web server can handle a higher number of sustained connections. The server can grab a new connection while it waits for data to send on an existing one. We&amp;#8217;ll talk more about this in another post. Fast server times also keep high concurrent connections from arising in the first place.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;optimizing-the-network:117df65302b8db2107451cddb1557896&#34;&gt;Optimizing the Network&lt;/h2&gt;

&lt;p&gt;Lowering latency and optimizing data throughput are what dominate delivery optimization. It is important to keep the data which flows between client and server to a minimum. Downloading a 100kb page is a lot faster than downloading a 1000kb page. Compressing static content like css and javascript greatly reduces payload which is why tools like &lt;a href=&#34;http://documentcloud.github.com/jammit/&#34;&gt;Jammit&lt;/a&gt; and &lt;a href=&#34;http://code.google.com/closure/&#34;&gt;Google Closure&lt;/a&gt; are so ubiquitous. These tools can also merge files; because of http chatter it is faster to download one larger file than several small files. Remember the importance of knowing http? Each http request requires reusing or establishing a connection, a header request sent, the server handling the request, and the response. Doing this once is better than twice. Most web servers can also dynamically compress http responses and should be used when possible.&lt;/p&gt;

&lt;p&gt;Fixing latency can be done by using a content delivery network like &lt;a href=&#34;http://aws.amazon.com/cloudfront/&#34;&gt;Amazon Cloudfront&lt;/a&gt; or &lt;a href=&#34;http://www.akamai.com&#34;&gt;Akamai&lt;/a&gt;. They shorten the distance between a request and response by taking content from your server and spreading it on their infrastructure all over the world. When a user requests a resource the cdn routes the request to the server with the lowest latency. A user in Japan can download a file from Japan a lot faster than he can from Europe. Shorter distance, fewer hops, fewer retransmissions. A good cdn strategy rests on how easy it is to push content to a cdn and how easy it is to refresh it. Both concerns should be well researched when leveraging a cdn. You don&amp;#8217;t want stale css files in the wild when you release a new version of your app.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&#34;http://en.wikipedia.org/wiki/WAN_optimization&#34;&gt;WAN accelerator&lt;/a&gt; is also a cool technique. Let&amp;#8217;s say you want to deliver a dynamic web page from the US to Tokyo. You could have that travel over the open internet with a high latency connection. Or you could route that request to a data center in Tokyo with an optimized connection to the US. The user gets a low-latency connection to the Tokyo datacenter which in turn gets a low-latency high bandwidth connection to the US. This can greatly simplify issues with running and keeping multiple data centers in sync.&lt;/p&gt;

&lt;h2 id=&#34;the-bottom-line:117df65302b8db2107451cddb1557896&#34;&gt;The Bottom Line&lt;/h2&gt;

&lt;p&gt;There&amp;#8217;s a lot of effort underway in making the web faster by changing how tcp connections are leveraged on the web. Http 1.0 requires a new connection for every request/response and browsers limit the number of parallel connections between client and server between two and six. Http keep-alive and &lt;a href=&#34;http://en.wikipedia.org/wiki/HTTP_pipelining&#34;&gt;http pipelining&lt;/a&gt; offer mechanisms to push more content through existing connections. Rails 3.1 introduced http streaming via chunked responses. Browsers can fetch assets in parallel with the main html response as soon as tags appear in the main response stream. &lt;a href=&#34;http://www.chromium.org/spdy/spdy-whitepaper&#34;&gt;Spdy&lt;/a&gt;, an effort by Google, is worth checking out: it proposes a multi-pronged attack to leverage as much flow as possible on a single connection. The docs also illustrate interesting pain points with the network on the web. The bottom line is simple: reduce the amount of data that needs to go from one place to another and make the travel time as fast as possible. Small amounts of data over existing parallel connections make a fast web.&lt;/p&gt;

&lt;dl&gt;
&lt;dt&gt;This approach shouldn&amp;#8217;t be limited to users and servers; optimizing network communication within your datacenter is extremely important. You have total control over your infrastructure and can tune your network accordingly. You can also choose your communication protocols: using something like &lt;a href=&#34;http://thrift.apache.org/&#34;&gt;thrift&lt;/a&gt; or &lt;a href=&#34;http://code.google.com/p/protobuf/&#34;&gt;protocol buffers&lt;/a&gt; can save a tremendous amount of bandwidth over xml-based web services on http.&lt;/p&gt;&lt;/dt&gt;
&lt;dt&gt;:&lt;a href=&#34;http://www.scala-lang.org/&#34;&gt;http://www.scala-lang.org/&lt;/a&gt;&lt;/p&gt;&lt;/dt&gt;
&lt;/dl&gt;

&lt;p&gt;:&lt;a href=&#34;http://python.org/&#34;&gt;http://python.org/&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Building for the Web: What Are We Trying to Accomplish?</title>
          <link>http://blog.michaelhamrah.com/2012/01/web-technology-what-are-we-trying-to-accomplish/</link>
          <pubDate>Wed, 04 Jan 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/01/web-technology-what-are-we-trying-to-accomplish/</guid>
          <description>

&lt;p&gt;The web technology landscape is huge and growing every day. There are hundreds of options from servers to languages to frameworks for building the next big thing. Is it &lt;a href=&#34;http://www.nginx.org/en/&#34;&gt;nginx&lt;/a&gt; + &lt;a href=&#34;http://unicorn.bogomips.org/&#34;&gt;unicorn&lt;/a&gt; + &lt;a href=&#34;http://rubini.us/&#34;&gt;rubinus&lt;/a&gt; or a &lt;a href=&#34;http://nodejs.org/&#34;&gt;node.js&lt;/a&gt; restful service on &lt;a href=&#34;http://cassandra.apache.org/&#34;&gt;cassandra&lt;/a&gt; running with &lt;a href=&#34;http://emberjs.com/&#34;&gt;ember.js&lt;/a&gt; and html5 on the front end? Should I learn or ? What&amp;#8217;s the best nosql database for a socially powered group buying predicative analysis real-time boutique mobile aggregator that scales to 100 million users and never fails?&lt;/p&gt;

&lt;p&gt;It is true there are many choices out there but web technology boils down to a very simple premise. You want to respond to a user action as quickly as possible, under any circumstance, while easily changing functionality. Everything from the technology behind this blog to what goes into facebook operates on that simple idea. The problem comes down to doing it at the scale and speed of the modern web. You have hundreds of thousands of users; you have hundreds of thousands of things you want them to see; you want them to buy, share, create, and/or change those things; you want to deliver a beautiful, customized experience; everything that happens needs to be instantaneous; it can never stop working; and the cherry on the cake is that everything is constantly changing; different features, different experiences, different content; different users. The simple &amp;#8220;hello world&amp;#8221; app is easy. But how do you automatically translate that into every language with a personal message and show a real-time graph with historical data of every user accessing the page &lt;em&gt;at scale&lt;/em&gt;? What if we wanted to have users leave messages on the same page? Hopefully by the end of this series you will get a sense of how all the pieces fit together and what&amp;#8217;s involved from going to 10 users to 10 thousand to 10 million.&lt;/p&gt;

&lt;h2 id=&#34;the-web-at-50-000-feet:ba587cddf41afc4433f829b6dc01b418&#34;&gt;The Web at 50,000 Feet&lt;/h2&gt;

&lt;p&gt;The web breaks down to three distinct areas: what happens with the client, what happens on the server, and what happens in between. Usually this is rendering a web page: a user clicks a link, a page delivered, the browser displays the content. It could also involve handling an ajax request, calling an API, posting a search form. Either way it boils down to &lt;em&gt;client action, server action, result&lt;/em&gt;. Doing this within a users attention span given any amount of meaningful content in a fail-safe way with even the smallest amount of variation is why we have all this technology. It all works together to combine flexibility and speed with power and simplicity. The trick is using the right tool in the swiss-army knife of tech to get the job done.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s break all this down a bit. Handling a user action well comes down to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Having the server-side handle the request quickly (Serving)&lt;/li&gt;
&lt;li&gt;A quick travel time between the client and server (Delivering)&lt;/li&gt;
&lt;li&gt;Quickly displaying the result (Rendering)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And developing and managing all this successfully comes down to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Changing any aspect of what is going on quickly and easily (Developing)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As sites grow and come under heavier load doing any one of these things becomes increasingly difficult. More features, more users, more servers, more code. There is only so much one server can do. There is also only so much a server &lt;em&gt;needs&lt;/em&gt; to do. Why hit the database if you can cache the result? Why render a page if you don&amp;#8217;t need to? Why download a one meg html page when it&amp;#8217;s only 100kb compressed? Why download a page if you don&amp;#8217;t have to? How do you do all this and keep your code simple? How do you ensure everything still works even if your &lt;a href=&#34;http://aws.amazon.com/message/65648/&#34;&gt;data center goes down&lt;/a&gt;?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Http plays an important role in all of this. I didn&amp;#8217;t truly appreciate http until I read &lt;a href=&#34;http://www.amazon.com/Restful-Web-Services-Leonard-Richardson/dp/0596529260&#34;&gt;Restful Web Services&lt;/a&gt; by Sam Ruby and Leonard Richardson. Http as an application protocol offers an elegant, scalable mechanism for transferring data and defining intent. Understanding http verbs, various http headers and how http sits on top of tcp/ip can go along way in mastering the web.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;making-it-all-work-together:ba587cddf41afc4433f829b6dc01b418&#34;&gt;Making it all work together&lt;/h2&gt;

&lt;p&gt;So how do you choose and use all the tools out there to serve, deliver, render and develop for the web? What does &lt;em&gt;client action/server action/result&lt;/em&gt; have to do with &lt;a href=&#34;http://rack.rubyforge.org/&#34;&gt;rack&lt;/a&gt; and &lt;a href=&#34;http://en.wikipedia.org/wiki/Web_Server_Gateway_Interface&#34;&gt;wsgi&lt;/a&gt;? I naïvely thought I could write everything I wanted to in a single post: from using &lt;a href=&#34;http://sass-lang.com/&#34;&gt;sass&lt;/a&gt; for compressed, minimized css to sharding databases for horizontal scalability. It will be easier to spread it out a bit so stay tuned. But remember: any language, framework or tool out there is really about improving &lt;em&gt;client action/server action/result&lt;/em&gt;. Even something like &lt;a href=&#34;http://en.wikipedia.org/wiki/WebSocket&#34;&gt;websockets&lt;/a&gt;. Websockets eliminates the client request completely. Why wait for a user to tell you something when you can push them content? Knowing your problem domain, your bottlenecks, and your available options will help you choose the right tool and make the right time/cost/benefit decision.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ll dig into the constraints and various techniques to effectively &lt;a href=&#34;http://wp.me/pnRto-af&#34;&gt;deliver&lt;/a&gt;, &lt;a href=&#34;http://wp.me/pnRto-al&#34;&gt;serve, develop&lt;/a&gt; and &lt;a href=&#34;http://wp.me/pnRto-at&#34;&gt;render&lt;/a&gt; for the web in upcoming posts.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Thoughts on Kanban</title>
          <link>http://blog.michaelhamrah.com/2011/12/thoughts-on-kanban/</link>
          <pubDate>Thu, 15 Dec 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/12/thoughts-on-kanban/</guid>
          <description>

&lt;p&gt;One of my favorite achievements in the agile/lean world has been the progression from standard Scrum practices to a &lt;a href=&#34;http://en.wikipedia.org/wiki/Kanban_(development)&#34;&gt;Kanban&lt;/a&gt; approach of software development. In fact, Kanban, in my opinion, is such an ideal approach to software development I cannot imagine approaching team-based development any other way.&lt;/p&gt;

&lt;h2 id=&#34;what-8217-s-wrong-with-scrum:d7ec680e7904e258131968165612fab2&#34;&gt;What&amp;#8217;s Wrong With Scrum?&lt;/h2&gt;

&lt;p&gt;Before answering this, I want to mention Kanban came only after altering, tweaking, and refining the Scrum process as much as possible. If anything, Kanban represents a &lt;em&gt;graduation&lt;/em&gt; from Scrum. Scrum worked, and worked well, but it was time to take the approach to the next level. Why? The Scrum process was failing. It became too constrained, too limiting. As I mentioned in my &lt;a href=&#34;http://www.michaelhamrah.com/blog/2008/12/adventures-in-agile-practical-scrum-intro/&#34;&gt;three-year old (as of this writing) post on Scrum&lt;/a&gt; one needs to constantly iterate in refining the practice. Pick one thing that isn&amp;#8217;t working, fix it, and move one. Quite simply, there was nothing with Scrum left to refine except Scrum itself.&lt;/p&gt;

&lt;h3 id=&#34;why-scrum-was-failing:d7ec680e7904e258131968165612fab2&#34;&gt;Why Scrum Was Failing&lt;/h3&gt;

&lt;p&gt;The main issue was simply it was time to break free from the time-boxed approach of sprints. Too much effort went into putting stories into iterations. Too much effort went into managing the process. This process took away from releasing new functionality. &lt;em&gt;Nothing can be more important than releasing new functionality.&lt;/em&gt; Tweaking iteration length did not help; one week caused too many meetings to happen too frequently. Two weeks and the early sprint planning effort was lost on stories which would not occur until the second week. Too much time went into making stories &amp;#8220;the right size&amp;#8221;. Some where too small; not worth discussing in a group. Some were too big but they did not make sense to break down to fit into the iteration. Worse, valuable contributions in meetings only occurred with a few people. This had nothing to do with the quality of dev talent; some really good developers did not jive with the story time/sprint review/retrospective/group think model. Why would they? Who really likes meetings?&lt;/p&gt;

&lt;h3 id=&#34;rethinking-constraints:d7ec680e7904e258131968165612fab2&#34;&gt;Rethinking Constraints&lt;/h3&gt;

&lt;p&gt;Scrum has a specific approach to constraints: limit by time. Focus on what can be accomplished in X timeframe (sprints). Add those sprints into releases. Wash, rinse, repeat. Kanban, however, rethinks constraints. Time is irrelevant; the constraint is how much work can occur at any one time. This is, essentially, your work in progress. Limit your work in work in progress (WIP) to work you can be actively doing at any one time. In order to do new work, old work must be done.&lt;/p&gt;

&lt;h3 id=&#34;always-be-releasing:d7ec680e7904e258131968165612fab2&#34;&gt;Always Be Releasing&lt;/h3&gt;

&lt;p&gt;The beauty of this approach is that it lends itself well to a continuous deployment approach. If you work on something, and work on it until it is done, when it is done, it can be released. So release it. Why wait until an arbitrary date? The development pipeline in Kanban is similar to Scrum. Stories are prioritized, they are sized, they are ready for work, they are developed, they are tested, they are released. The main difference is instead of doing these at set times, they are done &lt;em&gt;just-in-time&lt;/em&gt;. In order to move from one stage of the process (analysis, development, testing, etc) there must be an open &amp;#8220;slot&amp;#8221; in the next stage. This is your WIP limit. If there isn&amp;#8217;t an open slot, it cannot move, and stays as is. People can be focused on moving stories through the pipeline rather than meeting arbitrary deadlines, no matter how those deadlines came to be. Even blocking items can have WIP limits. The idea is simple: you have X resources. Map those resources directly to work items as soon as they are available, and see them through to the end. Then start again.&lt;/p&gt;

&lt;h3 id=&#34;everything-is-just-in-time:d7ec680e7904e258131968165612fab2&#34;&gt;Everything is Just In Time&lt;/h3&gt;

&lt;p&gt;All of the benefits of Scrum are apparent in Kanban. Transparency into what is being worked on and the state of stories. Velocity can still be measured; stories are sized and can be timed through the pipeline. Averages can be calculated over time for approximate release dates. The business can prioritize what is next up to the point of development. Bugs can be weaved into the pipeline as necessary, without having to detract from sprints. With the right build and deploy setup releases can occur as soon as code is merged into the master branch. Standup meetings are still important.&lt;/p&gt;

&lt;h3 id=&#34;the-goal:d7ec680e7904e258131968165612fab2&#34;&gt;The Goal&lt;/h3&gt;

&lt;p&gt;The theory of constraints is nothing new. My first encounter was with &lt;a href=&#34;http://www.amazon.com/Goal-Process-Ongoing-Improvement/dp/0884270610&#34;&gt;The Goal by Eliyahu Goldratt&lt;/a&gt;. The goal, in this case, is to release new functionality as efficiently (not quickly, not regularly; efficiently) as possible. There is a process to this: an idea happens, a request comes in. It is evaluated, it is fleshed out, given a cost. It is planned, implemented, and tested. It is released. Some are small, some are big. Some can be broken down. But in teams large and small, they go from inception to implementation to release. Value must be delivered efficiently. It can happen quickly, but it does not need to be arbitrarily time-boxed.&lt;/p&gt;

&lt;p&gt;Scrum is a great and effective approach to software development. It helps focus the business and dev teams on thinking about what is next. It is a great way to get teams on board with a goal and working, in sync, together. It follows a predictable pattern to what will happen when. It offers the constraint of time. Kanban offers the constraint of capacity. For software development this is a far more effective constraint to managing work. You still need solid, manageable stories. You just don&amp;#8217;t have to fit a square peg in a round hole. Kanban streamlines the development process so resources, which always have a fixed limit, are the real limit you are dealing with. They are matched directly to the current state of work so a continuous stream of value can be delivered without the stop-and-go Scrum approach.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>My post about image delivery on the Getty Images Blog</title>
          <link>http://blog.michaelhamrah.com/2011/12/my-post-about-image-delivery-on-the-getty-images-blog/</link>
          <pubDate>Thu, 08 Dec 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/12/my-post-about-image-delivery-on-the-getty-images-blog/</guid>
          <description>&lt;p&gt;I wrote an article covering how we move images to our customers on the new &lt;a href=&#34;http://blog.gettyimages.com/2011/12/06/from-camera-to-customer-faster-than-ever-before/&#34;&gt;Getty Images blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The system is a suite of .NET applications which handle various steps in our workflow. It features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A custom C# module which sits with IIS FTP to alert when a new image arrives&lt;/li&gt;
&lt;li&gt;Services built with the &lt;a href=&#34;https://github.com/Topshelf/Topshelf&#34;&gt;Topshelf framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;WCF services which wrap a rules engine featuring dynamic code generation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Getty Images blog be covering more insight into the system as well as other technology developed at Getty Images. So check out the new blog and subscribe to the RSS feed!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Solr: Improving Performance and Other Considerations</title>
          <link>http://blog.michaelhamrah.com/2011/11/solr-improving-performance-and-other-considerations/</link>
          <pubDate>Tue, 29 Nov 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/11/solr-improving-performance-and-other-considerations/</guid>
          <description>

&lt;p&gt;We use &lt;a href=&#34;http://lucene.apache.org/solr/&#34;&gt;Solr&lt;/a&gt; as our search engine for one of our internal systems. It has been awesome; before, we had to deal with very messy sql statements to support many search criteria. Solr allows us to stick our denormalized data into an index and search on an arbitrary number of fields via an elegant, RESTful interface. It&amp;#8217;s extremely fast, easy to use, and easy to scale. I wanted to share some lessons learned from our experience with Solr.&lt;/p&gt;

&lt;h2 id=&#34;know-your-use-cases:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Know Your Use Cases&lt;/h2&gt;

&lt;p&gt;There are two worlds of Solr: writing data (committing) and reading data (querying). Solr should not be treated like a database or some nosql solution; it is a search indexer built on top of Lucene. Treat it like a search indexer and not a permanent data store; it doesn&amp;#8217;t behave like a database. There are plenty of tools to keep data in database in sync with Solr; the worst case scenario is you have to sync it yourself. You should know how heavy you will query it, how much you&amp;#8217;ll write to it, and have a rough idea what your schema will be (but it doesn&amp;#8217;t have to be 100%). Knowing your use cases will allow you to configure your instance and define your schema appropriately.&lt;/p&gt;

&lt;p&gt;Solr offers a variety of ways to index and parse data; when you&amp;#8217;re starting out, you don&amp;#8217;t need to pick one. Solr has a great copyField feature that allows you to index the same data in multiple ways. This can be great for trying out new things or doing A/B comparisons. Once your patterns are well defined, you can tune your index and configuration as needed.&lt;/p&gt;

&lt;p&gt;Our use cases are pretty straight forward; we simply need to search many different fields and aggregate results. We don&amp;#8217;t need to deal with lexical analyzation or sorting on score. Our biggest issue was actually commits because we didn&amp;#8217;t thoroughly vet our update patterns. Remember, Solr is about commits as much as it is about querying. You need to realize there will be some lag between when you update Solr and when you see the results. There are a large number of factors that go into how long that delay will be (it could be very quick), but it will be there, and you should design your system in knowing there will be a delay rather than trying to avoid it. The commit section covers why you shouldn&amp;#8217;t try and commit on every update, even under moderate load.&lt;/p&gt;

&lt;h2 id=&#34;know-configuration-options:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Know Configuration Options&lt;/h2&gt;

&lt;p&gt;Go through the solrconfig.xml and schema.xml files. It&amp;#8217;s well documented and there are lots of good bits in there (solrconfig.xml is often missed!). The caches are what matter most, and explained in later sections. If you know your usage patterns you can get a good sense of how you can tune your caches for optimal results. Autowarming is also important; it allows Solr to reuse caches from previous indexes when things change.&lt;/p&gt;

&lt;p&gt;Don&amp;#8217;t forget that Solr sits on top of Java, so you should also tune the JVM as appropriate. This probably will revolve around how much memory to allocate to the JVM. Be sure to give as much as possible, especially in production.&lt;/p&gt;

&lt;h2 id=&#34;understand-commits:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Understand Commits&lt;/h2&gt;

&lt;p&gt;You should control the number of commits being made to Solr. Load testing is important; you need to know how often and what happens when Solr will rebuilds an index. You shouldn&amp;#8217;t commit on every update; you will surely hit memory and performance issues. When a commit occurs, an index and search warmer need to be built. A search warmer is a view onto an index. Caches may need to be pre-populated. Locking occurs. You don&amp;#8217;t want to have that overhead if you don&amp;#8217;t need it. If you have any post commit listeners those will also run. Finally, updating without forcing a commit is a lot faster than forcing a commit on update. The downside is simply that data will not be immediately available.&lt;/p&gt;

&lt;p&gt;This is where autocommit comes into play. We use an autocommit every 5 seconds or 5000 docs. We never hit 5000 commits in less than 5 seconds; we just don&amp;#8217;t want data to be too stale. 5000 docs allows us to re-index in production if we need to without killing the system. This ratio provides a good enough index time for searches to work appropriately without causing too many commits from choking the system. Again, know your usage patterns and you can get this number right.&lt;/p&gt;

&lt;h2 id=&#34;search-warmers-and-cold-searches:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Search Warmers and Cold Searches&lt;/h2&gt;

&lt;p&gt;Solr caching works by creating a view on an index called a searcher. A commit will create a warming search to prep the index and the cache. How long this takes is tricky to say, but the more rows, indexable fields, and the more parsing that is done the longer it takes. The default is to only allow two warming searches at once, and depending on how you’re doing commits, you can easily surpass that limit. If you read the solrconfig.xml file you&amp;#8217;ll see that 1-2 is useful for read-only slaves. So you&amp;#8217;re going to want to increase this number on your main instance; but be aware, you can kill your available memory if you&amp;#8217;re committing so much you have a high number of warmers.&lt;/p&gt;

&lt;p&gt;By default Solr will block if a search warmer isn&amp;#8217;t available. Depending on how and when you&amp;#8217;re committing, you may not want this. For instance, if the first search is warming an index, it could be a while before it returns. Be sure to reuse old warmers and see if you can live with a semi-built index. This is all handled in the solrconfig.xml file. Read it!&lt;/p&gt;

&lt;h2 id=&#34;increase-cache-sizes:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Increase Cache Sizes&lt;/h2&gt;

&lt;p&gt;Don&amp;#8217;t forget out-of-the-box mode is not production mode. We&amp;#8217;ve touched on a committing and search warmers. Cache sizes are another important aspect and should be as big as possible. This allows more warmers to be reused and offers a greater opportunity to search against cached search results (fq parameters) versus new query results (q parameters). The more we can cache the better; it also allows Solr to carry over search warmers when rebuilding indices which is very helpful.&lt;/p&gt;

&lt;h2 id=&#34;lock-types:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Lock Types&lt;/h2&gt;

&lt;p&gt;Luckily the default lock type is now &amp;#8220;Native&amp;#8221; which means Solr uses OS level locking. Previously it was single and this killed the system in concurrent update scenarios. Go native.&lt;/p&gt;

&lt;h2 id=&#34;understand-and-leverage-q-and-fq-parameters:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Understand and Leverage Q and FQ parameters&lt;/h2&gt;

&lt;p&gt;Q is the original query, fq is a filter query.  For larger sets this is important. If the original query is cached an fq query will just search the cached original query, rather than the entire index.  So if you have an index with one million records, and a query returns 100k results, a q/fq combination will only search the 100k cached records. This is a big performance win. Ensure your cache settings are big enough for your usage patterns to create more cache hits.&lt;/p&gt;

&lt;h2 id=&#34;minimize-use-of-facets:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Minimize Use of Facets&lt;/h2&gt;

&lt;p&gt;Calculating facets is time consuming and can easily increase a search 2-5x than normal.  This is the slowest bottleneck we have with Solr (but still, it’s minimal compared to sql).  If you can avoid facets than do so.  If you can’t, only calculate them once on initial load, and design a UI that doesn’t need to refresh them (i.e. paging via ajax, etc).  When searching from a facet, use the fq parameter to minimize the set you&amp;#8217;re searching on from your q query. This also reduces the required number of entries that are calculated for a facet and greatly increases performance.&lt;/p&gt;

&lt;h2 id=&#34;avoid-dynamic-fields-in-solr:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Avoid Dynamic Fields in Solr&lt;/h2&gt;

&lt;p&gt;This is more of an application architectural decision rather than anything else, and probably somewhat controversial. I feel you should avoid the use of dynamic fields and focus on defining your schema. I feel you can easily lose control over your schema if your model changes often as you have no base to work from. That can have unintended consequences depending on how you wrap your Solr instance and how you serialize and deserialize Solr data. It’s not too much up front work to define your schema during development that would call for the use of dynamic fields in production, unless of course your app necessitates using dynamic fields for one reason or another.&lt;/p&gt;

&lt;p&gt;The other, more valid argument is that on a per-field level you can specify multi-values, required, and indexable fields. Solr handles multi valued and indexable fields differently on commits. If you are using dynamic fields and are indexing each one, and are not actually searching nor returning these fields, you have a really high and unnecessary commit cost. At the very least, consider turning off indexing for dynamic fields if you don&amp;#8217;t need it.&lt;/p&gt;

&lt;h2 id=&#34;use-field-lists:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Use Field Lists&lt;/h2&gt;

&lt;p&gt;You should always specify what data you want returned from the query with fl (field list). This is extremely important!  Depending on how you’ve set up your schema, you probably have a ton of fields you don’t actually need returned to the UI.  This is common when you are indexing the same field with different parsers via the copyField functionality. Use fl to get back only the data you need- this will greatly reduce the amount data (and network traffic) returned, and speed up the query because Solr will not have to fetch unnecessary fields from its internal storage. In a high-read environment, you can greatly reduce both memory and network load by trimming the fat from your dataset.&lt;/p&gt;

&lt;h2 id=&#34;have-a-reindex-strategy:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Have a Reindex strategy&lt;/h2&gt;

&lt;p&gt;There will come a time when you need to reindex your Solr instance. Most likely this will be when you&amp;#8217;re releasing a new feature. It&amp;#8217;s important to have a reindexing strategy ready to go. Let&amp;#8217;s say you add a new field to your UI which you want to search on. You release your code, but that field is not in Solr yet so you get no results. Or, you get a doc back from Solr, you deserialize it to your object model, and get an error because you expect the field to be there and it&amp;#8217;s not. You must prepare for that. You could change your schema file, reindex in a background process, and then release code when ready. In this scenario make sure you can reindex without killing the system. It&amp;#8217;s also important to know how long it will take. Having to reindex like this may not be practical if takes a couple of days. You could also reindex to a second, unused Solr instance, and when you deploy you cut over to the new instance. By looking at your db update timestamps you can sync any missed data. (Remember how I said Solr is not a data store? This is a reason.)&lt;/p&gt;

&lt;h2 id=&#34;final-thoughts:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;Remember that data in Solr needs to be stored, indexed and returned. If you are only using dynamic fields, indexing all of them, defining copyField settings left and right and returning all that data because you are not using field lists (and potentially calculating facets on everything), you are generating a lot of unnecessary overhead. Keep it small and keep it slim. You&amp;#8217;ll lower your storage needs, your memory requirements, and your result set. You&amp;#8217;ll speed up commits as well.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Data Modeling at Scale:  MongoDb &#43; Mongoid, Callbacks, and Denormalizing Data for Efficiency</title>
          <link>http://blog.michaelhamrah.com/2011/08/data-modeling-at-scale-mongodb-mongoid-callbacks-and-denormalizing-data-for-efficiency/</link>
          <pubDate>Fri, 12 Aug 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/08/data-modeling-at-scale-mongodb-mongoid-callbacks-and-denormalizing-data-for-efficiency/</guid>
          <description>

&lt;p&gt;I found myself confronted with a MongoDb data modeling problem. I have your vanilla User model which has many Items. The exact nature of an Item is irrelevant, but let us say a User can have lots of Items. I struggled with trying to figure out how to model this data in a flexible way while still leveraging the documented-orientated nature of MongoDb. The answer may seem obvious to some but it is interesting to weigh the options available.&lt;/p&gt;

&lt;h3 id=&#34;to-embed-or-not-to-embed:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;To Embed or Not to Embed&lt;/h3&gt;

&lt;p&gt;The main choice was to embed Items in a User or have that as a separate collection. I do not think it makes sense to go vice versa, as Users are unique and clearly a top level entity. It would not make sense to have thousands of the same User in an Items collection. So the choice was between having Items in its own collection or embedding it in Users. A couple of factors came into play: How can I access, sort, or page through Item results if it is embedded in a User? What happens if I had so many Items in a User class I hit the MongoDb 4mb document size limit? (Unlikely: 4mb is a lot of data, but I would certainly not want to have to refactor that logic later on!) What would sharding look like with a large number of very large User documents? Most importantly, at what point would the number of Items be problematic with this approach? A hundred? A thousand? A hundred thousand?&lt;/p&gt;

&lt;h3 id=&#34;when-to-embed:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;When to Embed&lt;/h3&gt;

&lt;p&gt;I think embedded documents are an awesome feature of MongoDb, and the general approach, as recommended on the docs, is to say &amp;#8220;Why wouldn&amp;#8217;t I put this in an embedded document?&amp;#8221;. I would say if the number of Items a User would have is relatively small (say, enough that you would not need to page them on a UI, or if it would not create large network io by just accessing that field) then it can be an embedded document. The decision is a lot simpler if it is a 1..1 relationship as the potential size is clearly defined. 1..N relationships break down with embedded relations when N becomes so large that accessing it as a whole is impractical. As far as I know there does not seem to be a way to page or sort through an embedded array directly within MongoDb: you need to pull the entire field out of the database with field selection and then page on the client. Note MongoDb offers numerous ways to find data within a document no matter how it is stored within the document (see the &lt;a href=&#34;http://www.mongodb.org/display/DOCS/Dot+Notation+%28Reaching+into+Objects%29&#34;&gt;docs on dot notation&lt;/a&gt; for more). You can even query on the position of elements in an array, which is helpful with sorted embedded lists (find me all Users who have Item Z as the first element). But sadly you cannot say &amp;#8220;give me the first to the Nth element in an embedded array&amp;#8221;. It is all or nothing.&lt;/p&gt;

&lt;p&gt;Now Mongoid does offer the ability to page through an embedded association using a gem (seems like people use &lt;a href=&#34;https://github.com/amatsuda/kaminari&#34;&gt;Kaminari&lt;/a&gt; as will_paginate was removed from Mongoid some time ago). However, this paging is done within the ruby object for embedded relations. More importantly, it is only done on a per-document basis. Under the hood you need to grab the entire embedded relation &lt;em&gt;embedded within its root document&lt;/em&gt; (think an array of Users containing an array of Items, not a plain array of Items). This means you cannot grab a collection of embedded documents which span multiple root documents. You cannot say &amp;#8220;give me all Items of type &amp;#8216;X&amp;#8217;. You need to say &amp;#8220;give me all Users and its Items containing Items of type &amp;#8216;X&amp;rsquo;&amp;#8221;. If you ever ran into the &amp;#8220;Access to the collection for XXX is not allowed since it is an embedded document, please access a collection from the root document&amp;#8221; error you are probably trying to issue an unsupported Mongoid query by bypassing a root document. You think you can treat embedded relations like normal collections, but you can&amp;#8217;t.&lt;/p&gt;

&lt;h3 id=&#34;when-to-have-separate-collections:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;When to Have Separate Collections&lt;/h3&gt;

&lt;p&gt;So where does that leave us: If the relation is small enough, than an embedded relation is fine: we just need to realize that we can never really treat elements in that collection across its top level document and that getting those elements is an all-or-nothing decision for each parent document. For the sake of argument, let us say a User can have thousands of Items, and we wanted the ability to list Items across Users in a single view. That would be too much to manage as its own field as an embedded document, and we could not aggregate Items across Users easily. So it needs to be in its own collection. This now gives us numerous sorting options and paging features like skip and limit to reduce network traffic. If we have Items as its own collection then we can create a DBRef between the two. This is a classical relational breakdown. The thing that smells with this approach, specifically when using MongoDb, is that if I were viewing a list of Items, and wanted to show the username associated with them, I would either have to use a DBRef command to pull user information or make two queries. Less than ideal. A JOIN would certainly be easier (albeit at scale, impractical, but probably for the DbRef approach too).&lt;/p&gt;

&lt;h3 id=&#34;the-solution:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;The Solution&lt;/h3&gt;

&lt;p&gt;So what I&amp;#8217;m really looking for is the ability to show the username with a list of Items when each has its own collection. The trick is I do not need to aggregate this data when I am pulling it out of the database. Instead I can assemble it before I put in the database and it will all be there when I take it out. Classic denormalization. With Mongoid and &lt;a href=&#34;http://mongoid.org/docs/callbacks.html&#34;&gt;Callbacks&lt;/a&gt; this becomes extremely easy.&lt;/p&gt;

&lt;p&gt;On my Items class I add a _:belongs&lt;em&gt;to :user&lt;/em&gt; property along with a &lt;em&gt;:username&lt;/em&gt; property. I want to ensure that a &lt;em&gt;:user&lt;/em&gt; always exists, so I add a &lt;em&gt;validates_presence_of :user&lt;/em&gt; validation. I do not need to add &lt;em&gt;:username&lt;/em&gt; to this validation as we will see below. Then I leverage callbacks like so:&lt;/p&gt;

&lt;pre class=&#34;syntax ruby&#34;&gt;before_save :add_username

protected
def add_username
  if user_id_changed?
    self.username = user.username
  end
end&lt;/pre&gt;

&lt;p&gt;What will happen is if the User property changed Mongoid will set the current Item&amp;#8217;s username value to the user.username property value. The username field is now stored within the Item document, and I can query on this field as easily as any other Item property (including the user_id relation on the Item document). More importantly, it is already available in a query result so there is no need to make an additional query on User.username for display. Any time the user changes (if Items can switch Users) the username will be updated automatically before the save to maintain consistency. Because the :user object is required, there is no need to also make :username required. Username will read from the required User property before each save. There is a slight catch with this approach: callbacks will only be run on document which received the save call, so be careful with cascading updates. As always a great test suite will always ensure the behavior you want is enforced.&lt;/p&gt;

&lt;h3 id=&#34;sharding:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;Sharding&lt;/h3&gt;

&lt;p&gt;The other point about the user relation, whether it is via the username field or on user_id, is that it makes a good shard key. If we shard off of this field (probably in conjunction of another key) you can control things like write scaling while keeping relevant data close together for querying. For instance, sharding only on username will put all data in the same server to make querying a user&amp;#8217;s items extremely efficient. Sharding on username and something else will get writes distributed across servers at the expense of having to gather elements across servers when returning results. The bottom line is know your use case: are faster writes more important than faster reads? Which one are you doing more of?&lt;/p&gt;

&lt;h3 id=&#34;in-conclusion:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;In Conclusion&lt;/h3&gt;

&lt;p&gt;I think there are two important things to realize when it comes to modeling with not just Mongoid but with any type of data store, sql or nosql. First when you are dealing with scale you want to put your data in the same way you want to get it out. Know your data access patterns. Sql allows a tremendous amount of flexibility, but joining numerous tables across millions of rows is extremely inefficient. More importantly, if you model your data in NoSql incorrectly, you could end up with similar performance problems. In the case of the data denormalization exercise above, adding a username field to the Items collections saves us from a DbRef later. Plus, with the use of callbacks, getting our data into Mongoid in a denormalized way is easy. We could easily apply the same principle to a sql-based solution: add a username column to a Item table or create a materialized view/indexed view on the Users/Items data. If you are debating a no-sql solution over a sql one, take a look at the cost/benefit of one approach over another in terms of how easy it is to model your data around data access. I think MongoDb gives a good amount of flexibility, especially with querying and indices, while still promoting some of the NoSql goodies like easy sharding for scalability and easy replication for reliability and read scaling.&lt;/p&gt;

&lt;p&gt;Secondly, it is extremely important to know your toolset. With MongoDb, you get a tremendous amount of querying power: filtering on any field, no matter the nesting, even if it&amp;#8217;s an array; creating indices on said fields; map/reduce views; only retrieving specific fields from a document; the list is nearly endless. ORM features are important too: How does Mongoid map its API to MongoDB commands? How does it deal with dirty tracking? What callbacks are available? The coolest thing on the &lt;a href=&#34;http://www.mongoid.org&#34;&gt;Mongoid&lt;/a&gt; website is the statement &lt;em&gt;This is why the documentation provides the exact queries that Mongoid is executing against the database when you call a persistence operation. If we took the time to tell you, you should listen.&lt;/em&gt; VERY TRUE! I like that. The point being, there should be a purpose why you are choosing a NoSql solution: so know what it is and leverage it. It will mean the difference of succeeding at scale or failing at launch.&lt;/p&gt;

&lt;h3 id=&#34;cassandra:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;Cassandra&lt;/h3&gt;

&lt;p&gt;As an interesting footnote, I think Cassandra exemplifies the query-first approach to data modeling (I mean, it states so on its wiki!). Cassandra&amp;#8217;s uniqueness is in its masterless approach as a key/value store. It comes with some interesting features: the choice of using a secondary index vs. columnfamily as index, numerous comparison operators on columnfamily names, super columns vs. columns for storing data, replication and write consistency options across multiple data centers. This leads to plenty of benefits but with a certain cost. As for the know your tools/know your data philosophy, an example is the typical choice of &amp;#8220;Do you create a row and use its respected columns as an index, choosing an appropriate column comparison type, or do you treat your data as a key/value store and use a secondary index for queries?&amp;#8221; One the one hand, you have a pre-sorted list that queries from one machine and with one call with slices for paging; on the other, you may need to farm out to a lot of machines to get the data you want. Knowing your options is important, and knowing what you have to do to implement your choice is nearly as important. Even with the best Cassandra ORMs you still need to do a lot of prep to get your data into and out of Cassandra in a meaningful way.&lt;/p&gt;

&lt;h3 id=&#34;final-thought:f1205cf7689e3c00acdefcf31ce7d859&#34;&gt;Final Thought&lt;/h3&gt;

&lt;p&gt;In a bit of contradictory advice, I&amp;#8217;d say don&amp;#8217;t sweat it too much. Do some preliminary research, go with your hunch and trust your ability to refactor when needed. If you wait to figure out the perfect solution, you won&amp;#8217;t build anything!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Nina, My New Favorite Web (Micro)Framework</title>
          <link>http://blog.michaelhamrah.com/2011/05/nina-my-new-favorite-web-microframework/</link>
          <pubDate>Tue, 10 May 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/05/nina-my-new-favorite-web-microframework/</guid>
          <description>

&lt;p&gt;One of the things I&amp;#8217;m excited to see is the huge increase in Open Source projects in the .NET world. NuGet has certainly helped the recent explosion, but even before that there have been numerous projects gaining legs in the .NET community. Even better, the movement has been learning from other programming ecosystems to bring some great functionality into all kinds of .NET based systems.&lt;/p&gt;

&lt;p&gt;One of my favorite projects on the scene is &lt;a href=&#34;http://jondot.github.com/nina/&#34;&gt;Nina, a Web Micro Framework&lt;/a&gt; by &lt;a href=&#34;http://twitter.com/#!/jondot&#34;&gt;jondot&lt;/a&gt;. What exactly is a web micro framework? Quite simply it easily allows you to go from an HTTP request to a server side method call with little friction. The project is inspired by &lt;a href=&#34;http://www.sinatrarb.com/&#34;&gt;Sinatra&lt;/a&gt; a very popular ruby framework for server-side interaction which doesn&amp;#8217;t involve all the overhead of a convention based framework like Ruby on Rails.&lt;/p&gt;

&lt;h2 id=&#34;wait-isn-8217-t-this-mvc:340cb49b2e29e9bef84588c65c69d675&#34;&gt;Wait, Isn&amp;#8217;t This .MVC?&lt;/h2&gt;

&lt;p&gt;Sort of- but the two frameworks take very different approaches in how they map an HTTP request to a function call. .MVC is a huge improvement over &amp;#8220;that which must not be named&amp;#8221; but still abstracts the underlying HTTP request/response: controllers and actions to handle logic, models to represent data, views to render results, and routing to figure out what to do. This is usually a good thing as you can easily get fully formed objects into and out of the server in an organized way and has incredible benefits over WebForms. But sometimes that is too much for what you want or need. In our ajax driven world we simply want to do something&amp;#8211;GET or POST some data&amp;#8211;as quickly and easily as possible. We don&amp;#8217;t want to set up a routing for new controller, create a model or view model, invoke an action, return a view, and all that other stuff; we just want to look at the request and do something. That&amp;#8217;s where Nina comes in- it elegantly lets you &amp;#8220;think&amp;#8221; in HTTP by providing an API to do something based on a given HTTP request. It&amp;#8217;s extremely lightweight and extremely fast. It&amp;#8217;s the bare essentials of MVC by providing a minimalist view of functionality in a well defined DSL. On the plus side, the MVC framework and Nina can complement each other quite well (Nina can also stand on its own, too!). Let&amp;#8217;s take a look.&lt;/p&gt;

&lt;h2 id=&#34;how-it-works:340cb49b2e29e9bef84588c65c69d675&#34;&gt;How It Works&lt;/h2&gt;

&lt;p&gt;Nina is essentially functionality added to a web project in the same way the MVC bits are added to a web project. It&amp;#8217;s not an entirely new HTTP server implementation. It&amp;#8217;s powered off of the standard .NET HttpApplication class and unlike the various &lt;a href=&#34;http://owin.org/&#34;&gt;OWIN&lt;/a&gt; toolkits Nina doesn&amp;#8217;t try and rewrite the underlying HttpContext or IIS server stack. To start things off Nina is powered by creating a class that handles all requests to a given url, referred to as an endpoint. This class inherits from &lt;em&gt;Nina.Application&lt;/em&gt; and handles all requests to that endpoint- no matter what the rest of the url is. This is done by &amp;#8220;mounting&amp;#8221; the class to an endpoint in your Global.asax file. It&amp;#8217;s not too different than setting up a routing for MVC. However, instead of MVC, you&amp;#8217;re not routing directly to specific actions or a pattern of actions, but &amp;#8220;gobbing&amp;#8221; up all requests to that url endpoint. Below is an example of a global.asax file from the Nina demo project. There are two Nina applications- the Contacts class gets mounted to the contacts endpoint and Posts gets mounted to the blog endpoint.&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;private static void RegisterRoutes()
    {
            RouteTable.Routes.Add(new MountingPoint(&#34;contacts&#34;));
            RouteTable.Routes.Add(new MountingPoint(&#34;blog&#34;));
    }&lt;/pre&gt;

&lt;p&gt;When you&amp;#8217;re mounting an endpoint any request to that endpoint will go to that class- and that class will handle everything else. So anything with a url of /contacts, /contacts/123, /contacts/some/long/path/with/file.html?x=1&amp;amp;y=1 will go to the Contacts class. There&amp;#8217;s no automatic mapping of url parts to action names, or auto filling of parameters. That&amp;#8217;s all handled by the class you specify which inherits from Nina.Application. Routing to individual methods is handled within these classes by leveraging the Nina DSL. I like this approach, as it keeps routing logic tied to specific endpoints rather than requiring you to centrally locate everything or to dictate globally how routing should work via conventions. Of course, there are pros and cons in either case. In very complex systems the Global.asax can get quite large; you can certainly refactor routing logic into helper functions as necessary, but moving routing definitions closer to the logic has its benefits. I&amp;#8217;m also not too big of a fan when it comes to attribute based programming so not having to pepper your action methods with specific filters- whether for a Uri template in the case of WCF or Http Verbs for .MVC- is a big plus.&lt;/p&gt;

&lt;h2 id=&#34;handling-requests:340cb49b2e29e9bef84588c65c69d675&#34;&gt;Handling Requests&lt;/h2&gt;

&lt;p&gt;This is where the beauty of Nina comes in. Once we&amp;#8217;ve mounted an application to an endpoint we can handle what to do based on two variables: the HTTP method and the path of the request. This is done via four function calls which are part of the Nina.Application class and map to the four HTTP verbs: Get(), Put(), Post() and Delete(). Each function takes in two parameters: the first is a Uri template which determines when this method gets invoked. The second is a lambda with a signature of Func&lt;NameValueCollection, HttpContext, ResourceResult&gt;. This lambda is what gets invoked when the current requests matches the Uri template. The first parameter are the template parts (explained later), the second parameter is the underlying HttpContext object, and the Function returns a Nina.ResourceResult class. For all intents and purposes a ResourceResult is similar to an ActionResult in .MVC. Nina provides quite a number of ResourceResults, from Html views to various serialization objects to binary data.&lt;/p&gt;

&lt;p&gt;This setup is powered by an extremely nice DSL for handling function invocation from HTTP requests and yields a very nice description of your endpoint. You specify the HTTP verb required to invoke the function. You specify the Uri template to when that match should occur&amp;#8211;very similar to setting up routes&amp;#8211;and your handler is actually a parameter, which you can specify inline or elsewhere if needed. The Uri templating is pretty slick, as it allows any level of fuzzy matching. Because the template is automatically parsed and passed as a variable to your handler, you can easily get out elements of the Uri using the template tokens in your Uri. Let&amp;#8217;s take a look at a simple example:&lt;/p&gt;

&lt;p&gt;Take a look at the example application below.&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;public class Contacts : Nina.Application
    {
        public Contacts()
        {
            Get(&#34;&#34;, (m, c) =&amp;gt;
                        {
                            //Returns anything at the root endpoint, i.e. /contacts
                            var data = SomeRepository.GetAll();
                            return Json(data);
                        });
            Get(&#34;Detail/{id}&#34;, (m, c) =&amp;gt;
                                   {
                                       //Returns /contacts/detail/XYZ

                                       //m is the bound parameters in the template
                                       //this will be a collection with m[&#34;ID&#34;] returning XYZ
                                       var id = m[&#34;ID&#34;]; //returns XYZ

                                       var data = SomeRepository.GetDetail(id);
                                       return View(&#34;viewname&#34;, data); //Nina has configurable ViewEngines!
                                   });

            Post(&#34;&#34;, (m,c) =&amp;gt;
                         {
                             //A post request to the root endpoint.

                             return Nothing(); 
                         });
         }
}&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;re exposing three operations: two GET calls and one POST. We&amp;#8217;re handling a GET and POST operation at the endpoint root. In our global.asax we&amp;#8217;ve mounted this application at /contacts, so everything here is relative to /contacts. A template of &amp;#8220;&amp;#8221; will simply match a Uri of /contacts. If we wrote  &lt;code class=&#34;syntax c#&#34;&gt;RouteTable.Routes.Add(new MountingPoint(&amp;ldquo;contacts&amp;rdquo;));&lt;/code&gt; in our Global.asax than this class would be at the root of our application, i.e. &amp;#8220;&lt;a href=&#34;http://localhost/&amp;amp;#8221&#34;&gt;http://localhost/&amp;amp;#8221&lt;/a&gt;;. Finally, we have another GET call at /detail/{id}. This is actually a URI template, similar to a Route, so anything which matches that template will be handled by that function. In this case /detail/123 or /detail/xyz would match. The template variables are passed as a key/value array in the &amp;#8220;m&amp;#8221; parameter of the lambda and can easily be pulled out. These are your template parts that are automatically parsed out for you.&lt;/p&gt;

&lt;p&gt;Using this DSL we can create any number of handlers for any GET, POST, PUT or even DELETE request. We can easily access HTTP Headers, Form variables, or the Request/Response objects from the HttpContext class. Most importantly we can easily view how a request will get handled by our system. The abstraction that MVC brings via Routes, Controllers and Actions is helpful; but not always necessary. Nina provides a different way of describing what you want done that serves a variety of purposes.&lt;/p&gt;

&lt;h2 id=&#34;returning-results:340cb49b2e29e9bef84588c65c69d675&#34;&gt;Returning Results&lt;/h2&gt;

&lt;p&gt;So far we&amp;#8217;ve focused on the Request side of Nina and haven&amp;#8217;t delved too much in the Response side. Nina&amp;#8217;s response system is very similar to .MVC&amp;#8217;s ActionResult infrastructure. Nina has a suite of classes which inherit from ResourceResult that allows you to output a response in a variety of ways. You can serialize an object into Json or Xml, render straight text, return a file, return only a status code, or even return a view. Nina supports numerous View engines&amp;#8211;including Razor but also NHaml, NDjango and Spark&amp;#8211;that&amp;#8217;s beyond the scope of this blog but worth checking out. I&amp;#8217;m a big fan of Haml. Results are returned using one of the method calls provided through the Nina.Application class and should serve all your needs. The best thing to do is explore the Nina.Application class itself and find out which methods return ResourceResults objects.&lt;/p&gt;

&lt;h2 id=&#34;this-is-cool-but-why-use-it:340cb49b2e29e9bef84588c65c69d675&#34;&gt;This is cool, but why use it?&lt;/h2&gt;

&lt;p&gt;The great part about Nina is that even though it can stand alone as an application, it can just as easily augment an existing WebForms (Blah!) or MVC application via mounting endpoints using the Routing engine. There are times when you want speed and simplicity for your web app rather than a fully-fledged framework. MVC is great, but requires quite a few moving parts and abstracts away underlying HTTP. The new Restful Web API&amp;#8217;s Microsoft is rolling out for WCF are also nice, but I&amp;#8217;ve never been a fan of attribute based programming and the WCF endpoints are service specific. Nina offers much more flexibility. Nina strikes the right balance by honoring existing HTTP conventions while providing flexibility of output. Sinatra, Nina&amp;#8217;s inspiration, came about by those who didn&amp;#8217;t want to follow the Rails bandwagon and the MVC convention it implemented. They wanted an easier, lightweight way of parsing and handling HTTP requests, and that&amp;#8217;s exactly what Nina does.&lt;/p&gt;

&lt;p&gt;Here are some use cases where Nina works well:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Json powered services. Even though MVC has JsonResult, Nina provides a low friction way of issuing a get request to return Json data, useful for Autosuggest lists or other Json powered services. JQuery thinks in terms of get/post commands so mapping these directly to mounted endpoints becomes much more fluid. One of my more popular articles is the &lt;a href=&#34;http://www.michaelhamrah.com/blog/2010/08/the-new-webapp-architecture-asp-net-mvc-3-jquery-templating-with-pure-and-the-json-value-provider/&#34;&gt;New Web App Architecture&lt;/a&gt;. Nina provides a nice alternative to Json powered services that can augment one of the newer javascript frameworks like &lt;a href=&#34;knockoutjs.com&#34;&gt;Knockout&lt;/a&gt; or &lt;a href=&#34;http://documentcloud.github.com/backbone/&#34;&gt;Backbone&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Better file delivery. HttpHandlers work well, but exist entirely outside the domain of your app. Powering file delivery through Nina- either because the info is in a data store or required specific authentication, works well.&lt;/li&gt;
&lt;li&gt;Conventions aren&amp;#8217;t required. Setting up routings, organizing views, and implementing action methods all require work and coding. Most of the time, you just want to render something or save something. Posting a search form, save a record via ajax, polling for alerts are all things that could be done with the conventions of MVC but aren&amp;#8217;t necessarily needed. Try the lightweight approach of Nina and you&amp;#8217;ll be glad you did. With support for View engines you may even want to come up with your own conventions for organizing content.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When the time it takes to do something simple simply becomes too great you&amp;#8217;re using the wrong tool. I strongly encourage you to play around with Nina&amp;#8211; you&amp;#8217;ll soon learn to love the raw power of HTTP and the simplicity of the API. It will augment your existing tool belt quite well and you&amp;#8217;ll find how much you can do when when you can express yourself in different ways.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Updated MVC3 Html5 Boilerplate Template: Now with Twitter and Facebook</title>
          <link>http://blog.michaelhamrah.com/2011/03/updated-mvc3-html5-boilerplate-template-now-with-twitter-and-facebook/</link>
          <pubDate>Mon, 21 Mar 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/03/updated-mvc3-html5-boilerplate-template-now-with-twitter-and-facebook/</guid>
          <description>&lt;p&gt;I pushed a major update to the MVC3/Html5 Boilerplate Template found on the &lt;a href=&#34;https://github.com/mhamrah/Html5OpenIdTemplate&#34;&gt;github&lt;/a&gt; page. The new update includes the latest boilerplate code and uses the DotnetOpenAuth CTP for logging in via Twitter and Facebook. Thanks to &lt;a href=&#34;http://www.twitter.com/jacob4u2&#34;&gt;@jacob4u2&lt;/a&gt; for making some necessary web.config changes (he has an alternate template on his &lt;a href=&#34;https://bitbucket.org/jacob4u2/mothereffin-html5-site&#34;&gt;bitbucket&lt;/a&gt; site you should also check out.&lt;/p&gt;

&lt;p&gt;Your best option is to &lt;code class=&#34;syntax bash&#34;&gt;git clone git@github.com:mhamrah/Html5OpenIdTemplate.git&lt;/code&gt; the template with your own app. That way you&amp;#8217;ll get the latest nu-get packages with the bundle. You can also use the template, but you&amp;#8217;ll need to manually pull &lt;a href=&#34;http://sourceforge.net/projects/dnoa/files/CTP/OAuth2/&#34;&gt;the latest CTP for DotNetOpenAuth&lt;/a&gt; to get the latest dlls.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Quicktip: Use Negative Margins with CSS Transforms to Fix Clipping</title>
          <link>http://blog.michaelhamrah.com/2011/03/quicktip-use-negative-margins-with-css-transforms-to-fix-clipping/</link>
          <pubDate>Wed, 02 Mar 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/03/quicktip-use-negative-margins-with-css-transforms-to-fix-clipping/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve been playing around with CSS Transforms and had an annoying issue: when rotating divs at an angle, the edge of the div also rotated leaving a gap where I didn&amp;#8217;t want one. See the pic:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2011/03/rotated-div.png&#34;&gt;&lt;img src=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2011/03/rotated-div.png?resize=246%2C117&#34; alt=&#34;&#34; title=&#34;rotated div2&#34; class=&#34;aligncenter size-full wp-image-531&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this case, the div is actually a header tag I wanted to span the length of the page, like a ribbon stretching the width of browser. But I did not want that gap. So what to do? I thought about using Transforms to skew the header the angle required to maintain a vertical line, but that&amp;#8217;s annoying.&lt;/p&gt;

&lt;p&gt;Instead, I simply added a negative margin to the width to stretch the header enough to hide the gap. Here&amp;#8217;s the css and final result:&lt;/p&gt;

&lt;pre class=&#34;syntax css&#34;&gt;header 
{
  background-color: #191919;
  margin: 75px -20px;
  -webkit-transform: rotate(-10deg);
  -moz-transform: rotate(-10deg);
  tranform:rotate(-10deg);
}
&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://i0.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2011/03/rotated-div2.png&#34;&gt;&lt;img src=&#34;http://i0.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2011/03/rotated-div2.png?resize=246%2C117&#34; alt=&#34;&#34; title=&#34;rotated div2&#34; class=&#34;aligncenter size-full wp-image-531&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You may find rotate and skew is a better combination to achieve the desired result- however, if you have text in your containing element, that text will also be skewed if you use skew.&lt;/p&gt;

&lt;p&gt;To achieve the desired result within a page (where you don&amp;#8217;t have the benefit of viewport clipping) you can always put the rotated element within another element, use negative margins, and set &lt;code class=&#34;syntax css&#34;&gt;overflow:hidden&lt;/code&gt; to achieve the desired result.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Loving Vim</title>
          <link>http://blog.michaelhamrah.com/2011/02/loving-vim/</link>
          <pubDate>Sun, 06 Feb 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/02/loving-vim/</guid>
          <description>

&lt;p&gt;Vim has quickly become my go-to editor of choice for Windows, Mac and Linux. So far I&amp;#8217;ve had about three months of serious Vim usage and I&amp;#8217;m just starting to hit that vim-as-second-nature experience where the power really starts to shine. I&amp;#8217;m shocked I&amp;#8217;ve waited this long to put in the time to seriously learn it. Now that I&amp;#8217;m past the beginner hump I wish I learned Vim long ago- when I tried Vim in the past, I just never got over that WTF-is-going-on-here frustration! Better late than never I suppose!&lt;/p&gt;

&lt;h3 id=&#34;why-i-like-vim-8211-mac:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Why I Like Vim &amp;#8211; Mac&lt;/h3&gt;

&lt;p&gt;Coming from Visual Studio, I longed for a VS like experience for programming on my Mac, whether it&amp;#8217;s html/css/js or for my recent focus on Ruby and Rails. I checked out both &lt;a href=&#34;http://www.aptana.com&#34;&gt;Aptana&lt;/a&gt; and &lt;a href=&#34;http://www.eclipse.org&#34;&gt;Eclipse&lt;/a&gt; but quickly became frustrated- it was kind of like VS but not really and it was just too weird going back and forth. Plus, my biggest pet peave with development started to emerge: I wasn&amp;#8217;t learning a language, I was learning a tool that abstracted the language away. There&amp;#8217;s nothing that could be worse- once your tool hides the benefits of the underlying infrastructure, you&amp;#8217;re missing the point, and you&amp;#8217;ll usually be behind the curve because the language always moves faster than the support.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://macromates.com&#34;&gt;TextMate&lt;/a&gt; then became the go-to: it&amp;#8217;s widely used, powerful, and there&amp;#8217;s a lot of resources for learning. The simplified environment mixed with the command line really created a higher degree of fluidity, and I realized how nice it can be to develop outside an integrated environment. Textmate has its features- Command-T is slick, the project drawer is helpful, and the Rails support is great along with the other available bundles. But it lacked split windows which drove me crazy. There&amp;#8217;s nothing more essential than split windows: I want to see my specs and code side-by-side. I want to see my html and js side-by-side. And you can&amp;#8217;t do that with the Textmate. So I turned to &lt;a href=&#34;http://code.google.com/p/macvim&#34;&gt;MacVim&lt;/a&gt; and haven&amp;#8217;t looked back.&lt;/p&gt;

&lt;h3 id=&#34;why-i-like-vim-8211-windows:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Why I Like Vim &amp;#8211; Windows&lt;/h3&gt;

&lt;p&gt;Don&amp;#8217;t get me wrong: I love me some Visual Studio. Visual Studio was my first IDE when programming professionally and my thought was &amp;#8220;wow, I can really focus on building stuff rather than pulling my hair out with every build, every exception and every bug&amp;#8221;. It was so much better than the emacs days of college. It&amp;#8217;s my go-to for anything .NET, as it should be. But there are some text-editor needs that aren&amp;#8217;t related to coding or .NET, and VS is too much of a beast to deal with for those things. First, for html/js/css editing that&amp;#8217;s not part of a Visual Studio project, VS not great to work with. It&amp;#8217;s annoying to be forced to create a Visual Studio project to house related content, especially when it&amp;#8217;s already grouped together in the file system. Quickly checking out an html template or a js code samples becomes tedious when you just want to look around. The VS File Explorer is a step in the right direction, but it&amp;#8217;s not there yet; I know there&amp;#8217;s shell plugins for a &amp;#8220;VS Project Here&amp;#8221; shortcut but really? Is that necessary?&lt;/p&gt;

&lt;p&gt;Then there&amp;#8217;s the notepad issue. Notepad is barely an acceptable editor for checking out the occasional config file or random text file. Everyone knows how incredibly limited it can be, not to mention how much it sucks for large files. Pretty much everything about it sucks, actually, and everyone knows it. &lt;a href=&#34;http://notepad-plus-plus.org/&#34;&gt;Notepad++&lt;/a&gt; is a nice alternative, but it&amp;#8217;s no Vim. So after I got modestly comfortable with MacVim, I thought, why not do this on Windows too? So I started using &lt;a href=&#34;http://www.vim.org/download.php&#34;&gt;gVim&lt;/a&gt; and haven&amp;#8217;t looked back.&lt;/p&gt;

&lt;h3 id=&#34;why-i-like-vim-8211-linux:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Why I Like Vim &amp;#8211; Linux&lt;/h3&gt;

&lt;p&gt;This blog runs WordPress on a Linux box hosted by Rackspace. Occasionally, I need to pop in via ssh, edit a config file, push some stuff, tweak some settings, etc. &lt;a href=&#34;http://www.nano-editor.org/&#34;&gt;Nano&lt;/a&gt; was the lightweight go-to editor of choice, and works well. There are many differences between nano and Vim, the biggest being nano is a &amp;#8220;modeless&amp;#8221; editor while Vim&amp;#8217;s power comes from the Normal, Insert, and Visual modes. Like always, stick with what works. But once you&amp;#8217;re hooked on Vim, I&amp;#8217;d be surprised how often you go back to Nano, especially when you get your configuration files synced up with source control, and Vim&amp;#8217;s ubiquity on Linux.&lt;/p&gt;

&lt;h3 id=&#34;why-i-like-vim:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Why I Like Vim&lt;/h3&gt;

&lt;p&gt;There&amp;#8217;s plenty of resources out there about Vim and what makes it great- a quick &lt;a href=&#34;http://www.google.com/search?q=why+i+love+vim&#34;&gt;google search&lt;/a&gt; &lt;a href=&#34;http://www.google.com/search?q=coming+home+to+vim&#34;&gt;will give you some great resources&lt;/a&gt;. But I really appreciated Vim when I became comfortable with the following features:&lt;/p&gt;

&lt;h4 id=&#34;splits-and-buffers:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Splits and Buffers&lt;/h4&gt;

&lt;p&gt;Splits are one of my favorite features- it allows you to view more than one file at once on the same screen. The power of splits allows you to have multiple vertically and horizontally split windows so you can see anything you want. There&amp;#8217;s also tab support, but I find using splits and managing buffers a better way to cycle through files. Buffers allow you to keep files open and active but in the background, so you can quickly swap them into the current window in a variety of ways. It&amp;#8217;s like having a stack of papers on a table while easily going to any page instantaneously. This is different than having files within a project, which would be like having those papers in a folder- buffers provide another level of abstraction allowing you to manipulate a set of content together. In summary, you have a set of papers in a folder (the current directory) and put them on the desk to work on (using buffers) and arrange them in front of you (using splits).&lt;/p&gt;

&lt;h4 id=&#34;modes:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Modes&lt;/h4&gt;

&lt;p&gt;A major feature of Vim, which really sets it apart from other editors, is how it separates behavior into different modes- specifically, normal and insert modes. Insert mode is simple: it allows you to write text. But normal mode is all about navigation and manipulation: finding text, cutting lines, moving stuff around, substituting words, running commands, etc. It offers a whole new level of functionality including shell commands interaction for doing anything you would on the command line. With the plethora of plugins around you can pretty much do anything within Vim you could imagine- from simple editing, to testing, to source control management, to deployments. Modes break you free from a whole slew of Ctrl + whatever commands required in other editors, allowing for precision movement with a minimal set of keystrokes. The best analogy is you can &amp;#8220;program your editing&amp;#8221; in a way unmatched from any other editor.&lt;/p&gt;

&lt;h4 id=&#34;search-and-replace-aka-substitute:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Search and Replace (aka Substitute)&lt;/h4&gt;

&lt;p&gt;Vim&amp;#8217;s Search and Substitute features make any old find and replace dialog box seem stupid. I&amp;#8217;ve barely started unlocking the power of search and replace, but already, I wish every application behaved this way. In normal mode you can easily create everything from simple string searches to complex regex to find what your looking for in a couple of keystrokes. On top of that, it&amp;#8217;s only a few more keystrokes to replace text. Because it&amp;#8217;s all driven by key commands, you can easily change or alter what you&amp;#8217;re doing without having to start an entire search and you never have that &amp;#8220;context switch&amp;#8221; of filling out a form in a dialog box. It&amp;#8217;s all right in front of you. Highlighting allows you to see matches and you can lock in a search to easily jump between results. Viewing and editing configuration files is the real win for me over other editors: whatever the size, I can open a file and type a few keystrokes to go to exactly where I need to be, even if I&amp;#8217;m not sure where to go. This is so much better than using notepad or even Visual Studio.&lt;/p&gt;

&lt;h4 id=&#34;source-controlled-configuration:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Source controlled configuration&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/mhamrah/vimfiles&#34;&gt;I put my vimfiles&lt;/a&gt; on &lt;a href=&#34;https://github.com/mhamrah/&#34;&gt;github&lt;/a&gt; so I can synchronize them across platforms. This offers an unparalleled level of uniformity across environments with minimal effort. A lot of people do this, and it&amp;#8217;s helpful to see how others have configured their environment. You&amp;#8217;ll pick up a lot of neat tidbits by reading people&amp;#8217;s Vimfiles!&lt;/p&gt;

&lt;h3 id=&#34;vim-across-platforms:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Vim Across Platforms&lt;/h3&gt;

&lt;p&gt;Vim can run in either a gui window (like &lt;a href=&#34;http://code.google.com/p/macvim&#34;&gt;MacVim&lt;/a&gt; and &lt;a href=&#34;http://www.vim.org/download.php&#34;&gt;gVim&lt;/a&gt;) or from a command line. These are two different executables with a slightly different feature set. Usually, you get a little more with a gui vim, especially around OS integration (like cutting and pasting text) while running shell commands are easier with command line vim. Gui Vim also offers better color support for syntax highlighting.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://code.google.com/p/macvim&#34;&gt;MacVim&lt;/a&gt; is the Vim app for the Mac. It has a really nice full screen mode and native Mac commands alongside the Vim ones. I love the &lt;a href=&#34;http://peepcode.com/products/peepopen&#34;&gt;Peepopen&lt;/a&gt; search plugin which is only available on the Mac (and can be used with Textmate). It&amp;#8217;s a slick approach which is better than Textmate&amp;#8217;s Command-T. I like running MacVim in full screen mode with the toolbar off to get the most screen real estate.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.vim.org/download.php&#34;&gt;gVim&lt;/a&gt; is the Windows gui version of Vim, and I find it preferable over the command line Vim via cygwin. gVim has a shell extension which lets you open any file in gVim- set it as the default to avoid notepad. Note that Vim on Windows reads configuration files from the _vimrc, _gvimrc, and _vimfiles directory, which is different than the normal .vimrc and .vim location on other platforms. That hung me up when I was trying to sync configuration via git.&lt;/p&gt;

&lt;p&gt;As for command line Vim, that&amp;#8217;s probably what you&amp;#8217;ll be using on Linux. In &lt;a href=&#34;http://peepcode.com/products/play-by-play-bernhardt&#34;&gt;Gary Bernhardt&amp;#8217;s Play-by-Play Peepcode&lt;/a&gt; video I learned a cool trick of running Vim via the command line, then using jobs to suspend (Ctrl-Z) to return to the command line, than going back to Vim via foreground (fg).&lt;/p&gt;

&lt;h3 id=&#34;learning-vim:e1d90ec891499580029eaa8c80ca6228&#34;&gt;Learning Vim&lt;/h3&gt;

&lt;p&gt;There are plenty of resources on the web for getting started with Vim. Steve Losh&amp;#8217;s &lt;a href=&#34;http://stevelosh.com/blog/2010/09/coming-home-to-vim/&#34;&gt;Coming Home To Vim&lt;/a&gt; is a great overview that points you to a lot of other helpful posts. I bought a subscription to &lt;a href=&#34;http://www.peepcode.com&#34;&gt;Peepcode&lt;/a&gt; and picked up the &lt;a href=&#34;http://peepcode.com/products/smash-into-vim-ii&#34;&gt;Smash Into Part II&lt;/a&gt; episode. I didn&amp;#8217;t watch Part I because I felt like it was too basic, but Part II has a lot of substantial content. Peepcode offers a high quality product so you probably can&amp;#8217;t go wrong with getting both if you&amp;#8217;d rather be safe than sorry. I also watched the &lt;a href=&#34;http://peepcode.com/products/play-by-play-bernhardt&#34;&gt;Gary Bernhardt Play by Play&lt;/a&gt; which focuses a lot on using Vim with Rspec and Ruby, and use the &lt;a href=&#34;http://peepcode.com/products/peepopen&#34;&gt;Peepopen&lt;/a&gt; plugin for file search with Vim on my Mac.&lt;/p&gt;

&lt;p&gt;Here are some tips to avoid the beginner frustration:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Take it slow. There&amp;#8217;s a learning curve, but it&amp;#8217;s worth it.&lt;/li&gt;
&lt;li&gt;Don&amp;#8217;t sweat plugins when you&amp;#8217;re starting out. Yes, everyone says &amp;#8220;use Pathogen, use Rails.vim, use xyz&amp;#8221; and it&amp;#8217;s absolutely correct. But it&amp;#8217;s not essential when you&amp;#8217;re starting out.&lt;/li&gt;
&lt;li&gt;Take it one step at a time. Learn about Vim via tutorials and blogs so you know what&amp;#8217;s there, but don&amp;#8217;t try and do everything at once. Keep that knowledge in the back of your head, get comfortable with one thing, then move onto the next. You&amp;#8217;ll just end up confusing yourself if you do everything at once.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Focusing on items in the following order will allow you to build on your knowledge:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;editing text and searching, as that&amp;#8217;s what you&amp;#8217;ll be doing most&lt;/li&gt;
&lt;li&gt;file management, like opening, saving, and navigating to files&lt;/li&gt;
&lt;li&gt;other navigation, like jumping to lines or words and the power of hjkl (don&amp;#8217;t use arrow keys!)&lt;/li&gt;
&lt;li&gt;manipulation like replacing text, cutting and pasting&lt;/li&gt;
&lt;li&gt;window and buffer management, including splits&lt;/li&gt;
&lt;li&gt;start using and learning plugins to see where you can eliminate friction&lt;/li&gt;
&lt;li&gt;start customizing your vimrc file to make the vim experience more comfortable now that you know the basics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Good luck!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Machinist 2 &#43; Mongoid &#43; Embeds_Many Goodness</title>
          <link>http://blog.michaelhamrah.com/2010/12/machinist-2-mongoid-embeds_many-goodness/</link>
          <pubDate>Fri, 10 Dec 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/12/machinist-2-mongoid-embeds_many-goodness/</guid>
          <description>&lt;p&gt;I had a heck of a time getting fixtures working with &lt;a href=&#34;http://mongoid.org/&#34;&gt;Mongoid&lt;/a&gt; when it came to a required embeds_many property.  No matter what I did, I kept getting an error: &amp;#8220;&lt;em&gt;Access to the collection for&lt;/em&gt; XXX &lt;em&gt;is not allowed since it is an embedded document, please access a collection from the root document.&amp;#8221;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Then I stumbled upon &lt;a href=&#34;https://github.com/notahat/machinist&#34;&gt;Machinist&lt;/a&gt; v2 and the &lt;a href=&#34;https://github.com/nmerouze/machinist_mongo&#34;&gt;machinist_mongo&lt;/a&gt; gem which solved the problem.  And it had a nice API, to boot!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As of this post, you&amp;#8217;ll need to pull the machinist_mongo gem directly from git and get the machinist2 branch.  That&amp;#8217;s easy with Rails 3:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;gem &amp;#8216;machinist_mongo&amp;#8217;, :git =&amp;gt; &amp;#8216;&lt;a href=&#34;https://github.com/nmerouze/machinist_mongo.git&amp;amp;#8217&#34;&gt;https://github.com/nmerouze/machinist_mongo.git&amp;amp;#8217&lt;/a&gt;;, :require =&amp;gt; &amp;#8216;machinist/mongoid&amp;#8217;, :branch =&amp;gt; &amp;#8216;machinist2&amp;#8242;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Next, run&lt;/p&gt;

&lt;p&gt;&lt;code class=&#34;syntax bash&#34;&gt;bundle&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;to update your gems.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m using RSpec, so I put my blueprints in the spec/support/ directory so they get automatically loaded. Let&amp;#8217;s say I have a User class with a :username and an embeds_many :authentications property (as if you&amp;#8217;re following the Railscasts episode on using Devise and Omniauth).  The blueprint will look like this:&lt;/p&gt;

&lt;pre class=&#34;syntax ruby&#34;&gt;Authentication.blueprint do
     uid { &#34;user#{serial_number}&#34; }
     provider { &#34;machinist&#34; }
end

User.blueprint do
     username { &#34;user#{serial_number}&#34; }
     authentications(1) { Authentication.make }
end
&lt;/pre&gt;

&lt;p&gt;My authentications blueprint sets a unique id using the #{serial_number} counter in machinist 2. Then I&amp;#8217;m declaring an array of one item in my authentications array, and calling Authentication.make to load the Authentication blueprint. This essentially lazy loads the authentications property via the root document, which is exactly what Mongoid wants, as there&amp;#8217;s no Authentications table or root-level document.&lt;/p&gt;

&lt;p&gt;Now you can build away using Machinist 2&amp;#8217;s User.make (for creating an object without saving it) or make! (which makes and saves the object).&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Updated: Visual Studio Html5OpenId Template.  Now with MVC 3 RC and Html5 Boilerplate 0.9.5</title>
          <link>http://blog.michaelhamrah.com/2010/11/updated-visual-studio-html5openid-template-now-with-mvc-3-rc-and-html5-boilerplate-0-9-5/</link>
          <pubDate>Mon, 15 Nov 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/11/updated-visual-studio-html5openid-template-now-with-mvc-3-rc-and-html5-boilerplate-0-9-5/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;http://www.michaelhamrah.com/blog/2010/08/mvc-3-preview-1-vs-2010-template-w-razor-html-5-boilerplate-and-openid-authentication/&#34;&gt;In my previous I created a Visual Studio Template which uses OpenId via DotNetOpenAuth and Html5 Boilerplate by Paul Irish&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;del datetime=&#34;2011-02-17T16:21:27+00:00&#34;&gt;That template has been updated to use MVC 3 RC and Html5 Boilerplate .0.9.5.&lt;/del&gt; That template has been updated to use MVC 3 full and html5 boilerplate&amp;#8217;s master as of 2/17/11.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/downloads/mhamrah/Html5OpenIdTemplate/Html5OpenIdTemplate.zip&#34;&gt;You can grab the new template&lt;/a&gt; or &lt;a href=&#34;https://github.com/mhamrah/Html5OpenIdTemplate&#34;&gt;view/clone the project on Github.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.michaelhamrah.com/blog/2010/08/mvc-3-preview-1-vs-2010-template-w-razor-html-5-boilerplate-and-openid-authentication/&#34;&gt;Read the original post to learn what it&amp;#8217;s all about.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;font-size: 13.2px;&#34;&gt;&lt;/span&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Rails: Fixing Bundle “No Metadata Found” issues</title>
          <link>http://blog.michaelhamrah.com/2010/11/rails-fixing-bundle-no-metadata-found-issues/</link>
          <pubDate>Sun, 14 Nov 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/11/rails-fixing-bundle-no-metadata-found-issues/</guid>
          <description>&lt;p&gt;In playing around with Rails this weekend, I ran into an annoying error when trying to set up some bundles- specifically with Webrat and Cucumber, which I found very odd:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;/Users/Michael/.rvm/rubies/ruby-1.9.2-p0/lib/ruby/1.9.1/rubygems/package/tar_input.rb:111:in
`initialize&#39;: No metadata found! (Gem::Package::FormatError)
&lt;/pre&gt;

&lt;p&gt;Removing the dependencies in my Gemfile fixed the issue, but obviously left me without Cucumber and Webrat gems.&lt;/p&gt;

&lt;p&gt;Googling didn&amp;#8217;t provide an immediate solution to my problem, which is why I&amp;#8217;m writing this post. &lt;a href=&#34;https://github.com/carlhuda/bundler/issuesearch?state=closed&amp;amp;q=metadata#issue/603&#34;&gt;An issue hidden in the Bundle Github tracker had a solution&lt;/a&gt;: delete the cache directory in your Ruby&amp;#8217;s gem directory. The problem isn&amp;#8217;t necessarily specific to Webrat or Cucumber; the problem appears to be when the cache directory gets out of sync with the actual repository, and gems which should be installable cannot be found.&lt;/p&gt;

&lt;p&gt;After deleting the cache, &lt;strong&gt;bundle install&lt;/strong&gt; ran without error with my new Webrat and Cucumber gems.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Expressions and Lambdas: Oh My!</title>
          <link>http://blog.michaelhamrah.com/2010/11/expressions-and-lambdas-oh-my/</link>
          <pubDate>Sat, 13 Nov 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/11/expressions-and-lambdas-oh-my/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve been working on a toolkit called &lt;a href=&#34;https://github.com/mhamrah/Redaculous&#34;&gt;Redaculous&lt;/a&gt;&amp;#8211; it&amp;#8217;s a .NET Library for the really cool key/value store &lt;a href=&#34;http://code.google.com/p/redis/&#34;&gt;Redis&lt;/a&gt;.  It&amp;#8217;s built on top of the &lt;a href=&#34;http://code.google.com/p/servicestack/wiki/ServiceStackRedis&#34;&gt;ServiceStack.Redis&lt;/a&gt; library, which provides various .NET clients for Redis.  Redaculous is meant to make aggregating Redis commands a little easier- but don&amp;#8217;t get too excited.  The project __is in its infancy, and will undergo many changes, if it even gets off the ground.  This post isn&amp;#8217;t about Redis nor Redaculous- it&amp;#8217;s about how parts of Redaculous leverage &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/bb506649.aspx&#34;&gt;Expressions&lt;/a&gt; and &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/bb397687.aspx&#34;&gt;Lambdas&lt;/a&gt; to drive a lot of the functionality Redaculous is meant to provide, and how you can leverage Expressions to make your programming life easier.  ASP.NET MVC and lots of other great frameworks do it, so why can&amp;#8217;t you?&lt;/p&gt;

&lt;p&gt;The problem was simple:  I have a class, with a bunch of properties, and those properties have values.  In order to put them into Redis, I need to know the property name and the value it contains.  I need to know, at runtime, that myObj.SomeProperty has a property called &amp;#8220;SomeProperty&amp;#8221; and the value of that property.  This is a problem shared with most serialization tools and ORM mappers:  how does a &amp;#8220;SomeProperty&amp;#8221; make it to a column in table in a database, or make it to a node in xml?&lt;/p&gt;

&lt;p&gt;This problem can (and has been) solved in a variety of ways.  The most common was attributes to decorate classes and properties- which is how WCF constructs contracts or how some ORM toolkits work.  But that&amp;#8217;s extremely intrusive, in this pseudo example which combines WCF and DataAccess :&lt;/p&gt;

&lt;pre class=&#34;syntax c# escaped&#34;&gt;[Table(&#34;DtoTb&#34;)]
[DataContract]
public class Dto
{
     [PrimaryKey]
     [DataMember]
     public int Id { get; set; }

     [ColumnName(&#34;Name&#34;)]
     [DataMember]
     public string Name { get; set; }
}
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a weird combination of data storage and data definition which goes on in toolkits which use that approach.  It&amp;#8217;s not really a smooth way to operate- and the approach falls out when the model greatly diverages from the underlying table, or when a class has numerous other complex types.  Worse, when we want to also expose that class via WCF, we&amp;#8217;re essentially tacking on both data access description and web service schema description.  How many attributes can you tack on there? Validation attributes, too?&lt;/p&gt;

&lt;p&gt;Another approach is to use code-generation.  This is essentially what Linq-to-Sql does; it provides the underlying property descriptions at design time, saving a lot of manual typing.  I&amp;#8217;m not a big fan of code generators; they have their place, but you lose a lot of control in defining explicit functionality.  You&amp;#8217;re boxed into what is generated and most customizations usually don&amp;#8217;t fit in well: when you stray outside of what the code generator provides (or even outside of what the core generation is focused on) you find yourself trying to shove a square peg in a round hole.&lt;/p&gt;

&lt;p&gt;The best approach is to do what great tools like &lt;a href=&#34;https://github.com/atheken/NoRM&#34;&gt;Norm&lt;/a&gt;, &lt;a href=&#34;http://fluentnhibernate.org/&#34;&gt;Fluent NHibernate,&lt;/a&gt; &lt;a href=&#34;http://automapper.codeplex.com/&#34;&gt;AutoMapper&lt;/a&gt;, and .MVC&amp;#8217;s Html helpers do: they use &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/bb397687.aspx&#34;&gt;Expressions&lt;/a&gt;.  Most often the configuration is offloaded to separate classes which rely heavily on Expressions for property introspection.  This allows an end user to write code like:&lt;/p&gt;

&lt;pre class=&#34;syntax c# escaped&#34;&gt;//For MVC:
Html.TextBoxFor(m =&amp;gt; m.ProductName);
//Or for Fluent NHibernate:
Id(x =&amp;gt; x.ImageId);
&lt;/pre&gt;

&lt;p&gt;The TextBoxFor creates an input html element with a name=&amp;#8221;ProductName&amp;#8221; attribute; the Id() says NHIbernate should use a class&amp;#8217;s ImageId Property as the Id for the table.&lt;/p&gt;

&lt;p&gt;Expressions where introduced in .NET 3.5, and have been heavily leveraged ever since to provide a level of meta-programming which previously didn&amp;#8217;t exist in the .NET world.   Expressions greatly differ from Reflection by the fact Expressions are code which has been parsed into a set of various descriptive classes, while Reflection is compiled code which has been deconstructed into a descriptive semantic. An Expression statement will not actually &amp;#8220;do&amp;#8221; anything.  It can, however, be compiled into a callable function- which is the core advantage over Reflection.  With Expressions, you can have both the description of code and the actual, runnable code together.  Most frameworks create a package or container around the two as a performance optimization- it prevents an application from having to compile the expression multiple times.  The performance is much greater than using reflection to dynamically invoke or inspect properties.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How It Works&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Expressions and Lambdas go hand-in-hand, and can often be confused with one another. Let&amp;#8217;s take a look at the following code:&lt;/p&gt;

&lt;pre class=&#34;syntax c# escaped&#34;&gt;Expression&amp;lt;Func&amp;lt;SomeDto, string&amp;gt;&amp;gt; expressionLambda = m =&amp;gt; m.SomeStringProp;
Func&amp;lt;SomeDto, string&amp;gt;&amp;gt; funcLambda = m =&amp;gt; m.SomeStringProp;
&lt;/pre&gt;

&lt;p&gt;What&amp;#8217;s the difference between the two?  It&amp;#8217;s the same value, right?  Well, running that snippet in Visual Studio, and viewing the debugger properties, you&amp;#8217;ll find two different results for the m=&amp;gt;m.SomeStringProp statement. m=&amp;gt;m.SomeStringProp is the lambda: it&amp;#8217;s simply an alternative way of writing C# for various purposes. Usually, Lambdas are either Func or Action statements used as an alternative for delegate methods. The compiler will generate runnable IL code for the m =&amp;gt; m.SomeStringProp statement and create a callable method expecting an instance of SomeDto as the input. In the above example, you could get the value of a SomeStringProp using funcLambda like so:&lt;/p&gt;

&lt;pre class=&#34;syntax c# escaped&#34;&gt;var dto = new SomeDto() { SomeStringProp = &#34;Hell Yeah!&#34; };
funcLambda(dto); //Returns &#34;Hell Yeah!&#34;
&lt;/pre&gt;

&lt;p&gt;This can provide a level of agnosticism by passing around logic as variables in a much easier way than vanilla delegates.&lt;/p&gt;

&lt;p&gt;Expressions, on the other hand, don&amp;#8217;t compile into runnable IL code.  You can&amp;#8217;t write &lt;em&gt;funcExpression(dto)&lt;/em&gt; in the same was a normal lambda.  The compiler does something different: it actually parses the m=&amp;gt;m.SomeStringProp into various expression components which can be traversed, manipulated, rearranged and even compiled into a callable action, as if it were a Func all along. The [I&amp;#8217;ve been working on a toolkit called &lt;a href=&#34;https://github.com/mhamrah/Redaculous&#34;&gt;Redaculous&lt;/a&gt;&amp;#8211; it&amp;#8217;s a .NET Library for the really cool key/value store &lt;a href=&#34;http://code.google.com/p/redis/&#34;&gt;Redis&lt;/a&gt;.  It&amp;#8217;s built on top of the &lt;a href=&#34;http://code.google.com/p/servicestack/wiki/ServiceStackRedis&#34;&gt;ServiceStack.Redis&lt;/a&gt; library, which provides various .NET clients for Redis.  Redaculous is meant to make aggregating Redis commands a little easier- but don&amp;#8217;t get too excited.  The project __is in its infancy, and will undergo many changes, if it even gets off the ground.  This post isn&amp;#8217;t about Redis nor Redaculous- it&amp;#8217;s about how parts of Redaculous leverage &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/bb506649.aspx&#34;&gt;Expressions&lt;/a&gt; and &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/bb397687.aspx&#34;&gt;Lambdas&lt;/a&gt; to drive a lot of the functionality Redaculous is meant to provide, and how you can leverage Expressions to make your programming life easier.  ASP.NET MVC and lots of other great frameworks do it, so why can&amp;#8217;t you?&lt;/p&gt;

&lt;p&gt;The problem was simple:  I have a class, with a bunch of properties, and those properties have values.  In order to put them into Redis, I need to know the property name and the value it contains.  I need to know, at runtime, that myObj.SomeProperty has a property called &amp;#8220;SomeProperty&amp;#8221; and the value of that property.  This is a problem shared with most serialization tools and ORM mappers:  how does a &amp;#8220;SomeProperty&amp;#8221; make it to a column in table in a database, or make it to a node in xml?&lt;/p&gt;

&lt;p&gt;This problem can (and has been) solved in a variety of ways.  The most common was attributes to decorate classes and properties- which is how WCF constructs contracts or how some ORM toolkits work.  But that&amp;#8217;s extremely intrusive, in this pseudo example which combines WCF and DataAccess :&lt;/p&gt;

&lt;pre class=&#34;syntax c# escaped&#34;&gt;[Table(&#34;DtoTb&#34;)]
[DataContract]
public class Dto
{
     [PrimaryKey]
     [DataMember]
     public int Id { get; set; }

     [ColumnName(&#34;Name&#34;)]
     [DataMember]
     public string Name { get; set; }
}
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a weird combination of data storage and data definition which goes on in toolkits which use that approach.  It&amp;#8217;s not really a smooth way to operate- and the approach falls out when the model greatly diverages from the underlying table, or when a class has numerous other complex types.  Worse, when we want to also expose that class via WCF, we&amp;#8217;re essentially tacking on both data access description and web service schema description.  How many attributes can you tack on there? Validation attributes, too?&lt;/p&gt;

&lt;p&gt;Another approach is to use code-generation.  This is essentially what Linq-to-Sql does; it provides the underlying property descriptions at design time, saving a lot of manual typing.  I&amp;#8217;m not a big fan of code generators; they have their place, but you lose a lot of control in defining explicit functionality.  You&amp;#8217;re boxed into what is generated and most customizations usually don&amp;#8217;t fit in well: when you stray outside of what the code generator provides (or even outside of what the core generation is focused on) you find yourself trying to shove a square peg in a round hole.&lt;/p&gt;

&lt;p&gt;The best approach is to do what great tools like &lt;a href=&#34;https://github.com/atheken/NoRM&#34;&gt;Norm&lt;/a&gt;, &lt;a href=&#34;http://fluentnhibernate.org/&#34;&gt;Fluent NHibernate,&lt;/a&gt; &lt;a href=&#34;http://automapper.codeplex.com/&#34;&gt;AutoMapper&lt;/a&gt;, and .MVC&amp;#8217;s Html helpers do: they use &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/bb397687.aspx&#34;&gt;Expressions&lt;/a&gt;.  Most often the configuration is offloaded to separate classes which rely heavily on Expressions for property introspection.  This allows an end user to write code like:&lt;/p&gt;

&lt;pre class=&#34;syntax c# escaped&#34;&gt;//For MVC:
Html.TextBoxFor(m =&amp;gt; m.ProductName);
//Or for Fluent NHibernate:
Id(x =&amp;gt; x.ImageId);
&lt;/pre&gt;

&lt;p&gt;The TextBoxFor creates an input html element with a name=&amp;#8221;ProductName&amp;#8221; attribute; the Id() says NHIbernate should use a class&amp;#8217;s ImageId Property as the Id for the table.&lt;/p&gt;

&lt;p&gt;Expressions where introduced in .NET 3.5, and have been heavily leveraged ever since to provide a level of meta-programming which previously didn&amp;#8217;t exist in the .NET world.   Expressions greatly differ from Reflection by the fact Expressions are code which has been parsed into a set of various descriptive classes, while Reflection is compiled code which has been deconstructed into a descriptive semantic. An Expression statement will not actually &amp;#8220;do&amp;#8221; anything.  It can, however, be compiled into a callable function- which is the core advantage over Reflection.  With Expressions, you can have both the description of code and the actual, runnable code together.  Most frameworks create a package or container around the two as a performance optimization- it prevents an application from having to compile the expression multiple times.  The performance is much greater than using reflection to dynamically invoke or inspect properties.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How It Works&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Expressions and Lambdas go hand-in-hand, and can often be confused with one another. Let&amp;#8217;s take a look at the following code:&lt;/p&gt;

&lt;pre class=&#34;syntax c# escaped&#34;&gt;Expression&amp;lt;Func&amp;lt;SomeDto, string&amp;gt;&amp;gt; expressionLambda = m =&amp;gt; m.SomeStringProp;
Func&amp;lt;SomeDto, string&amp;gt;&amp;gt; funcLambda = m =&amp;gt; m.SomeStringProp;
&lt;/pre&gt;

&lt;p&gt;What&amp;#8217;s the difference between the two?  It&amp;#8217;s the same value, right?  Well, running that snippet in Visual Studio, and viewing the debugger properties, you&amp;#8217;ll find two different results for the m=&amp;gt;m.SomeStringProp statement. m=&amp;gt;m.SomeStringProp is the lambda: it&amp;#8217;s simply an alternative way of writing C# for various purposes. Usually, Lambdas are either Func or Action statements used as an alternative for delegate methods. The compiler will generate runnable IL code for the m =&amp;gt; m.SomeStringProp statement and create a callable method expecting an instance of SomeDto as the input. In the above example, you could get the value of a SomeStringProp using funcLambda like so:&lt;/p&gt;

&lt;pre class=&#34;syntax c# escaped&#34;&gt;var dto = new SomeDto() { SomeStringProp = &#34;Hell Yeah!&#34; };
funcLambda(dto); //Returns &#34;Hell Yeah!&#34;
&lt;/pre&gt;

&lt;p&gt;This can provide a level of agnosticism by passing around logic as variables in a much easier way than vanilla delegates.&lt;/p&gt;

&lt;p&gt;Expressions, on the other hand, don&amp;#8217;t compile into runnable IL code.  You can&amp;#8217;t write &lt;em&gt;funcExpression(dto)&lt;/em&gt; in the same was a normal lambda.  The compiler does something different: it actually parses the m=&amp;gt;m.SomeStringProp into various expression components which can be traversed, manipulated, rearranged and even compiled into a callable action, as if it were a Func all along. The]&lt;a href=&#34;http://msdn.microsoft.com/en-us/library/bb506649.aspx&#34;&gt;4&lt;/a&gt; has all of the available types a lambda expression can be broken down into.  Essentially, a hierarchal tree is generated, with each distinct component of the lambda expression being represented by one of the many Expression classes.  These &lt;span style=&#34;font-size: 13.2px;&#34;&gt;classes can be used to inspect various aspects of that part.  The above lambda is simply a &lt;a href=&#34;http://msdn.microsoft.com/en-us/library/system.linq.expressions.memberexpression.aspx&#34;&gt;MemberExpression&lt;/a&gt;, which is used for field and properties. Using MemberExpressions you can get the name of the Member, the Property Type, you can use the NodeType property- that&amp;#8217;s available with all Expression classes- to learn that it&amp;#8217;s a MemberAccess call.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;font-size: 13.2px;&#34;&gt;Because Expressions can be compiled into runnable code, frameworks get the best of both worlds: metadata about the call, and the call itself.  This is the precisely how ASP.NET MVC Html Helpers are built: that TextBoxFor method takes in an expression which it uses to generate the Html output. The Html helpers in MVC inspect the input expression to figure out what the name of the property for the html name attribute value, and then it runs the compiled expression against the current ViewModel object to get the value for the value html attribute.  The expression metadata and compiled function is actual cached in various static classes for performance: you don&amp;#8217;t want to have to compile the expression every time you use it.  The performance is much better than using reflection alone.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Redaculous is using Expressions to avoid the magic string conundrum- by using expressions, Redaculous can parse a statement like &amp;#8220;m = m.Name&amp;#8221; and know it should put the value of a class&amp;#8217;s Name property in the store using a key involving the word &amp;#8220;Name&amp;#8221; in some way. You should explore the &lt;a href=&#34;http://aspnet.codeplex.com/wikipage?title=MVC&amp;amp;referringTitle=Home&#34;&gt;MVC Framework&amp;#8217;s source code&lt;/a&gt; to dig in to how they&amp;#8217;re using expressions for strongly typed helpers. Both Norm and Automapper have some pretty straightforward usage too. By inspecting how other tools use these features you can more easily integrate them into your own projects- and increase productivity by eliminating redundant code and code smells involving magic strings.&lt;/p&gt;

&lt;p&gt;Other languages, like Ruby, have a similar level of functionality built in. This is inherit in all dynamic languages: functionality to not only provide an operation, but functionality to describe that operation as well.  I still get some hits on my &lt;a href=&#34;http://www.michaelhamrah.com/blog/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;ASP.NET MVC and Rails&lt;/a&gt; article, and I&amp;#8217;d say one of the biggest differences is how Rails, and Ruby in general, use dynamic language features like code metadata to drive functionality.  .NET has always had reflection, but expressions provide a much easier, and much more performant way of dealing with meta-programming.  &lt;span style=&#34;font-size: 13.2px;&#34;&gt;Expressions provide a core of the functionality in the Dynamic Language Runtime and have always been a strong part of Linq&amp;#8217;s roots.  DLR capabilities, using expressions, will continue to increase its surface area in the .NET world and should be a part of any developers toolkit.&lt;/span&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>QuickTip: Use CommonServiceLocator and MvcServiceLocator together in ASP.NET MVC 3 Pre-Release Projects</title>
          <link>http://blog.michaelhamrah.com/2010/09/quicktip-use-commonservicelocator-and-mvcservicelocator-together-in-asp-net-mvc-3-projects/</link>
          <pubDate>Thu, 23 Sep 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/09/quicktip-use-commonservicelocator-and-mvcservicelocator-together-in-asp-net-mvc-3-projects/</guid>
          <description>&lt;p&gt;&lt;strong&gt;UPDATE: This post is outdated since ASP.NET MVC Beta.  Use the DependencyResolver static class instead.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The integration of the CommonServiceLocator pattern within ASP.NET MVC is a positive step forward for the .MVC framework. Dependency management via ServiceLocation is a smart way to go, especially for large codebases with complex dependency needs. ServiceLocation keeps constructors clean, and prevents bloat in higher-level classes which don&amp;#8217;t need to know about lower-level dependencies.&lt;/p&gt;

&lt;p&gt;However, the fact that .MVC now has its own ServiceLocation infrastructure, via the System.Web.Mvc.IServiceLocator interface, is a little troublesome for code which already uses the Microsoft CommonServiceLocator class found in the Unity Enterprise Application Block. But don&amp;#8217;t fret- luckily, the IServiceLocator interface is exactly the same in the System.Web.Mvc namespace and the Microsoft.Practices.ServiceLocation namespace. This means you can have one class implement both interface simultaneously, like so:&lt;/p&gt;

&lt;pre class=&#34;brush: csharp; title: ; notranslate&#34; title=&#34;&#34;&gt;public class  SomeServiceLocatorWrapper : System.Web.Mvc.IServiceLocator, Microsoft.Practices.ServiceLocation.IServiceLocator
{
 //Implicity Implementation of Methods
}
&lt;/pre&gt;

&lt;p&gt;What&amp;#8217;s even easier is when there&amp;#8217;s already a wrapper class around the IServiceLocator for you, such as the one provided by Unity via the UnityServiceLocator class in Microsoft.Practices.Unity&amp;#8217;s namespace. The following code below provides all the functionality you need to use both ServiceLocators:&lt;/p&gt;

&lt;pre class=&#34;brush: csharp; title: ; notranslate&#34; title=&#34;&#34;&gt;public class UnityMvcServiceLocator : UnityServiceLocator, System.Web.Mvc.IServiceLocator
{
 public UnityMvcServiceLocator(IUnityContainer container)
 : base(container)
 {

 }

}
&lt;/pre&gt;

&lt;p&gt;Once you have that class in place, then it&amp;#8217;s just a matter of hooking both up in your Global.asax file like so:&lt;/p&gt;

&lt;pre class=&#34;brush: csharp; title: ; notranslate&#34; title=&#34;&#34;&gt;//In Global.asax&#39;s Application_Start hook:
var container = UnityContainerBuilder.CreateContainer();
var locator = new UnityMvcServiceLocator(container);

ServiceLocator.SetLocatorProvider(() =&amp;gt; locator);
MvcServiceLocator.SetCurrent(locator);
&lt;/pre&gt;

&lt;p&gt;This allows you to access the same locator either via the MvcServiceLocator.Current instance or the ServiceLocator.Current instance.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>MVC 3 RC VS 2010 Template w/ Razor, Html 5 Boilerplate and OpenId Authentication</title>
          <link>http://blog.michaelhamrah.com/2010/08/mvc-3-preview-1-vs-2010-template-w-razor-html-5-boilerplate-and-openid-authentication/</link>
          <pubDate>Wed, 25 Aug 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/08/mvc-3-preview-1-vs-2010-template-w-razor-html-5-boilerplate-and-openid-authentication/</guid>
          <description>

&lt;p&gt;### Updated 3/21/2011:&lt;/p&gt;

&lt;p&gt;Now with twitter and oauth support. See [the latest update](&lt;a href=&#34;http://www.michaelhamrah.com/blog/2011/03/updated-mvc3-html5-boilerplate-template-now-with-twitter-and-facebook/&#34;&gt;http://www.michaelhamrah.com/blog/2011/03/updated-mvc3-html5-boilerplate-template-now-with-twitter-and-facebook/&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;You can pull the source files from the &lt;a href=&#34;http://github.com/mhamrah/Html5OpenIdTemplate&#34;&gt;GitHub page&lt;/a&gt;, or &lt;a href=&#34;https://github.com/downloads/mhamrah/Html5OpenIdTemplate/Html5OpenIdTemplate.zip&#34;&gt;download the zip containing the template&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve created an (arguably) bare-minimum Visual Studio 2010 Template for getting started with MVC 3 Preview 1 using the Razor view engine and some other web goodies. I got tired of the vanilla &amp;#8220;Welcome to MVC&amp;#8221; homepage and I&amp;#8217;m not a fan of the MembershipProvider abstraction and all the Account junk included by default.  This template is meant to provide a bare-bones setup of Html 5 and provide OpenId authentication for users (but not full-on user management)- it&amp;#8217;s a simple cocktail of Html5 Boilerplate and DotNetOpenAuth.  The rest is left up to you to build up!&lt;/p&gt;

&lt;h3 id=&#34;span-style-font-weight-normal-paul-irish-8217-s-html5-boilerplate-span:c85e8e12a9a51fb10d19524cb483b402&#34;&gt;&lt;span style=&#34;font-weight: normal;&#34;&gt;Paul Irish&amp;#8217;s Html5 Boilerplate&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;Paul Irish created a nice starting point for Html5 websites with his &lt;a href=&#34;http://html5boilerplate.com/&#34;&gt;Html5 Boilerplate&lt;/a&gt;.  There&amp;#8217;s a lot of goodies in it which go beyond just changing the DOCTYPE.  &lt;a href=&#34;http://net.tutsplus.com/tutorials/html-css-techniques/the-official-guide-to-html5-boilerplate/?utm_source=feedburner&amp;amp;utm_medium=feed&amp;amp;utm_campaign=Feed:+nettuts+(NETTUTS)&#34;&gt;Check out the  NetTuts+ official guide to Html5 Boilerplate explained by Paul himself&lt;/a&gt;.  I&amp;#8217;ve ported over the .9 version with comments so you can see what&amp;#8217;s what.  The only changes made where to replace the url&amp;#8217;s in script and link tags with @Url.Content methods.  The original index page is at the root (you&amp;#8217;ll want to delete, but it&amp;#8217;s a handy reference) and the _Layout.cshtml now has a Header and Script section to use in child pages.  This functionality is used in the LogOn view to set up OpenId authentication.&lt;/p&gt;

&lt;h3 id=&#34;span-style-font-weight-normal-openid-with-dotnetopenauth-span:c85e8e12a9a51fb10d19524cb483b402&#34;&gt;&lt;span style=&#34;font-weight: normal;&#34;&gt;OpenId with DotNetOpenAuth&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;I was never a fan of the stock MembershipProvider infrastructure and setup of the sql tables required for user management.  Usually, user management requires very specific needs site to site, and the default functionality gets butchered anyway.  So all that stuff gets pulled and replaced with the ease and simplicity of OpenId.  The template uses the &lt;a href=&#34;http://code.google.com/p/openid-selector/&#34;&gt;OpenId Selector&lt;/a&gt; on the login page and &lt;a href=&#34;http://www.dotnetopenauth.net/&#34;&gt;DotNetOpenAuth&lt;/a&gt; with Forms Authentication on the backend.  There&amp;#8217;s no Register page, as the specifics of the required user schema varies too much.  But you should be able to combine the OpenId logic on the Login page with a new Register page to suit your needs.  There&amp;#8217;s also no persistence store integrated into the template as that is left entirely up to you.&lt;/p&gt;

&lt;h3 id=&#34;span-style-font-weight-normal-future-plans-span:c85e8e12a9a51fb10d19524cb483b402&#34;&gt;&lt;span style=&#34;font-weight: normal;&#34;&gt;Future Plans&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;&lt;span style=&#34;font-weight: normal;&#34;&gt;I really have no set goals for this template, so it will evolve in a free form manner.  The &lt;a href=&#34;http://github.com/mhamrah/Html5OpenIdTemplate&#34;&gt;code is on GitHub&lt;/a&gt; so feel free to branch and edit as necessary.  I&amp;#8217;m going to add some unit tests around the OpenId logic, and possibly evolve the Account creation in some way.  So that&amp;#8217;s about it!&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&#34;span-style-font-weight-normal-downloads-span:c85e8e12a9a51fb10d19524cb483b402&#34;&gt;&lt;span style=&#34;font-weight: normal;&#34;&gt;Downloads&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;&lt;span style=&#34;font-weight: normal;&#34;&gt;You can pull the source files from the &lt;a href=&#34;http://github.com/mhamrah/Html5OpenIdTemplate&#34;&gt;GitHub page&lt;/a&gt;, or &lt;a href=&#34;http://www.michaelhamrah.com/blog/wp-content/uploads/2010/11/Html5OpenIdTemplate.zip&#34;&gt;download the zip containing the template&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>The New Web App Architecture: ASP.NET MVC 3, jQuery Templating with PURE and the Json Value Provider</title>
          <link>http://blog.michaelhamrah.com/2010/08/the-new-webapp-architecture-asp-net-mvc-3-jquery-templating-with-pure-and-the-json-value-provider/</link>
          <pubDate>Wed, 04 Aug 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/08/the-new-webapp-architecture-asp-net-mvc-3-jquery-templating-with-pure-and-the-json-value-provider/</guid>
          <description>

&lt;p&gt;Over the past couple of years there has been a slow progression in the .NET web app world to fully separate out client/server interaction.  Long gone are the horrible days of ViewState and Events; MVC provided a nice step to better structure web applications for powerful Web 2.0 experiences.  But the barrier between client and server interaction has never really been clean-  MVC markup has always been littered with C# code and there hasn&amp;#8217;t always been widespread tools available to easily build desktop class applications in the browser.  Sure, spark and haml provide alternatives, but these are essentially make a core problem easier to bear.&lt;/p&gt;

&lt;p&gt;With the new preview of MVC 3, we can eliminate that core problem of server based html rendering: the built in Json Support via the JsonValueProviderFactory along with some jQuery goodies presents us with the new web app architecture: service- orientated web apps built with a rich, js based client driven by Json based services.  Using core tools of html, css, and js and not requiring the overhead of Silverlight, Flash, or JS based web frameworks.  This JSON functionality, combined with javascript templating using &lt;a href=&#34;http://beebole.com/pure/&#34;&gt;PURE&lt;/a&gt; gives us the ability to break free from the static html navigation and build dynamic apps in much more efficient ways.&lt;/p&gt;

&lt;h3 id=&#34;asp-net-mvc-3-and-the-jsonvalueproviderfactory:5f5fe724d58710160addd90aba3904ae&#34;&gt;ASP.NET MVC 3 and the JsonValueProviderFactory&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://weblogs.asp.net/scottgu/archive/2010/07/27/introducing-asp-net-mvc-3-preview-1.aspx&#34;&gt;Scott Guthrie writes about the new ASP.NET MVC 3 preview features&lt;/a&gt;, and among the hype is the default Json support for action methods.  In his blog post he forgets one crucial step to actually make this work, which is adding the new JsonValueProviderFactory to the global value provider factories class in the Application start method of the global.asax, like so:&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;protected void Application_Start()
{
    AreaRegistration.RegisterAllAreas();

    //Must add this factory explicitly (for now, at least):
    ValueProviderFactories.Factories.Add(new JsonValueProviderFactory());

    RegisterGlobalFilters(GlobalFilters.Filters);
    RegisterRoutes(RouteTable.Routes);
}
&lt;/pre&gt;

&lt;p&gt;Once this is in place we can treat our controller actions as usual, even when using Json.  The serialization mechanism is agnostic (this is pretty much exactly the same as Scott&amp;#8217;s code):&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;[HttpPost]
public ActionResult Search(ImageSearchInput input)
{
    //AssetSearchInput can be posted via Json
    return new JsonResult() { Data = new { ImageInfo = new Repository.ImageRepository().Search(input).Images } };
}
&lt;/pre&gt;

&lt;p&gt;ImageSearchInput, with a string property of &amp;#8220;Caption&amp;#8221;, will be built from the Json data posted to the server by the following Ajax call:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;$(&#39;#search&#39;).submit(function () {
  var input = { Caption: $(&#39;#caption&#39;).val() };

  $.ajax({url: &#39;/Home/Search&#39;,
    type: &#39;POST&#39;,
    data: JSON.stringify(input),
    dataType: &#39;json&#39;,
    contentType: &#39;application/json; charset=utf-8&#39;,
    success: function (data) {
        $(&#39;.imgContainer&#39;).autoRender(data);
    }
  });

  return false;
});
&lt;/pre&gt;

&lt;p&gt;The server will route the request to the Home/Search method, and will serialize the posted data in the ImageSearchInput class.  As long the properties match up and are of the correct type, the deserialization will be fluid.  Notice how we don&amp;#8217;t actually need to specify the ImageSearchInput class name when building the Json object.&lt;/p&gt;

&lt;p&gt;Those with keen eyes may have noticed the success: callback containing the autoRender() function.  This is the [Over the past couple of years there has been a slow progression in the .NET web app world to fully separate out client/server interaction.  Long gone are the horrible days of ViewState and Events; MVC provided a nice step to better structure web applications for powerful Web 2.0 experiences.  But the barrier between client and server interaction has never really been clean-  MVC markup has always been littered with C# code and there hasn&amp;#8217;t always been widespread tools available to easily build desktop class applications in the browser.  Sure, spark and haml provide alternatives, but these are essentially make a core problem easier to bear.&lt;/p&gt;

&lt;p&gt;With the new preview of MVC 3, we can eliminate that core problem of server based html rendering: the built in Json Support via the JsonValueProviderFactory along with some jQuery goodies presents us with the new web app architecture: service- orientated web apps built with a rich, js based client driven by Json based services.  Using core tools of html, css, and js and not requiring the overhead of Silverlight, Flash, or JS based web frameworks.  This JSON functionality, combined with javascript templating using &lt;a href=&#34;http://beebole.com/pure/&#34;&gt;PURE&lt;/a&gt; gives us the ability to break free from the static html navigation and build dynamic apps in much more efficient ways.&lt;/p&gt;

&lt;h3 id=&#34;asp-net-mvc-3-and-the-jsonvalueproviderfactory-1:5f5fe724d58710160addd90aba3904ae&#34;&gt;ASP.NET MVC 3 and the JsonValueProviderFactory&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://weblogs.asp.net/scottgu/archive/2010/07/27/introducing-asp-net-mvc-3-preview-1.aspx&#34;&gt;Scott Guthrie writes about the new ASP.NET MVC 3 preview features&lt;/a&gt;, and among the hype is the default Json support for action methods.  In his blog post he forgets one crucial step to actually make this work, which is adding the new JsonValueProviderFactory to the global value provider factories class in the Application start method of the global.asax, like so:&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;protected void Application_Start()
{
    AreaRegistration.RegisterAllAreas();

    //Must add this factory explicitly (for now, at least):
    ValueProviderFactories.Factories.Add(new JsonValueProviderFactory());

    RegisterGlobalFilters(GlobalFilters.Filters);
    RegisterRoutes(RouteTable.Routes);
}
&lt;/pre&gt;

&lt;p&gt;Once this is in place we can treat our controller actions as usual, even when using Json.  The serialization mechanism is agnostic (this is pretty much exactly the same as Scott&amp;#8217;s code):&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;[HttpPost]
public ActionResult Search(ImageSearchInput input)
{
    //AssetSearchInput can be posted via Json
    return new JsonResult() { Data = new { ImageInfo = new Repository.ImageRepository().Search(input).Images } };
}
&lt;/pre&gt;

&lt;p&gt;ImageSearchInput, with a string property of &amp;#8220;Caption&amp;#8221;, will be built from the Json data posted to the server by the following Ajax call:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;$(&#39;#search&#39;).submit(function () {
  var input = { Caption: $(&#39;#caption&#39;).val() };

  $.ajax({url: &#39;/Home/Search&#39;,
    type: &#39;POST&#39;,
    data: JSON.stringify(input),
    dataType: &#39;json&#39;,
    contentType: &#39;application/json; charset=utf-8&#39;,
    success: function (data) {
        $(&#39;.imgContainer&#39;).autoRender(data);
    }
  });

  return false;
});
&lt;/pre&gt;

&lt;p&gt;The server will route the request to the Home/Search method, and will serialize the posted data in the ImageSearchInput class.  As long the properties match up and are of the correct type, the deserialization will be fluid.  Notice how we don&amp;#8217;t actually need to specify the ImageSearchInput class name when building the Json object.&lt;/p&gt;

&lt;p&gt;Those with keen eyes may have noticed the success: callback containing the autoRender() function.  This is the]&lt;a href=&#34;http://beebole.com/pure&#34;&gt;3&lt;/a&gt; javascript templating engine at work.  I came across PURE when reading about the &lt;a href=&#34;http://wiki.github.com/nje/jquery/jquery-templates-proposal&#34;&gt;jQuery templating proposal on GitHub&lt;/a&gt; and was immediately drawn to its simplistic syntax.&lt;/p&gt;

&lt;p&gt;The philosophy behind PURE is simple: instead of interleaving markup and template directives (which, to be honest, is just as bad as mixing code with markup), PURE can make assumptions about what to repeat and what to bind based on css class names and Json property names.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s say I had the following json markup:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;var data = {
    Image:
        { Filename: &#39;mypic1.jpg&#39;, ImageUrl: &#39;/images/mypic1.jpg&#39;},
        { Filename: &#39;agoodphoto.jpg&#39;, ImageUrl: &#39;/images/agoodphoto.jpg&#39;}
}
&lt;/pre&gt;

&lt;p&gt;This is simply an Array with two objects in it.  Using PURE&amp;#8217;s autoRender function, we can specify binding directives using CSS classes, building li elements and populating content as appropriate:&lt;/p&gt;

&lt;pre class=&#34;syntax html escaped&#34;&gt;&amp;lt;ul class=&#34;imgContainer&#34;&amp;gt;
    &amp;lt;li class=&#34;Image&#34;&amp;gt;
        &amp;lt;p&amp;gt;&amp;lt;img class=&#34;ImageUrl@src&#34; /&amp;gt;&amp;lt;/p&amp;gt;
        &amp;lt;p class=&#34;info&#34;&amp;gt;&amp;lt;span class=&#34;Filename&#34;&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/p&amp;gt;
    &amp;lt;/li&amp;gt;
&amp;lt;/ul&amp;gt;
&lt;/pre&gt;

&lt;p&gt;Because the li has the Image css class, and we have an array of Image objects, PURE will duplicate the li contents for each element in the array.  The span tag will show the filename property because it has the Filename css class.  The src attribute of the img tag will get the ImageUrl property because it is using the @ directive, which simply says &amp;#8220;put the ImageUrl property in the src attribute&amp;#8221;.  PURE can easily get it&amp;#8217;s own post, but the point is simple:  we no longer need to generate server side html for displaying complex content.  Even though this has always been possible before, it hasn&amp;#8217;t been as fluid as PURE.  Another primary benefit is that we can send incredibly complex Json data back to the client which can update numerous page snippets.  Orchestrating multi-updates, like a detail page, shopping cart, or other areas simultaneously has not been trivial, but because we have json data and templating these complex scenarios are easily achievable.&lt;/p&gt;

&lt;h3 id=&#34;gotchas:5f5fe724d58710160addd90aba3904ae&#34;&gt;Gotchas&lt;/h3&gt;

&lt;p&gt;Client side templating is dangerous- you don&amp;#8217;t really know how well the client&amp;#8217;s computer it is.  Be mindful of performance and memory requirements, and ensure you&amp;#8217;re targeting the right browsers.  Chrome&amp;#8217;s V8 javascript engine has evolved remarkably well in handling javascript intensive applications.&lt;/p&gt;

&lt;p&gt;As for the MVC 3 preview, I wish we saw dynamic results from controllers: specifically, one action to return either  Json, Xml, or Html output based on some request directive.  This is possible with various ActionResults or ActionFilters, and there&amp;#8217;s some functionality already out there to do it, but it isn&amp;#8217;t as nice as Ruby on Rail&amp;#8217;s respond_to functionality.  A single action supporting multiple outputs will allow developers to easily target various platforms and scenarios in the web world.&lt;/p&gt;

&lt;h3 id=&#34;final-thoughts:5f5fe724d58710160addd90aba3904ae&#34;&gt;Final Thoughts&lt;/h3&gt;

&lt;p&gt;Despite techniques of Json serialization and jQuery templating already existing in the ecosystem, it has never been as robust and integrated as with the new MVC 3 support (sure, Rails has had this for a while).  Combining this functionality with templating tools like PURE will open up new development paths for rich web applications based entirely on standard tools like javascript, html, and css without relying on chunky js frameworks.  Keeping core tools simple means having the ability to build out specific functionality as needed, rather than getting boxed into more complete frameworks.  From the managing side, it also keeps the available pool of developers large so you&amp;#8217;re not requiring knowledge on a specific tool.  Play around with these tools and see what you can do!&lt;/p&gt;

&lt;p&gt;&lt;a rev=&#34;vote-for&#34; href=&#34;http://dotnetshoutout.com/The-New-Web-App-Architecture-ASPNET-MVC-3-jQuery-Templating-with-PURE-and-the-Json-Value-Provider-Adventures-in-HttpContext&#34;&gt;&lt;img alt=&#34;Shout it&#34; src=&#34;http://dotnetshoutout.com/image.axd?url=http%3A%2F%2Fwww.michaelhamrah.com%2Fblog%2F2010%2F08%2Fthe-new-webapp-architecture-asp-net-mvc-3-jquery-templating-with-pure-and-the-json-value-provider%2F&#34; style=&#34;border:0px&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Building a Single Speed: Wheel Building</title>
          <link>http://blog.michaelhamrah.com/2010/03/building-a-single-speed-wheel-building/</link>
          <pubDate>Tue, 16 Mar 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/03/building-a-single-speed-wheel-building/</guid>
          <description>

&lt;p&gt;By far the trickiest part of building the single speed bike (and, consequently, the most fun) was building the wheel.  I was a little hesitant to take on wheel assembly, but I couldn&amp;#8217;t cop out and not try to give it a shot.  It turns out, it&amp;#8217;s rather easy and a lot of fun.  Once you get the pattern down lacing the spokes is pretty straightforward, and out of all the parts of bike building this step really connects you to the bike.&lt;figure id=&#34;attachment_372&#34; style=&#34;width: 300px;&#34; class=&#34;wp-caption alignleft&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i0.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0198.jpg&#34;&gt;&lt;img class=&#34;size-medium wp-image-372&#34; title=&#34;Wheel building, step one&#34; src=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0198-300x200.jpg?resize=300%2C200&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;The first round of spokes are in. Luckily, the Rebel Yell was close at hand.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h4 id=&#34;the-wheels:9e3d2c3958c1dead2d18ffd299d6579c&#34;&gt;The Wheels&lt;/h4&gt;

&lt;p&gt;The wheels are a pair of &lt;a href=&#34;http://www.velocityusa.com/default.asp?contentID=583&#34;&gt;Velocity Deep V&lt;/a&gt;&amp;#8216;s with Origin-8 branded hubs (they&amp;#8217;re actually made by Formula).  The spokes are DT Champions.  Luckily, I didn&amp;#8217;t have to worry about &lt;a href=&#34;http://www.bikeschool.com/spokes/&#34;&gt;calculating spoke length&lt;/a&gt; because I ordered everything from &lt;a href=&#34;http://www.ridebrooklynny.com&#34;&gt;Ride Brooklyn&lt;/a&gt; (a great bike store in Park Slope) and they took care of the details.&lt;/p&gt;

&lt;h4 id=&#34;videos:9e3d2c3958c1dead2d18ffd299d6579c&#34;&gt;Videos&lt;/h4&gt;

&lt;p&gt;I did a lot of research before I got started.  Books weren&amp;#8217;t that helpful, as you really need to learn by example to mimic what&amp;#8217;s going on as you lace the spokes.  I found the &lt;a href=&#34;http://www.youtube.com/watch?v=qTb3x5VO69Y&amp;amp;feature=related&#34;&gt;best video on building a 36 hole front wheel from the bike tube&lt;/a&gt; on YouTube. I&amp;#8217;d just skip to &lt;a href=&#34;http://www.youtube.com/watch?v=AOI3uBztvHc&#34;&gt;wheel building part two&lt;/a&gt; where the action starts.  &lt;a href=&#34;http://www.youtube.com/watch?v=OYl4NO5m16Q&amp;amp;feature=related&#34;&gt;The bike tube also has two videos on lacing a 32 hole rear wheel&lt;/a&gt;.  It&amp;#8217;s the same lacing pattern, but the video goes into a little more depth when building the wheel.  The three cross pattern (which means a spoke crosses three other spokes between the hub and rim) is the same between front and rear wheels.&lt;figure id=&#34;attachment_373&#34; style=&#34;width: 300px;&#34; class=&#34;wp-caption alignright&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0201.jpg&#34;&gt;&lt;img class=&#34;size-medium wp-image-373 &#34; title=&#34;MLH_0201&#34; src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0201-300x201.jpg?resize=300%2C201&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;The first set of spokes are in. Notice the spoke next to the valve hole. The second set of spokes should go in to the right of this one, so it doesn&amp;rsquo;t straddle the valve hole. Or, you can start one away from the valve hole, and string the second set between the valve hole and the first spoke. The alternate side spoke goes in the flange hole immediately adjacent to the spoke on the opposite side.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;I recommend watching both sets.  The approach is a little different in that the 32 hole front wheel has you lace one side first, then the other, while the 36 hole rear wheel has you lace alternate sides (you put in for sets of spokes: the &amp;#8220;innies&amp;#8221; and &amp;#8220;outies&amp;#8221; on both the left and right flange).  The valve hole ends up being in a different place with the rear wheel video- as the author explains, you want it to be between a parallel set of spokes to make using the valve easier.  With the front wheel video the valve ends up being between a slanted set of spokes (I could have also followed the video incorrectly).  Not a big deal; it&amp;#8217;s just a nuanced difference.&lt;/p&gt;

&lt;p&gt;I recommend following the 36 hole front wheel video- it&amp;#8217;s a little easier to get the hub centered in the flange because you lace alternate sides.  My rims have 32 holes and there really isn&amp;#8217;t any difference in building the wheel.  (A 36 hole wheel has 4 sets of 9 spokes each, while a 32 hole wheel only has 8 spokes per set).&lt;figure id=&#34;attachment_374&#34; style=&#34;width: 300px;&#34; class=&#34;wp-caption alignleft&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0202.jpg&#34;&gt;&lt;img class=&#34;size-medium wp-image-374&#34; title=&#34;MLH_0202&#34; src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0202-300x200.jpg?resize=300%2C200&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;I followed the 32 hole rear wheel video, which laces one side at a time. It proved difficult to lace the 2nd side because the hub wasn&amp;rsquo;t centered.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h4 id=&#34;getting-started:9e3d2c3958c1dead2d18ffd299d6579c&#34;&gt;Getting Started&lt;/h4&gt;

&lt;p&gt;I followed the rear wheel video because it was more succinct.  However, I messed up in the process and had to start again.  I laced one side, and the hub was off center- one side was flush with the rim.  This made lacing the second side very difficult, as there wasn&amp;#8217;t a lot of give in the hub.  I also had a hard time figuring out where to start lacing on the second side.  I started with the wrong hole, and the hub got very twisted half way through the second side.  I really had to pull to get the nipples connected.  When it proved extremely difficult, I knew I must be doing something wrong.  &lt;strong&gt;If you&amp;#8217;re fighting the wheel, stop and restart.  You missed something.&lt;/strong&gt; After starting again, I realized where I went wrong the first time with the second set of spokes on the other side.&lt;figure id=&#34;attachment_377&#34; style=&#34;width: 300px;&#34; class=&#34;wp-caption alignright&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0229.jpg&#34;&gt;&lt;img class=&#34;size-medium wp-image-377&#34; title=&#34;MLH_0229&#34; src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0229-300x200.jpg?resize=300%2C200&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;Notice how the second set of spikes line up with the valve hole. This will put the valve hole between parallel spokes.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h4 id=&#34;try-and-try-again:9e3d2c3958c1dead2d18ffd299d6579c&#34;&gt;Try, and Try Again&lt;/h4&gt;

&lt;p&gt;When I attempted to build the wheel again, using the front wheel video, the process was a piece of cake.  Every spoke went in without a hitch.  The benefit of alternating sides while lacing is the hub is centered in the rim which makes connecting the spokes to the nipples very easy (although, I could have messed that up too when following the rear wheel video).&lt;/p&gt;

&lt;p&gt;The only glitch (which was a minor issue) is when I did the first wheel the valve hole didn&amp;#8217;t end up between parallel spots.  I either messed up following the video, or the front wheel video didn&amp;#8217;t make a point to lace the wheel in such a way.  The rear wheel video does explicitly call this out.  As long you don&amp;#8217;t end up with the valve hole in a cross spoke triangle you&amp;#8217;re all set.  Then it will be impossible to fill the tire with air.&lt;figure id=&#34;attachment_378&#34; style=&#34;width: 300px;&#34; class=&#34;wp-caption alignleft&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/comparison.jpg&#34;&gt;&lt;img class=&#34;size-medium wp-image-378 &#34; title=&#34;comparison&#34; src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/comparison-300x201.jpg?resize=300%2C201&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;It&amp;rsquo;s a nuanced difference, but you want the valve hole between parallel spots.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Correcting the error when I did the rear wheel was easy- I just started one hole away from the valve hole, rather than right next to the valve hole.  After I twisted the hub to get the spoke slant to start the second side, I made sure I started the second side next to the valve hole.  This way the two slanted spokes are next to the valve hole, rather than straddling it.  This will put the valve hole between parallel spokes.&lt;figure id=&#34;attachment_375&#34; style=&#34;width: 300px;&#34; class=&#34;wp-caption alignright&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0211.jpg&#34;&gt;&lt;img class=&#34;size-medium wp-image-375&#34; title=&#34;MLH_0211&#34; src=&#34;http://i0.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0211-300x200.jpg?resize=300%2C200&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;The wheel after lacing the second side.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h4 id=&#34;truing:9e3d2c3958c1dead2d18ffd299d6579c&#34;&gt;Truing&lt;/h4&gt;

&lt;p&gt;Once I got all the spokes on the wheel I dropped some wet lube on the nipples and tightened every spoke with a screwdriver until only a couple threads remained.  The front wheel came out pretty straight, but the rear wheel was way off.  It just took a little more work with the spoke wrench to get it center.  I came up with a make shift truing stand with the front fork and some plastic sticky tabs (I needed to use the frame to do the rear wheel, but it&amp;#8217;s the same idea).&lt;figure id=&#34;attachment_379&#34; style=&#34;width: 300px;&#34; class=&#34;wp-caption alignleft&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0008.jpg&#34;&gt;&lt;img class=&#34;size-medium wp-image-379&#34; title=&#34;MLH_0008&#34; src=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0008-300x200.jpg?resize=300%2C200&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;Awesome makeshift truing stand, with helpful plastic sticky tabs used in lieu of calipers.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;I placed the sticky tabs as close as possible to the frame and spun the wheel (the wheel spun forever- the hubs must be really slick!).  When the frame hit the tab it would make a noise, and I would adjust.  It&amp;#8217;s really hard to see the space shift between the tab and the rim.  The noise thing really helped.  Slow and calculated increments worked best- you really only need a quarter turn.  This approach worked for both the horizontal and vertical trueness.&lt;/p&gt;

&lt;p&gt;I have no way of properly checking the dish, so I may need to take a trip to the bike store (dish is making sure the hub is centered on the rim).  I&amp;#8217;m also worried that the spokes are too tight.  I read that you shouldn&amp;#8217;t have a lot of spoke tension- the spokes feel firm but I definitely need a second opinion.&lt;/p&gt;

&lt;h4 id=&#34;what-8217-s-next:9e3d2c3958c1dead2d18ffd299d6579c&#34;&gt;What&amp;#8217;s Next&lt;/h4&gt;

&lt;p&gt;I&amp;#8217;m hoping to get the rest of the parts tomorrow to finish this build.  I&amp;#8217;ll have another post to show how it all came together!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Building a Single Speed Bike with a 183rd Street Frame</title>
          <link>http://blog.michaelhamrah.com/2010/03/building-a-single-speed-bike/</link>
          <pubDate>Mon, 15 Mar 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/03/building-a-single-speed-bike/</guid>
          <description>

&lt;p&gt;As we&amp;#8217;re getting ready to gear up for summer, my wife was looking for a new bike.  I wanted to get her something cool and unique, and (selfishly) use the opportunity to indulge myself in a bike building project.  I heard building a bike was a pretty straightforward process, and in a worse case scenario I&amp;#8217;ll just bring the parts to a bike shop and have them finish it up.  We knew we wanted to go with a single speed- a friend has a &lt;a href=&#34;http://www.trekbikes.com/us/en/bikes/urban/soho/sohos/&#34;&gt;Trek Soho S&lt;/a&gt; which I fell in love with.  A single speed is perfect for jetting around the city- really light and nimble.  The acceleration is surprisingly easy and you hit a nice cruising speed quickly.  I&amp;#8217;m anxiously awaiting to compare this custom build with other commercial bikes.&lt;/p&gt;

&lt;h4 id=&#34;the-bike:5375a95adfae0af00b50d08588b7f5bf&#34;&gt;The Bike&lt;/h4&gt;

&lt;p&gt;We headed over to the good folks at &lt;a href=&#34;http://www.ridebrooklynny.com&#34;&gt;Ride Brooklyn&lt;/a&gt; for a recommendation on what to buy.  It&amp;#8217;s a great bike shop with a really friendly and helpful staff.  My wife wanted something light and we both wanted to keep the cost down.  They had two frames handy for comparison: An Origin-8 track frame and a slightly more expensive &lt;a href=&#34;http://183rdstreet.com&#34;&gt;183rd Street&lt;/a&gt; track frame.  The 183rd Street frame was a lot lighter than the Origin-8 (surprisingly so) and has a nice powder coat paint job in black which gives it a cool matte finish.  The owner built a bike using the same frame, so we figured it was the way to go.  My wife is 5&amp;#8217;4&amp;#8243; and we got the 51cm frame and fork.&lt;figure id=&#34;attachment_357&#34; style=&#34;width: 300px;&#34; class=&#34;wp-caption aligncenter&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0196.jpg&#34;&gt;&lt;img class=&#34;size-medium wp-image-357  &#34; title=&#34;Getting Ready for the Bike Build&#34; src=&#34;http://i0.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0196-300x200.jpg?resize=300%2C200&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;Half the parts ready to go. My boss says only in NYC do you build a bike in the kitchen.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p style=&#34;text-align: left;&#34;&gt;
  We also got a pair of Velocity Deep V rims in lime green with Formula hubs and a purple Origin-8 crank.  Once I saw everything together this bike is definitely getting a nickname: &amp;#8220;The Joker&amp;#8221;.
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Build: Bottom Bracket&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0003.jpg&#34;&gt;&lt;img class=&#34;alignleft size-medium wp-image-360&#34; title=&#34;MLH_0003&#34; src=&#34;http://i0.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0003-300x200.jpg?resize=300%2C200&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;With parts in hand, it&amp;#8217;s time to get building!  I started with the bottom bracket, which is an Origin-8 cartridge style bracket with a length of 107mm.  It turns out I should have gotten the 103mm, but the 107 will work just fine.  Check the specifications of your crank to find out for sure (I thought it would have to do with the frame, but it&amp;#8217;s sized for the crank).  I followed this &lt;a href=&#34;http://bicycletutor.com/cartridge-bottom-bracket/&#34;&gt;video from Bicycle Tutor&lt;/a&gt; to find out what to do.  It&amp;#8217;s pretty straightforward.  Honestly, the hardest part was figuring out how to get the lock cup off the bottom bracket.  Hint: You just pull it off!  I kept turning it and was afraid to pull too hard.  Turns out, not a big deal.  I borrowed a bottom bracket tool, but didn&amp;#8217;t have a torque wrench to find the right &lt;a href=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0007-Edit.jpg&#34;&gt;&lt;img class=&#34;alignright size-medium wp-image-361&#34; title=&#34;MLH_0007-Edit&#34; src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0007-Edit-300x209.jpg?resize=300%2C209&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;resistance.  Using some polylube 1000 to grease the threads,  I just screwed the bracket until it was pretty tight and checked the turn on the spindle.  Seemed alright.  I couldn&amp;#8217;t get the lock cup flush with the frame on the non drive side, but it turns out this isn&amp;#8217;t a big deal.&lt;/p&gt;

&lt;h4 id=&#34;the-headset:5375a95adfae0af00b50d08588b7f5bf&#34;&gt;The Headset&lt;/h4&gt;

&lt;p&gt;The 183rd Street frame has a threadless 1 &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt;&amp;#8243; headtube.  I picked up a Crane Creek threadless headset, and, funny enough, used their great &lt;a href=&#34;http://www.canecreek.com/tech-headsets?view=video&#34;&gt;video on how to install a threadless headset&lt;/a&gt;.  I didn&amp;#8217;t have a threadless headset press, and tried using a 2&amp;#215;4 and a hammer to get the bottom and top cups on.  It didn&amp;#8217;t work.  Lesson learned: &lt;strong&gt;Don&amp;#8217;t try using a hammer to bang your threadless headset on the frame.&lt;/strong&gt;&lt;figure id=&#34;attachment_362&#34; style=&#34;width: 150px;&#34; class=&#34;wp-caption alignleft&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0005.jpg&#34;&gt;&lt;img class=&#34;size-thumbnail wp-image-362  &#34; title=&#34;MLH_0005&#34; src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0005-150x150.jpg?resize=150%2C150&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;Headset fully pressed on frame, but text is off center&lt;/figcaption&gt;&lt;/figure&gt; &lt;figure id=&#34;attachment_363&#34; style=&#34;width: 150px;&#34; class=&#34;wp-caption alignright&#34;&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0006.jpg&#34;&gt;&lt;img class=&#34;size-thumbnail wp-image-363 &#34; title=&#34;MLH_0006&#34; src=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/03/MLH_0006-150x150.jpg?resize=150%2C150&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;Fork with crown race installed. You can see the sticky tabs I used to true the wheel.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;You need the right tool for the job.  I didn&amp;#8217;t want to buy a tool, so I brought the headset and frame back to Ride Brooklyn (I should start a side bet to see how many times I need to go back there!) and they pressed the cups on the frame, and used a crown race setter to put the crown race on the fork.  I did mess up a little bit, because I forgot to center the logo on the cups when I put them on the frame. I already had them half on when I went to the store and didn&amp;#8217;t want to deal with taking them off again.&lt;/p&gt;

&lt;h4 id=&#34;next-steps:5375a95adfae0af00b50d08588b7f5bf&#34;&gt;Next Steps&lt;/h4&gt;

&lt;p&gt;Lacing and truing the wheel was a fun but tedious process.  I&amp;#8217;ll write about that next as well as putting the other parts together, so check out the &lt;a href=&#34;http://www.michaelhamrah.com/blog/tag/bikebuild&#34;&gt;BikeBuild tag&lt;/a&gt; to follow on the progress.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Proposal: Let’s call ASP.NET MVC “.MVC”</title>
          <link>http://blog.michaelhamrah.com/2010/02/proposal-lets-call-asp-net-mvc-mvc/</link>
          <pubDate>Thu, 18 Feb 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/02/proposal-lets-call-asp-net-mvc-mvc/</guid>
          <description>&lt;p&gt;I hereby propose renaming ASP.NET MVC to just &amp;#8220;.MVC&amp;#8221;. It&amp;#8217;s just so much easier to type.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Performance Tracing For Your Applications via Enterprise Library</title>
          <link>http://blog.michaelhamrah.com/2010/02/performance-tracing-for-your-applications-via-enterprise-library/</link>
          <pubDate>Sat, 13 Feb 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/02/performance-tracing-for-your-applications-via-enterprise-library/</guid>
          <description>&lt;p&gt;Performance is one of the most important, yet often overlooked, aspects of programming.  It&amp;#8217;s usually not something you worry about until it gets really bad.  And at that point, you have layers of code you need to sift through to figure out where you can remove bottlenecks.  It&amp;#8217;s also tricky, especially on complex, process orientated systems, to aggregate performance information for analysis.  That&amp;#8217;s where this &lt;a href=&#34;http://gist.github.com/302225&#34;&gt;gist for a utility performance tracer class comes into play&lt;/a&gt;.  It&amp;#8217;s meant to hook into Enterprise Library Logging Block by aggregating the elapsed time or information statements and pass them to the Event Log (or other listeners you&amp;#8217;ve hooked up).  Even though the gist is designed for the Enterprise Library, it can easily be modified for other utilities like Log4Net.  Below is an example of how the entry looks in the EventLog.&lt;/p&gt;

&lt;p style=&#34;text-align: center;&#34;&gt;
  &lt;a href=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/02/event-log-examoke.png&#34;&gt;&lt;img class=&#34;size-full wp-image-350 aligncenter&#34; title=&#34;event log example&#34; src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2010/02/event-log-examoke.png?resize=483%2C526&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why Do It?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I was disappointed with the built in trace listener provided with Enterprise Library.  It would write a begin and end message to the trace utility, and output a rather verbose message containing only a few relevant lines.  Even though it was easy to use in a using() statement, it was rather difficult to view related trace statements for a single high-level call.  I could have explored using the Tracer class more effectively, but isn&amp;#8217;t always easier to write your own?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How it Works&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Usually you&amp;#8217;ll create an instance of the PerformanceTracer in a using statement, just like the Tracer class.&lt;/p&gt;

&lt;pre class=&#34;brush: csharp; title: ; notranslate&#34; title=&#34;&#34;&gt;using(new PerformanceTracer(&#34;How long does this function take?&#34;) {
     //Your Code Here
}
&lt;/pre&gt;

&lt;p&gt;This will write the current message as the Title of the LogEntry and write the elapsed time between instantiation and disposal (thanks to the IDisposable pattern).  But the real benefit comes when you use the instantiated class for tracing within the using block:&lt;/p&gt;

&lt;pre class=&#34;brush: csharp; title: ; notranslate&#34; title=&#34;&#34;&gt;using(var tracer = new PerformanceTracer(&#34;Some long process&#34;) {
    //Step One
    tracer.WriteElapsedTime(&#34;Step One Complete&#34;);
    //Step Two
    tracer.WriteElapsedTime(&#34;Step Two Complete&#34;);
}
&lt;/pre&gt;

&lt;p&gt;This will write both the elapsed time and the difference between statement calls, and will allow you to easily gain insight into long running steps.  There&amp;#8217;s also another method &lt;em&gt;WriteInfo&lt;/em&gt;, which allows you to write important information without clogging the performance messages.  This is important for information such as the current user, or information about the request:&lt;/p&gt;

&lt;pre class=&#34;brush: csharp; title: ; notranslate&#34; title=&#34;&#34;&gt;tracer.WriteInfo(&#34;CurrentUser&#34;, Identity.User);
tracer.WriteInfo(&#34;QueryString&#34;, Request.QueryString);
&lt;/pre&gt;

&lt;p&gt;More often than not you probably differ high level function calls (those that orchestrate complex logic) to sub routines.  In Domain Driven Design, your Domain objects will probably interact with support services or other entities.  You may need to gain insight into these routines, but you don&amp;#8217;t want to create separate tracers- that will create multiple EventLog entries and won&amp;#8217;t give you a clear picture of the entire process.  In a production environment, where there could be hundreds of concurrent requests, aggregating those calls in a meaningful way is a nightmare.  That&amp;#8217;s the main issue I had with the default Tracer class.  The solution for the PerformanceTracer is simple-  In your dependent classes you create a property of type IPerformanceTracer.  There are two extension methods, &lt;em&gt;LogPerformanceTrace&lt;/em&gt; and &lt;em&gt;LogInfoTrace&lt;/em&gt; you can use dependent classes.  These simply do a null object check before writing the trace so you don&amp;#8217;t get any nasty null reference errors- helpful if you need to add logging and don&amp;#8217;t want to update dependencies in your unit tests.  Notice how in the example above there was one call which took the majority of the time? I should probably get more insight into that function to see what&amp;#8217;s going on.  Here&amp;#8217;s an example of a property/extension method combination:&lt;/p&gt;

&lt;pre class=&#34;brush: csharp; title: ; notranslate&#34; title=&#34;&#34;&gt;//Create a property to store the tracer
public IPerformanceTracer Tracer { get; set; }
public void MySubRoutineCalledFromAnotherFunction(string someParam)
{
    //Do Work
    Tracer.LogPerformanceTracer(&#34;Subroutine Step One&#34;);
    //Do More Work
    Tracer.LogPerformanceTrace(&#34;Subroutine Step Two&#34;);
}
&lt;/pre&gt;

&lt;p&gt;It doesn&amp;#8217;t matter if the Tracer property is set, because the extension method handles null objects correctly. You could also put the tracer entity in an IoC framework and pull it out that way.  I prefer this approach when dealing with web apps.  You can create the tracer in a BeginRequest HttpModule, plug it into a container, and pull it out in the EndRequest method and dispose of it.  Using the tracer with a Per WebRequest Lifetime Container, supported with most frameworks, is even better.  This way it&amp;#8217;s available everywhere without have to wire up new properties, and you can scatter it around in various places.  Here&amp;#8217;s some code which pulls it out of a ServiceLocator ActionFilterAttribute in a ASP.NET MVC App:&lt;/p&gt;

&lt;pre class=&#34;brush: csharp; title: ; notranslate&#34; title=&#34;&#34;&gt;public class PerformanceCounterAttribute : ActionFilterAttribute

{

IPerformanceTracer tracer;

public override void OnActionExecuting(ActionExecutingContext filterContext)

{

     tracer = ServiceLocator.Current.GetInstance&amp;lt;IPerformanceTracer&amp;gt;();

     tracer.LogPerformanceTrace(&#34;Action Executing&#34;);

}

public override void OnResultExecuted(ResultExecutedContext filterContext)

{

     tracer.LogPerformanceTrace(&#34;Result Finished&#34;);

}

public override void OnActionExecuted(ActionExecutedContext filterContext)

{

     tracer.LogPerformanceTrace(&#34;Action Finished&#34;);

}

}

&lt;/pre&gt;

&lt;p&gt;The final aspect of this setup is formatting the output.  EnterpriseLibrary provides a ton of information about the the context which logged the entry- not all of it may be applicable to you.  I use a customer formatter to only write the Title, Message and Category of the trace, like so:&lt;/p&gt;

&lt;pre class=&#34;brush: xml; title: ; notranslate&#34; title=&#34;&#34;&gt;&amp;lt;formatters&amp;gt;
 &amp;lt;add template=&#34;Title:{title}&amp;#xA;Message: {message}&amp;#xA;Category: {category}&#34; type=&#34;Microsoft.Practices.EnterpriseLibrary.Logging.Formatters.TextFormatter, Microsoft.Practices.EnterpriseLibrary.Logging, Version=4.0.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35&#34; name=&#34;Text Formatter&#34;/&amp;gt;
 &amp;lt;/formatters&amp;gt;
&lt;/pre&gt;

&lt;p&gt;This makes entries so much more readable. By default, the EventLog takes care of important information like the Computer name and Timestamp. So I can keep the message info very clean.&lt;/p&gt;

&lt;p&gt;﻿One thing I&amp;#8217;m specifically not trying to do is get an average of the same function call over a period of time.  That insight is also very important in understanding bottlenecks.  But what I&amp;#8217;m trying to do is get a broad sense of the flow of a large process and see which chunks can be improved.  With the ability to aggregate EventLogs in a single place, I can easily sift through and see how my app is behaving in production.  By linking that with relevant information like the current user, url, and query string, I can find specific scenarios which are problematic that may not have gotten caught in lower environments.&lt;/p&gt;

&lt;p&gt;A copy of the entire class is below, and available from GitHub.  It doesn&amp;#8217;t make sense to roll out an entire project for it- just roll it somewhere in your app and tweak as necessary.  Then enjoy finding all the places you could optimize!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Using Flickr and jQuery to learn JSONP</title>
          <link>http://blog.michaelhamrah.com/2010/02/using-flickr-and-jquery-to-learn-jsonp/</link>
          <pubDate>Fri, 05 Feb 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/02/using-flickr-and-jquery-to-learn-jsonp/</guid>
          <description>&lt;p&gt;I was playing around with the Flickr API recently and got a little stuck: when using jQuery to call the &lt;a href=&#34;http://www.flickr.com/services/feeds/&#34;&gt;Flickr public feed&lt;/a&gt; using jQuery&amp;#8217;s $.getJSON method, I wasn&amp;#8217;t getting any results.  I thought maybe I was parsing the response incorrectly, but when I went to check out the data coming back in firebug, nothing was there.  I couldn&amp;#8217;t believe it- the response headers were present, but the body was blank.  Calling the public feed url from the browser worked fine.  What&amp;#8217;s more interesting was everything worked in IE.  So I did some experimenting and learned the issue: I wasn&amp;#8217;t correctly using the endpoint to work with &lt;a href=&#34;http://bob.pythonmac.org/archives/2005/12/05/remote-json-jsonp/&#34;&gt;JSONP&lt;/a&gt;, which is required when using jQuery with Flickr.  Then I thought I better learn more about JSONP.&lt;/p&gt;

&lt;p&gt;There are &lt;a href=&#34;http://www.insideria.com/2009/03/what-in-the-heck-is-jsonp-and.html&#34;&gt;plenty of good articles&lt;/a&gt; about &lt;a href=&#34;http://en.wikipedia.org/wiki/JSON#JSONP&#34;&gt;JSONP&lt;/a&gt; on the net.  Essentially, JSONP allows you to specify custom callbacks when making remote ajax calls.  Firefox seems to be more strict when dealing with jsonp, which is why I didn&amp;#8217;t get a response body.  What did the trick was adding the&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;jsoncallback=?&lt;/pre&gt;

&lt;p&gt;query string parameter to the end of my Flickr url.  This allows the jQuery framework to route to the default success function you pass to .getJSON(), like so:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;$.getJSON(
            &#39;http://api.flickr.com/services/feeds/photos_public.gne?format=json&amp;jsoncallback=?&#39;,
               function(data) {
                   $.each(data.items, function(i, item) {
                       $(&#34;img&#34;).attr(&#34;src&#34;, item.media.m).appendTo(&#34;#images&#34;);
                   });
               });
&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://www.michaelhamrah.com/blog/wp-content/FlickrJsonp.html&#34;&gt;Here&amp;#8217;s an example of this call&lt;/a&gt;.   So what happens if we replace the ? with something else? Well, passing a simple text string will be treated like a normal function call.  Take a look at the example below:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;function GetItemsUsingExternalCallback() {
            $.ajax({ url: &#39;http://api.flickr.com/services/feeds/photos_public.gne?format=json&amp;jsoncallback=myCallbackFunction&#39;,
                dataType: &#39;jsonp&#39;
            });
        }
  function myCallbackFunction(data) {
  //dostuff
  }
&lt;/pre&gt;

&lt;p&gt;jQuery will try and call &amp;#8220;myCallbackFunction&amp;#8221; as a normal function call instead of routing to the standard success function that&amp;#8217;s part of the getJSON call.  &lt;a href=&#34;http://www.michaelhamrah.com/blog/wp-content/FlickrJsonp.html&#34;&gt;The example page also includes this approach.&lt;/a&gt; There&amp;#8217;s only a slight difference between the two approaches, but it&amp;#8217;s cool that jQuery can call a function outside of the normal success callback.  Of course, if you needed to reuse a common callback across multiple ajax calls, you&amp;#8217;d probably want to call that function directly from the success method, rather than inlining the function in the .ajax call.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How eAccelerator Improved WordPress on My Fedora Server</title>
          <link>http://blog.michaelhamrah.com/2010/02/how-eaccelerator-improved-wordpress-on-my-fedora-server/</link>
          <pubDate>Mon, 01 Feb 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/02/how-eaccelerator-improved-wordpress-on-my-fedora-server/</guid>
          <description>&lt;p&gt;I recently moved this blog and some other smaller websites to a virtual machine running on &lt;a href=&#34;http://www.michaelhamrah.com/blog/2010/01/rocking-the-rackspace-cloud/&#34;&gt;Rackspace Cloud&lt;/a&gt;. So far I&amp;#8217;m loving having my own server, and have been able to get my hands dirty with Linux administration, apache, and mysql.&lt;/p&gt;

&lt;p&gt;But one quirk was really bothering me: at times, my WordPress blog would hang. I don&amp;#8217;t get too much traffic, and using&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;top&lt;/pre&gt;

&lt;p&gt;showed no real load on the cpu. But there were a lot of Apache threads with a good chunk of memory allocated, and I had a little free memory available.  I tried adding more memory to the server, but no luck.  I thought the issue could be mysql, but running queries wasn&amp;#8217;t a problem.  Neither was static html- the root index.html loaded quickly.  So that left php itself.&lt;/p&gt;

&lt;p&gt;A couple of apache config changes didn&amp;#8217;t help.  The thing which really did the trick was installing &lt;a href=&#34;http://eaccelerator.net/&#34;&gt;eAccelerator&lt;/a&gt;.  This tool simply keeps the compiled php script available, so apache doesn&amp;#8217;t have to recompile the php script on every load.  No, the blog is a lot faster and much more reliable.&lt;/p&gt;

&lt;p&gt;Installation is easy on Fedora (or CentOS, or whatever distro uses yum):&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo yum install php-accelerator&lt;/pre&gt;

&lt;p&gt;then just restart apache and you&amp;#8217;re good to go:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo /sbin/service httpd reload&lt;/pre&gt;

&lt;p&gt;If you don&amp;#8217;t notice an immediate improvement, and want to check to make sure it&amp;#8217;s loaded, then create a info.php file with the following code:&lt;/p&gt;

&lt;pre class=&#34;brush: php; title: ; notranslate&#34; title=&#34;&#34;&gt;&amp;lt;?php phpinfo(); ?&amp;gt;&lt;/pre&gt;

&lt;p&gt;and you should see the accelerator info in the list.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>An Outlook Macro to Archive Items like GMail</title>
          <link>http://blog.michaelhamrah.com/2010/01/an-outlook-macro-to-archive-items-like-gmail/</link>
          <pubDate>Mon, 25 Jan 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/01/an-outlook-macro-to-archive-items-like-gmail/</guid>
          <description>&lt;p&gt;I&amp;#8217;m a big fan of Gmail&amp;#8217;s archive feature- I can move items out of the inbox quickly and find them later with search.  To mimic this behavior at work with outlook, I&amp;#8217;ve created this quick and dirty macro which does the job.  Simply create a folder named &amp;#8220;archive&amp;#8221; in your mailbox.  When you run the macro, it will move the current item in your inbox to that folder.  I&amp;#8217;ve created a keystroke which lets me run the macro easily.&lt;/p&gt;

&lt;pre class=&#34;brush: vb; title: ; notranslate&#34; title=&#34;&#34;&gt;Sub ArchiveItem()

On Error Resume Next

 Dim objFolder As Outlook.MAPIFolder, objInbox As Outlook.MAPIFolder
 Dim objNS As Outlook.NameSpace, objItem As Outlook.MailItem

 Set objNS = Application.GetNamespace(&#34;MAPI&#34;)
 Set objInbox = objNS.GetDefaultFolder(olFolderInbox)
 Set objFolder = objInbox.Parent.Folders(&#34;Archive&#34;)

&#39;Assume this is a mail folder
 If objFolder Is Nothing Then
 MsgBox &#34;This folder doesn&#39;t exist!&#34;, vbOKOnly + vbExclamation, &#34;INVALID FOLDER&#34;
 End If

 If Application.ActiveExplorer.Selection.Count = 0 Then
 &#39;Require that this procedure be called only when a message is selected
 Exit Sub
 End If

 For Each objItem In Application.ActiveExplorer.Selection
 If objFolder.DefaultItemType = olMailItem Then
 If objItem.Class = olMail Then
 objItem.Move objFolder
 End If
 End If
 Next

 Set objItem = Nothing
 Set objFolder = Nothing
 Set objInbox = Nothing
 Set objNS = Nothing
End Sub

&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Rocking the Rackspace Cloud</title>
          <link>http://blog.michaelhamrah.com/2010/01/rocking-the-rackspace-cloud/</link>
          <pubDate>Wed, 13 Jan 2010 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2010/01/rocking-the-rackspace-cloud/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve been unhappy with my current hosting provider and in looking for an alternative I tried &lt;a href=&#34;http://www.rackspacecloud.com/cloud_hosting_products/servers&#34;&gt;Rackspace Cloud Server&lt;/a&gt;.  I&amp;#8217;ve wanted a dedicated server for a while, but it&amp;#8217;s been cost prohibitive compared to shared machine hosting.  This blog, as well as some other, smaller sites, are now running on a Rackspace Cloud Server.  Needless to say I love it.  Rackspace Cloud Server is simply awesome.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Signing up and setup was fast and simple.  There was a weird account verification process where Rackspace had to call me to finalize and confirm setup, but I only had to wait about ten minutes after I signed up for the call.  After that, it was only a couple of clicks before I was logging in to my new server.  There are several Linux images to choose from- you select the size of the machine you want and the os- and within seconds you have a public ip v4 address and root access. For me that was the most amazing part of cloud computing- I wanted a new server, and I got one with instant gratification.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m not a Linux expert coming from the Windows world, but I was able to configure everything and install mysql/apache in no time.  Rackspace has a &lt;a href=&#34;http://cloudservers.rackspacecloud.com/index.php/Main_Page&#34;&gt;series of extremely well written and straightforward how-to&amp;#8217;s to get your server configured correctly&lt;/a&gt;.  Each OS image is a stripped down bare-bones install so there&amp;#8217;s some setup required, but by using the knowledge base I was up and running quickly.  Backups are integrated into the management website with Rackspace&amp;#8217;s Cloud Files product, so I took a snapshot for reuse later.  You can even schedule snapshots as needed.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s really the knowledge base which I loved- I got ftp, apache, php, ssh, mysql all installed and secured quickly.  There&amp;#8217;s a wealth of information there.  If you need to learn Linux or do anything with popular OS applications check out their kb. I&amp;#8217;ve never seen so much easy to access documentation on open source software in one place.&lt;/p&gt;

&lt;p&gt;**Other Goodies&lt;/p&gt;

&lt;p&gt;**&lt;/p&gt;

&lt;p&gt;The reason why I wanted to try Rackspace is because the cost of entry is extremely cheap- it starts at 1.5 cents/hr which comes out to about $11/month.  Much cheaper than Amazon&amp;#8217;s EC2, which starts around $20 only if you prepay.  There are a lot of major differences between the two products which I talk about below.&lt;/p&gt;

&lt;p&gt;Rackspace Servers come in different sizes all differentiated by the amount of RAM.  I chose the 256mb option because it was the cheapest.  It turns out it was a little too light on power for what I wanted, so I upgraded to the 512mb option.  The upgrade was seemless- you just say &amp;#8220;make this a 512mb server&amp;#8221;, and Rackspace queues the request, reboots when appropriate, you verify, and then you get the same server with better &amp;#8220;hardware&amp;#8221;.  Loved it.  I queued the request before I left work and it was good to go when I got home.&lt;/p&gt;

&lt;p&gt;The best part about Rackspace is the support.  There&amp;#8217;s a live chat link from the management page.  I had a question about DNS- I clicked live chat- had a quick IM conversation- got the answer- and I was done.  That&amp;#8217;s service.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rackspace vs. EC2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The main reason I chose Rackspace over EC2 was cost.  But in looking into the differences between Rackspace and EC2 I&amp;#8217;m happy I chose Rackspace.  The biggest difference is with Rackspace you can reboot the OS without losing state- so if you make configuration change or install some software, a reboot behaves like you&amp;#8217;d expect- everything stays the same.  EC2, on the other hand, resets the server state back to what it was on the first boot.  You need to have your EC2 server link to EBS to persist state. WIth Rackspace you get a hard drive which acts, surprisingly, just like a hard drive.&lt;/p&gt;

&lt;p&gt;With Rackspace, the default is ip v4, but with AWS, it&amp;#8217;s ip v6 and you need to link to an ip v4 address.  The initial setup is also slightly different- EC2 involves the use of a keypair for the initial login, but Rackspace gives you root access which you then change and configure as needed.  I haven&amp;#8217;t actually set up an EC2 server but by looking at the how to I an safely say Rackspace is a lot simpler.&lt;/p&gt;

&lt;p&gt;Another major difference in options is the &amp;#8220;size&amp;#8221; of the server.  EC2 is explicit in the hardware configuration- RAM, cpu power, cores, etc.  Size is capped.  Rackspace takes a different approach: all CPUs are 64 bit with four cores and you only choose the amount of RAM.  Storage and CPU power change with the size of RAM.  You&amp;#8217;re guaranteed a certain minimum power (rather than a capped maximum), and if more power is available you get it.&lt;/p&gt;

&lt;p&gt;I can say EC2 has much more options available for OSes- EC2 offers Windows in addition to Linux (apparently this is coming soon to Rackspace).  I also like the idea of EC2&amp;#8217;s community AMI&amp;#8217;s.  You only get a bare bones OS with Rackspace, but with EC2 you can get a fully configured LAMP stack or some other ready-to-go server.  But it was fun getting everything set up myself with Rackspace.  I haven&amp;#8217;t done a long haul with either product, and like I said I haven&amp;#8217;t actually spun up an EC2 instance, so I can&amp;#8217;t get into any more descriptive details.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rocking the Cloud&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I can&amp;#8217;t tell you how fun it is to be able to spin up a new server out of the blue.  There are a million things I want to do- being able to run a server full time, with great bandwidth, without having to find a place for it in my house (or buy and salvage hardware) is just geeky awesome.  And for only $11/mo!  It&amp;#8217;s crazy!  Whether with Rackspace or Amazon I encourage you to give it a shot.  I encode a lot of video for my ipod, which means running my laptop at night or slow performance when doing other things.  I&amp;#8217;d love to be able to spin up a server, upload a video, encode it, pull it down, and shut down the server to save money.  Power on demand!&lt;/p&gt;

&lt;p&gt;Disclaimer: I own stock in both Rackspace and Amazon, but I wasn&amp;#8217;t paid nor asked to write this article.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Getting Ruby 1.9, Readline, Rails, and Mysql all running on Snow Leopard</title>
          <link>http://blog.michaelhamrah.com/2009/12/getting-ruby-1-9-readline-rails-and-mysql-all-running-on-snow-leopard/</link>
          <pubDate>Sun, 27 Dec 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/12/getting-ruby-1-9-readline-rails-and-mysql-all-running-on-snow-leopard/</guid>
          <description>

&lt;p&gt;In my never ending love/hate relationship with Ruby, Rails and my Mac I&amp;#8217;ve finally gotten Ruby 1.9 up and running with Rails 2.3 and MySql 64 bit.  All on Snow Leopard.  There was an even a little detour with Readline.  If you&amp;#8217;ve scoured other posts about Snow Leopard, Ruby, Rails and Mysql and ended up here I feel your pain.  I hope this helps you on your way. &lt;a href=&#34;http://wonko.com/post/how-to-compile-ruby-191&#34;&gt;Most of this info&lt;/a&gt; is from other places which I&amp;#8217;ve explained in a little (just a little) but more depth.&lt;/p&gt;

&lt;h3 id=&#34;install-xcode:c926fa09a3f264a43863b502ab5b6f5a&#34;&gt;Install XCode&lt;/h3&gt;

&lt;p&gt;You need XCode to do any of this, so install it.  If you&amp;#8217;re upgrading to Snow Leopard, reinstall XCode so you get the correct c compiler.&lt;/p&gt;

&lt;h3 id=&#34;your-profile:c926fa09a3f264a43863b502ab5b6f5a&#34;&gt;Your profile&lt;/h3&gt;

&lt;p&gt;Here&amp;#8217;s the deal.  We&amp;#8217;re going to install ruby 1.9 to your /usr/local directory.  It will dump stuff in /usr/local/bin and other stuff in /usr/local/lib.  Why here?  That&amp;#8217;s where it goes.  The default install of ruby on Snow Leopard 1.8, lives in /System/Library/Frameworks/Ruby.framework/Versions/Current.  Current is really an alias (actually, symlink) to the 1.8 directory at the same level.  For some it may seem like a good idea to install ruby 1.9 here.  It&amp;#8217;s not.  Just put in /usr/local like everyone else.&lt;/p&gt;

&lt;p&gt;Because ruby 1.9 will live in our /usr/local you have to help out your terminal a little.  You have to tell it where to look for the bin of ruby 1.9.  So when you run &amp;#8220;ruby&amp;#8221; from terminal you get the 1.9 version in /usr/local, not the 1.8 version in the System Library.  That&amp;#8217;s why you have to add a path to /usr/local in your profile.  Do this from the terminal:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;mate ~/.bash_profile
&lt;/pre&gt;

&lt;p&gt;This says, &amp;#8220;open or create the file .bash_profile, in the home directory (~/), using textmate&amp;#8221;.  You can use any other editor if you know how- but if you did you probably wouldn&amp;#8217;t need to read this.  So just buy- and use- textmate.  It&amp;#8217;s a nice app.  Now, .bash_profile is a file used by bash, aka the terminal app, for settings.  Some places you&amp;#8217;ll see &amp;#8220;mate .profile&amp;#8221; instead.  &lt;a href=&#34;http://hayne.net/MacDev/Notes/unixFAQ.html&#34;&gt;This will work too&lt;/a&gt;&amp;#8211; but if you have a .profile and .bash_profile you may run in to problems.  Just have one, preferably .bash_profile, and write this in it:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;export PATH=/usr/local/bin:/usr/local/sbin:/usr/local/mysql/bin:$PATH
&lt;/pre&gt;

&lt;p&gt;This appends our /usr/local/bin and sbin to the current list of directories to search for when trying to find out where all those little commands live which you type into the terminal.  Keen eyes may have noticed the mysql/bin thrown in there.  This is for later.  The :$PATH at the end is extremely important- it includes other paths which are included in other places. Once this is done then type &amp;#8220;source .bash_profile&amp;#8221; from terminal to load the changes.&lt;/p&gt;

&lt;h3 id=&#34;try-installing-ruby:c926fa09a3f264a43863b502ab5b6f5a&#34;&gt;Try Installing Ruby&lt;/h3&gt;

&lt;p&gt;One way to install ruby is by using &lt;a href=&#34;http://www.macports.org/&#34;&gt;MacPorts&lt;/a&gt;.  If you want to get a little more hands on, we&amp;#8217;re going to pull the source down and build it ourselves.  MacPorts is probably the easiest option.  We&amp;#8217;re not doing the easy option.&lt;/p&gt;

&lt;p&gt;Note: Read all this first!  In the terminal, make sure you&amp;#8217;re in your home directory by doing a simple &amp;#8220;cd&amp;#8221;.  Then, do this:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;mkdir src
cd src
&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;re creating a new directory called src for our source files, and moving into said directory.  Now we can get the code:&lt;/p&gt;

&lt;p&gt;curl -O &lt;a href=&#34;ftp://ftp.ruby-lang.org/pub/ruby/1.9/ruby-1.9.1-p376.tar.gz&#34;&gt;ftp://ftp.ruby-lang.org/pub/ruby/1.9/ruby-1.9.1-p376.tar.gz&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;tar xzvf ruby-1.9.1-p376.tar.gz&lt;/p&gt;

&lt;p&gt;cd ruby-1.9.1-p376/&lt;/p&gt;

&lt;p&gt;Curl is a nice little app to pull things from the internet.  The -O option simply names the local file the same as the remote file.  Ruby 1.9.1-p376 was the latest version as of this writing, but [In my never ending love/hate relationship with Ruby, Rails and my Mac I&amp;#8217;ve finally gotten Ruby 1.9 up and running with Rails 2.3 and MySql 64 bit.  All on Snow Leopard.  There was an even a little detour with Readline.  If you&amp;#8217;ve scoured other posts about Snow Leopard, Ruby, Rails and Mysql and ended up here I feel your pain.  I hope this helps you on your way. &lt;a href=&#34;http://wonko.com/post/how-to-compile-ruby-191&#34;&gt;Most of this info&lt;/a&gt; is from other places which I&amp;#8217;ve explained in a little (just a little) but more depth.&lt;/p&gt;

&lt;h3 id=&#34;install-xcode-1:c926fa09a3f264a43863b502ab5b6f5a&#34;&gt;Install XCode&lt;/h3&gt;

&lt;p&gt;You need XCode to do any of this, so install it.  If you&amp;#8217;re upgrading to Snow Leopard, reinstall XCode so you get the correct c compiler.&lt;/p&gt;

&lt;h3 id=&#34;your-profile-1:c926fa09a3f264a43863b502ab5b6f5a&#34;&gt;Your profile&lt;/h3&gt;

&lt;p&gt;Here&amp;#8217;s the deal.  We&amp;#8217;re going to install ruby 1.9 to your /usr/local directory.  It will dump stuff in /usr/local/bin and other stuff in /usr/local/lib.  Why here?  That&amp;#8217;s where it goes.  The default install of ruby on Snow Leopard 1.8, lives in /System/Library/Frameworks/Ruby.framework/Versions/Current.  Current is really an alias (actually, symlink) to the 1.8 directory at the same level.  For some it may seem like a good idea to install ruby 1.9 here.  It&amp;#8217;s not.  Just put in /usr/local like everyone else.&lt;/p&gt;

&lt;p&gt;Because ruby 1.9 will live in our /usr/local you have to help out your terminal a little.  You have to tell it where to look for the bin of ruby 1.9.  So when you run &amp;#8220;ruby&amp;#8221; from terminal you get the 1.9 version in /usr/local, not the 1.8 version in the System Library.  That&amp;#8217;s why you have to add a path to /usr/local in your profile.  Do this from the terminal:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;mate ~/.bash_profile
&lt;/pre&gt;

&lt;p&gt;This says, &amp;#8220;open or create the file .bash_profile, in the home directory (~/), using textmate&amp;#8221;.  You can use any other editor if you know how- but if you did you probably wouldn&amp;#8217;t need to read this.  So just buy- and use- textmate.  It&amp;#8217;s a nice app.  Now, .bash_profile is a file used by bash, aka the terminal app, for settings.  Some places you&amp;#8217;ll see &amp;#8220;mate .profile&amp;#8221; instead.  &lt;a href=&#34;http://hayne.net/MacDev/Notes/unixFAQ.html&#34;&gt;This will work too&lt;/a&gt;&amp;#8211; but if you have a .profile and .bash_profile you may run in to problems.  Just have one, preferably .bash_profile, and write this in it:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;export PATH=/usr/local/bin:/usr/local/sbin:/usr/local/mysql/bin:$PATH
&lt;/pre&gt;

&lt;p&gt;This appends our /usr/local/bin and sbin to the current list of directories to search for when trying to find out where all those little commands live which you type into the terminal.  Keen eyes may have noticed the mysql/bin thrown in there.  This is for later.  The :$PATH at the end is extremely important- it includes other paths which are included in other places. Once this is done then type &amp;#8220;source .bash_profile&amp;#8221; from terminal to load the changes.&lt;/p&gt;

&lt;h3 id=&#34;try-installing-ruby-1:c926fa09a3f264a43863b502ab5b6f5a&#34;&gt;Try Installing Ruby&lt;/h3&gt;

&lt;p&gt;One way to install ruby is by using &lt;a href=&#34;http://www.macports.org/&#34;&gt;MacPorts&lt;/a&gt;.  If you want to get a little more hands on, we&amp;#8217;re going to pull the source down and build it ourselves.  MacPorts is probably the easiest option.  We&amp;#8217;re not doing the easy option.&lt;/p&gt;

&lt;p&gt;Note: Read all this first!  In the terminal, make sure you&amp;#8217;re in your home directory by doing a simple &amp;#8220;cd&amp;#8221;.  Then, do this:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;mkdir src
cd src
&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;re creating a new directory called src for our source files, and moving into said directory.  Now we can get the code:&lt;/p&gt;

&lt;p&gt;curl -O &lt;a href=&#34;ftp://ftp.ruby-lang.org/pub/ruby/1.9/ruby-1.9.1-p376.tar.gz&#34;&gt;ftp://ftp.ruby-lang.org/pub/ruby/1.9/ruby-1.9.1-p376.tar.gz&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;tar xzvf ruby-1.9.1-p376.tar.gz&lt;/p&gt;

&lt;p&gt;cd ruby-1.9.1-p376/&lt;/p&gt;

&lt;p&gt;Curl is a nice little app to pull things from the internet.  The -O option simply names the local file the same as the remote file.  Ruby 1.9.1-p376 was the latest version as of this writing, but]&lt;a href=&#34;http://www.ruby-lang.org/en/&#34;&gt;4&lt;/a&gt; for the latest release.  Tar xzvf simply unpacks the compressed download.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Very helpful hint:&lt;/strong&gt; &lt;em&gt;If you ever are unsure about a command, simply type the command and &amp;#8211;help.  As in, curl &amp;#8211;help.  This is very helpful.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Next, we get to the good stuff.  First, run autoconf simply by typing &amp;#8220;autoconf&amp;#8221;:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;autoconf&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://www.gnu.org/software/autoconf/&#34;&gt;Autoconf&lt;/a&gt; is a tool used for generating configuration scripts.  It&amp;#8217;s important you run this.  Then, run the configuration script.  The thing which worked for me was:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;./configure --prefix=/usr/local/ --enable-shared --with-readline-dir=/usr/local
&lt;/pre&gt;

&lt;p&gt;This tells us to install ruby in the /usr/local directory, build a shared library for ruby, and use the readline installation found in /usr/local. &lt;strong&gt;Very Important:&lt;/strong&gt; some users prefer adding a suffix to the ruby 1.9 install so it doesn&amp;#8217;t interfere with the system install of ruby.  By adding the &amp;#8211;program-suffix=19 option to configure you&amp;#8217;ll append &amp;#8220;19&amp;#8221; to all commands, like &amp;#8220;ruby19&amp;#8243; and &amp;#8220;gem19&amp;#8243;.  This is a smart idea as it won&amp;#8217;t interfere with the default ruby installation.  Using this technique there are ways to easily switch between ruby installations.  If you don&amp;#8217;t care about 1.8, and just want the ease of typing &amp;#8220;ruby&amp;#8221; and getting the latest 1.9, omit the &amp;#8211;program-suffix option.&lt;/p&gt;

&lt;p&gt;If you run ./configure and get an error of:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;configure: WARNING: unrecognized options: &amp;#8211;with-readline-dir&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;You did not run autoconf.  Type it and run it, then run configure again.&lt;/p&gt;

&lt;p&gt;Sometimes you&amp;#8217;ll see the &amp;#8211;enable-pthread option.  There seems to be some debate on whether this is a good idea.  I say omit it unless you know what you&amp;#8217;re doing.  You can google for more info.  Feel free to explore and google other configure options- simply type &amp;#8220;configure &amp;#8211;help&amp;#8221; to list them all.&lt;/p&gt;

&lt;p&gt;Next we need to run:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;make&lt;/pre&gt;

&lt;p&gt;This builds all the source files needed for the install.  If you run this and get an error of:&lt;/p&gt;

&lt;p&gt;readline.c: In function â€˜username_completion_proc_callâ€™:&lt;/p&gt;

&lt;p&gt;readline.c:1159: error: â€˜username_completion_functionâ€™ undeclared (first use in this function)&lt;/p&gt;

&lt;p&gt;You don&amp;#8217;t have readline- or at least the proper version of readline- installed.  This is a problem.  Let&amp;#8217;s get it.&lt;/p&gt;

&lt;h3 id=&#34;readline:c926fa09a3f264a43863b502ab5b6f5a&#34;&gt;Readline&lt;/h3&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd ~/src
curl -O ftp://ftp.gnu.org/gnu/readline/readline-6.0.tar.gz
tar xzvf readline-6.0.tar.gz
cd readline-6.0
./configure --prefix=/usr/local
make
sudo make install
&lt;/pre&gt;

&lt;p&gt;You&amp;#8217;ve now installed readline.  You may get a warning of &lt;em&gt;install: you may need to run ldconfig at&lt;/em&gt; at the end&lt;em&gt;.&lt;/em&gt; Don&amp;#8217;t worry about it.  At least I didn&amp;#8217;t have to worry about it.&lt;/p&gt;

&lt;h3 id=&#34;do-everything-again:c926fa09a3f264a43863b502ab5b6f5a&#34;&gt;Do everything again&lt;/h3&gt;

&lt;p&gt;By now, you know the drill.  Hop back to the ruby source code directory and try it again.  But if you ran make in the ruby install step and got errors, just run &amp;#8220;make clean&amp;#8221; to reset everything.  It&amp;#8217;s a good idea.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;make clean
autoconf
./configure --prefix=/usr/local/ --with-readline-dir=/usr/local --enable-shared
make
sudo make install
&lt;/pre&gt;

&lt;p&gt;That should be it.  Hopefully you&amp;#8217;re error free.  Typing:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;which ruby&lt;/pre&gt;

&lt;p&gt;should return:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;/usr/local/bin/ruby&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;and typing &amp;#8220;ruby -v&amp;#8221; should return the version of ruby you&amp;#8217;ve just downloaded.  Unless you used the &amp;#8211;program-suffix option above, then it&amp;#8217;s probably &amp;#8220;ruby19 -v&amp;#8221;&lt;/p&gt;

&lt;h3 id=&#34;r-ails:c926fa09a3f264a43863b502ab5b6f5a&#34;&gt;&lt;strong&gt;R&lt;/strong&gt;&lt;strong&gt;ails&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Now you can download and install rails.  Simply run:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo gem update --system
sudo gem install rails
&lt;/pre&gt;

&lt;h3 id=&#34;mysql:c926fa09a3f264a43863b502ab5b6f5a&#34;&gt;Mysql&lt;/h3&gt;

&lt;p&gt;Mysql is a piece of cake.  Simply grab the x86_64 &lt;a href=&#34;http://dev.mysql.com/downloads/mysql/5.1.html#macosx-dmg&#34;&gt;install package from the Mysql site.&lt;/a&gt; Even though it&amp;#8217;s for 10.5, it works fine on 10.6 (Snow Leopard).  Once this is done, you can build the mysql gem:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo env ARCHFLAGS=&#34;-arch x86_64&#34; gem install mysql -- --with-mysql-config=/usr/local/mysql/bin/mysql_config
&lt;/pre&gt;

&lt;h3 id=&#34;fin:c926fa09a3f264a43863b502ab5b6f5a&#34;&gt;Fin&lt;/h3&gt;

&lt;p&gt;Now, try to get everything running.  Go back to your home directory, create a rails app, and see if it works:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;cd ~/
rails playground --databse=mysql
cd playground
rake db:create
script/server
&lt;/pre&gt;

&lt;p&gt;Go to &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;, check your environment settings, and you should see:&lt;/p&gt;

&lt;table style=&#34;height: 164px;&#34; width=&#34;256&#34;&gt;
  &lt;tr&gt;
    &lt;td&gt;
      Ruby version
    &lt;/td&gt;
    
    &lt;td&gt;
      1.9.1 (i386-darwin10.2.0)
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      RubyGems version
    &lt;/td&gt;
    
    &lt;td&gt;
      1.3.5
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      Rack version
    &lt;/td&gt;
    
    &lt;td&gt;
      1.0
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      Rails version
    &lt;/td&gt;
    
    &lt;td&gt;
      2.3.5
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      Active Record version
    &lt;/td&gt;
    
    &lt;td&gt;
      2.3.5
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      Active Resource version
    &lt;/td&gt;
    
    &lt;td&gt;
      2.3.5
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      Action Mailer version
    &lt;/td&gt;
    
    &lt;td&gt;
      2.3.5
    &lt;/td&gt;
  &lt;/tr&gt;
  
  &lt;tr&gt;
    &lt;td&gt;
      Active Support version
    &lt;/td&gt;
    
    &lt;td&gt;
      2.3.5
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;And voila, you&amp;#8217;re done!&lt;/p&gt;

&lt;p&gt;UPDATE:&lt;/p&gt;

&lt;p&gt;If you use Textmate to develop with Rails, your Textmate Ruby path will point to the system&amp;#8217;s 1.8 version, so you&amp;#8217;ll get awakard issues of Gems not being available or other weird stuff when trying to run Ruby within Textmate (like when you want to RSpec tests).  This fix is simple: go to Textmate -&amp;gt; Preferences -&amp;gt; Advanced -&amp;gt; Shell Variables and add TM_RUBY with a value of /usr/local/bin/ruby and you&amp;#8217;ll be good to go.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Organizing Javascript for Event Pooling with jQuery</title>
          <link>http://blog.michaelhamrah.com/2009/12/organizing-javascript-for-event-pooling-with-jquery/</link>
          <pubDate>Mon, 21 Dec 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/12/organizing-javascript-for-event-pooling-with-jquery/</guid>
          <description>

&lt;p&gt;It turns out my most popular article of the past year was &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2008/12/event-pooling-with-jquery-using-bind-and-trigger-managing-complex-javascript/&#34;&gt;Event Pooling with jQuery&amp;#8217;s Bind and Trigger&lt;/a&gt;.&amp;nbsp; I wanted to write a follow up article taking this approach one step further by discussing how to logically organize the relationship between binders and triggers on a javascript heavy UI.&amp;nbsp; It&amp;#8217;s important to properly design the code structure of your javascript to create a flexible and maintainable system.&amp;nbsp; This is essential for any software application. &amp;nbsp;For javascript development, you don&amp;#8217;t want to end up with odd dependencies hindering changes or randomly bubbled events causing bugs.&lt;/p&gt;

&lt;h3 id=&#34;event-pooling-a-quick-review:b1aa778a626daa40c556fbd74954875f&#34;&gt;Event Pooling: A Quick Review&lt;/h3&gt;

&lt;p&gt;What is event pooling and why is it important?&amp;nbsp; Quite simply it&amp;#8217;s a way to manage dependencies.&amp;nbsp; You create a loosely coupled system between the thing which triggers an action to happen and the thing which responds to the action, called the binder.&amp;nbsp; jQuery has some cool bind and trigger functionality which allows you to create custom events for event pooling- and you can use this to easily wire up multiple functions to write complex javascript with ease.&amp;nbsp; I encourage you to check out the original how-to article &lt;a href=&#34;../index.php/2008/12/event-pooling-with-jquery-using-bind-and-trigger-managing-complex-javascript/&#34;&gt;Event Pooling with jQuery&amp;#8217;s Bind and Trigger&lt;/a&gt; to learn more. &amp;nbsp;It&amp;#8217;s a very powerful technique.&lt;/p&gt;

&lt;p&gt;Now, as we all know, with great power comes great responsibility.&amp;nbsp; If you structure the relationship between binders and triggers incorrectly you&amp;#8217;ll end up with a mess on your hands, a spider web of logic which is almost impossible to unwind.&amp;nbsp; Here are some tips to better structure your binders and triggers for a logical and maintainable application&lt;/p&gt;

&lt;h3 id=&#34;presenting-the-problem:b1aa778a626daa40c556fbd74954875f&#34;&gt;Presenting the Problem&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2009/12/Stupid-Form1.png&#34;&gt;&lt;img class=&#34;alignleft size-medium wp-image-292&#34; title=&#34;Stupid Form&#34; src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2009/12/Stupid-Form1-300x179.png?resize=300%2C179&#34; alt=&#34;Stupid Form&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s take a look at an example app. &amp;nbsp;Here&amp;#8217;s a simple form where a user can draft a report, save it for later, or send it immediately. There are three options to enter data in the name/email/department field.&amp;nbsp; The user can enter it manually, choose from the autosuggest area at left, or simply select one of the favorite links.&amp;nbsp; The autosuggest/favorite link would pre-populate the textboxes used in the form. &amp;nbsp;If all the data is there, the send button is enabled. &amp;nbsp;If not, it&amp;#8217;s disabled.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s discuss how we&amp;#8217;d deal with enabling and disabling the send button. &amp;nbsp;There are many ways to approach this. &amp;nbsp;A traditional way is to wire up some standard event to our textboxes:&lt;/p&gt;

&lt;pre class=&#34;brush: jscript; title: ; notranslate&#34; title=&#34;&#34;&gt;$(&#39;#name&#39;).keyup(function(){
    //Logic (hopefully structured in a reusable way across all controls).
});
//Do this for all other input controls.
&lt;/pre&gt;

&lt;p&gt;This solution is perfectly valid and works, but it has some shortcomings. &amp;nbsp;First, you need to wire up all three controls and route them to the same function. &amp;nbsp;Next, even though the keyup will handle user input, the textboxes could change in other ways- either from the favorite link or the auto suggest box. &amp;nbsp;We&amp;#8217;ll need to call our validation function in numerous places. &amp;nbsp;There may also be other logic we want to incorporate within the text change event unrelated to enabling of the send button- email validation, for instance, which is only applicable to the email textbox, or a certain length requirement on the name textbox. &amp;nbsp;What happens if there&amp;#8217;s a new requirement where the subject/body must be filled to enable the send button? Where does all this functionality fall into our larger requirements?&lt;/p&gt;

&lt;p&gt;You can imagine the complexity of writing the javascript required for this UI. &amp;nbsp;We need a slew of functions to deal with validation logic: Both for enabling the send button and other input controls. &amp;nbsp;We need to wire up all our onkeyup and on click events across the auto suggest field, textboxes, and favorite links. &amp;nbsp;Function calls are everywhere. &amp;nbsp;It will take all your code complete skills to manage those dependencies and keep this code lean.&lt;/p&gt;

&lt;p&gt;This is where event pooling comes into play: instead of direct dependencies between these controls and logic, you bubble events to a middle man and allow interested parties to respond accordingly. &amp;nbsp;Instead of all the controls telling the send button to enable/disable itself, the send button &amp;#8220;listens&amp;#8221; for changes it cares about- when the value of the textbox changes- and responds accordingly. &amp;nbsp;The responsibility is reversed- the function which needs to be called can choose when it&amp;#8217;s called, rather than waiting for something to call it.&lt;/p&gt;

&lt;h3 id=&#34;dependencies-between-binders-and-triggers:b1aa778a626daa40c556fbd74954875f&#34;&gt;Dependencies Between Binders and Triggers&lt;/h3&gt;

&lt;p&gt;With event pooling there are two things you need to be conscience of:&amp;nbsp; the events themselves and the data passed between the binder and trigger.&amp;nbsp; These items form the two dependencies between binders and triggers.&amp;nbsp; The event serves as a link between the binder and trigger and the data is the information which is passed between the two.&amp;nbsp; It&amp;#8217;s essential to properly structure your events and choose the right data transfer option to avoid pitfalls over the life of the application.&amp;nbsp; We&amp;#8217;ll deal with each aspect individually. &amp;nbsp;This post will be about the structure of events, and I&amp;#8217;ll write various ways to pass data between parties in another post.&lt;/p&gt;

&lt;h3 id=&#34;structuring-custom-events:b1aa778a626daa40c556fbd74954875f&#34;&gt;Structuring Custom Events&lt;/h3&gt;

&lt;p&gt;&lt;span style=&#34;font-weight: normal;&#34;&gt;Most people are familiar with the standard javascript events: click, blur, enter, exit, etc.&amp;nbsp; These are what you usually wire up to functions when you want to do something.&amp;nbsp; However, they only go so far- you need custom events when you have a lot of stuff happening. &amp;nbsp;Why use them?&amp;nbsp; Quite simply, it&amp;#8217;s more logical for your application control flow. &amp;nbsp;For our send button functionality, we want our validation function to act when something happens. &amp;nbsp;This &amp;#8220;something&amp;#8221; will be a custom event we create. &amp;nbsp;We have two options for naming our event: we can name the event after what is intended, like &amp;#8220;ENABLE_DISABLE_SEND_BUTTON&amp;#8221;, or name the event after what has happened, like &amp;#8220;NAME_CHANGED&amp;#8221; or &amp;#8220;FAVORITE_LINK_SELECTED&amp;#8221;. &amp;nbsp;The former option requires multiple events to be fired when something happens. &amp;nbsp;The autosuggest box, for instance, would require the ENABLE_DISABLE_SEND_BUTTON trigger, a SELECTED_CONTACT event for setting the textboxes, and whatever else happens after a contact is selected. &amp;nbsp;The latter option requires just one trigger to be fired, but the binder must subscribe to multiple events. &amp;nbsp;The choice can also be presented like this: do we want each element to fire a trigger for each action which should occur, or have a function to listen for multiple triggered events?&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;font-weight: normal;&#34;&gt;&lt;a href=&#34;http://i0.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2009/12/Event_Pooling-2-e1261347350823.png&#34;&gt;&lt;img class=&#34;aligncenter size-full wp-image-300&#34; title=&#34;Event_Pooling-2&#34; src=&#34;http://i0.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2009/12/Event_Pooling-2-e1261347350823.png?resize=600%2C317&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Naming events may seem like an arbitrary and thoughtless act, but it&amp;#8217;s essential to have a naming strategy with event pooling. &amp;nbsp;Just like wiring up functions directly can be unwieldy, so can a slew of events wired up every which way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For event pooling, it&amp;#8217;s more important to name events after what has happened, rather than what is intended.&lt;/strong&gt; So the correct approach is to go with things like &amp;#8220;NAME_CHANGED&amp;#8221;, &amp;#8220;CONTACT_SELECTED&amp;#8221;, or &amp;#8220;EMAIL_CHANGED&amp;#8221;. &amp;nbsp;The rationale has to do with dependencies themselves: &amp;nbsp;we don&amp;#8217;t want functions called from random parts of the system via specific events: &amp;nbsp;this is no better than calling functions directly from disparate parts of the system. &amp;nbsp;And with specific action related events like &amp;#8220;ENABLE_SEND&amp;#8221; you just know someone is going to take a shortcut and wire some random binder to a totally unrelated trigger, because that trigger is fired from the control they want to monitor. &amp;nbsp;Binders- and the functions they are wired to- should proactively &amp;#8220;listen&amp;#8221; for things which it&amp;#8217;s interested in. &amp;nbsp;This allows you to easily know why a function is being called. &amp;nbsp;If you need to change why or how something is handled, you go to the recipient, not the caller. &amp;nbsp;The caller, after all, could be anything, and more importantly the caller could be triggering multiple things.&lt;/p&gt;

&lt;pre class=&#34;brush: jscript; title: ; notranslate&#34; title=&#34;&#34;&gt;//Funnel all possibilities to single custom trigger
$(&#39;#name&#39;).bind(&#39;keyup enter&#39;, function() {
 $(document).trigger(&#39;NAME_CHANGED&#39;);
});

//Funnel all possibilites to single custom trigger
$(&#39;#email&#39;).bind(&#39;keyup enter&#39;, function() {
$(document).trigger(&#39;EMAIL_CHANGED&#39;);
});

//I know why this is being called because I&#39;ve subscribed
//to the events I&#39;m interested in.
$(document).bind(&#39;NAME_CHANGED EMAIL_CHANGED&#39;, function()
{
 //Handle validation, etc.
});
&lt;/pre&gt;

&lt;p&gt;The cool thing about this approach is there may be something else which will cause an email or name changed event to happen: specifically, when someone has selected a contact from the autosuggest list or a favorite link is selected. &amp;nbsp;You can fire the name_changed event from multiple places and not worry about wiring up any new triggers. &amp;nbsp;The code would look something like this:&lt;/p&gt;

&lt;pre class=&#34;brush: jscript; title: ; notranslate&#34; title=&#34;&#34;&gt;//Fired from elsewhere
$(document).bind(&#39;FAVORITE_LINK_SELECTED&#39;, function()
{
  //Handle selection
  //Set Name, Email, etc.
  //Fire event:
  $(document).trigger(&#39;EMAIL_CHANGED&#39;);
  $(document).trigger(&#39;NAME_CHANGED&#39;);
});
&lt;/pre&gt;

&lt;p&gt;How nice: &amp;nbsp;another part of the system changes the name indirectly, and we don&amp;#8217;t have to worry about hooking anything up because the binder is already subscribed to the NAME_CHANGED event.&lt;/p&gt;

&lt;p&gt;Note the name doesn&amp;#8217;t necessarily correspond to a specific element: It&amp;#8217;s not a textbox_name_changed event, nor are any specific html id&amp;#8217;s involved except for wiring up a trigger from a standard keyup event. &amp;nbsp;This is an important difference: we could rename the id&amp;#8217;s, or switch to some other input control, and not have to rewire everything. &amp;nbsp;We don&amp;#8217;t care about the textbox nor that the textbox&amp;#8217;s value has changed. &amp;nbsp;We care that the textbox respresents the name entered or the email address- and we want to know when the name or email has changed. &amp;nbsp;The favorite link clicked is a good example of this nuanced difference. &amp;nbsp;Take a look at the following html:&lt;/p&gt;

&lt;pre class=&#34;brush: xml; title: ; notranslate&#34; title=&#34;&#34;&gt;&amp;lt;a href=&#39;#&#39; id=&#39;fav1&#39; class=&#39;favorite&#39;&amp;gt;My Favorite 1&amp;lt;/a&amp;gt;
&amp;lt;a href=&#39;#&#39; id=&#39;fav2&#39; class=&#39;favorite&#39;&amp;gt;My Favorite 2&amp;lt;/a&amp;gt;
&lt;/pre&gt;

&lt;p&gt;Imagine we&amp;#8217;ve wired the click event to fire a FAVORITE_LINK_SELECTED trigger. &amp;nbsp;With this trigger, we don&amp;#8217;t care that the link with id fav1 or fav2 has been clicked, nor the a.favorite selector has been clicked. &amp;nbsp;We don&amp;#8217;t care about ids or css classes. &amp;nbsp;We care a FAVORITE_LINK_SELECTED event has happened because that&amp;#8217;s what the id fav1 and favorite class represents- the favorite link- and we want to know when that has been selected. &amp;nbsp;We can rename the id, change the class, or even change the entire element. &amp;nbsp;As long as FAVORITE_LINK_SELECTED is fired we&amp;#8217;re good to go. &amp;nbsp;Our custom FAVORITE_LINK_SELECTED trigger is the abstraction which creates the loosely coupled system.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:b1aa778a626daa40c556fbd74954875f&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;One thing I haven&amp;#8217;t discuss is how data is passed from trigger to binder. &amp;nbsp;It&amp;#8217;s the other important dependency between triggers and binders which we&amp;#8217;ll discuss in another post. &amp;nbsp;The important thing to take away is why you&amp;#8217;d want to use custom named events to create a loosely coupled system. &amp;nbsp;For very straightforward pages it&amp;#8217;s probably not worth the overhead- the abstraction isn&amp;#8217;t required. &amp;nbsp;However, in a complex form where there are lots of validation dependencies, or many routes to the same function, or when multiple events can trigger an update- you want to use event pooling. &amp;nbsp;More importantly, you want to think about your strategy when it comes to naming events, because it&amp;#8217;s the description linking the caller and method together. &amp;nbsp;You do not want to code yourself into a corner!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Help Your Business Be A Technology Company</title>
          <link>http://blog.michaelhamrah.com/2009/12/help-your-business-be-a-technology-company/</link>
          <pubDate>Wed, 16 Dec 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/12/help-your-business-be-a-technology-company/</guid>
          <description>&lt;p&gt;It doesn&amp;#8217;t matter what industry you work for- every company is a technology company.  At the end of the day a business needs to provide something somebody wants- quickly and efficiently.  This is- quite simply- driven by technology.  And you- as a software developer- are directly responsible for making that happen.  What will separate you from other developers is not how well you program-  it&amp;#8217;s how well you create the right technology to help your business work better.&lt;/p&gt;

&lt;p&gt;Unfortunately most people don&amp;#8217;t really understand technology nor how to use it.  I&amp;#8217;m not talking about silly things like printing a pdf or syncing your Outlook calendar- I&amp;#8217;m talking about &lt;em&gt;how technology can optimize business processes.  How technology can both improve efficiency and quickly provide accurate, meaningful information.&lt;/em&gt; &lt;em&gt;How technology can be the engine keeping your business two steps ahead of everyone else.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Luckily you have an advantage to accomplish this goal: you know how to create technology.  The important thing, however, is knowing both what the business is and also needs so you can help make it work better.  Building software isn&amp;#8217;t nearly as important as knowing what to build.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Don&amp;#8217;t just Program-  Provide Solutions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Don&amp;#8217;t be the type of heads down programmer that wants to be told what to do just so they can code.  People that wait around for requirements and specs are a waste of time and energy.  Everything has to be articulated to a T, then assumptions and misunderstandings lead to bad a deliverable which can sink a ship.  Yes, it&amp;#8217;s important to code and code well, but all too often code eclipses the bigger picture.&lt;/p&gt;

&lt;p&gt;As a programmer, your job is to create something to fill a gap.  You need to understand why that gap exists so you can figure out the best way to fill it.  You&amp;#8217;re the programmer- you literally create the solution.  You know what&amp;#8217;s possible and what&amp;#8217;s not- what&amp;#8217;s easy and what&amp;#8217;s hard.  And if you&amp;#8217;re experienced you know the consequences of decisions.  So don&amp;#8217;t blindly accept solutions.  Spend time understanding the problem- come up with the solution- and then work with people to figure out if your solution will solve the problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lead by Listening&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Coming up with the solution should not equate to arrogance.  The world, unfortunately, does not revolve around you.  Despite how annoying your users may seem, they are your responsibility and you have an obligation to them.  Nobody likes to be told what to do or have something shoved in their face.  Yes, it is a lot easier for you to just do it yourself and say &amp;#8220;here&amp;#8221;.  Unfortunately, for everybody else, that way sucks.  You need to lead by listening: sit with people, listen to what they have to say, and show that you get &amp;#8220;it&amp;#8221;.&lt;/p&gt;

&lt;p&gt;If you listen well you&amp;#8217;re in a better position to explain why you&amp;#8217;re making the decisions you&amp;#8217;re making.  Work with users: build consensus and understanding.  If you have a business advocate who&amp;#8217;s on board with your plan, you have a powerful ally to help create the change you want.  Bounce ideas off of people- and if you think an idea is bad, say so, but always provide an alternative solution.  Being negative and impatient is the quickest way to alienate yourself and have the best idea shot down.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Change the Process&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can only make a car go so fast- at some point, you need to fly.  So don&amp;#8217;t just help the driver drive; help them get to the destination.  Efficiency is key, but it&amp;#8217;s found in unusual ways.  People want to get their job done as quickly as possible with the least amount of annoyance.  Helping them achieve this is often a good thing, but beware: making them do their job better could make a lot of other stuff worse.  It can&amp;#8217;t just be about speeding up the process- the process may need to change.  It&amp;#8217;s up to you to see the big picture and help everybody, not just a single person.  It&amp;#8217;s just like scaling a system: at some point the current process will hit a limit; you need to know what that limit is, when it will happen, and what the hell to do next.&lt;/p&gt;

&lt;p&gt;Usually what&amp;#8217;s being asked for is different than what&amp;#8217;s actually needed.  You need to create an &amp;#8220;ah ha&amp;#8221; moment- when you can take all the frustrations and problems of your users, combine them into one solution, and have everyone say &amp;#8220;yes, that&amp;#8217;s perfect&amp;#8221;!  Have a vision.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Roadmap&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Sadly, you always have two things against you: time and patience.  Nobody has either.  You can&amp;#8217;t drastically change a users day to day activity.  Imagine if all of a sudden we had to start driving on the wrong side of the road- total chaos would ensue.  It&amp;#8217;s the same thing with drastically changing an application- for the end user, it&amp;#8217;s hard for the long term benefit to outweigh the short term difficulties.  It&amp;#8217;s also a horrible idea to wait a long time for your great solution to be delivered: by that time, it&amp;#8217;s no longer relevant, and everyone&amp;#8217;s forgotten why it&amp;#8217;s important.&lt;/p&gt;

&lt;p&gt;Small, incremental changes are key: you need a roadmap to the goal.  That&amp;#8217;s why we have agile.  You need to shepherd the heard to the promise land, and constantly remind them of the goal.  As new requirements come up and new requests come in you need to make sure everything is fitting into that big picture.  People may even come in with quick one-offs as stop gap measures- the biggest setback for moving forward.  You need to be able to say &amp;#8220;Here&amp;#8217;s why this is bad&amp;#8221; and provide better alternatives. If something must happen quicker, prioritize, refactor and meet half way.  Agile development has numerous benefits:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can provide iterative releases.  People can see progress unfold.&lt;/li&gt;
&lt;li&gt;You get in refactor/response cycles.  It&amp;#8217;s amazing how excited and appreciative people are if you make a change they ask for- no matter how small.  If something isn&amp;#8217;t working as envisioned, and gets changed quickly, you are a rock star.&lt;/li&gt;
&lt;li&gt;You can deliver faster: software can be used when it&amp;#8217;s usable, not when it&amp;#8217;s finished.&lt;/li&gt;
&lt;li&gt;Things can be prioritized.  When you can&amp;#8217;t do something somebody wants, it&amp;#8217;s not a surprise.  If you communicate well they know why it didn&amp;#8217;t happen.&lt;/li&gt;
&lt;li&gt;The learning curve upon delivery is minimized.  People know what they&amp;#8217;re getting and how it works.  After all, they were part of the design.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;The Deliverable is not the Release&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Good software is something that people like using.  At the end of the day, it doesn&amp;#8217;t matter which design patterns were used, how cool your ajax widget was, or even how many bugs the product has.  Writing code well does not equal a good product.  If it doesn&amp;#8217;t help somebody do something better you wasted a lot of time- particularly your own and even worse your users.  That&amp;#8217;s why time to market is so essential- it gets the product used sooner so reactions can be gauged and adjustments made.  Bells and whistles are nice to haves, but at the end of the day you need to ask yourself how important was it really?  Would a simple web 1.0 form have been better? Would it have been easier to use and quicker to develop? Is this thing that I&amp;#8217;m doing actually solving a problem?&lt;/p&gt;

&lt;p&gt;All of these questions equate to a simple rational:  Am I giving my business what it needs? The better you are at answering that question the better you&amp;#8217;ll be at programming the solution.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>HDR Photography</title>
          <link>http://blog.michaelhamrah.com/2009/12/hdr-photography/</link>
          <pubDate>Sat, 05 Dec 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/12/hdr-photography/</guid>
          <description>&lt;div style=&#34;text-align: left; padding: 3px;&#34;&gt;
  &lt;a title=&#34;photo sharing&#34; href=&#34;http://www.flickr.com/photos/hamrah/4141374402/&#34;&gt;&lt;img style=&#34;border: solid 2px #000000;&#34; src=&#34;http://i0.wp.com/farm3.static.flickr.com/2716/4141374402_d8eb9257df.jpg?w=660&#34; alt=&#34;&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt; 
  
  &lt;p&gt;
    &lt;span style=&#34;font-size: 0.8em; margin-top: 0px;&#34;&gt;&lt;a href=&#34;http://www.flickr.com/photos/hamrah/4141374402/&#34;&gt;fox glacier sunset at lake-24&lt;/a&gt;, originally uploaded by &lt;a href=&#34;http://www.flickr.com/people/hamrah/&#34;&gt;mhamrah&lt;/a&gt;.&lt;/span&gt;&lt;/div&gt; 
    

&lt;pre&gt;&lt;code&gt;&amp;lt;p&amp;gt;
  I&amp;amp;#8217;ve been playing around with HDR photography after coming back from an incredible vacation to New Zealand. This photo came out especially well.
&amp;lt;/p&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>AppStorm Giveaway!</title>
          <link>http://blog.michaelhamrah.com/2009/06/appstorm-giveaway/</link>
          <pubDate>Wed, 24 Jun 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/06/appstorm-giveaway/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;http://mac.appstorm.net&#34;&gt;AppStorm&lt;/a&gt; is an incredible site which reviews Mac software. I&amp;#8217;ve followed their reviews for a while and it&amp;#8217;s made my Mac 8000% more awesome. They frequently give away software- currently copies of Flux and Forklift are up for grabs. &lt;a href=&#34;http://mac.appstorm.net/general/competitions/the-mega-giveaway-forklift-flux/&#34;&gt;so check it out!&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Great Site for the Ruby on Rails API</title>
          <link>http://blog.michaelhamrah.com/2009/05/great-site-for-the-ruby-on-rails-api/</link>
          <pubDate>Fri, 29 May 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/05/great-site-for-the-ruby-on-rails-api/</guid>
          <description>&lt;p&gt;I came across this great site for the Ruby on Rails API:  &lt;a href=&#34;http://railsapi.com&#34; title=&#34;Ruby on Rails API&#34;&gt;http://railsapi.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://railsapi.com/doc/rails-v2.3.2.1_ruby-v1.9/&#34;&gt;Here&amp;#8217;s a link to Ruby 2.3.2.1 with Ruby 1.9.&lt;/a&gt; Why is this great? It has a slick interface with a fast AJAX powered search filter.  From a UI perspective, it makes great use of whitespace and font sizing to present information clearly and legibly.  Finally, the simplistic color scheme is easy on the eyes.  Also includes links for source and github.&lt;/p&gt;

&lt;p&gt;Bookmark it!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Weird Issue with rspec and authlogic-openid</title>
          <link>http://blog.michaelhamrah.com/2009/05/weird-issue-with-rspec-and-authlogic-openid/</link>
          <pubDate>Sun, 24 May 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/05/weird-issue-with-rspec-and-authlogic-openid/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve been playing around rspec in my application which also uses &lt;a href=&#34;http://github.com/binarylogic/authlogic_openid&#34;&gt;AuthLogic&amp;#8217;s OpenID&lt;/a&gt;.  When doing&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;rake spec &lt;/pre&gt;

&lt;p&gt;I get the following, very weird error:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;(in /Users/Michael/r/wc)
/Users/Michael/.gem/ruby/1.8/gems/activerecord-2.3.2/lib/active_record/base.rb:1964:in `method_missing&#39;: undefined method `config&#39; for #&amp;lt;Class:0x263efdc&amp;gt; (NoMethodError)
from /Library/Ruby/Gems/1.8/gems/authlogic-oid-1.0.3/lib/authlogic_openid/acts_as_authentic.rb:35:in `optional_fields=&#39;
from /Users/Michael/r/wc/app/models/user.rb:3
from /Library/Ruby/Gems/1.8/gems/authlogic-2.0.13/lib/authlogic/acts_as_authentic/base.rb:37:in `acts_as_authentic&#39;
from /Users/Michael/r/wc/app/models/user.rb:2
from /Library/Ruby/Site/1.8/rubygems/custom_require.rb:31:in `gem_original_require&#39;
from /Library/Ruby/Site/1.8/rubygems/custom_require.rb:31:in `require&#39;
&lt;/pre&gt;

&lt;p&gt;As seen above I&amp;#8217;m using version 1.0.3 of the source code.  For the life of me, I couldn&amp;#8217;t figure out what was wrong.  What was even stranger was the code was working when running the site.  It turns out, config got changed to rw_config, and the gem repository at rubyforge hasn&amp;#8217;t been updated to the latest 1.0.4 &lt;a href=&#34;http://github.com/binarylogic/authlogic_openid/commit/8a8c7729cf5590636892535167a86fffd40650b4&#34;&gt;which includes this change&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;Change from using config to the new rw_config. Requires the latest version 
of authlogic. config was too general of a name and was causing conflicts 
for people in their own projects.

&lt;/pre&gt;

&lt;p&gt;So I changed the acts_as_authentic.rb file so config is named rw_config, and everything worked fine.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;d love to find a good reference on how rspec loads gems, and why this isn&amp;#8217;t an issue when running the site.  I&amp;#8217;m also interested in knowning how to pull content as a gem from github source (the openid gem isn&amp;#8217;t at gems.github.com).&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>I Hate Persistence (but love Azure Table Storage)</title>
          <link>http://blog.michaelhamrah.com/2009/05/i-hate-persistence-but-love-azure-table-storage-with-data-services/</link>
          <pubDate>Tue, 12 May 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/05/i-hate-persistence-but-love-azure-table-storage-with-data-services/</guid>
          <description>&lt;p&gt;If there&amp;#8217;s one thing I can&amp;#8217;t stand in the development process it&amp;#8217;s writing code to save data.  In fact, there are only a few things which I&amp;#8217;d consider more useless than dealing with data persistence- one of them being data migration.  I hate dealing with persistence because it&amp;#8217;s totally mundane and repetitive code.  Worse, nobody outside of IT really understands the details of persistence.  Which means it has no business value.  And why should users care about unit of work or active record vs. repository?   Should saving an object really be that complicated of a task? Absolutely not.  Which is why no one cares.  It&amp;#8217;s like caring about how your beer was delivered.  Really, I just want to drink the damn beer.  Yet somehow the thing which should be a no brainer takes the most time and effort to do.  And causes the most debate.  And causes the most problems.  However, I recently got my first glimmer of hope- a &lt;em&gt;cloud&lt;/em&gt; with a silver lining- that maybe, just maybe, someday, we&amp;#8217;ll actually have simple data access.&lt;/p&gt;

&lt;p&gt;We as a development community have been transfixed on data access strategies.  It&amp;#8217;s the great elephant in the room, and has totally consumed all our attention.  It&amp;#8217;s distracting.  Only a .NET/Java language war in &amp;#8217;04 could rival such a fierce debate as between &lt;a href=&#34;http://ayende.com/Blog/archive/2009/04/26/the-repositoryrsquos-daughter.aspx&#34;&gt;Ayende&lt;/a&gt; and &lt;a href=&#34;http://codebetter.com/blogs/gregyoung/archive/2009/04/24/more-on-repository.aspx&#34;&gt;Greg Young&lt;/a&gt; over the repository pattern. (My two cents: ActiveRecord. Just kidding, ActiveRecord sucks (not SRP). Use &lt;a href=&#34;http:/www.subsonicproject.com&#34;&gt;Subsonic&lt;/a&gt;&amp;#8211; which lost me at &amp;#8220;drag and drop&amp;#8221;, but &lt;a href=&#34;http://blog.wekeroad.com/subsonic/subsonic-to-acquire-nhibernate/&#34;&gt;I would not want to piss off Rob Conery&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Why do I hate persistence?  It&amp;#8217;s never really gotten any better.  The first major advancement came with sprocs.  Everything had to be a sproc, and if you weren&amp;#8217;t using sprocs then you just weren&amp;#8217;t cool.  You can do these great things with sprocs- like writing select statements with inner joins.  But not in your vbscript!  This mostly lead to awesome interview questions like &amp;#8220;why are sprocs important&amp;#8221; and dba&amp;#8217;s who would whine about developers writing bad sql (guess what- they can write bad code too)!&lt;/p&gt;

&lt;p&gt;Next, and silently from the Java community, we were given NHibernate and the concept of an object relational mapper.  Suddenly, an awakening occurred, and we as developers could now stick it to those dba&amp;#8217;s and there precious sprocs.  Try changing my auto generated sql, you bastards! Unfortunately, the NHibernate craze did not take off.  We gave up sprocs for those awful configuration files- as if things could get any worse!  I personally have a big chip on my shoulder because NHibernate comes from the Java community.  Java, after all, just sucks (sidenote: this a totally baseless claim).  Fluent NHibernate doesn&amp;#8217;t make it that much better: I&amp;#8217;m not digging Cascade. All(). Inverse(). WithForeignKey(). WithIndex(). WithConvention(). ButNotForThisClass(). How. Many. Can. I. Add() and double code associations for parent/child relationships etc.  Not a big win.  But at least we can rub Fluent NHibernate in Hibernate&amp;#8217;s xml hell!  Yes, there&amp;#8217;s the auto mapper configuration which I admit is slick- but you&amp;#8217;re kind of locked into the default convention (over configuration).  Stray from the path and you&amp;#8217;re back in the configuration quicksand.  And don&amp;#8217;t even get me started on the NHibernate query syntax.  Yes, there&amp;#8217;s Linq to NHibernate, but why use that when you go directly to the best thing to come out of Microsoft since &lt;a href=&#34;http://www.youtube.com/watch?v=8To-6VIJZRE&#34;&gt;the Steve Ballmer video&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally there was a breath of fresh air, and Microsoft was awesome again.  We were given Linq to Sql.  But when Microsoft realized they did something cool, they quickly pushed for Linq to Entities (oh wait- Entity Framework- Linq to Entities sounds like the cool Linq to Sql).  We were quickly back to the WTF relationship we have with Microsoft.  &lt;strong&gt;If you want to know why EF sucks, just try using it.&lt;/strong&gt; Or check out &lt;a href=&#34;http://naspinski.net/post/Linq-to-SQL-vs-Linq-to-Entities--Revisited.aspx&#34;&gt;this good series of articles about l2s vs. l2e.&lt;/a&gt; I gave up on Entity Framework when I tried setting up an auto generated db value on an insert.  It&amp;#8217;s a pain in the ass and not nearly as simple as Linq To Sql&amp;#8217;s &amp;#8220;This is an auto generated property&amp;#8221; property.&lt;/p&gt;

&lt;p&gt;What are we left with?  Just the endless data access debate.  Which in a way is good for us because &lt;strong&gt;it keeps us from actually delivering something our users care about- like an app that saves data&lt;/strong&gt;.  (Um, why can&amp;#8217;t I change the customer address and overcharge them for adventure works gear at the same time??!?!?!)&lt;/p&gt;

&lt;p&gt;So where&amp;#8217;s the glimmer of hope?  Azure Table Storage- which might put us back in the Microsoft is cool again category.  Don&amp;#8217;t worry Microsoft- you only support sorting on only one field (&amp;#8220;Choose&amp;#8230; Choose Wisely&amp;#8221;), so I&amp;#8217;m sure you can still spark lots of criticism.  But please give large bonuses to the people behind ATS.  Why is it so great?  The same reason Linq to Sql is great- it just works. ATS with ADO.NET Data Services is about as easy as you can do persistence without actually having to do it. (Side note: I&amp;#8217;m ignoring object databases).  But you really have true POCO storage.  No messy attributes, no base classes, no schema generators, no xml configuration files, no .edmx files.  You have an object- it has public getters and setters- and you can save it.  That&amp;#8217;s all you have to do.  The catch?  A row key and a partition key (Sidenote: I&amp;#8217;m ignoring the whole deploy/run cloud part).  I could live without the partition key, but you&amp;#8217;re not really asking for much.  What else? It&amp;#8217;s built on Linq- and Linq, as we all know, is awesome (just like sql, but no strings!). (Sidenote: more criticism fuel: not all Linq operators are supported).&lt;/p&gt;

&lt;p&gt;What else?  ATS is also schema free, so no more entity/table mappings.  Just save an object.  In fact, save different objects in the same table.  Like parent/child classes.  Together.  Like an aggregate root.  Then search on any field (sorry, still no sorting!)  That&amp;#8217;s all you have to do.  And did I mention there&amp;#8217;s no schema migration?  Because there&amp;#8217;s no schema!  We&amp;#8217;re free! Freedom! Sure, it&amp;#8217;s &lt;a href=&#34;http://aws.amazon.com/simpledb/&#34;&gt;not the first schema free storage solution&lt;/a&gt;&amp;#8211; but it&amp;#8217;s the only one backed with ADO.NET Data Services.  And that&amp;#8217;s the special sauce which makes it yummy!&lt;/p&gt;

&lt;p&gt;Yes, ATS isn&amp;#8217;t perfect.  In fact it&amp;#8217;s far from perfect.  But it&amp;#8217;s a great step in the right direction.  NHibernate, Linq to Sql, EF, sprocs are all usable tools (sorry, didn&amp;#8217;t mean to include EF).  There are some great 3rd party toolkits too, but if I have to pay for data access I&amp;#8217;d like you to just do it or me.  There has been a great effort in the OS community (FluentNHibernate is a huge win) to make this stuff easier, but the bottom line is you&amp;#8217;re doing double programming: you got your db and your code, and all that stuff that sits between is just crap.  Even with FluentNHibernate you&amp;#8217;re just shifting your db programming to configuration.  At the end of the day doing data access well isn&amp;#8217;t even a win because the user still doesn&amp;#8217;t have a damn form!  ATS is different- it&amp;#8217;s just really slick.  It&amp;#8217;s scary slick.  Yes, it&amp;#8217;s not ready for prime time, but it&amp;#8217;s a huge step in the right direction.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Authlogic and OpenID on Rails</title>
          <link>http://blog.michaelhamrah.com/2009/05/authlogic-and-openid-on-rails/</link>
          <pubDate>Mon, 11 May 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/05/authlogic-and-openid-on-rails/</guid>
          <description>&lt;p&gt;So, I&amp;#8217;m working on a Rails App and I want to use OpenID (and only OpenID) for authentication.  I was going to use Restful_Authentication with the open_id_authentication extension, but then I saw Ryan Bates&amp;#8217; &lt;a href=&#34;http://www.railscasts.com&#34;&gt;Railscast&lt;/a&gt; on &lt;a href=&#34;http://github.com/binarylogic/authlogic/tree/master&#34;&gt;AuthLogic&lt;/a&gt;.  Authlogic has an &lt;a href=&#34;http://github.com/binarylogic/authlogic_openid/tree/master&#34;&gt;OpenID extention&lt;/a&gt; which looked perfect for my needs, and Authlogic seemed like a great gem for authentication.  My goal was simple: I wanted to support, and only support, authentication via OpenID.  None of this username/password/salt stuff.&lt;/p&gt;

&lt;p&gt;Now, I should warn you: I HAVE NO IDEA WHAT I AM DOING.  I&amp;#8217;m just getting started with Rails so I hit a few bumps getting this working.  I learned a lot along the way, so here&amp;#8217;s a quick rundown of my adventure with Authlogic and OpenID on Rails.&lt;/p&gt;

&lt;p&gt;First, getting started with sample code is always a good idea.  Authlogic has an &lt;a href=&#34;http://github.com/binarylogic/authlogic_example/tree/master&#34;&gt;example app&lt;/a&gt; on github where you can check out how to use the gem.  After pulling down the code, I was surprised I couldn&amp;#8217;t find anything relating to OpenID.  It turns out, the master repository doesn&amp;#8217;t have the OpenID example.  But there&amp;#8217;s a branch of the master example that does have the OpenID functionality.  Here&amp;#8217;s how to get it:&lt;/p&gt;

&lt;p&gt;First, clone the Authlogic example like so:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;git clone git://github.com/binarylogic/authlogic_example.git&lt;/pre&gt;

&lt;p&gt;Then, cd into the new Authlogic directory and get the OpenID fork like so:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;git checkout --track -b authlogic_with_openid origin/with-openid&lt;/pre&gt;

&lt;p&gt;Then, you can explore away.  My approach was simple: using the OpenID example as a guide, I&amp;#8217;d start a new app and add OpenID support from Authlogic.&lt;/p&gt;

&lt;p&gt;First, I need the required gems.  Authlogic and authlogic-oid are obvious, but I also need ruby-openid.  Authlogic-oid is built on the rails plugin open_id_authentication, which in turn is built on ruby-openid.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;sudo gem install ruby-openid
sudo gem install authlogic
sudo gem install authlogic-oid
&lt;/pre&gt;

&lt;p&gt;You can also set these in your environment.rb file, like so:&lt;/p&gt;

&lt;pre class=&#34;brush: ruby; title: ; notranslate&#34; title=&#34;&#34;&gt;config.gem &#34;authlogic&#34;
config.gem &#34;authlogic-oid&#34;, :lib =&amp;gt; &#34;authlogic_openid&#34;
config.gem &#34;ruby-openid&#34;, :lib =&amp;gt; &#34;openid&#34;
&lt;/pre&gt;

&lt;p&gt;Note: I got thwarted when I tried running the app using the config.gem approach.  I kept getting an &amp;#8220;Uninitialized constant Authlogic&amp;#8221; in User_sessions.rb when I ran the app.  It sucked!  It took me a while to figure this out, but it was a horrible beginner mistake.  I was doing rake gems:install instead of sudo rake gems:install.  So when I ran my app, the proper gems weren&amp;#8217;t in the right place.  Ouch!&lt;/p&gt;

&lt;p&gt;Next, it&amp;#8217;s time to install the open_id_authentication plugin which is required by Authlogic-oid.  It&amp;#8217;s not available as a gem.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;script/plugin install git://github.com/rails/open_id_authentication.git&lt;/pre&gt;

&lt;p&gt;The next step is to create the necessary migration scripts for the open_id_authentication plugin.&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;rake open_id_authentication:db:create&lt;/pre&gt;

&lt;p&gt;Another beginner mistake: I got a failure when I tried doing open_id_authentication:db:create before I installed the Authlogic gem.  Something about &amp;#8220;acts_as_authenticated&amp;#8221; wasn&amp;#8217;t there.  So the order of installation is important!&lt;/p&gt;

&lt;p&gt;Next, use the authlogic command to create the user_session model.  I&amp;#8217;m prettying much following instructions from the Authlogic github pages at this point:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;script/generate session user_session
script/generate model user
&lt;/pre&gt;

&lt;p&gt;Another beginner mistake: if you&amp;#8217;re making changes to your migration files without creating a new migration, make sure your schema is correct. I don&amp;#8217;t know the best way to do this except by using rake db:drop, rake db:create, and rake db:migrate.  This was due to an error I was seeing in my view: &amp;#8220;undefined method :openid_identifier&amp;#8221;.  I had a text_field in my form for the openid field, and I had the openid_identifier field in my model.  The problem? It wasn&amp;#8217;t in the database schema so Rails couldn&amp;#8217;t do its thing to make it a property of the user model and render the textfield correctly.&lt;/p&gt;

&lt;p&gt;From there, the views and controllers are pretty much the same as the example.  My user model is also pretty slim:&lt;/p&gt;

&lt;pre class=&#34;brush: ruby; title: ; notranslate&#34; title=&#34;&#34;&gt;class CreateUsers &amp;lt; ActiveRecord::Migration
def self.up
create_table :users do |t|
t.string :email, :null=&amp;gt;false
t.string :persistence_token, :null=&amp;gt; false
t.string :openid_identifier, :null=&amp;gt; false
t.datetime :last_request_at
t.timestamps
end
add_index :users, :openid_identifier
add_index :users, :persistence_token
end
def self.down
drop_table :users
end
end
&lt;/pre&gt;

&lt;p&gt;Now, when I put everything together to run the app, I got some weird failures.  When trying to login using my OpenID, I got a nasty error:&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;undefined local variable or method &#39;crypted_password_field&#39;&lt;/pre&gt;

&lt;p&gt;Not having a crypted_pasword field made sense seeing how I wasn&amp;#8217;t supporting login or password fields. But I wasn&amp;#8217;t expecting &lt;em&gt;not&lt;/em&gt; having it to be an issue.  In fact, from the docs the only required field on the user model is :persistance_token. So what&amp;#8217;s going on?&lt;/p&gt;

&lt;p&gt;Well, it turns out the Authlogic OpenID extension is designed to work with a login/password by default.  I had to dig through the stack trace and source code, but was able to figure out what&amp;#8217;s going on.  There&amp;#8217;s a method called &lt;span class=&#34;name&#34;&gt;attributes_to_save which is responsible for persisting form fields across the OpenID process.  By default, &lt;/span&gt;&lt;span class=&#34;name&#34;&gt;attributes_to_save includes password related information.  It treats the crypted password and password salt fields a little differently, which causes a problem when you don&amp;#8217;t have a :crypted_password attribute on your model.  The solution is simple: just override the method with one which doesn&amp;#8217;t include the password fields.  The user model will look like this:&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;name&#34;&gt;&lt;/p&gt;

&lt;pre class=&#34;brush: ruby; title: ; notranslate&#34; title=&#34;&#34;&gt;
class User &amp;lt; ActiveRecord::Base
acts_as_authentic do |c|
def attributes_to_save # :doc:
attrs_to_save = attributes.clone.delete_if do |k, v|
[ :persistence_token, :perishable_token, :single_access_token, :login_count,
:failed_login_count, :last_request_at, :current_login_at, :last_login_at, :current_login_ip, :last_login_ip, :created_at,
:updated_at, :lock_version].include?(k.to_sym)
end
end
end
end
&lt;/pre&gt;

&lt;p&gt;
  &lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;
  The second mistake was because I got ahead of myself.  In the example app there&amp;#8217;s code in the application.html.erb file rendering a login/register link or user info based on the current_user method in the application controller.  I was getting an error:
&lt;/p&gt;

&lt;pre class=&#34;brush: bash; title: ; notranslate&#34; title=&#34;&#34;&gt;unknown method &#39;logged_out?&#39;&lt;/pre&gt;

&lt;p&gt;
  &lt;span class=&#34;n&#34;&gt;which was occurring deep in the Authlogic codebase.  The problem was I didn&amp;#8217;t go a good job copying everything I needed from the example files!  The authlogic example project used the&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;span class=&#34;n&#34;&gt;logout_on_timeout true&lt;/p&gt; 
  

&lt;p&gt;&lt;p&gt;
    &lt;/span&gt;
  &lt;/p&gt;&lt;/p&gt;

&lt;p&gt;&lt;p&gt;
    &lt;span class=&#34;n&#34;&gt;filter on the UserSession method.  After digging through the documentation, this callback relies on the &lt;/span&gt;
  &lt;/p&gt;&lt;/p&gt;

&lt;p&gt;&lt;p&gt;
    &lt;span class=&#34;n&#34;&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;pre class=&amp;quot;brush: ruby; title: ; notranslate&amp;quot; title=&amp;quot;&amp;quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;t.datetime :last_request_at
&lt;/pre&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;p&amp;gt;
  &amp;lt;/span&amp;gt;
&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;
  &amp;lt;span class=&amp;quot;n&amp;quot;&amp;gt;field on the user model, which I didn&amp;amp;#8217;t have at the time.  And not having this field was throwing everything off.  (The best part was the documentation clearly states you need that attribute on the model for the callback to work correctly.&amp;lt;br /&amp;gt; &amp;lt;/span&amp;gt;
&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;
  &amp;lt;span class=&amp;quot;n&amp;quot;&amp;gt;Lesson learned: always know what you&amp;amp;#8217;re doing. (Also: don&amp;amp;#8217;t be afraid of source code).&amp;lt;br /&amp;gt; &amp;lt;/span&amp;gt;
&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;
  The whole process has really made me appreciate Authlogic.  It&amp;amp;#8217;s very extensible and extremely easy to customize.  If you know what you&amp;amp;#8217;re doing it&amp;amp;#8217;s pretty slick- the best way to figure it out is by reading the documentation and playing around with some code.
&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;
  Good luck!
&amp;lt;/p&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Scrum Tips: Managing Task Items</title>
          <link>http://blog.michaelhamrah.com/2009/03/scrum-tips-managing-task-items/</link>
          <pubDate>Wed, 11 Mar 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/03/scrum-tips-managing-task-items/</guid>
          <description>

&lt;p&gt;One of the biggest hurdles in transitioning to scrum is getting everyone- especially the dev team- on the same page.  Sprint planning and even scrum meetings can be painfully boring and seem useless.  There&amp;#8217;s a lack of understanding of what to do, communication is either too little or too much, and a few people bad mouth the process and continue their heads down development.  You&amp;#8217;re left wondering what&amp;#8217;s the point- thinking it&amp;#8217;s never going to work.  Well, have faith: proper task management is key in making scrum work for you and your team.  Task management is an aspect of scrum that&amp;#8217;s entirely contained within the dev group, which makes implementation a lot easier.&lt;/p&gt;

&lt;h4 id=&#34;have-a-purpose:8e5df32093653ec1649dd5400ba44174&#34;&gt;Have a Purpose&lt;/h4&gt;

&lt;p&gt;The key is you- the scrummaster- need to provide an environment (like a framework) for scrum.  If you think by simply sending some scrum info around, meeting every day, and having a backlog will magically make everything better you&amp;#8217;re wrong and you&amp;#8217;ll fail.  You&amp;#8217;ll also make everyone miserable.  The reason why &amp;#8220;It gets worse before it gets better&amp;#8221; is because it takes time to transition the team into the scrum process.  The key to success is facilitating that transition as quickly as possible, and the solution is having a purpose for everything you do.&lt;/p&gt;

&lt;p&gt;That purpose comes via two simple rules which will help you and your team transition to scrum faster:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;All developers should work on the same backlog item.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No task item should take more than four hours.&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Think of these rules as grease in the wheels of scrum.  It keeps the machine moving.&lt;/p&gt;

&lt;h4 id=&#34;a-quick-refresher:8e5df32093653ec1649dd5400ba44174&#34;&gt;A Quick Refresher&lt;/h4&gt;

&lt;p&gt;A backlog item and task item are two different things.  A backlog item is something that&amp;#8217;s business focused.  It&amp;#8217;s specifically created by the product owner for the development team to implement.  It&amp;#8217;s not, and shouldn&amp;#8217;t be, developer focused in any way.  A task item, on the other hand, is developer focused.  It&amp;#8217;s a list of things needed to be done to finish a backlog item.  Task items are created in either a sprint planning or a developer meeting at the start of a sprint.  They can also be refactored in the middle of the sprint.&lt;/p&gt;

&lt;p&gt;A crucial step in scrum is having the team gauge complexity of backlog items.  This requires developers to have an understanding of what needs to be done from a user perspective- which is not something developers are usually good at.  Without understanding what needs to be done, they don&amp;#8217;t know how to do it, and they can&amp;#8217;t tell you how long it will take.  Another popular excuse is &amp;#8220;I need to start coding to figure this out&amp;#8221;.&lt;/p&gt;

&lt;p&gt;The key is to make developers think and plan first so there are fewer variables mid sprint.  This is not mini-waterfall.  It&amp;#8217;s strategy.&lt;/p&gt;

&lt;h4 id=&#34;how-the-rules-help:8e5df32093653ec1649dd5400ba44174&#34;&gt;How the Rules Help&lt;/h4&gt;

&lt;p&gt;Having everyone work on the same item is essential for scrum.  Why? If people are working on different items there&amp;#8217;s little reason for them to communicate.  If Developer A is working on feature X, and Developer B is working on feature Y, there&amp;#8217;s no reason for A or B to care what the other is doing.  In fact, it&amp;#8217;s a waste of time.  However, if both A and B are working on feature X, there&amp;#8217;s every reason in the world for the two to talk.  Not only talk, but collaborate.  It&amp;#8217;s very XP without the side by side awkwardness.  The major benefit is the entire team is focused on the highest priority item- which means something will get completed in the sprint.  By working on single items together each item gets finished faster.  QA has more time to test- and they do it one at a time rather than dealing with multiple things coming together at the end of the sprint.&lt;/p&gt;

&lt;h4 id=&#34;break-things-down:8e5df32093653ec1649dd5400ba44174&#34;&gt;Break Things Down&lt;/h4&gt;

&lt;p&gt;The four hour rule is key to facilitate working together.  By breaking down every task into small pieces, more analysis is done on each backlog item.  The trick is each task is a single thing which can be done and checked in-  a class creation, a new method- all with unit tests.  The benefits materialize in different ways.  First, sprint planning becomes much more effective as communication occurs via the process of breaking down items.  Analysis is done and development ideas are thought out and debated amongst team members.  A roadmap is created outlining steps needed to complete each item- everyone is in the know and on the bus.&lt;/p&gt;

&lt;p&gt;Why four hours? Easy: in theory, a developer should be able to get two four-hour tasks done between scrums.  A developer should never be in a position to say, &amp;#8220;I&amp;#8217;m working on X, and I&amp;#8217;m still planning on working on X&amp;#8221;.   (Unless, of course, they fall behind- in which case, break down the task again).  The point is you want to focus on granularity.  If you have to do a controller, a task of &amp;#8220;Write a controller&amp;#8221; is not adequate.  What are you doing? What actions are needed? What dependencies are there? Are you using ModelBinders, or method parameters? Where&amp;#8217;s the error handling, if any?  The API and code structure evolves out of granular task items- and the whole team is part of the process, everyone on the same page as to what needs to get done. This level of discussion, collaboration, and task tracking are what you&amp;#8217;re looking for.&lt;/p&gt;

&lt;p&gt;Of course, it&amp;#8217;s hard to get the entire team working on backlog items when they&amp;#8217;re small.  It&amp;#8217;s really your judgment call- but in my experience, there are only a few tasks which are too small to be worked on by more than one person in less than four hours.  The end of completing a backlog item is the most difficult when it&amp;#8217;s a lot of little to dos.  But the things which take longer then they should are always the items worked on by one person- which usually aren&amp;#8217;t properly tasked out.  The bottom line is there&amp;#8217;s a constant cycle of breaking things down and redistribution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TDD&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A key factor to success is TDD.  TDD allows isolation of the moving parts.  In this way, a developer can work on a class, a method, or a service without having to worry about dependent objects.  The task description and sprint planning identify what the moving parts are, and how the moving parts fit together.  TDD is the environment to implement those moving parts, and make sure expectations are being met.  You&amp;#8217;ll usually end up rewriting task items mid sprint.  That&amp;#8217;s okay.  Sprint planing is really about establishing guidelines for development.  TDD is where the code and project materializes, and you need to change course when necessary as the project evolves.&lt;/p&gt;

&lt;p&gt;Unit Tests provide a great way to review code.  If you&amp;#8217;re working on a data layer for a business object, the business object developer can look at the data layer tests and get all the usage available.  Little time is spent &amp;#8220;figuring it out&amp;#8221;.  And if the business object developers mock objects don&amp;#8217;t match the concrete ones, somebody wasn&amp;#8217;t on the same page.  However, with small, iterative tasks taking a step back and refactoring is a non-issue.&lt;/p&gt;

&lt;h4 id=&#34;a-better-perspective-from-above:8e5df32093653ec1649dd5400ba44174&#34;&gt;A Better Perspective from Above&lt;/h4&gt;

&lt;p&gt;A great result of this process is the development lead or architect has great insight to how the application shapes up.  During sprint plannings, guidelines can be set on how to implement backlog items.  If something isn&amp;#8217;t tasked out correctly, point out the issue and discuss.  Dependencies can be identified and spun off into new classes.  When issues arise during scrums, refactoring can happen and new directions can be taken- all in the presence of senior devs, which minimize one offs or crazy spaghetti code.  BTW writing spaghetti code is nearly impossible- it&amp;#8217;s hard to make a mess in less than four hours when you&amp;#8217;re only doing one specific thing and everyone is watching.&lt;/p&gt;

&lt;h4 id=&#34;mentoring:8e5df32093653ec1649dd5400ba44174&#34;&gt;Mentoring&lt;/h4&gt;

&lt;p&gt;This process works great for junior developers- they can learn from the entire team.  Similar to an architect having insight to a system, a junior developer has better access to seasoned developers.  One on one time is reduced in favor sprint planning and scrums.  Their work can be evaluated when code comes together for a backlog item.  Any developer is capable of answering a question because everyone is working on the same thing.&lt;/p&gt;

&lt;h4 id=&#34;cleaner-code:8e5df32093653ec1649dd5400ba44174&#34;&gt;Cleaner Code&lt;/h4&gt;

&lt;p&gt;Code reviews happen naturally when code is shared for feature development.  Consider the usual application stack: UI, Controller, Application, Domain, Data Layer.   This stack is no longer in the hands of one person, who either blurs the line between layers or creates a million helper classes.  Code review happens when the pieces gets fused together.  If the puzzle pieces don&amp;#8217;t fit, refactor.  They usually will fit because everyone is clear on task items- because they were small, granular, and created together.&lt;/p&gt;

&lt;p&gt;No one person is left to the whim of their own devices- the expectation is clear and apparent to the team during scrums.  And the best developers have a better surface area for showing off, by being exposed to the entire application and not regulated to a single feature. You&amp;#8217;ll also get better exposure to cross functional teams, like UI.  When the Javascript developer and the backend developer are in the same room, creating the same roadmap, knowing what needs to be done, development is truly fluid.&lt;/p&gt;

&lt;h4 id=&#34;conclusion:8e5df32093653ec1649dd5400ba44174&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;Scrum, at its core, is really about structure.  Each part is an important gear on the clock which helps the hands move smoothly.  A key to getting the team working in that structure is setting up an environment that meets the .  Task management is key for managing developers and the sprint.  The four hour task breakdown and single item focus are two tools for managing complexity and fostering communication.  The end result are the following benefits:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Better gauge complexity of backlog items.&lt;/li&gt;
&lt;li&gt;Complete work in priority order.&lt;/li&gt;
&lt;li&gt;Fully meet Got Done criteria at end of sprint.&lt;/li&gt;
&lt;li&gt;Understand where things are with development mid-sprint.&lt;/li&gt;
&lt;li&gt;Understand why things are delayed with development mid-sprint.&lt;/li&gt;
&lt;li&gt;Fix cohesion with development.  Specifically, enforce development standards and architecture guidelines, write clean code, and minimize hacks and bugs.&lt;/li&gt;
&lt;li&gt;Top down visibility for architects, bottom up visibility for developers.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Remember, task management is only part of the entire scrum process.  In order for scrum to be effective, everything must fall into place.  But task management will have an immediate benefit for you and your development team.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.dotnetkicks.com/kick/?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2009%2f03%2fscrum-tips-managing-task-items%2f&#34;&gt;&lt;img src=&#34;http://www.dotnetkicks.com/Services/Images/KickItImageGenerator.ashx?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2009%2f03%2fscrum-tips-managing-task-items%2f&amp;bgcolor=000099&#34; border=&#34;0&#34; alt=&#34;kick it on DotNetKicks.com&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Prevent Js and Css Browser Caching Issues with ASP.NET</title>
          <link>http://blog.michaelhamrah.com/2009/03/prevent-js-and-css-browser-caching-issues-with-aspnet/</link>
          <pubDate>Mon, 02 Mar 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/03/prevent-js-and-css-browser-caching-issues-with-aspnet/</guid>
          <description>&lt;p&gt;You&amp;#8217;ve seen this problem before- you deploy a new version of your website but the style is off and you&amp;#8217;re getting weird javascript errors. You know the issue: Firefox or IE is caching and old version of the css/js file and it&amp;#8217;s screwing up the web app. The user needs to clear the cache so the latest version is pulled. The solution: versionstamp your include files!&lt;/p&gt;

&lt;p&gt;Take a lesson from Rails and create a helper which appends a stamp to your include files (and takes care of the other required markup). It&amp;#8217;s simple- embed the following code in your views:&lt;/p&gt;

&lt;pre class=&#34;brush: csharp; title: ; notranslate&#34; title=&#34;&#34;&gt;&amp;lt;%=SiteHelper.JsUrl(&#34;global.js&#34;) %&amp;gt;
&lt;/pre&gt;

&lt;p&gt;will render&lt;/p&gt;

&lt;pre class=&#34;brush: xml; title: ; notranslate&#34; title=&#34;&#34;&gt;&amp;lt;script type=&#34;text/javascript&#34; src=&#34;http://blog.michaelhamrah.com/content/js/global.js?33433651&#34;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/pre&gt;

&lt;p&gt;The browser will invalidate the cache because of the new query string and you&amp;#8217;ll be problem free. Version stamps are better than timestamps because the version will only change if you redeploy your site.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the code, which is based on the AppHelper for Rob Conery&amp;#8217;s Storefront MVC:&lt;/p&gt;

&lt;pre class=&#34;brush: csharp; title: ; notranslate&#34; title=&#34;&#34;&gt;using System.Reflection;
using System.Web.Mvc;

namespace ViewSample
{
public static class SiteHelper
{
private static readonly string _assemblyRevision = Assembly.GetExecutingAssembly().GetName().Version.Build.ToString() + Assembly.GetExecutingAssembly().GetName().Version.Revision.ToString();

/// &amp;lt;summary&amp;gt;
/// Returns an absolute reference to the Content directory
/// &amp;lt;/summary&amp;gt;
public static string ContentRoot
{
get
{
return &#34;/content&#34;;
}
}

/// &amp;lt;summary&amp;gt;
/// Builds a CSS URL with a versionstamp
/// &amp;lt;/summary&amp;gt;
/// &amp;lt;param name=&#34;cssFile&#34;&amp;gt;The name of the CSS file&amp;lt;/param&amp;gt;
public static string CssUrl(string cssFile)
{

string result = string.Format(&#34;&amp;lt;link rel=&#39;Stylesheet&#39; type=&#39;text/css&#39; href=&#39;{0}/css/{1}?{2}&#39; /&amp;gt;&#34;, ContentRoot, cssFile, _assemblyRevision);
return result;
}
/// &amp;lt;summary&amp;gt;
/// Builds a js URL with a versionstamp
/// &amp;lt;/summary&amp;gt;
/// &amp;lt;param name=&#34;cssFile&#34;&amp;gt;The name of the CSS file&amp;lt;/param&amp;gt;
public static string JsUrl(string jsPath)
{
return string.Format(&#34;&amp;lt;script type=&#39;text/javascript&#39; src=&#39;{0}/js/{1}?{2}&#39;&amp;gt;&amp;lt;/script&amp;gt;&#34;, ContentRoot, jsPath, _assemblyRevision);
}

}
}

&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Wiping Out Inherited CSS Styles</title>
          <link>http://blog.michaelhamrah.com/2009/02/wiping-out-inherited-css-styles/</link>
          <pubDate>Fri, 27 Feb 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/02/wiping-out-inherited-css-styles/</guid>
          <description>&lt;p&gt;Sure, you can use the !important css keyword in your stylesheets to override inherited styles, but what about when you want to simply wipe out or remove a style defined in a parent? Use the auto keyword.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s say you have a style defined in a global.css sheet defining a position for a class called menu:&lt;/p&gt;

&lt;pre class=&#34;brush: css; title: ; notranslate&#34; title=&#34;&#34;&gt;ul.menu {
position:absolute;
left: 10px;
top: 10px;
}
&lt;/pre&gt;

&lt;p&gt;This will position a &lt;ul class=&amp;#8221;menu&amp;#8221;&gt; element&lt;/ul&gt; 10 pixels away from the top left corner of the parent element.  But what if you want to move that menu to the right side of the parent element, but you can&amp;#8217;t change the defined style in global.css?&lt;/p&gt;

&lt;p&gt;You create a new stylesheet or define an inline style, and override the ul.menu class by specifying&lt;/p&gt;

&lt;pre class=&#34;brush: css; title: ; notranslate&#34; title=&#34;&#34;&gt;ul.menu {
right:10px;
}
&lt;/pre&gt;

&lt;p&gt;But this simply stretches the menu to the other corner.   We need to override the left:10px; style with the default style (as if we never specified the left:10px; to begin with).  left:none; won&amp;#8217;t work and left:0; won&amp;#8217;t either.  The solution is the auto keyword:&lt;/p&gt;

&lt;pre class=&#34;brush: css; title: ; notranslate&#34; title=&#34;&#34;&gt;ul.menu {
right:10px;
left:auto; /* removes left:10px from parent */
}
&lt;/pre&gt;

&lt;p&gt;Voila, you&amp;#8217;re done!  It&amp;#8217;s as if the left:10px was never defined!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Tips for Managing ASP.NET MVC Views</title>
          <link>http://blog.michaelhamrah.com/2009/02/tips-for-managin-aspnet-mvc-views/</link>
          <pubDate>Wed, 25 Feb 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/02/tips-for-managin-aspnet-mvc-views/</guid>
          <description>&lt;p&gt;After working on an ongoing ASP.NET MVC project for a couple of months I&amp;#8217;ve learned a couple of lessons when it comes to dealing with Views.  Keep as much logic out of the views as possible! This can be tricky because it&amp;#8217;s so easy to let code sneak into your views. But by following the tips below you&amp;#8217;ll be able to keep your logic organized and views clean.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1) Follow the Rails pattern of having a Helper class for each controller.  The helper class deals with html snippets or formatting functions on a per controller level.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2009/02/helpersfolder.png&#34;&gt;&lt;img class=&#34;alignnone size-medium wp-image-174&#34; title=&#34;ASP.NET MVC Helpers&#34; src=&#34;http://i1.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2009/02/helpersfolder.png?resize=211%2C159&#34; alt=&#34;ASP.NET MVC Helpers&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Pushing out bits of code like date formatting into helper classes not only cleans up the views, but aids in testability.  Logic is consolidated in one place simplifying maintenance.  Creating Helpers on a per controller level also creates a namespace where functionality can be found intuitively.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2) Don&amp;#8217;t overload the Html helper with one-off functions. Organize functions into their respective helper classes.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s so easy to want to add extension methods to the HtmlHelper class.  It&amp;#8217;s like a siren crying out &amp;#8220;do it, do it&amp;#8221;.  This is bad!  Extension methods are good for some things, but can easily garble an API.  The HtmlHelper class should be reserved for rendering core html elements.  As an alternative, leverage helper classes you create on a per-controller basis.&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;//These shouldn&#39;t belong to HtmlHelper!
public static string ShowSomethingOnlyForHomeController((this HtmlHelper helper) ...
public static string RenderDateTimeNowInSpan(this HtmlHelper helper) ...&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3) Leverage partial views for iterative content, even if it&amp;#8217;s not reused elsewhere (hint: it probably will be eventually).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So instead of:&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;&amp;lt;%= foreach (var photo in photos)
{ %&gt;


&lt;div class=&#34;photo&#34;&gt;
  &lt;span&gt;&amp;lt;%= photo.Name %&gt;&lt;/span&gt;
  
  
  &lt;p&gt;
    &amp;lt;%= photo.Description %&gt;
  &lt;/p&gt;
  
  
  &lt;p&gt;
    &amp;lt;%= photo.Date %&gt;
  &lt;/p&gt;
  
&lt;/div&gt;
&amp;lt;%= } %&gt;
&lt;/pre&gt;

&lt;p&gt;use:&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;&amp;lt;% foreach (var photo in photos)
{ %&gt;
&amp;lt;% Html.RenderPartial(&#34;~/Views/Photos/PhotoTemplate&#34;, photo); %&gt;
&amp;lt;% } %&gt;
&lt;/pre&gt;

&lt;p&gt;This will greatly minimize the amount of markup scattered throughout a view, keeping view files focused on a specific task (just like good class design).  It&amp;#8217;ll also make version control easier because changes will be isolated to their respective file, allowing someone to better see what changes happened where.  It also eliminates excessive merging.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4) Organize partial views within their respective controller, not a shared folder- even if it&amp;#8217;s a global skin.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Dumping partials within a shared folder can cause overcrowding and jumbling in the long term.  Prefer organization and grouping of related content (again, just like good class design).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5) Prefer a strongly typed view and leverage specialized ViewData types for aggregating random models under one root.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It can be easy to dump stuff into the ViewData hash.  However, prefer using a custom ViewData class instead.  It&amp;#8217;s easier to know what data is available for the view.  This is a lot more intuitive than rummaging through a controller, which happens when teams share code.  Also leverage the &lt;a href=&#34;http://en.wikipedia.org/wiki/Null_Object_pattern&#34;&gt;null object pattern&lt;/a&gt; for properties to avoid having to do null reference checks in the views.&lt;/p&gt;

&lt;p&gt;Instead of:&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;ViewData[&#34;Title&#34;] = &#34;My Photos&#34;;
ViewData[&#34;Photos&#34;] = myPhotos;
ViewData[&#34;User&#34;] = currentUser;

return View();
&lt;/pre&gt;

&lt;p&gt;use:&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;//Title and User can be properties of a base view data class.
var vd = new PhotoListViewData() 
     { Photos = myPhotos, Title = &#34;My Photos&#34;, User = currentUser };
return  View(vd);

//Sample null object pattern (always returns a valid object, so no if null or Count == 0):
private List&amp;lt;Photo&gt; _photos = new List&amp;lt;Photo&gt;();
public List&amp;lt;Photo&gt; Photos 
{ 
get 
     { if (_photos == null) _photos = new List&amp;lt;Photo&gt;(); 
            return _photos; 
     } 
set { 
        _photos = value; } 
}
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;6) Minimize code snippets in views.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Code snippets occur in many ways.  They can be as simple as formatting a date, to string concatenation, to even doing a grouping/projection to get data correct.  Doing this in a view easily leads to bugs and isn&amp;#8217;t testable.  Any data processing specifically for views should occur in the controller action, custom ViewData object itself, or in a helper class.  Code in views should be simplified to looping, calling a property, or calling a single method.  Anything more than that will get you into trouble!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Leverage RenderAction or MvcContrib&amp;#8217;s SubController for dealing with shared isolated functionality not relevant to a view.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, as of RC1 there still isn&amp;#8217;t a great way to deal with aggregating disparate content in an action.  You&amp;#8217;ll need to resort to the RenderAction in the Future&amp;#8217;s dll or use MvcContrib&amp;#8217;s SubController.  The point is the same- keep actions specific to what you&amp;#8217;re doing.  If you need to aggregate disparate content in a view (like a menu in the header, or a shopping cart) offload functionality into an action and call RenderAction.  Having actions do multiple, random things leads to messy code.  Prefer a single point of entry into supporting view content.&lt;/p&gt;

&lt;p&gt;Good luck and share your tips!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.dotnetkicks.com/kick/?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2009%2f02%2ftips-for-managin-aspnet-mvc-views%2f&#34;&gt;&lt;img src=&#34;http://www.dotnetkicks.com/Services/Images/KickItImageGenerator.ashx?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2009%2f02%2ftips-for-managin-aspnet-mvc-views%2f&amp;bgcolor=0000FF&#34; border=&#34;0&#34; alt=&#34;kick it on DotNetKicks.com&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Using Moq to Implement Tests (and Avoid Stubs)</title>
          <link>http://blog.michaelhamrah.com/2009/02/using-moq-3-to-implement-tests-and-avoid-stubs/</link>
          <pubDate>Wed, 25 Feb 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/02/using-moq-3-to-implement-tests-and-avoid-stubs/</guid>
          <description>

&lt;p&gt;In &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2008/12/understand-unit-testing-and-td-getting-better-code-coverage/&#34;&gt;my previous post on understanding TDD&lt;/a&gt; I discussed how to analyze existing code for creating unit tests.  This was somewhat of &amp;#8220;reverse TDD&amp;#8221;, the idea being to look for what to needs to be tested- the relationship between what one class expects and what another class actually does.  Unfortunately I stopped short of actually implementing those tests- which is the subject of this post.&lt;/p&gt;

&lt;p&gt;As a quick refresher, our demo app has some simple functionality which involves getting an order from a data tier then uses a shipping service to ship the order.  You can &lt;a href=&#34;http://www.michaelhamrah.com/blog/wp-content/uploads/2009/02/unittestexamplewithmoq.zip&#34;&gt;download the sample app here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;prefer-mocking-over-stubs:dc6d1fe85225b3a1b9bf1fc833403f60&#34;&gt;Prefer Mocking Over Stubs&lt;/h3&gt;

&lt;p&gt;Had I written this post about two months ago, I would have favored creating a suite of stub classes to mimic the expected behavior of our interfaces.  The approach would involve creating various stub classes for our dependent interfaces- the IShippingService and IOrderRepository- and plugging those stubs into the various unit tests.  In the simplest sense these would be hard coded classes to do what I wanted- from throwing exceptions to hard coding return values.  Probably even use some fancy random number generation for ids. However, I&amp;#8217;m learning from a current project that the stub approach isn&amp;#8217;t the best route to take.&lt;/p&gt;

&lt;h4 id=&#34;the-problem-distilled:dc6d1fe85225b3a1b9bf1fc833403f60&#34;&gt;The Problem, Distilled&lt;/h4&gt;

&lt;p&gt;Stubbing is definitely easy, but code quickly spirals out of control.  With stub classes, especially those backed by interfaces, more classes are created which then need to be managed and supported.  In an evolving software project this management creates added infrastructure which gets in the way when making changes to the original functionality.  An interface change, for example (either adding a new method or refactoring parameters) causes numerous changes to be made in stubbed out classes.  What&amp;#8217;s worse, there can easily be a divergence in the behavior of the stubbed out class with the behavior it&amp;#8217;s supposed to represent.  True, this can happen with mocking, but as we&amp;#8217;ll see mocking is much more granular and can be tailored to specific circumstances with minimal code duplication.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s simply no need to create extensive, or multiple, stub objects with all those great mocking frameworks around.  Most mocking frameworks offer a ton of features creating granular control of expectations, return values, parameter verification, and method invocation checking.  I strongly recommend reading Martin Fowler&amp;#8217;s &lt;a href=&#34;http://martinfowler.com/articles/mocksArentStubs.html&#34;&gt;Mocks Aren&amp;#8217;t Stubs&lt;/a&gt; article for more information on the difference between mocks and stubs.&lt;/p&gt;

&lt;h3 id=&#34;using-moq-to-mimic-behavior:dc6d1fe85225b3a1b9bf1fc833403f60&#34;&gt;Using Moq to Mimic Behavior&lt;/h3&gt;

&lt;p&gt;We&amp;#8217;ll use &lt;a href=&#34;http://code.google.com/p/moq/&#34;&gt;Moq&lt;/a&gt; 3 to demo what you can do with mocking.  Let&amp;#8217;s start off with our first unit test, ShipOrder_Returns_New_Shipment_With_Order_Items.  We simply want to call our ShipOrder function and ensure we get a shipment back with shipment items.  To begin with, we&amp;#8217;ll use &lt;a href=&#34;http://code.google.com/p/moq/&#34;&gt;Moq&lt;/a&gt; to mock our IOrderStorage and IShipmentService so we can construct our OrderShipmentManager.&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;[TestMethod()]
public void OrderShipmentManager_ShipOrder_Returns_New_Shipment_With_Shipment_Items()
{
var mockOrderStorage = new Mock&amp;lt;IOrderStorage&gt;();
var mockShipmentService = new Mock&amp;lt;IShipmentService&gt;();

var osm = new OrderShipmentManager(mockOrderStorage.Object, mockShipmentService.Object);

var shipment = osm.ShipOrder(5);

Assert.IsNotNull(shipment, &#34;Returned Shipment was not null&#34;);
Assert.IsInstanceOfType(shipment, typeof(Shipment), &#34;Returned object was not of type shipment.&#34;);
Assert.IsNotNull(shipment.ShipmentProducts, &#34;ShipmentProducts were null&#34;);
Assert.IsTrue(shipment.ShipmentProducts.Count &gt; 0, &#34;ShipmentProducts were empty&#34;);
}
&lt;/pre&gt;

&lt;p&gt;This test will fail because we haven&amp;#8217;t specified any functionality for our interfaces!  But, we didn&amp;#8217;t get a failing test with a null reference exception when calling GetOrder()- we got a failing test because our returned shipment object from GetOrder was null.  Stepping through code you&amp;#8217;ll see that Moq returned void when calling the IOrderStorage.GetOrder method.  Moq creates a simple &amp;#8220;dumb&amp;#8221; proxy class for the interface automatically.  You can specify a strict behavior using MockBehavior.Strict in the constructor to throw an exception for anything that isn&amp;#8217;t explicitly set up.  This can be helpful for ensuring control flow.&lt;/p&gt;

&lt;p&gt;The solution to pass our test is to explicitly tell Moq what to do when this method is called, like so:&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;mockOrderStorage.Setup(os =&gt; os.GetOrder(It.IsAny&amp;lt;int&gt;())).Returns((int id) =&gt; {
var order = new Order() { OrderId = id };
return order;
});
&lt;/pre&gt;

&lt;p&gt;When working with Moq you may struggle with the extensive use of Lambda expressions the framework requires.  But after a couple of tests you&amp;#8217;ll become a lambda ninja.  Moq relies on a pair of Setup/Return calls to dictate behavior.  Also notice how we didn&amp;#8217;t have to implement every method in our interfaces- we only had to implement the ones we needed to make the test pass.  That saves a lot of code from being written!&lt;/p&gt;

&lt;p&gt;The previous expression states &amp;#8220;when you call GetOrder, with any int value, return a new order with the input id&amp;#8221;.  Moq gives a lot of power in dictating the behavior of the return call based on the expected function.  By specifing (int id) in our return Lambda we can get a reference to our input parameter, which helps us construct a valid Order object based on any input. This is helpful when setting up exceptions.  By using a predicate expression with It.Is instead of It.IsAny, we can have our call throw an exception for an invalid input:&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;mockShipmentService.Setup(ss =&gt; ss.CreateShipment(It.Is&amp;lt;int&gt;(id =&gt; id &amp;lt; 0))).Throws&amp;lt;ArgumentException&gt;();
&lt;/pre&gt;

&lt;p&gt;Setting up exceptions using Moq is instrumental in properly dealing with exception handling in your code.  Also, using It.Is to tailor return methods can help out when dealing with validation or verifying state.  It&amp;#8217;s much faster and easier to use Moq to create behavior than hard coding stub classes.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s one final advantage to using Moq which you can&amp;#8217;t easily do with stubs.  It&amp;#8217;s verifying method invocation.  In our sample project, we have a call to _shipmentService.Ship(shipment).  What does Ship() do? It doesn&amp;#8217;t return anything so checking a return parameter is difficult.  It doesn&amp;#8217;t manipulate the shipment (or, let&amp;#8217;s pretend it doesn&amp;#8217;t).  We want to ensure this function is called.  The solution? Use Moq&amp;#8217;s Verify() method, like so:&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;mockShipmentService.Verify(svc =&gt; svc.Ship(shipment));
&lt;/pre&gt;

&lt;p&gt;This code will throw an exception if Ship wasn&amp;#8217;t called with the shipment variable, ensuring that Ship was actually called if it should.  Using Verify() is helpful when testing out methods returning void- like sending an e-mail, doing a file operation, or ftp.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:dc6d1fe85225b3a1b9bf1fc833403f60&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Spending some time getting up to speed with a mocking framework will be instramental in learning TDD.  It&amp;#8217;ll also come in handy with just writing unit tests.  Mocking provides a simple, slimmed down solution that&amp;#8217;s much easier to manage than stub classes.&lt;/p&gt;

&lt;p&gt;I like Moq because it&amp;#8217;s quick to express what you want done- but the heavy use of lambdas can be a struggle with getting started.  The &lt;a href=&#34;http://code.google.com/p/moq/wiki/QuickStart&#34;&gt;moq quickstart&lt;/a&gt; is a great guide to the framework and covers all the bases.  The bottom line is don&amp;#8217;t worry about which mocking framework you choose.  They all have their respective advantages.  Pick one, learn it, use it.  If you don&amp;#8217;t like how it does something, switch to something else down the line which does what you want.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.dotnetkicks.com/kick/?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2009%2f02%2fusing-moq-3-to-implement-tests-and-avoid-stubs%2f&#34;&gt;&lt;img src=&#34;http://www.dotnetkicks.com/Services/Images/KickItImageGenerator.ashx?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2009%2f02%2fusing-moq-3-to-implement-tests-and-avoid-stubs%2f&amp;#038;bgcolor=0000FF&#34; border=&#34;0&#34; alt=&#34;kick it on DotNetKicks.com&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to Hire a Software Developer: The In House Interview (Part Two)</title>
          <link>http://blog.michaelhamrah.com/2009/02/how-to-hire-a-software-developer-the-in-house-interview-part-two/</link>
          <pubDate>Mon, 23 Feb 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/02/how-to-hire-a-software-developer-the-in-house-interview-part-two/</guid>
          <description>

&lt;p&gt;_Note: This is part of a series on hiring software developers.  See &lt;a title=&#34;Interview&#34; href=&#34;http://www.michaelhamrah.com/blog/index.php/tag/interview/&#34; target=&#34;_self&#34;&gt;articles tagged with interview for the series&lt;/a&gt;.  The in house interview post has two parts.  This part focuses on non-development questions._&lt;/p&gt;

&lt;h3 id=&#34;key-concepts:3dcafca7ca783ca88343fc510ece7ff5&#34;&gt;&lt;strong&gt;Key Concepts&lt;/strong&gt;&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Present yourself well- the candidate is checking you out too.&lt;/li&gt;
&lt;li&gt;Prefer a group style interview approach over one-on-ones.&lt;/li&gt;
&lt;li&gt;Ask specific non-development questions and evlove the follow ups.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;they-8217-re-interviewing-you-too:3dcafca7ca783ca88343fc510ece7ff5&#34;&gt;They&amp;#8217;re Interviewing You Too&lt;/h3&gt;

&lt;p&gt;It&amp;#8217;s important to note that you- both as a company, a potential co-worker, or a future boss- are being interviewed too.  A smart candidate (read: one you want to hire) will be sizing up the situation and the interview process themselves.  What&amp;#8217;s the office like?  Do people know what&amp;#8217;s going on?  Where is the interview going to happen? Am I, the candidate, waiting around a lot for something to happen, or someone to do something? How fluid is the entire process?  Are people droning on at their desks or do they look happy?&lt;/p&gt;

&lt;p&gt;What goes on during the interview will give the developer hints as to what to expect during the day to day of the job. If the office is awesome, the people are cool, and things are moving smoothly, a much better impression will be made on the candidate than a sloppy process where people are totally frazzled, have no interest in what&amp;#8217;s going on, or are treated/act like sheep.&lt;/p&gt;

&lt;p&gt;Bottom line: be respectful.  Present yourself- and your company- as the epitome of who you&amp;#8217;re expecting to hire.  Joel Spolsky, of &lt;a href=&#34;http://www.joelonsoftware.com&#34;&gt;Joel on Software&lt;/a&gt;, puts great effort into making his office and company a kick-ass place.  Even his blog makes you sit at your desk wishing you worked at Fog Creek.  Hell, the New York Times &lt;a href=&#34;http://www.nytimes.com/2009/02/08/realestate/commercial/08sqft.html?_r=1&amp;amp;partner=permalink&amp;amp;exprod=permalink&#34;&gt;even wrote an article&lt;/a&gt; on the office architecture!  Don&amp;#8217;t be afraid to sell yourself.&lt;/p&gt;

&lt;h3 id=&#34;the-interviewers:3dcafca7ca783ca88343fc510ece7ff5&#34;&gt;The Interviewers&lt;/h3&gt;

&lt;p&gt;It&amp;#8217;s important to have the candidate meet with a variety of team members.  This shouldn&amp;#8217;t be considered a chore for people, but an honor.  It&amp;#8217;s important when building a cohesive team to give current members a voice as to who&amp;#8217;s joining the team.  Most people have no idea how to interview- so make sure to give current members specific questions, role playing scenarios or guidelines on how to conduct the interview.  Make sure meetings are timed box- to between 5 to 20 minutes- so the interviewer knows the correct pace.  There&amp;#8217;s nothing worse than awkward silence.&lt;/p&gt;

&lt;h3 id=&#34;the-interview-structure:3dcafca7ca783ca88343fc510ece7ff5&#34;&gt;The Interview Structure&lt;/h3&gt;

&lt;p&gt;There are essentially two ways to structure the interview: a series of one on one interviews where different developers ask a distinct set of questions, or a group style interview where a group of developers have a round table discussion with a candidate.  The way you decide to structure the interview says a lot about your company and how you conduct your development (that whole thing about presenting yourself).&lt;/p&gt;

&lt;p&gt;Personally, I don&amp;#8217;t like the one-on-one style.  It&amp;#8217;s pretty exhaustive, both for you and the candidate.  The candidate is constantly bombarded with questions in an interrogation-like manner.  If the questions aren&amp;#8217;t planned ahead of time people will also ask the same set of questions (Um,  so, why are you leaving your job? Why do you want to work here?).  More importantly, it&amp;#8217;s hard for the candidate to get into a rhythm- the mandatory warm up time, body, and wrap up is just too cramped.  Finally, the one on one approach offers poor breadth of a candidate.  If each person is only focusing on a specific set of questions, no one person can see the big picture.&lt;/p&gt;

&lt;p&gt;A roundtable discussion is a much better approach, even if there are only two interviewers.  This is much more representative of an agile development team- you want to see how the group dynamic plays with a new person.  It also gives every person an opportunity to determine breadth- you see the whole package, and not just individual parts.  The conversation has a much better chance to evolve in a solid discussion- the positive attributes of each interviewer can shine while the negative attributes are mitigated by the group.&lt;/p&gt;

&lt;p&gt;The one advantage to the one-on-one style is you can interview more people easily.  It&amp;#8217;s just math- let&amp;#8217;s say you&amp;#8217;re interviewing four candidates with four developers.  An interview should last about two hours.  That&amp;#8217;s eight hours using all four developers!  On the other hand, if each developer spent 30 minutes with each candidate concurrently, that&amp;#8217;s only four hours total- half the time!&lt;/p&gt;

&lt;h3 id=&#34;personal-non-technical-questions:3dcafca7ca783ca88343fc510ece7ff5&#34;&gt;Personal/Non-Technical Questions&lt;/h3&gt;

&lt;p&gt;It&amp;#8217;s important to ask non-development questions.  You want to get a sense of how well rounded a candidate is and what their personality is like.  Here are some example questions:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do you stay on top of technology?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m looking to know what their go-to resources are for learning and keeping up with stuff.  Google is not a good answer.  Do they read blogs? What are their favorite books? Have they been to any conferences?  Subscribe to any magazines?  What can they bring to the table?&lt;/p&gt;

&lt;p style=&#34;margin-left: 1.6pt;&#34;&gt;
  &lt;strong&gt;What personal program projects have you worked on, if any?&lt;/strong&gt;
&lt;/p&gt;

&lt;p style=&#34;margin-left: 1.6pt;&#34;&gt;
  I&amp;#8217;m interested to know if they have any programming experience outside of work.  Not too bad if they don&amp;#8217;t, but it shows how into programming they are.
&lt;/p&gt;

&lt;p style=&#34;margin-left: 1.6pt;&#34;&gt;
  &lt;strong&gt;What is the coolest thing you&amp;#8217;ve done with technology? What are you proud of?&lt;/strong&gt;
&lt;/p&gt;

&lt;p style=&#34;margin-left: 1.6pt;&#34;&gt;
  One of my favorites.  It&amp;#8217;s important to hire people who are proud of what they do.  You can&amp;#8217;t beat someone who&amp;#8217;s about personal responsibility.
&lt;/p&gt;

&lt;p style=&#34;margin-left: 1.6pt;&#34;&gt;
  &lt;strong&gt;What are some websites you&amp;#8217;ve used where you appreciate the design of the site? Why?&lt;/strong&gt;
&lt;/p&gt;

&lt;p style=&#34;margin-left: 1.6pt;&#34;&gt;
  An analytical question.  Developers with design insight are rare.  A developer who shows appreciation for good design, or can &amp;#8220;talk&amp;#8221; design, is invaluable.
&lt;/p&gt;

&lt;p style=&#34;margin-left: 1.6pt;&#34;&gt;
  &lt;strong&gt;What do you want to be doing in a couple of years? What is one area where you want to be developing your skill set?&lt;/strong&gt;
&lt;/p&gt;

&lt;p style=&#34;margin-left: 1.6pt;&#34;&gt;
  I used to hate asking this question because its so cookie cutter.  But it&amp;#8217;s important to ask and to have the candidate elaborate on.  It does two things- let you know how driven the candidate is, and what they&amp;#8217;re interested in doing and evolving.  Both important things to know.  A good follow up is &amp;#8220;How are you going to do that?&amp;#8221;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&#34;color: black;&#34;&gt;Describe how you would handle a situation if you were required to finish multiple tasks by the end of the day, and there was no conceivable way that you could finish them. &lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Answer: Prioritize.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&#34;color: black;&#34;&gt;If you could get rid of any one of the US states, which one would you get rid of, and why? &lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;color: black;&#34;&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Answer: Canada.  Just kidding.  This one&amp;#8217;s good to change pace of the conversation if needed.  Ask if the candidate completely failed another question or is feeling insecure.  It can help them get on track.  It&amp;#8217;s one of those silly filler questions that&amp;#8217;s good at wasting time if you need too or changing the mood.&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;color: black;&#34;&gt;&lt;strong&gt;With your eyes closed, tell me step-by-step how to tie my shoes.&lt;/strong&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&#34;color: black;&#34;&gt;If selected for this position, can you describe your strategy for the first 90 days?&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You may not need this if you&amp;#8217;re doing role-playing development questions, but it&amp;#8217;s a good question to gauge how well a candidate can explain something.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span style=&#34;color: black;&#34;&gt;Describe a criticism you were given at a job and how you worked to improve it.&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;color: black;&#34;&gt;This is another good question to ask.  Most people don&amp;#8217;t respond well to criticism, or aren&amp;#8217;t that self critical.  You don&amp;#8217;t want someone to be cocky nor beat themselves up.  But there are times when people do things they shouldn&amp;#8217;t or simply make mistakes.  Being self-aware and improving on weaknesses is one of the best qualities a person can have.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;color: black;&#34;&gt;&lt;br /&gt; &lt;/span&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How I Got Started Learning Ruby on Rails</title>
          <link>http://blog.michaelhamrah.com/2009/02/how-i-got-started-learning-ruby-on-rails/</link>
          <pubDate>Fri, 20 Feb 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/02/how-i-got-started-learning-ruby-on-rails/</guid>
          <description>

&lt;p&gt;As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-1:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-1:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-1:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-1:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The]&lt;a href=&#34;https://peepcode.com/products/rails-2-pdf&#34;&gt;7&lt;/a&gt; e-book which is worth the $9, and the [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-2:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-2:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-2:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-2:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-3:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-3:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-3:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-3:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The]&lt;a href=&#34;https://peepcode.com/products/rails-2-pdf&#34;&gt;7&lt;/a&gt; e-book which is worth the $9, and the]&lt;a href=&#34;https://peepcode.com/products/textmate-for-rails-2&#34;&gt;8&lt;/a&gt; video (I use a Mac).  I&amp;#8217;m very happy with them.  I&amp;#8217;ve also heard great things about the &lt;a href=&#34;https://peepcode.com/products/rest-for-rails-2&#34;&gt;Rest&lt;/a&gt; screencast (now in version 2, I&amp;#8217;ve seen version 1 and liked it).  I plan on getting the Git and RSpec videos as well.&lt;/p&gt;

&lt;p&gt;If you don&amp;#8217;t want to spend money, check out &lt;a href=&#34;http://railscasts.com/&#34;&gt;Railscasts&lt;/a&gt;.  These are done by the same people from peepcode and offer shorter videos on specific topics.  Well worth it.  (For those .NET people, think of &lt;a href=&#34;http://dimecasts.net/&#34;&gt;Dimecasts&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;the-hardware-software-setup:07a84db2bfd60e17f47a574617953835&#34;&gt;The Hardware/Software Setup&lt;/h3&gt;

&lt;p&gt;I use a MacBook Pro.  It&amp;#8217;s pretty easy to get setup with Rails on a Mac.  Google for the how-to.  For an editor, I use &lt;a href=&#34;http://www.macromates.com&#34;&gt;Textmate&lt;/a&gt; and love it. (Especially after watching the Peepcode screencast).  I played around with [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-4:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-4:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-4:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-4:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-5:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-5:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-5:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-5:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The]&lt;a href=&#34;https://peepcode.com/products/rails-2-pdf&#34;&gt;7&lt;/a&gt; e-book which is worth the $9, and the [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-6:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-6:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-6:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-6:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-7:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-7:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-7:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-7:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The]&lt;a href=&#34;https://peepcode.com/products/rails-2-pdf&#34;&gt;7&lt;/a&gt; e-book which is worth the $9, and the]&lt;a href=&#34;https://peepcode.com/products/textmate-for-rails-2&#34;&gt;8&lt;/a&gt; video (I use a Mac).  I&amp;#8217;m very happy with them.  I&amp;#8217;ve also heard great things about the &lt;a href=&#34;https://peepcode.com/products/rest-for-rails-2&#34;&gt;Rest&lt;/a&gt; screencast (now in version 2, I&amp;#8217;ve seen version 1 and liked it).  I plan on getting the Git and RSpec videos as well.&lt;/p&gt;

&lt;p&gt;If you don&amp;#8217;t want to spend money, check out &lt;a href=&#34;http://railscasts.com/&#34;&gt;Railscasts&lt;/a&gt;.  These are done by the same people from peepcode and offer shorter videos on specific topics.  Well worth it.  (For those .NET people, think of &lt;a href=&#34;http://dimecasts.net/&#34;&gt;Dimecasts&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;the-hardware-software-setup-1:07a84db2bfd60e17f47a574617953835&#34;&gt;The Hardware/Software Setup&lt;/h3&gt;

&lt;p&gt;I use a MacBook Pro.  It&amp;#8217;s pretty easy to get setup with Rails on a Mac.  Google for the how-to.  For an editor, I use &lt;a href=&#34;http://www.macromates.com&#34;&gt;Textmate&lt;/a&gt; and love it. (Especially after watching the Peepcode screencast).  I played around with]&lt;a href=&#34;http://www.aptana.com&#34;&gt;13&lt;/a&gt; and &lt;a href=&#34;http://www.netbeans.com&#34;&gt;NetBeans&lt;/a&gt;, but stopped using them both.  It&amp;#8217;s not worth it if you&amp;#8217;re just starting out- these two programs are good IDE&amp;#8217;s, but they wrap a lot of functionality that I&amp;#8217;m finding is just easier to do on the command line.  Get comfortable with the simple first and add what you feel you&amp;#8217;re missing.  On Windows, I&amp;#8217;ve played around with &lt;a href=&#34;http://www.e-texteditor.com/&#34;&gt;e Text Editor&lt;/a&gt; and found it pretty nice.&lt;/p&gt;

&lt;p&gt;IDE&amp;#8217;s are very important.  As a .NET developer, I love Visual Studio.  It&amp;#8217;s awesome- I love the intellisense, resharper, the immediate window, debugging, tfs integration, I&amp;#8217;m spoiled.  You can pry code completion from my cold, dead hands.  I wanted that in Rails IDE.  But don&amp;#8217;t try finding Visual Studio for Rails- Rails doesn&amp;#8217;t need Visual Studio, and it will just get in the way.  Everything you need to do can be done in a simple text editor and the terminal window.  When you feel like you&amp;#8217;re lacking something, there&amp;#8217;s an easy way to do it.  Different is okay.&lt;/p&gt;

&lt;h3 id=&#34;other-stuff:07a84db2bfd60e17f47a574617953835&#34;&gt;Other Stuff&lt;/h3&gt;

&lt;p&gt;This is random:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://tryruby.hobix.com/&#34;&gt;Ruby in 15 minutes&lt;/a&gt; is a great site for learning Ruby.&lt;/li&gt;
&lt;li&gt;Learn and understand &lt;a href=&#34;http://github.com&#34;&gt;Git&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You seriously haven&amp;#8217;t checked out [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-8:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-8:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-8:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-8:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-9:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-9:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-9:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-9:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The]&lt;a href=&#34;https://peepcode.com/products/rails-2-pdf&#34;&gt;7&lt;/a&gt; e-book which is worth the $9, and the [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-10:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-10:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-10:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-10:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-11:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-11:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-11:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-11:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The]&lt;a href=&#34;https://peepcode.com/products/rails-2-pdf&#34;&gt;7&lt;/a&gt; e-book which is worth the $9, and the]&lt;a href=&#34;https://peepcode.com/products/textmate-for-rails-2&#34;&gt;8&lt;/a&gt; video (I use a Mac).  I&amp;#8217;m very happy with them.  I&amp;#8217;ve also heard great things about the &lt;a href=&#34;https://peepcode.com/products/rest-for-rails-2&#34;&gt;Rest&lt;/a&gt; screencast (now in version 2, I&amp;#8217;ve seen version 1 and liked it).  I plan on getting the Git and RSpec videos as well.&lt;/p&gt;

&lt;p&gt;If you don&amp;#8217;t want to spend money, check out &lt;a href=&#34;http://railscasts.com/&#34;&gt;Railscasts&lt;/a&gt;.  These are done by the same people from peepcode and offer shorter videos on specific topics.  Well worth it.  (For those .NET people, think of &lt;a href=&#34;http://dimecasts.net/&#34;&gt;Dimecasts&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;the-hardware-software-setup-2:07a84db2bfd60e17f47a574617953835&#34;&gt;The Hardware/Software Setup&lt;/h3&gt;

&lt;p&gt;I use a MacBook Pro.  It&amp;#8217;s pretty easy to get setup with Rails on a Mac.  Google for the how-to.  For an editor, I use &lt;a href=&#34;http://www.macromates.com&#34;&gt;Textmate&lt;/a&gt; and love it. (Especially after watching the Peepcode screencast).  I played around with [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-12:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-12:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-12:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-12:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-13:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-13:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-13:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-13:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The]&lt;a href=&#34;https://peepcode.com/products/rails-2-pdf&#34;&gt;7&lt;/a&gt; e-book which is worth the $9, and the [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-14:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-14:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-14:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-14:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The [As I wrote recently in a previous post, &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/&#34;&gt;I&amp;#8217;ve been playing around with Rails&lt;/a&gt;.   I wanted to touch on how I got up and running learning and digging into Rails.&lt;/p&gt;

&lt;h2 id=&#34;learning-ruby-on-rails-15:07a84db2bfd60e17f47a574617953835&#34;&gt;Learning Ruby on Rails&lt;/h2&gt;

&lt;h3 id=&#34;the-web-15:07a84db2bfd60e17f47a574617953835&#34;&gt;The Web:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://guides.rails.info/&#34;&gt;Ruby on Rails Guides&lt;/a&gt; is the best place to learn Rails.  People looking to get started should definitely start here.  The guides are broken down into various topics, and are extremely well written with good coverage.  You&amp;#8217;ll get exactly what you need, even if you&amp;#8217;re an experienced programmer.&lt;/p&gt;

&lt;h3 id=&#34;books-15:07a84db2bfd60e17f47a574617953835&#34;&gt;Books:&lt;/h3&gt;

&lt;p&gt;I bought &lt;a href=&#34;http://www.amazon.com/Simply-Rails-2-Patrick-Lenz/dp/0980455200/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1231523564&amp;amp;sr=1-1&#34;&gt;Simply Rails 2&lt;/a&gt; and almost immediately returned it.  It&amp;#8217;s geared towards someone who has no programming experience whatsover.  It has minimal depth, if any.  Don&amp;#8217;t buy this book.&lt;/p&gt;

&lt;p&gt;I then bought &lt;a href=&#34;http://www.amazon.com/Rails-Way-Addison-Wesley-Professional-Ruby/dp/0321445619/ref=pd_bxgy_b_img_c&#34;&gt;The Rails Way&lt;/a&gt; which I did not return, and now use as a reference.  It&amp;#8217;s a pretty big book with a decent amount of whitespace.  However, I like this book because it&amp;#8217;s not a how-to book.  It&amp;#8217;s also not really for learning Rails, but by explaining what Rails does and why.  It has a good format- with snippets from contributors highlighting certain points (think of the comments in &lt;a href=&#34;http://www.amazon.com/Framework-Design-Guidelines-Conventions-Development/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;amp;s=books&amp;amp;qid=1234999371&amp;amp;sr=1-1&#34;&gt;Framework Design Guidelines&lt;/a&gt;).  To be honest, you can get what you need from the Rails guides, but if you like books, get The Rails Way.&lt;/p&gt;

&lt;p&gt;I also found some PDF&amp;#8217;s of Rails Recipes and Advanced Rails Recipes which I found useful.  However, Rails changes pretty frequently, so a lot of books get outdated pretty quickly.  Rails Guides is simply your best bet.&lt;/p&gt;

&lt;h3 id=&#34;peepcode-and-railscasts-15:07a84db2bfd60e17f47a574617953835&#34;&gt;Peepcode and Railscasts:&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;m usually not a fan of screencasts, but I bought a 5 pack from &lt;a href=&#34;http://peepcode.com/&#34;&gt;Peepcode&lt;/a&gt; and am glad I did.  I wish I bought the unlimited.  Peepcode offers very well done videos about various topics on Rails.  There are also some pdf e-books you can download.  So far I&amp;#8217;ve bought The]&lt;a href=&#34;https://peepcode.com/products/rails-2-pdf&#34;&gt;7&lt;/a&gt; e-book which is worth the $9, and the]&lt;a href=&#34;https://peepcode.com/products/textmate-for-rails-2&#34;&gt;8&lt;/a&gt; video (I use a Mac).  I&amp;#8217;m very happy with them.  I&amp;#8217;ve also heard great things about the &lt;a href=&#34;https://peepcode.com/products/rest-for-rails-2&#34;&gt;Rest&lt;/a&gt; screencast (now in version 2, I&amp;#8217;ve seen version 1 and liked it).  I plan on getting the Git and RSpec videos as well.&lt;/p&gt;

&lt;p&gt;If you don&amp;#8217;t want to spend money, check out &lt;a href=&#34;http://railscasts.com/&#34;&gt;Railscasts&lt;/a&gt;.  These are done by the same people from peepcode and offer shorter videos on specific topics.  Well worth it.  (For those .NET people, think of &lt;a href=&#34;http://dimecasts.net/&#34;&gt;Dimecasts&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&#34;the-hardware-software-setup-3:07a84db2bfd60e17f47a574617953835&#34;&gt;The Hardware/Software Setup&lt;/h3&gt;

&lt;p&gt;I use a MacBook Pro.  It&amp;#8217;s pretty easy to get setup with Rails on a Mac.  Google for the how-to.  For an editor, I use &lt;a href=&#34;http://www.macromates.com&#34;&gt;Textmate&lt;/a&gt; and love it. (Especially after watching the Peepcode screencast).  I played around with]&lt;a href=&#34;http://www.aptana.com&#34;&gt;13&lt;/a&gt; and &lt;a href=&#34;http://www.netbeans.com&#34;&gt;NetBeans&lt;/a&gt;, but stopped using them both.  It&amp;#8217;s not worth it if you&amp;#8217;re just starting out- these two programs are good IDE&amp;#8217;s, but they wrap a lot of functionality that I&amp;#8217;m finding is just easier to do on the command line.  Get comfortable with the simple first and add what you feel you&amp;#8217;re missing.  On Windows, I&amp;#8217;ve played around with &lt;a href=&#34;http://www.e-texteditor.com/&#34;&gt;e Text Editor&lt;/a&gt; and found it pretty nice.&lt;/p&gt;

&lt;p&gt;IDE&amp;#8217;s are very important.  As a .NET developer, I love Visual Studio.  It&amp;#8217;s awesome- I love the intellisense, resharper, the immediate window, debugging, tfs integration, I&amp;#8217;m spoiled.  You can pry code completion from my cold, dead hands.  I wanted that in Rails IDE.  But don&amp;#8217;t try finding Visual Studio for Rails- Rails doesn&amp;#8217;t need Visual Studio, and it will just get in the way.  Everything you need to do can be done in a simple text editor and the terminal window.  When you feel like you&amp;#8217;re lacking something, there&amp;#8217;s an easy way to do it.  Different is okay.&lt;/p&gt;

&lt;h3 id=&#34;other-stuff-1:07a84db2bfd60e17f47a574617953835&#34;&gt;Other Stuff&lt;/h3&gt;

&lt;p&gt;This is random:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://tryruby.hobix.com/&#34;&gt;Ruby in 15 minutes&lt;/a&gt; is a great site for learning Ruby.&lt;/li&gt;
&lt;li&gt;Learn and understand &lt;a href=&#34;http://github.com&#34;&gt;Git&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You seriously haven&amp;#8217;t checked out]&lt;a href=&#34;http://guides.rails.info/&#34;&gt;2&lt;/a&gt; yet?&lt;/li&gt;
&lt;li&gt;I spent too much time searching the Internet learning rails and not enough time programming Rails.  Get to it!&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Digging into Ruby on Rails from C# and .MVC (Asp.Net MVC)</title>
          <link>http://blog.michaelhamrah.com/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/</link>
          <pubDate>Wed, 18 Feb 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/02/digging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc/</guid>
          <description>&lt;p&gt;One of the things I&amp;#8217;ve been doing lately is digging into Ruby on Rails. I&amp;#8217;ve always wanted to learn Rails since I was first exposed to Rails at an AjaxWorld conference in &amp;#8217;06 (at least, I think in 06). David Heinemeier Hansson actually presented!&lt;/p&gt;

&lt;p&gt;Alas, I never could devote enough time to get past the tipping point. I&amp;#8217;m comfortable with C# and ASP.NET and evolving those skills has always been the priority. But the perfect storm has happened recently- in order to save space in my apartment, I got rid of my PC desktop and now only use my MacBook. I got tired of using Parallels and Visual Studio, and a new project came up in which I could either use ASP.NET MVC (which I&amp;#8217;m calling .MVC from now on) or RoR. I thought it was time to finally try RoR.&lt;/p&gt;

&lt;p&gt;The verdict is Rails is great. I still can&amp;#8217;t &amp;#8220;express&amp;#8221; myself as well as I want to with Rails, but in comparison to .MVC Rails is pretty slick, and the evolution of Rails (specifically, &lt;a href=&#34;http://rubyonrails.org/merb&#34;&gt;combining Merb with Rails&lt;/a&gt;) is pretty exciting.  Most importantly, learning Rails has actually made me a better C#/ASP.NET MVC developer- if you&amp;#8217;re working with .MVC you have to spend at least an afternoon playing around with Rails- you&amp;#8217;ll get an invaluable perspective on MVC and web programming.&lt;/p&gt;

&lt;p&gt;Now, a discussion on any programming language/framework always causes a heated debated.  I&amp;#8217;m not an expert (or even a beginner) on Ruby or Rails, but these are my initial impressions.&lt;/p&gt;

&lt;p&gt;**Ruby, as a Language&lt;/p&gt;

&lt;p&gt;**&lt;/p&gt;

&lt;p&gt;One major leap between .MVC and Rails is Ruby as a language. Yes, Ruby and C# are both OO languages, but Ruby is a &lt;a href=&#34;http://en.wikipedia.org/wiki/Dynamic_programming_language&#34;&gt;dynamic language&lt;/a&gt;&amp;#8211; and if you&amp;#8217;re up on C# 4.0, you&amp;#8217;ll know that C# is on it&amp;#8217;s way to &lt;a href=&#34;http://ironpython-urls.blogspot.com/2008/12/c-becomes-dynamic-language.html&#34;&gt;becoming a dynamic language too&lt;/a&gt;. So if you want keep your C# skills on the cutting edge, get a head start on a full fledged dynamic language!&lt;/p&gt;

&lt;p&gt;I originally made the mistake of jumping in and assuming Ruby was more vb- or js- esque than it actually was.  It&amp;#8217;s pretty smart in the way it behaves, almost a tailored version of C#.  Ruby&amp;#8217;s use of &lt;a href=&#34;http://glu.ttono.us/articles/2005/08/19/understanding-ruby-symbols&#34;&gt;symbols&lt;/a&gt; is pretty big difference over other languages that&amp;#8217;s extensively used in Rails and pretty handy.&lt;/p&gt;

&lt;p&gt;Lambda expressions are also core part of Ruby, and are becoming a more integral part of C#.  This allows Ruby to be extremely concise in getting things done- which is awesome when you know what you&amp;#8217;re doing.  When you don&amp;#8217;t it can be confusing.  But lambdas make sense and are awesome when you know how to use them- and knowing Ruby can help you grasp lambda expressions in C#.&lt;/p&gt;

&lt;p&gt;Rails leverages Ruby&amp;#8217;s dynamic language to make a lot happen under the hood.  I personally found letting the Rails framework do its thing as one of the biggest hurdles in learning Rails.  I wanted to either program or explicitly orchestrate everything!  One prime example is the persistence model in Rails-  I struggled to figure out how properties where set in models from migration classes- but it&amp;#8217;s entirely automatic!  Also, a lot of methods are created dynamically.  This allows an extremely fluid expression in writing code, making Ruby a pretty natural programming language.  (Example: the find_by ActiveRecord methods; declaring links like edit_xxx_path).&lt;/p&gt;

&lt;p&gt;C# is making its way into being a more fluid programming language similar to Ruby.  Meaning, writing code and describing code are converging to be the same thing.  This is seen in lambda expressions and fluent interfaces, where you can daisy chain methods together.  &lt;a href=&#34;http://code.google.com/p/moq/&#34;&gt;Moq&lt;/a&gt; is a good example of the fluent interface approach.  I wouldn&amp;#8217;t be surprised if there&amp;#8217;s even a larger convergence with C# and Ruby in the years to come.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rails, as a Framework&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Those in the .NET world who follow &lt;a href=&#34;http://codebetter.com/blogs/jeremy.miller/&#34;&gt;Jeremy Miller&lt;/a&gt; have probably heard the term of &lt;a href=&#34;http://codebetter.com/blogs/jeremy.miller/archive/2008/10/23/our-opinions-on-the-asp-net-mvc-introducing-the-thunderdome-principle.aspx&#34;&gt;opinionated software.&lt;/a&gt; He talks about opinionated software in the context of &lt;a href=&#34;http://code.google.com/p/fubumvc/&#34;&gt;FubuMVC&lt;/a&gt;, an alternative to .MVC written in an opinionated style.  It&amp;#8217;s worth checking out- it&amp;#8217;s very interesting to use but Rails still cracks the MVC shell wide open because it leverages Ruby&amp;#8217;s language features.&lt;/p&gt;

&lt;p&gt;Opinionated software originally came from the Adam and Eve of Rails, &lt;a href=&#34;http://gettingreal.37signals.com/ch04_Make_Opinionated_Software.php&#34;&gt;37Signals&lt;/a&gt;.  Rails is highly opinionated- which is a blessing and curse.  The great thing about Rails is that it does what it does extremely well.  The MVC triangle are completely separate but seamlessly integrated, and extension points are explicit.  It&amp;#8217;s how MVC should be- and .MVC doesn&amp;#8217;t come close to Rails as an MVC framework.  Hate NHibernate configuration?  You&amp;#8217;ve never seen an ORM framework until you&amp;#8217;ve used ActiveRecord.  Really want to abandon code-behind files and excessive code in views? Really want to do TDD (and even BDD)?  Rails right now is the iPhone in a tin cup and string world.&lt;/p&gt;

&lt;p&gt;Here are some highlights:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Respond_to method for rendering html, xml, json or whatever from a single controller.  .MVC ActionResult could learn a thing or two.&lt;/li&gt;
&lt;li&gt;Rails&amp;#8217; partial layouts, partial actions and partial views are pretty slick compared .MVC&amp;#8217;s master pages, partial views, and partial layouts.  A prime example is using Rails partial views to render a collection of objects.&lt;/li&gt;
&lt;li&gt;Passing data between controllers and actions is a lot slicker than .MVC ViewData&lt;/li&gt;
&lt;li&gt;Rails helpers are a lot more helpful.&lt;/li&gt;
&lt;li&gt;Rails is pretty slick when it comes to mapping between posted data and Models- a lot better than binders.&lt;/li&gt;
&lt;li&gt;And of course, Rails&amp;#8217; database integration will make you wonder why you ever spent more than 5 minutes learning about databases- it&amp;#8217;s such a model centric approach with migrations you&amp;#8217;ll hate doing any data tier work in .MVC.  (Although learning databases are extremely important no matter what language/framework you use).&lt;/li&gt;
&lt;li&gt;Plugins.  Rails Plugins are simply awesome- extremely fluid integration into your Rails application.  And combining plugins and other support with gem makes maintenance and upgrades a breeze (forget the GAC!)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Where .MVC Shines&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve been hyping up Rails a lot, but there is a major disadvantage with Rails: good luck straying from the Rails track.  If you get off the train, you&amp;#8217;re walking alone.  This is something .MVC does well- allows you to change the game with whatever architecture pattern you wish.  It&amp;#8217;s one reason I love .NET- you can do literally do whatever want (yes, you can whatever you want with any language, but the .NET framework is pretty awesome).  .MVC is extremely extensible- from replaceable view engines, routing engines, controller factories, coupled with whatever architecture pattern you want.  And all very testable.  C#&amp;#8217;s usage of extension methods also add pretty nice extensibility to classes, too.&lt;/p&gt;

&lt;p&gt;In fact, Rails&amp;#8217; plans to integrate with Merb as way to be more modular is a prime example of something .MVC already does well- allows you to mix and match what you want instead of forcing conformance.  Allowing total and explicit control over the application stack- including your architecture- is a great advantage in maintaining the health of ongoing software. Yes, you can evolve Rails to do what you want, but the transparency .NET offers is something I don&amp;#8217;t see in Rails just yet.  It&amp;#8217;s definitely a pro and a con- on the one hand, Rails offers fluid interaction between components.  On the other, .NET doesn&amp;#8217;t force you into any specific pattern.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The bottom line is, check out Rails.  The best way to get started is with &lt;a href=&#34;http://guides.rails.info/&#34;&gt;the Rails guides&lt;/a&gt;.  From a .MVC perspective, you&amp;#8217;ll learn a lot about MVC and what an MVC framework can do- and it will help you out in your .NET development.  Knowing Ruby will also keep your C# skills sharp and help you in becoming a well rounded developer.  I&amp;#8217;ll even bet you carry over some Rails tricks to your next .MVC app!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.dotnetkicks.com/kick/?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2009%2f02%2fdigging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc%2f&#34;&gt;&lt;img src=&#34;http://www.dotnetkicks.com/Services/Images/KickItImageGenerator.ashx?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2009%2f02%2fdigging-into-ruby-on-rails-from-c-and-mvc-aspnet-mvc%2f&amp;#038;bgcolor=000099&#34; border=&#34;0&#34; alt=&#34;kick it on DotNetKicks.com&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>All those jQuery fans: new event delegation</title>
          <link>http://blog.michaelhamrah.com/2009/01/all-those-jquery-fans-new-event-delegation/</link>
          <pubDate>Wed, 14 Jan 2009 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2009/01/all-those-jquery-fans-new-event-delegation/</guid>
          <description>&lt;p&gt;jQuery 1.3 now has a &amp;#8216;live&amp;#8217; function which acts similar to liveQuery. It lets you wire up events automatically for newly injected DOM elements. Not bad!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Understand Unit Testing and TDD: Getting Better Code Coverage</title>
          <link>http://blog.michaelhamrah.com/2008/12/understand-unit-testing-and-td-getting-better-code-coverage/</link>
          <pubDate>Thu, 18 Dec 2008 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2008/12/understand-unit-testing-and-td-getting-better-code-coverage/</guid>
          <description>&lt;p&gt;One of the biggest challenges in unleashing the power of unit testing is getting good code coverage.  Most of the time, especially when teams are just starting out with Test Driven Development, unit testing usually gets in the way.  A lot of people (I succumb to this syndrome sometimes) add tests &lt;em&gt;after&lt;/em&gt; they develop code.  The audacity! Some people just don&amp;#8217;t unit test at all- especially when struggling with mocking a dependency, like a database or the HttpContext object.  And forget about finding more than one unit test for functions- you already have one for a function which passes- so you&amp;#8217;re done, right?&lt;/p&gt;

&lt;p&gt;The secret in unit testing is understanding what you need to test.  You are not just testing something works- you&amp;#8217;re testing specific functionality behaves &lt;em&gt;as expected&lt;/em&gt;.  Once you understand this difference, not only will TDD be easier to achieve, but developing large projects, ensuring business rules, enforcing API guidelines, and concurrent development on the same feature will be easier and faster for you and your team.&lt;/p&gt;

&lt;p&gt;This post will help you think about testing: what you need to do to add unit tests to your existing project or add to what you already have.  We&amp;#8217;re not doing test first- hopefully, this will be a precursor to TDD in getting you thinking about what to get out of your unit tests so you can get into the TDD groove.&lt;/p&gt;

&lt;p&gt;**Expectations And Reactions&lt;/p&gt;

&lt;p&gt;**&lt;/p&gt;

&lt;p&gt;Each class- and each function- has a specific purpose.  Systems are built on the interaction of these classes and functions.  Classes call functions.  Functions call other functions.  Data is processed.  Objects are returned.  Exceptions are thrown.  All of this interaction is like a clock- the pendulum swings, the gears turn, the hands move.  When everything does what it should, you have a healthy, usable, working system.  When something doesn&amp;#8217;t, chaos ensues and thing don&amp;#8217;t work- the hands of the clock don&amp;#8217;t move.&lt;/p&gt;

&lt;p&gt;This boils down to a simple concept of expectations and reactions.  Class A expects Class B to do something when Class A calls a function on Class B.  Class A is &lt;em&gt;dependent&lt;/em&gt; on Class B&amp;#8217;s behavior.  What you&amp;#8217;re looking to do is create a suite of tests to ensure Class B is actually doing the behavior Class A expects- from valid return values to handling exceptions- so the hands on the clock always turn.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Code&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://michaelhamrah.com/blog/wp-content/uploads/unittestexample.zip&#34;&gt;Download the sample code here.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve created some simple code which represents an ordering system we&amp;#8217;ll say is part of typical three tier MVC application, similar to &lt;a href=&#34;http://blog.wekeroad.com/mvc-storefront/&#34; target=&#34;_blank&#34;&gt;Rob Conery&amp;#8217;s MVC Storefront&lt;/a&gt;, I&amp;#8217;ve backed everything with interfaces.  To keep things simple, I&amp;#8217;ve done this as a single class library- you can imagine all the other mumbo jumbo thrown in there.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re simply going to be shipping an order- which involves getting an order from our data tier and then using a shipping service to ship the order.  For this series, I&amp;#8217;m only going to focus on a single method in our business tier: the OrderShipmentManager.ShipOrder() function.  This will help us look at one of the two main components for a successful test suite: knowing what to tests and setting up mock objects.  In a real world application, you&amp;#8217;ll want to apply the expectation/reaction approach to each layer.&lt;/p&gt;

&lt;p&gt;Currently, I&amp;#8217;m only going to focus on what tests we need to have.  I&amp;#8217;ll dig into how to set up mocking in another post.  Let&amp;#8217;s start by looking at how our ShipOrder is being called in the controller:&lt;/p&gt;

&lt;p&gt;IOrderShipmentManager opm = new OrderShipmentManager(new DataTier.OrderStorage(), new DataTier.ShipmentService());&lt;/p&gt;

&lt;p&gt;Shipment shipment = null;&lt;/p&gt;

&lt;p&gt;try&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;shipment = opm.ShipOrder(orderId);&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;catch (OrderShipmentException ope)&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;System.Diagnostics.Debug.Write(&amp;#8220;Whoops: {0}&amp;#8221;, ope.Message);&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;View(shipment);&lt;/p&gt;

&lt;p&gt;Most controllers should look like this: very light, with the bulk of your application in your business logic layer.  This is a fairly simple call: in my controller, I take in an orderId I want to ship, and pass it to ShipOrder(). The controller expects two things to happen: a shipment to be returned, which it can pass to the view; or an exception to be thrown.&lt;/p&gt;

&lt;p&gt;So let&amp;#8217;s make sure we have two unit tests to cover these two expectations (Note: I always prefix unit tests with CLASS_FUNCTION.  This makes viewing test results easier. I didn&amp;#8217;t add that here because of redundancy- but the test project will have each test prefixed with OrderShipmentManager_ShipOrder):&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void Returns_Shipment_Object()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void Throws_OrderShipmentException_When_What()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;So I have my two unit tests- but the problem is, when should ShipOrder return a shipment? What should it actually do? And when does ShipOrder throw an exception?  What the hell do I write in these tests? We could look at the view to figure out what it does with the Shipment object.  We could read the requirements to figure out what ShipOrder should do (or, in an agile world, figure this out from our user story).  But we don&amp;#8217;t have these things.  So let&amp;#8217;s look at the function to see if we can make heads or tells of this:&lt;/p&gt;

&lt;p&gt;public Shipment ShipOrder(int orderId)&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;Order order;&lt;/p&gt;

&lt;p&gt;Shipment shipment;&lt;/p&gt;

&lt;p&gt;if (orderId &amp;lt; 0) throw new OrderShipmentException(&amp;ldquo;OrderId cannot be less than zero&amp;rdquo;); order = _orderStorage.GetOrder(orderId); if (order == null) return null; if (order.ShipmentStatus == ShipmentStatus.Shipped) { throw new OrderShipmentException(&amp;ldquo;Can&amp;rsquo;t ship an order that&amp;rsquo;s shipped!&amp;rdquo;); } shipment = _shipmentService.CreateShipment(order.Customer.CustomerId); shipment.ShipmentProducts = new List&lt;Product&gt;();&lt;/p&gt;

&lt;p&gt;order.OrderItems.ForEach(oi =&amp;gt; shipment.ShipmentProducts.Add(oi.Product));&lt;/p&gt;

&lt;p&gt;_shipmentService.CalculateShipment(shipment);&lt;/p&gt;

&lt;p&gt;_shipmentService.Ship(shipment);&lt;/p&gt;

&lt;p&gt;order.ShipmentStatus = ShipmentStatus.Shipped;&lt;/p&gt;

&lt;p&gt;order.ShipmentId = shipment.ShipmentId;&lt;/p&gt;

&lt;p&gt;_orderStorage.SaveOrder(order);&lt;/p&gt;

&lt;p&gt;return shipment;&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;Ahh, I get it now! We get the order from our order storage.  If it&amp;#8217;s null, we can&amp;#8217;t ship, so we return null- n0 need for an exception.  If the order&amp;#8217;s been shipped, throw an exception.  Next, create a shipment with our shipment service, calculate the shipment cost, and save the order.  Return the shipment and we&amp;#8217;re done.  Now we can add a suite a tests.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s focus on exceptions.  Here, we plan on throwing exceptions when either the orderId is bad or the order has already been shipped.  Other code will expect an exception to be thrown under these circumstances.  If we change this logic, things could get screwed up- code can&amp;#8217;t react accordingly, and chaos ensues.  People may have already written code based on these expectations.  So let&amp;#8217;s write some unit tests to ensure we&amp;#8217;re always throwing an exception under these circumstances.&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void Throws_OrderShipmentException_When_OrderId_Is_Less_Than_Zero()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void Throws_OrderShipmentException_When_Order_Status_Is_Shipped()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ve essentially enforced two business rules with these unit tests: when to throw our two exceptions.  (You can argue when or when not to throw an exception.  Personally, I follow the rule: If a function can&amp;#8217;t do what it should, throw an exception).&lt;/p&gt;

&lt;p&gt;Coincidentally, we&amp;#8217;re also guaranteeing something else: that our ShipOrder function will always react the same way under certain circumstances.  Here, ShipOrder will always throw an exception with an invalid orderId.  It won&amp;#8217;t return null- and it won&amp;#8217;t continue.  That&amp;#8217;s important for processing logic- we wouldn&amp;#8217;t want to create a shipment when we don&amp;#8217;t have an order.&lt;/p&gt;

&lt;p&gt;Next, let&amp;#8217;s go into functional logic.  I want to make sure a couple of things are happening, mainly that the workflow is consistent.  Let&amp;#8217;s add some unit tests around the order object:&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void Gets_Order_From_Storage()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void Saves_Order_With_ShipmentId_And_Status()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void Returns_Null_When_No_Order()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;Next, our shipment object does a couple of more things:&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void OrderShipmentManager_ShipOrder_Creates_Shipment_For_Customer()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void OrderShipmentManager_ShipOrder_Copies_OrderItems_To_Shipment()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void OrderShipmentManager_ShipOrder_Calculates_Shipment()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void OrderShipmentManager_ShipOrder_Ships_Product()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;Finally, we now know what this function should return:&lt;/p&gt;

&lt;p&gt;[TestMethod()]&lt;/p&gt;

&lt;p&gt;public void OrderShipmentManager_ShipOrder_Returns_New_Shipment_With_Order_Items()&lt;/p&gt;

&lt;p&gt;{&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;We have a pretty robust suite of tests around our ship order function.  We&amp;#8217;re not only testing how the controller deals with this function, but the internal workings of the function as well- the expectations other objects have of our ShipOrder method.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What This Means&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The core principal of testing is to make sure something works as expected.  In principal that&amp;#8217;s totally obvious and makes perfect sense- why the hell am I even writing about it?  In reality, implementing something which meets that definition is extremely vague and easily debatable.  The pitfall most people get into in attempting TDD- or even writing unit tests- is their definition of &amp;#8220;I&amp;#8217;m proving it works as expected&amp;#8221; is wrong.&lt;/p&gt;

&lt;p&gt;Evolving systems- especially those built with agile development- change a great deal over time. What we want to do is make sure the behavior of everything we write is always the same as these changes occur.  Ideally, nothing can be taken for granted- return values, exceptions, processing.  All must be tested.  Changes create little ripples in the interaction between all the dependencies in a system. Unchecked, ripples grow bigger and bigger into tsunamis of horrible, horrible bugs.  It&amp;#8217;s important to understand what these interactions are, and make sure new changes don&amp;#8217;t alter current behavior.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Where TDD Fits In&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TDD simply makes you think about these interactions before you code, so you know what each class and function does for the greater good.  For most people, especially getting started with TDD, that&amp;#8217;s a lot to wrap your head around.  Especially when you&amp;#8217;re the type of coder that likes to code first and have the system morph into what you need it to be.  With TDD, you can easily end up over analyzing the situation, testing too much or too little.  That&amp;#8217;s why TDD is very iterative- figure out what you need, write your single test to enforce it, then write code to pass your test.  But it&amp;#8217;s important to note the level you need from your tests.  If you need a ShipOrder function that returns a shipment, don&amp;#8217;t just check it returns a shipment.  Enforce it&amp;#8217;s doing every step it should- your unit tests are guaranteeing every assumption every other piece of code makes is valid.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Up Next&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I didn&amp;#8217;t talk about mocking at all.  Mocking is the second most important factor of testing: after knowing what to test!  In another post, I&amp;#8217;ll talk about how to write the unit tests I outlined above.  Unit tests also provide numerous benefits I mentioned earlier in this post: enforcing standards, allowing concurrent development and faster coding (no need for those console apps for class library development!).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.dotnetkicks.com/kick/?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2008%2f12%2funderstand-unit-testing-and-td-getting-better-code-coverage%2f&#34;&gt;&lt;img src=&#34;http://www.dotnetkicks.com/Services/Images/KickItImageGenerator.ashx?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2008%2f12%2funderstand-unit-testing-and-td-getting-better-code-coverage%2f&amp;#038;bgcolor=0000FF&#34; border=&#34;0&#34; alt=&#34;kick it on DotNetKicks.com&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Saving dynamically added list items using jQuery and ASP.NET MVC</title>
          <link>http://blog.michaelhamrah.com/2008/12/saving-dynamically-added-list-items-using-jquery-and-aspnet-mvc/</link>
          <pubDate>Mon, 15 Dec 2008 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2008/12/saving-dynamically-added-list-items-using-jquery-and-aspnet-mvc/</guid>
          <description>&lt;p&gt;There are going to be times when you want to allow a user to enter multiple copies of a single form on a web page.  This frequently happens when adding items to a list- like products in a shopping cart or tasks in a task list.  You want the user to add as many &amp;#8220;items&amp;#8221; as they want to the list, then save the entire list at once.&lt;/p&gt;

&lt;p&gt;Dynamically adding elements to a page is easy with jQuery, but parsing out list items on the server can be difficult- especially when you don&amp;#8217;t know how many items are on the page!   Things get even trickier when the number of input controls for each item increases- you have to keep all these input controls in sync so each item gets saved correctly.  Luckily, ASP.NET MVC has &lt;a href=&#34;http://haacked.com/archive/2008/10/23/model-binding-to-a-list.aspx&#34; target=&#34;_blank&#34;&gt;a built in feature to pull out a list of complex types posted to a page&lt;/a&gt; and automatically put them in a model.  We&amp;#8217;re going to combine this feature with jQuery to dynamically add form elements on a page which end up in a list of complex types that can be accessed in a controller action.&lt;/p&gt;

&lt;p&gt;In our example,  a user will be creating a shopping list and will  have the ability to add items to the shopping list dynamically.  When the user hits &amp;#8220;Save&amp;#8221; the entire list with all items will be posted to the server and serialized into a ShoppingList model.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re going to use:&lt;/p&gt;

&lt;pre class=&#34;syntax html&#34;&gt;&lt;ul&gt;
  &lt;li&gt;
    jQuery to dynamically drive the client side and dynamically add list items
  &lt;/li&gt;
    
  
  &lt;li&gt;
    ASP.NET MVC as our web infrastructure
  &lt;/li&gt;
    
  
  &lt;li&gt;
    The DefaultModelBinder to build a list of complex types which can be passed to an action method.
  &lt;/li&gt;
  
&lt;/ul&gt;
&lt;/pre&gt;

&lt;p&gt;Our model is simple- we have a ShoppingList with properties of Name and ShoppingItems:&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;public class ShoppingList
{
public string ListName { get; set; }
public IList&amp;lt;ShoppingItem&gt; Items { get; set; }
}

public class ShoppingItem
{
public string Name { get; set; }
public int Quantity { get; set; }
}

&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Posting Multiple Items At Once&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The key to this feature is posting multiple related elements which end up in a list.  In order for the DefaultModelBinder to build a list, we have to post elements in a certain way.  Essentially, each set of related input elements are grouped together using a uniquer key.  The key can be any string- it doesn&amp;#8217;t have to be an integer index.&lt;/p&gt;

&lt;p&gt;Keys are specified with a hidden input element.  In our sample, we want our list to end up in the myList.Items property.  So for every object in the Items list, we need a hidden input field with a name of &amp;#8220;myList.Items.Index&amp;#8221;.  The key we specify for the element will be the key we use in the name attribute the input element.  If our key is &amp;#8220;foo&amp;#8221;, and our property is &amp;#8220;Quantity&amp;#8221;, we&amp;#8217;ll have:&lt;/p&gt;

&lt;pre class=&#34;syntax html&#34;&gt;&lt;input type=&#34;hidden&#34; name=&#34;myList.Items.Index&#34; value=&#34;foo&#34; /&gt;

&lt;input type=&#34;hidden&#34; name=&#34;myList.Items[foo].Quantity&#34; value=&#34;5&#34; /&gt;

[/pre]

Even though we couldn&#39;t do that in c#, we can do it in our markup.  Think of keys for a hashtable- the key can be anything you want.  The hashtable is a list of the complex type your building, so typing hashtable[&#34;myKey&#39;] returns the complex type, and you have access to all properties of that type.

I&#39;ve created a unit test which shows off this magic for multiple items:



&lt;pre class=&#34;syntax c#&#34;&gt;

shoppingController.Request.Form.Add(&#34;myList.ListName&#34;, &#34;My Shopping List&#34;);
shoppingController.Request.Form.Add(&#34;myList.Items.Index&#34;, &#34;1&#34;);
shoppingController.Request.Form.Add(&#34;myList.Items[1].Name&#34;, &#34;Chocolate&#34;);
shoppingController.Request.Form.Add(&#34;myList.Items[1].Quantity&#34;, &#34;5&#34;);
shoppingController.Request.Form.Add(&#34;myList.Items.Index&#34;, &#34;Alpha&#34;);
shoppingController.Request.Form.Add(&#34;myList.Items[Alpha].Name&#34;, &#34;Graham Crackers&#34;);
shoppingController.Request.Form.Add(&#34;myList.Items[Alpha].Quantity&#34;, &#34;10&#34;);

&lt;/pre&gt;

&lt;p&gt;
  Notice how we&#39;re passing multiple &#34;myList.Items.Index&#34; values.  Don&#39;t worry- they won&#39;t overwrite eachother.  Servers turn multiple name values into a comma delimited list (you&#39;ll end up with: &#34;1,Alpha&#34; has your values for &#34;myList.Items.Index&#34;.  All the DefaultModelBinder is doing is parsing the list of  keys for whatever array you want, then matching those values to the form package. My naming convention of myList.Items is simply allowing the myList parameter in the controller action to be populated correctly by the DefaultModelBinder.
&lt;/p&gt;

&lt;p&gt;
  To reiterate, the indexer I use for each array object is simply grouping related html elements together- Graham Crackers has a quantity of 10 because I&#39;m using the same indexer for each name: Items[Alpha].Name and Items[Alpha].Quantity, just like a hashtable.
&lt;/p&gt;

&lt;p&gt;
  The rest of the unit test shows how the DefaultModelBinder will build the ShoppingList Item&#39;s property using these form values:
&lt;/p&gt;

&lt;pre class=&#34;syntax c#&#34;&gt;

var defaultBinding = ModelBinders.GetBinder(typeof(ShoppingList));
var bindingContext = new ModelBindingContext(shoppingController.ControllerContext,
shoppingController.ValueProvider,
typeof(ShoppingList),
&#34;myList&#34;, null, shoppingController.ModelState, null);
var binderResult = defaultBinding.BindModel(bindingContext);

Assert.IsNotNull(binderResult);
Assert.IsNotNull(binderResult.Value);
Assert.IsInstanceOfType(binderResult.Value, typeof(ShoppingList));

var myList = binderResult.Value as ShoppingList;

Assert.IsTrue(myList.ListName == &#34;My Shopping List&#34;);
Assert.IsTrue(myList.Items.Count &gt; 0);
Assert.IsTrue(myList.Items[0].Name == &#34;Chocolate&#34;);
Assert.IsTrue(myList.Items[0].Quantity == 5);

Assert.IsTrue(myList.Items[1].Name == &#34;Graham Crackers&#34;);
Assert.IsTrue(myList.Items[1].Quantity == 10);

&lt;/pre&gt;

&lt;p&gt;
  &lt;strong&gt;The View&lt;/strong&gt;
&lt;/p&gt;

&lt;p&gt;
  I want a button to &#34;add another item&#34; to the shopping list.  If the user clicks this button, two new textboxes should appear: one for name, and another for quantity.  These need to have the same indexer.  When the page first loads, there should already be an item to enter.
&lt;/p&gt;

&lt;p&gt;
  I have a couple of choices to build this logic.  I could specify one set of input elements in the view, and use jQuery to add individual elements to the DOM when the user hits the button.  I don&#39;t like this idea because it means I have two places to build the list items: one in the view, the other in jQuery.  When you build the same code in multiple places, you&#39;re going to get discrepancies, which lead to bugs-  I guarantee it. I could use jQuery to add the default form when the page loads, but I don&#39;t like this- html is easy and simply, it allows me to see what I&#39;m doing.  It&#39;s much easier to write html than write javascript to build html.
&lt;/p&gt;

&lt;p&gt;
  Instead, I&#39;m going for a partial view approach using actions.  By putting the injected section of markup into its own UserControl with a controller action I can embed the markup in the parent view and use ajax to get the rendered html snippet to the client via a url.  I can also choose how to generate the indexer in the action method&#39;s body: I&#39;m going to use Guids.  With Guids, I won&#39;t have to track any other indexers- I&#39;m guaranteed a unique value I can use for all the related elements in the partial view.  Here&#39;s the markup:
&lt;/p&gt;

&lt;p&gt;
  The parent view, which the user sees on load:
&lt;/p&gt;

&lt;pre class=&#34;syntax html&#34;&gt;



&lt;/pre&gt;

&lt;p&gt;
  The partial view for each item:
&lt;/p&gt;

&lt;pre class=&#34;syntax html&#34;&gt;



&lt;div class=&#34;itemContent&#34;&gt;
  &amp;lt;input type=&#34;hidden&#34; name=&#34;&amp;lt;%= ViewData[&#34;Prefix&#34;] + &#34;.Index&#34; %&gt;&#34; value=&#34;&amp;lt;%=ViewData[&#34;GUID&#34;] %&gt;&#34; /&gt;
  &lt;label&gt;Item: &lt;/label&gt;&amp;lt;input type=&#34;text&#34; name=&#34;&amp;lt;%= ViewData[&#34;Prefix&#34;]  + &#34;[&#34; + ViewData[&#34;GUID&#34;] + &#34;].Name&#34; %&gt;&#34; /&gt;
  &lt;br /&gt;
  &lt;label&gt;Quantity: &lt;/label&gt;&amp;lt;input type=&#34;text&#34; name=&#34;&amp;lt;%= ViewData[&#34;Prefix&#34;] + &#34;[&#34; + ViewData[&#34;GUID&#34;] + &#34;].Quantity&#34; %&gt;&#34; /&gt;
  &lt;br /&gt;
  
&lt;/div&gt;

&lt;/pre&gt;

&lt;p&gt;
  I chose to make the &#34;prefix&#34; I need a variable so I can potentially use this same view in other forms if needed.  I may want to put this form somewhere else, where the server argument isn&#39;t myList, but something else- I could forgo a parameter of ShoppingList and want to post a list of ShoppingItems.  This is useful when adding some sort of &#34;update&#34; feature in another section of page- say, when I already have a ShoppingList and I&#39;m updating with a new list of items.
&lt;/p&gt;

&lt;p&gt;
  On the client side I simply wire up my button to request the html snippet from the server and inject that snippet with jQuery:
&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;

$(document).ready(function() {
$(&#34;#btnAddAnother&#34;).click(function() {
$.ajax(
{
type: &#34;GET&#34;,
url: &#34;/Shopping/ShoppingItemFormContent/myList.Items&#34;,
success: function(result) {
var toInject = $(result);
$(&#34;#itemContainer&#34;).append(toInject);
}
});

})

});

&lt;/pre&gt;

&lt;p&gt;
  There&#39;s something I don&#39;t like about this: I need to call the server every time I need a view.  This isn&#39;t that snappy, and could create a lot of chatter with the server.  There&#39;s a way around this for brevity I&#39;m only going to explain: render the indexer as a specific value which can be parsed out and replaced with something else later.  My original goal is only wanting one place to specify markup: I do not want to have to duplicate code across a project.  But that shouldn&#39;t mean I need to call the server every time I need an html snippet.  I could make the snippet regular html which can be cached, then use a string or regular expression replace function to replace the hard coded indexer with something unique.
&lt;/p&gt;

&lt;p&gt;
  &lt;a href=&#34;http://michaelhamrah.com/blog/wp-content/uploads/postingalist.zip&#34;&gt;You can download the sample project here.&lt;/a&gt;&lt;br /&gt;
  &lt;a href=&#34;http://www.dotnetkicks.com/kick/?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2008%2f12%2fsaving-dynamically-added-list-items-using-jquery-and-aspnet-mvc%2f&#34;&gt;&lt;img src=&#34;http://www.dotnetkicks.com/Services/Images/KickItImageGenerator.ashx?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2008%2f12%2fsaving-dynamically-added-list-items-using-jquery-and-aspnet-mvc%2f&amp;#038;bgcolor=0000CC&#34; border=&#34;0&#34; alt=&#34;kick it on DotNetKicks.com&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Hiring Software Developers: The In House Interview (Open Ended Questions)</title>
          <link>http://blog.michaelhamrah.com/2008/12/hiring-software-developers-the-in-house-interview-open-ended-questions/</link>
          <pubDate>Wed, 10 Dec 2008 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2008/12/hiring-software-developers-the-in-house-interview-open-ended-questions/</guid>
          <description>&lt;p&gt;_Note: This is part of a series on hiring software developers.  See &lt;a title=&#34;Interview&#34; href=&#34;http://www.michaelhamrah.com/blog/index.php/tag/interview/&#34; target=&#34;_self&#34;&gt;articles tagged with interview for the series&lt;/a&gt;.  I needed to break the in house interview post into two parts.  This part focuses on open ended questions._&lt;/p&gt;

&lt;p&gt;The in house interview is by far the most important part of the hiring process.  It&amp;#8217;s also the most difficult.  You need to vet a person thoroughly in a very short time.  If you don&amp;#8217;t have a strategy for the in house interview you will not effectively gauge a candidate nor be able to make a confident &amp;#8216;yes&amp;#8217; decision.  Furthermore, if you enter into a bargaining/negotiating process, you want enough information to know how far to go in the negotiations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Open Ended Questions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Quite simply, you&amp;#8217;re testing a developer on how well they solve problems.  No, not those ridiculous how many dentists are in San Fransisco questions.  Development problems- that measure a candidate in how well they analyze information, gather requirements, structure applications, organize data, implement software and scale for the future.  These are development- not programming- problems.  They&amp;#8217;re not algorithms or set theory or knowing the best collection class to use.  These are problems regarding how well you go from a customer or product owner telling you something to a living, breathing software system.  If you&amp;#8217;re an agile development shop, or want to be one, it&amp;#8217;s essential you hire developers- not programmers- who do this well.&lt;/p&gt;

&lt;p&gt;Open ended questions offer a suite of scenarios where you explore various skills that define a good developer.  These scenarios are really up to you and can include everything from data modeling to application architecture to user interface design.  Essentially, you simply tee up a situation and see how the developer deals with the situation.  There&amp;#8217;s no right or wrong answer, just good or bad discussions.  More importantly, it requires solid communication- the ability to understand and explain ideas- which is the cornerstone of a good developer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Questions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The first example focuses on application design and analysis.  Here in New York, we have MTA vending machines.  These are straightforward machines that let you buy either a prepaid or an all-you-can ride pass with credit or cash.  It&amp;#8217;s a simple system, everyone in New York has probably used it, and is perfect system to talk system design.  A regular ATM machine or other common system also works well- you want the developer already have an understanding of the system so they can focus on application design.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Design the system architecture for an MTA vending machine.  How would you break up the system into various components? How would the system work? What problems can occur? How can you deal with them?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I get to see how well a candidate can come up with all the different things you&amp;#8217;d need to know about a vending machine, and how thorough they are in their application design.  If they say &amp;#8220;I&amp;#8217;ll create a user input class and a payment processor&amp;#8221; they&amp;#8217;re not thinking through the issues. If they ramble on for ten minutes, they&amp;#8217;re not organized with their thoughts or approach.  I want to know how well they understand tiers and class design- roles and responsibilities and program organization.  Don&amp;#8217;t be afraid to ask what classes they would create to implement this system.&lt;/p&gt;

&lt;p&gt;This question should focus on a system the user is already comfortable with.  Don&amp;#8217;t have them gather requirements- it throws another dimension which can be difficult to maneuver, and takes away from the core point of system design.  It&amp;#8217;s important to ease them into this interview approach, so they don&amp;#8217;t get flustered or confused (however, it probably says a lot about them if they do).&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s a data modelling question inspired by Flickr:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Design a data model for photo album software.  There should be photos, where each photo belongs to one, and only one album.  Albums belong to users.  Users can view other people&amp;#8217;s photos if they are marked as public.  If a photo is private, no one can see the photo.  Each photo can be tagged using a simple &amp;#8220;tagging&amp;#8221; system.  Photos can have comments, which are linked to both the photo and the user who wrote the comment.  Finally, photos can belong to sets.  A set is a group of photos from various users.  A set is owned by a single user, but anyone can add a photo to a set.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This question works well on two fronts.  First, it obviously covers data modelling.  More importantly, it gauges the candidates ability to absorb information.  The question is explaining what we need from a business perspective- it doesn&amp;#8217;t list any specific attributes.  The candidate should think and ask what we need to put in these tables.  The question also highlights entity relationships, to see how well the candidate can deal with joins and data organization.  Unlike the MTA question, I&amp;#8217;m not approaching this as if they new what system I was talking about.  I could make this like the MTA question by saying, &amp;#8220;Datamodel Flickr&amp;#8221;.  I didn&amp;#8217;t.  I&amp;#8217;m looking how well they go from what the customer wants to an implementation- specifically, a data model.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s a web service related question:&lt;/p&gt;

&lt;p style=&#34;margin-left: 1.6pt;&#34;&gt;
  &lt;em&gt;Let&amp;#8217;s say I have two existing systems- an ordering system and a warehouse system.  The ordering system handles billing customers and fulfilling orders.  The warehouse system tracks inventory and sends shipments.  Currently, each night, orders are printed from the ordering system and faxed to the warehouse, where they are checked to see if they have any inventory.  If they are, they are fulfilled.  If not, they get put on hold and the warehouse calls the sales rep to have them update the ordering system to mark the order â€œon holdâ€ until the product comes in.  Your job is to make the ordering system and warehouse system talk to each other via web services.  What does the communication between the two system will look like?  What functions need to be exposed at which points? &lt;/em&gt;
&lt;/p&gt;

&lt;p style=&#34;margin-left: 1.6pt;&#34;&gt;
  There&amp;#8217;s a part two:
&lt;/p&gt;

&lt;p style=&#34;margin-left: 1.6pt;&#34;&gt;
  &lt;em&gt;A problem has occurred: Unfortunately, the connection between the two sites may be unreliable, but will never be down for more than a day.  Itâ€™s too cost prohibitive to upgrade the connection, and you&amp;#8217;re asked if there&amp;#8217;s anything you can do with the new service.  How would you design the system so a user can enter an order on a web page that will eventually get fulfilled by the warehouse?&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;I like this because not only does it talk about vanilla web services, it also hints on extensibility.  Here, the candidate is asked to lay out a web service design and come up with some schemas.  If they can boil this down to a simple client/server service, great.  If they come up with a complex two way communication system bad.  But, after they laid out the initial design, they&amp;#8217;re confronted with a major issue.  What do they need to do to deal with this problem given what the original solution?&lt;/p&gt;

&lt;p&gt;The big kahuna question- everything rolled into one:&lt;/p&gt;

&lt;p&gt;This one is mostly geared towards senior level programmers.  I was asked this question once in an interview and loved it- it was very draining, but in the end, it allowed me to show the company I was worth it.  It essentially wraps up all kinds of role playing questions into one big one.  Be warned- it also takes a while to do thoroughly.&lt;/p&gt;

&lt;p&gt;Essentially, you, as the interviewer, sketch out a set of web pages on paper.  Do this before hand.  When I got asked the question it was an insurance site.  A CMS system, blog site, or project tracking system works well too.  For the insurance system, there were four pages: The account owner, with a bunch of the usual fields (name, address, city, SS#).  Then, there was a form for cars- an owner insured a car.  Make, model, year, etc.  Next, there was a form for insured drivers.  This could just be account owner, or members of their family.  Cars were linked to insured drivers, and offered a price for the insurance.  The owner could buy the insurance or not.  You need to have four or five hand drawn web pages, each with a form, all related.&lt;/p&gt;

&lt;p&gt;So you walk the candidate through the hand drawn forms and what needs to happen, and you have them do numerous things.  They need a data model.  They need to design the application- data tier, business objects, web pages, whatever classes they need- see what they come up with.  They need to figure out how to get from one page to another and maintain state.  Sometimes I throw in a curve ball- in order to figure out the quote price, you need to call a long web service which could take ten to twenty seconds.  What&amp;#8217;s the user experience like? How do you deal with this?  Finally, you tell them they&amp;#8217;re running a dev team, and ask how they would break up the work to the different developers.  See how they approach this- it will give you a sense of well they know how to delegate and run a team.&lt;/p&gt;

&lt;p&gt;**What You&amp;#8217;re looking For&lt;/p&gt;

&lt;p&gt;**&lt;/p&gt;

&lt;p&gt;Remember, with the phone interview done, you should have a solid understanding of the technical ability of the candidate.  But that&amp;#8217;s not the most important quality of a candidate.  Google will tell me how to use ThreadPool.QueueUserWorkItem for multithreaded programming.  The real quality is knowing when I need multithreaded programming and knowing the difference between the ThreadPool API and asynchronous delegates. That&amp;#8217;s what you&amp;#8217;re looking for in these answers- the meat, not the special sauce.&lt;/p&gt;

&lt;p&gt;You can also create your own questions depending on the work you do.  The point is to focus on a specific thing you&amp;#8217;re looking for- data modelling,  SOA, application architecture, user interface design, business objects, or everything all in one- and try and stear the conversation in the general direction you want.  But let the candidate do most of the driving.&lt;/p&gt;

&lt;p&gt;With each question, have a specific area of focus.  See what their toolbox is like if you want to know about implementation or programming.  If it&amp;#8217;s about requirements, see how well the analyze the problem and cover the bases.  You know the problems you need to solve- so set up sandbox environments to see if they have the skills to solve your problems in various areas.  You decide how specific to get- and whether these are one, five or 20 minute questions.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re not getting what they want, you may need to nudge them a little.  But be careful not to ask lead in questions.  If you set up a question like, &amp;#8220;Okay, tell me how you would handle error in the MTA machine?&amp;#8221; and they reply with, &amp;#8220;Exceptions&amp;#8221; they simply don&amp;#8217;t get it.  If the struggle with the answer they probably don&amp;#8217;t know what to do- you&amp;#8217;ll easily spot this when you see it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What&amp;#8217;s Next&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s a lot more to the in house interview- everything isn&amp;#8217;t solved with open ended questions.  My next post will cover what else to ask- those personal, character building questions as well as how to structure the interview.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Best Comment Ever</title>
          <link>http://blog.michaelhamrah.com/2008/12/best-comment-ever/</link>
          <pubDate>Fri, 05 Dec 2008 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2008/12/best-comment-ever/</guid>
          <description>&lt;p&gt;I found this when going through some code:&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;color: #008000;&#34;&gt;//I have no idea what the following code does&lt;br /&gt; //but the ui seems to function better without it&lt;br /&gt; //except for when it crashes&lt;/span&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Event Pooling with jQuery Using Bind and Trigger: Managing Complex Javascript</title>
          <link>http://blog.michaelhamrah.com/2008/12/event-pooling-with-jquery-using-bind-and-trigger-managing-complex-javascript/</link>
          <pubDate>Fri, 05 Dec 2008 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2008/12/event-pooling-with-jquery-using-bind-and-trigger-managing-complex-javascript/</guid>
          <description>&lt;p&gt;&lt;strong&gt;Managing Complexity in the UI&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As everyone knows, the more dependencies you have in a system, the harder maintaining that system is.  Javascript is no exception- and orchestrating actions across complex user interfaces can be a nightmare if not done properly.&lt;/p&gt;

&lt;p&gt;Luckily, there&amp;#8217;s a great pattern for orchestrating complex interaction in a disconnected way. No, it&amp;#8217;s not the Observer pattern.  It&amp;#8217;s a take on the Observer pattern called Event Pooling which is a piece of cake with jQuery&amp;#8217;s &lt;a href=&#34;http://docs.jquery.com/Events/bind&#34; title=&#34;jQuery Bind&#34;&gt;bind&lt;/a&gt; and &lt;a href=&#34;http://docs.jquery.com/Events/trigger&#34; title=&#34;jQuery Trigger&#34;&gt;trigger&lt;/a&gt; functions.  &lt;a title=&#34;jQuery bind and trigger example&#34; href=&#34;http://www.michaelhamrah.com/blog/wp-content/uploads/jqueryEventPool/index.html&#34; target=&#34;_blank&#34;&gt;For the get to the code folks, here&amp;#8217;s an example of using jQuery&amp;#8217;s bind and trigger for event pooling&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problems with the Observer Pattern&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The observer pattern is great for some things, but it still requires a dependency between the observer and the subject.  The publish/subscribe scenario creates a direct relationship between two objects- and makes orchestrating a lot of events difficult when you have to manage so many direct references.&lt;figure id=&#34;attachment_56&#34; style=&#34;width: 300px;&#34; class=&#34;wp-caption aligncenter&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2008/12/observer.png&#34;&gt;&lt;img class=&#34;size-medium wp-image-56&#34; title=&#34;observer pattern&#34; src=&#34;http://i0.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2008/12/observer-300x91.png?resize=300%2C91&#34; alt=&#34;observer pattern&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;observer pattern&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Event Pooling&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Event pooling is simply a variation on the Observer pattern, where a &amp;#8220;middle man&amp;#8221; is used to orchestrate the publish/subscribe system.  First, an observer will register with the event pool by saying &amp;#8220;I need to call this function when this event is fired&amp;#8221;.  Next, the subject will tell the event pool &amp;#8220;I&amp;#8217;m firing this event&amp;#8221;.  Finally, the event pool will call the function the observer registered on behalf of the subject.  The observer and subject only need to know about the event pool, not each other.&lt;figure id=&#34;attachment_57&#34; style=&#34;width: 300px;&#34; class=&#34;wp-caption aligncenter&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2008/12/event-pool.png&#34;&gt;&lt;img class=&#34;size-medium wp-image-57&#34; title=&#34;event-pool&#34; src=&#34;http://i2.wp.com/www.michaelhamrah.com/blog/wp-content/uploads/2008/12/event-pool-300x147.png?resize=300%2C147&#34; alt=&#34;Event Pool&#34; data-recalc-dims=&#34;1&#34; /&gt;&lt;/a&gt;&lt;figcaption class=&#34;wp-caption-text&#34;&gt;Event Pool&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;This provides some cool functionality- especially because if you need to reference the subject (called the publisher), you can get a direct reference via the event pool.  This is similar to the object sender parameter in .NET&amp;#8217;s event system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Show me Code!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are going to do two things: First, update a span tag to show an address when a user enters a name, city, or state.  Second, show some complex behavior by daisy changing events and binding a single function to multiple triggers. &lt;a title=&#34;jQuery bind and trigger example&#34; href=&#34;http://www.michaelhamrah.com/blog/wp-content/uploads/jqueryEventPool/index.html&#34; target=&#34;_blank&#34;&gt;Check out the example.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We have an UpdateOutput() function that updates the span:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;function UpdateOutput() {
    var name = $(&#39;#txtName&#39;).val();
    var address = $(&#39;#txtAddress&#39;).val();
    var city = $(&#39;#txtCity&#39;).val();

    $(&#39;#output&#39;).html(name + &#39; &#39; + address + &#39; &#39; + city);
}
&lt;/pre&gt;

&lt;p&gt;This is wired up via the bind function:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;$(document).bind(&#39;NAME_CHANGE ADDRESS_CHANGE CITY_CHANGE&#39;, function() {
    UpdateOutput();
});
&lt;/pre&gt;

&lt;p&gt;Notice how we&amp;#8217;re wiring up multiple event names with a single function which calls UpdateOutput.  This allows us to encapsulate common functionality in a single function which could be called from various sources.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;re also binding and firing to $(document).  Document provides a global bucket all functionality has access to- a static class that can be referenced from anywhere, making pooling easy.&lt;/p&gt;

&lt;p&gt;We can also wire up another function with the NAME_CHANGE event without effecting any other logic:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;$(document).bind(&#39;NAME_CHANGE&#39;, function(e) {
    UpdateName();
    UpdateOtherText(e);
});
&lt;/pre&gt;

&lt;p&gt;Here, NAME_CHANGE will also trigger UpdateName and UpdateOther.&lt;/p&gt;

&lt;p&gt;Firing an event is done with the trigger function:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;$(&#39;#txtAddress&#39;).keyup(function() {
    $(document).trigger(&#39;ADDRESS_CHANGE&#39;);
});
$(&#39;#txtCity&#39;).keyup(function() {
    $(document).trigger(&#39;CITY_CHANGE&#39;);
});
&lt;/pre&gt;

&lt;p&gt;It can also be called directly from html:&lt;/p&gt;

&lt;pre class=&#34;syntax html escaped&#34;&gt;Name: &amp;lt;input type=&#34;text&#34; id=&#34;txtName&#34; onkeyup=&#34;javascript:$(document).trigger(&#39;NAME_CHANGE&#39;);&#34; /&amp;gt;
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Advanced Functionality&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are two more things you can do with bind and trigger: Inspect who fired the trigger (this can be important if you&amp;#8217;re wiring up a single function from multiple subjects) and pass data to the event.&lt;/p&gt;

&lt;p&gt;In our next example, our function definition takes in two parameters: e, which is the event&amp;#8217;s sender, and data, a generic data bucket:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;$(document).bind(&#39;FIRST_UPDATED&#39;, function(e, data) {
    UpdateOtherText(e, data);
});
&lt;/pre&gt;

&lt;p&gt;The first parameter will always be the event&amp;#8217;s sender.  You can wire up as many other parameters as you want.  You pass data as a comma delimited list between brackets [parm1,parm2] when calling trigger:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;$(document).trigger(&#39;FIRST_UPDATED&#39;, [data]);
&lt;/pre&gt;

&lt;p&gt;and from html:&lt;/p&gt;

&lt;pre class=&#34;syntax js escaped&#34;&gt;&amp;lt;input type=&#34;text&#34; id=&#34;txtData&#34; onkeyup=&#34;javascript:$(document).trigger(&#39;DATA_CHANGED&#39;, [$(this).val()]);&#34; /&amp;gt;
&lt;/pre&gt;

&lt;p&gt;The sender object has a couple of properties, but the most important is &amp;#8216;type&amp;#8217;, which let&amp;#8217;s you know what triggered the event:&lt;/p&gt;

&lt;pre class=&#34;syntax js&#34;&gt;function UpdateOtherText(e, text) {
var text;
if (e.type == &#39;FIRST_UPDATED&#39;)
 text = &#39;from first: &#39; + text;
else
 text = e.type;
$(&#39;#eventFired&#39;).html(text);
}
&lt;/pre&gt;

&lt;p&gt;Check out my follow up article &lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/12/organizing-javascript-for-event-pooling-with-jquery/&#34;&gt;Organizing Events for Event Pooling.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[]&lt;a href=&#34;http://www.michaelhamrah.com/blog/index.php/2009/12/organizing-javascript-for-event-pooling-with-jquery/&#34;&gt;5&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.dotnetkicks.com/kick/?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2008%2f12%2fevent-pooling-with-jquery-using-bind-and-trigger-managing-complex-javascript%2f&#34;&gt;&lt;img src=&#34;http://www.dotnetkicks.com/Services/Images/KickItImageGenerator.ashx?url=http%3a%2f%2fwww.michaelhamrah.com%2fblog%2findex.php%2f2008%2f12%2fevent-pooling-with-jquery-using-bind-and-trigger-managing-complex-javascript%2f&amp;bgcolor=0000FF&#34; border=&#34;0&#34; alt=&#34;kick it on DotNetKicks.com&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Adventures in Agile: Practical Scrum (Intro)</title>
          <link>http://blog.michaelhamrah.com/2008/12/adventures-in-agile-practical-scrum-intro/</link>
          <pubDate>Wed, 03 Dec 2008 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2008/12/adventures-in-agile-practical-scrum-intro/</guid>
          <description>&lt;p&gt;&lt;em&gt;Disclaimer: I am not a certified scrummaster, nor have I taken any formal scrum classes.  I have, however, worked with scrum a great deal at my job, and wanted to share my experiences.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s face it: Scrum is a Utopian pipe dream.  The user stories, working in sprints, task estimations, no deadlines, uncommitted backlog, visibility, transparency, business buy-in, everyone&amp;#8217;s happy, fast development, easily responding to change, everyone gets what they want, everybody gets it and works together- it&amp;#8217;s all a hot air and completely unrealistic.&lt;/p&gt;

&lt;p&gt;Okay, so maybe that&amp;#8217;s a little bit harsh.  How about this argument- real Scrum- true Scrum- ideal Scrum- is a Utopian dream.  It&amp;#8217;s a never ending struggle for a more perfect union.  The essential problem is we, as humans, are incapable of living up to the requirements of ideal Scrum; and business mentality will never allow Scrum to work in the practical world.  It&amp;#8217;s a round peg in a square hole.  But if you believe- and try- you can get partly there, and that, my friends, will be good enough.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Utopian&lt;/strong&gt; &lt;strong&gt;Scrum&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Scrum is dependent on the cohesion of too many disparate factors- mostly three sets of opposing forces.  First, the difference between practical feasibility and the unrealistic demands of requirements; second, the need for &amp;#8220;When can I get it?&amp;#8221; delivery dates from the business versus the complete impossibility of defining those dates accurately; and third, the need for a group of people to fluidly communicate and work together in a variety of situations.&lt;/p&gt;

&lt;p&gt;Scrum works- and delivers- when these three completely impossible scenarios are met.  Scrum simply sets groundrules on how these groups of people interact and manage their work.  But that requires disparate groups of people working and interacting effectively.  Unfortunately, most of the time, it&amp;#8217;s doesn&amp;#8217;t fully happen.  The business will never be satisfied with vague, long term deliverable goals.  The business wants concrete information so they can make decisions- they want to know when product &amp;#8220;A&amp;#8221; will be ready for launch so they can do x, y and z and estimate revenues.  An uncommitted backlog? Story points? WTF is that?  Nor will the business ever fully comprehend the cost/benefit analysis that goes into any sort of feature development.  What do you mean let&amp;#8217;s wait and see? I want to know what it&amp;#8217;s going to be like now! It needs to do x, y and z be be super simple to use- can&amp;#8217;t we just have it do this other thing?  Why will that take so long? I want to tell you how to do it- and if you already do it, why would we change it? I know how it should work!&lt;/p&gt;

&lt;p&gt;My personal favorite is the working together part.  Do you really think it&amp;#8217;s possible to get a group of introverts- all with their own egos and insecurities- to proactively communicate, share ideas, and work effectively as a team?  Heads down, opinionated, or really quiet developers?  This is the &amp;#8220;this conversation is too simplistic for me&amp;#8221; crowd! The &amp;#8220;I talk in code&amp;#8221; crowd!  And everyone on the team can operate without high level direction- and figure out requirements- and bring up impediments or issues when they arise- and ask questions- crowd?  Yeah, right! These are programmers!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Practical Scrum&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Don&amp;#8217;t worry- all is not lost.  The principals of Scrum are pretty solid and good ideas, and even if you only get half way there, you&amp;#8217;re better off than with anything else.  Visibility and transparency are extremely important with any development group, and knowing where you stand with the big picture is solid information anyone can appreciate.  Most of the time the business doesn&amp;#8217;t really care about when things happen, they just want an accurate estimate for planning.  Who wouldn&amp;#8217;t want that?  When you hit your stride with Scrum, it may not be perfect- but both you, the development team, and the business- are all better off because everyone gets enough of what they need with a manageable amount of overhead.&lt;/p&gt;

&lt;p&gt;The main problem with Scrum is that in principal everything make sense.  It&amp;#8217;s easy to see how it works- the backlog, the sprint, the scrum, the estimations- but when you try to do it, everything false apart. In practical terms, it&amp;#8217;s impossible to make all the chips falls as they need to- to make everyone understand what they need to do- and to get everyone in the organization on the same page.  Most people either end up with a bastardization of the process, or worse give up out of frustration.&lt;/p&gt;

&lt;p&gt;Practical Scrum is that middle ground between where you are now and Utopian Scrum.  It&amp;#8217;s the half way of making what you have work, and like Scrum, iterating to a better place one thing at a time.  Practical Scum is simply understanding how to implement Scrum to make your organization work effectively.  The key is not jumping into the deep end with Scrum hoping you know how to swim.  Hint: You&amp;#8217;ll drown.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ll delve into various aspects of Practical Scrum in future posts.  For now, it&amp;#8217;s important to remember a couple of things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Don&amp;#8217;t give up.  If it doesn&amp;#8217;t work, don&amp;#8217;t worry, it usually doesn&amp;#8217;t.&lt;/li&gt;
&lt;li&gt;Scrum is easy in principal; in practicality, it&amp;#8217;s extremely difficult.&lt;/li&gt;
&lt;li&gt;Like everything, you need to move in baby steps when adopting Scrum.&lt;/li&gt;
&lt;li&gt;Remember: If you do it, they will come. This simply means once your team is on Scrum, and you start delivering, people will recognize it works.&lt;/li&gt;
&lt;li&gt;The goal isn&amp;#8217;t learning Scrum- or what Scrum is.  The goal is learning how to adopt scrum.  Like the principals of Scrum, go with the flow.  Iterate.  Make subtle improvements.  Get to where you need to be at a healthy pace.&lt;/li&gt;
&lt;li&gt;Don&amp;#8217;t rush it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#8217;ll dig into how you get to where you need to be in Scrum in future posts.  I&amp;#8217;ve been on the brink of giving up too, but am happy I&amp;#8217;ve stuck with it.  Now we&amp;#8217;re starting to hit our stride, and it&amp;#8217;s great.  Yes, it&amp;#8217;s still rough around the edges, but that&amp;#8217;s a fact of life with everything.&lt;/p&gt;

&lt;p&gt;Getting there isn&amp;#8217;t half the battle- it is the battle.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to Hire a Software Developer: The Phone Interview</title>
          <link>http://blog.michaelhamrah.com/2008/12/how-to-hire-a-software-developer-the-phone-interview/</link>
          <pubDate>Mon, 01 Dec 2008 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2008/12/how-to-hire-a-software-developer-the-phone-interview/</guid>
          <description>&lt;p style=&#34;text-align: left;&#34;&gt;
  &lt;em&gt;Note: This is part of a series on hiring software developers.  See &lt;a title=&#34;Interview&#34; href=&#34;http://www.michaelhamrah.com/blog/index.php/tag/interview/&#34; target=&#34;_self&#34;&gt;articles tagged with interview for the series&lt;/a&gt;. &lt;/em&gt;
&lt;/p&gt;

&lt;p style=&#34;text-align: left;&#34;&gt;
  &lt;em&gt;&lt;/em&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;The Phone Interview&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;&lt;span&gt;The purpose of the phone interview is twofold.  First, a simple baseline for communication is established.  Communication is the most important quality a candidate can have.  You can always learn new technology, but if you can&amp;#8217;t communicate clearly- your ideas, your work, and your ability- you probably won&amp;#8217;t work well in a collaborative environment.  Secondly, the phone interview provides a benchmark for technical ability.  This is usually a list of straightforward programming questions in the particular language most important to the job. &lt;a title=&#34;.NET Questions&#34; href=&#34;http://www.michaelhamrah.com/blog/index.php/net-interview-questions/&#34; target=&#34;_self&#34;&gt;I use this list of .NET questions to get a sense of surface area and depth&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;Communication&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;How do you know a candidate is a good communicator? A two prong approach works best: first, have them go over their resume.&lt;span&gt; &lt;/span&gt;Ask about the projects theyâ€™ve worked on, and what they did on those projects.&lt;span&gt; &lt;/span&gt;How good are their answers? Do you have an understanding of the work they did?&lt;span&gt; &lt;/span&gt;Did they ramble on too much, or not provide enough depth, or repeat themselves?&lt;span&gt; &lt;/span&gt;If you asked any questions did they give a direct answer?&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Second, find out how well they answer technical questions.&lt;span&gt; &lt;/span&gt;This isnâ€™t an issue of right and wrong, itâ€™s about complexity.&lt;span&gt; &lt;/span&gt;Did they provide a clear explanation of the answer? Were they concise and confident they knew what they were talking about? Was there a hint of ego in their response? The more you phone interview the more you can tell someone who is good or bad at communication.&lt;span&gt; &lt;/span&gt;Itâ€™s that â€œthingâ€ that makes you say, â€œTell me more!â€&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;Technical Questions&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;There are plenty of sites out there for programming type questions- whether C#, PHP, Java, etc.  You know the technology you work with- so ask the candidate if they know it too.  Be flexible on this, we often interview people without any knowledge of the programming language we use.  If you&amp;#8217;re an expert in one language, you can probably pick up another pretty well.  Try and figure out how well the candidate has dug into their craft.  This will also give you the opportunity to rank a candidate as junior, mid-level, or senior.  It also lets you know how much they&amp;#8217;ve hyped up their ability (if you have the audacity to list php, .NET, java, perl, cobol and fortan as your &amp;#8220;expert languages&amp;#8221;, I&amp;#8217;m calling you out on it).&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Break up technical questions into a standard set of categories.&lt;span&gt; &lt;/span&gt;This includes SQL/Database work, class design/architecture, web programming, web services, html, javascript, language/framework how-toâ€™s. Go in with more than you can possibly fit into the allotted time: this will allow you to maneuver during the technical interview. &lt;span&gt; &lt;/span&gt;It doesnâ€™t make any sense to ask a candidate who has no sql experience a bunch of sql questions.&lt;span&gt; &lt;/span&gt;Instead, focus on their stronger skills or go for more surface area.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;How many questions to ask? Thatâ€™s easy: ask until they get something wrong.&lt;span&gt; &lt;/span&gt;This is important, because it allows you to see how they handle themselves when they donâ€™t know something.&lt;span&gt; &lt;/span&gt;Theyâ€™ll either flat out admit they donâ€™t know it, make an educated guess (hint: theyâ€™ll have a good sense of reasoning skills), or lie.&lt;span&gt; &lt;/span&gt;I prefer going for surface area then depth.&lt;span&gt; &lt;/span&gt;First, find out how much they know about different things.&lt;span&gt; &lt;/span&gt;Second, pick some of the most important areas youâ€™re looking for and go for depth: see how much they know in that area.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;a title=&#34;.NET Questions&#34; href=&#34;http://www.michaelhamrah.com/blog/index.php/net-interview-questions/&#34; target=&#34;_self&#34;&gt;Here are some questions I use for the phone interview, or the in house interview if needed&lt;/a&gt;.&lt;span&gt; &lt;/span&gt;Note, Iâ€™m not going for detailed how-toâ€™s.&lt;span&gt; &lt;/span&gt;I like to keep things high level, as I believe itâ€™s important to know concepts.&lt;span&gt; &lt;/span&gt;Google will tell me how to expire cache after ten minutes.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;Divide and Conquer&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Usually, the phone interview is handed out to a lot of different people, as itâ€™s somewhat time consuming and very hit or miss.&lt;span&gt; &lt;/span&gt;Some prefer to have one person do the phone interview so they can easily compare candidates to make it to the next round.&lt;span&gt; &lt;/span&gt;Either way, itâ€™s important to make a checklist.&lt;span&gt; &lt;/span&gt;A checklist lets you remember the differences between candidates, and allows other developers to easily summarize their phone interviews.&lt;span&gt; &lt;/span&gt;Iâ€™ve gotten a lot of blank stares and â€œHow do I do this?â€ after asking developers to do phone interviews.&lt;span&gt; &lt;/span&gt;Give them a set of questions- and a checklist for them to fill out- will make your and their lives easier.&lt;span&gt; &lt;/span&gt;If you canâ€™t into trouble (i.e. sued) for not bringing somebody in, a checklist will also give you cover.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;Conclusion&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;The most important thing for you to know is donâ€™t worry about ending the interview early.&lt;span&gt; &lt;/span&gt;If clearly you donâ€™t like them, you just say any number of things:&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;1)&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;â€œIâ€™m sorry; weâ€™re looking for someone with more experience. Thank you for applying.â€&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;2)&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;â€œThank you for your time.&lt;span&gt; &lt;/span&gt;Unfortunately, we wonâ€™t be able to move forward.â€&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;3)&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;If you want to postpone the face to face, thereâ€™s always â€œWeâ€™re in the process of interviewing candidates now and will let you know about the next round in XX days.â€&lt;span&gt; &lt;/span&gt;Then, e-mail them no in XX days. This works if someone else is doing the phone interview, or you want to mull it over. Giving them an e-mail date prevents them from living in limbo when youâ€™ll get back to them.&lt;span&gt; &lt;/span&gt;Itâ€™s simple respect.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;You can even add in â€œWeâ€™ll keep your resume on fileâ€ if you feel guilty.&lt;span&gt; &lt;/span&gt;My point is, be respectful: if you know theyâ€™re a no, donâ€™t waste their time or yours.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;How do you decide you makes it or who doesnâ€™t? Simple: if they impress you and you want to know more, invite them in.&lt;span&gt; &lt;/span&gt;Your gut check should be, â€œThis person has potential.â€ If not, donâ€™t move forward.&lt;span&gt; &lt;/span&gt;If youâ€™re on the fence, put them on hold until you seriously would want to hire them.&lt;span&gt; &lt;/span&gt;Donâ€™t bring them in for the sake of bringing them in- the in house interview should take some time, and if youâ€™re not serious about them, donâ€™t have them put on a suit, take time away from their current job/life, and make them travel just to use as a benchmark for something else.&lt;span&gt; &lt;/span&gt;Itâ€™s not right.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Ideally, 15% of your applicant pool you want to do a phone interview with.  Maybe 25% if you are in a tight technical market like NYC.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;p class=&#34;MsoNormal&#34;&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to Hire a Software Developer</title>
          <link>http://blog.michaelhamrah.com/2008/11/how-to-hire-a-software-developer/</link>
          <pubDate>Sat, 29 Nov 2008 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2008/11/how-to-hire-a-software-developer/</guid>
          <description>&lt;p&gt;_Note: This is part of a series on hiring software developers.  See &lt;a title=&#34;Interview&#34; href=&#34;http://www.michaelhamrah.com/blog/index.php/tag/interview/&#34; target=&#34;_self&#34;&gt;articles tagged with interview for the series&lt;/a&gt;._&lt;/p&gt;

&lt;p&gt;We just finished a round of recruiting at our company, and I&amp;#8217;m somewhat happy it&amp;#8217;s over.  Hiring software developers, or any other type of employee, is never an easy task.  How do you know you&amp;#8217;re picking the right person? How do you know if they&amp;#8217;re qualified? How do you know they&amp;#8217;ll deliver as well as they interview?  These are all questions we ask ourselves as we&amp;#8217;re harassed by recruiters, rummage over resumes, and get our interview questions ready.  However, I&amp;#8217;ve learned a great deal during this last round of recruiting, and am happy to say it&amp;#8217;s no longer a painful process.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the one liner: If you go into the hiring process without a strategy, like most things, you&amp;#8217;re doomed to fail.  It&amp;#8217;s important to get your strategy down before you even post the job ad.  You need a clear pipeline from start to finish, so you can manage the process easily.  Plus, hiring somebody who&amp;#8217;s part of a team should never be a one person job.  That doesn&amp;#8217;t mean give the bad parts (like phone screenings) to other people.  That means getting your team involved- and allowing them to give critical feedback on a candidate.  It&amp;#8217;s easy to do that if you have a strategy that allows clear benchmarks when comparing candidates.&lt;/p&gt;

&lt;p&gt;We break our hiring pipeline into three major parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Phone Interview&lt;/li&gt;
&lt;li&gt;The In House Interview&lt;/li&gt;
&lt;li&gt;The Follow Up Interview&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#8217;ll dig into each part in later posts, but it&amp;#8217;s important to note it all starts with getting resumes in the first place-  and that requires a solid ad.  You&amp;#8217;re not only announcing the job position, but you&amp;#8217;re selling yourself to the world.  The ad, as well as resumes, will be the subject of our next post.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to Interview a Software Developer: The Ad and Resumes</title>
          <link>http://blog.michaelhamrah.com/2008/11/how-to-interview-a-software-developer-the-ad-and-resumes/</link>
          <pubDate>Sat, 29 Nov 2008 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2008/11/how-to-interview-a-software-developer-the-ad-and-resumes/</guid>
          <description>&lt;p&gt;_Note: This is part of a series on hiring software developers.  See &lt;a title=&#34;Interview&#34; href=&#34;http://www.michaelhamrah.com/blog/index.php/tag/interview/&#34; target=&#34;_self&#34;&gt;articles tagged with interview for the series&lt;/a&gt;._&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Ad&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The ad is one of the most important parts of the hiring process, as it sets the tone for who you&amp;#8217;re asking to apply.  If you write a careless, generic ad you&amp;#8217;re going to get generic candidates.  Well, you&amp;#8217;re always going to get generic candidates, but with a generic ad you&amp;#8217;re definitely not going to get any good or great candidates.  It&amp;#8217;s as simple as this: If you want a good developer, you need a good ad.&lt;/p&gt;

&lt;p&gt;It doesn&amp;#8217;t take a lot of effort to see the difference between generic ads and good ads.  Just do a simple search of C# on Dice.com and you&amp;#8217;ll see them- bland titles, boring summaries.  Would you apply for these jobs? I don&amp;#8217;t even want to look at them!&lt;/p&gt;

&lt;p&gt;Here are some guidelines:**&lt;/p&gt;

&lt;p&gt;**&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Be proud of the company and the job, and be proud of the work you do.&lt;/strong&gt; Show that in the ad, and show why good people will excel at your company. There should be an exciting summary of the company, as well as the job.  It&amp;#8217;s important to put development work in the correct context, and allow people to see how they can fit and contribute.  A good ad will open the door to a positive answer when you ask, &amp;#8220;Why do you want to work for us?&amp;#8221; later.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Don&amp;#8217;t hide your company behind a recruiting agency.&lt;/strong&gt; Disclaimer: I hate recruiters.  99 out of 100 times they&amp;#8217;re awful people, shoveling the worst resumes in your face and harassing you.  About half my phone calls are recruiters- and I hang up on all them.  It&amp;#8217;s worthless.  More importantly, quality candidates probably aren&amp;#8217;t going to work with recruiters, because they don&amp;#8217;t need to.  It&amp;#8217;s their market, not yours.  Be open about the company, the job and the work.  A good candidate isn&amp;#8217;t going to apply just because the ad says &amp;#8220;C#/Javascript/Sql Server&amp;#8221;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Explain about the lifestyle of the company in the ad.&lt;/strong&gt; Be somewhat silly, but honest.  People want to feel at home at work- and relaxed.  Show off the perks a little.  This will get people excited about the opportunity, and will make them want it more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Explain what you&amp;#8217;re really looking for, not just certain skill sets.&lt;/strong&gt; If you&amp;#8217;re demanding people know one language or another, you&amp;#8217;re cutting off a lot of your applicant pool, and that&amp;#8217;s bad.  Sure, if you want someone who&amp;#8217;s going to do low-level device driver programming, you can be picky.  But for most things, it doesn&amp;#8217;t matter if a candidate knows C#, PHP, or Java.  A good candidate will learn a new language quickly.  It&amp;#8217;s more important they know the principals of programming.  Explain the skills on a high level- web programming, application architecture, content management systems, social websites- and let the candidate apply their knowledge to your needs.  If they can do that, you&amp;#8217;re golden.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Resumes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hopefully you&amp;#8217;ll get a lot of resumes- and you may think it&amp;#8217;s hard to separate the good from the bad.  It&amp;#8217;s not- and if you&amp;#8217;re on the fence about somebody, then they&amp;#8217;e a &amp;#8220;NO&amp;#8221;.  Don&amp;#8217;t waste your time!  Here are some tips:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;You&amp;#8217;re only goal is to see if the candidate is worth a phone call, that&amp;#8217;s it.  You&amp;#8217;re not hiring them based on their resume.&lt;/li&gt;
&lt;li&gt;If the have certain items in bold, they&amp;#8217;re no good.  Who bolds items on their resume? I see this all the time.  If you have to bold items in your resume, tailor your resume.&lt;/li&gt;
&lt;li&gt;A resume should be one page, two pages max.  If you get an eight page resume, pass.  It&amp;#8217;s all filler.&lt;/li&gt;
&lt;li&gt;A candidate should explain how they solved problems at their job.  You&amp;#8217;re hiring a problem solver.  If a candidate writes &amp;#8220;Created user controls and a site map for a website,&amp;#8221; don&amp;#8217;t call them.  That&amp;#8217;s a gimme.  Should they write they double clicked the visual studio icon also? I look for how they applied their knowledge to get things done.  You should too.&lt;/li&gt;
&lt;li&gt;Look for typos.  I see a lot of these- even small things, like capitalization errors.  If a candidate isn&amp;#8217;t going to take the time to clean up their resume, they&amp;#8217;re probably going to be sloppy programmers.&lt;/li&gt;
&lt;li&gt;I can live with someone specifying every technology since the dawn of man on their resume, but I&amp;#8217;m going to call you out on it.  I really don&amp;#8217;t believe people when I see &amp;#8220;PHP, ASP.NET 1.0/1.&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;.0/3.0/3.5, WCF, WWF, WPF,WebForms,Cobol, Java,EJB, Windows Vista, XP, NT, Server, Abacus, Sql Server 7.0/2000/2005/2008&amp;#8243;!  Really? The abacus? You used the abacus at a job?  Somehow, somewhere, someone told candidates to list every possible buzzword on their resume, and people blindly followed.  FYI: Nobody&amp;#8217;s going to say &amp;#8220;Oh, sorry, you didn&amp;#8217;t list VB 6 in your skillset, I&amp;#8217;m not calling you.&amp;#8221;  If they do- don&amp;#8217; worry- you don&amp;#8217;t want to work for them.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Luckily, you&amp;#8217;ll be calling about 15% of the resumes you get.  In a tight job market like NYC, maybe 20-25%.  If you hire a lot, I&amp;#8217;m sure you&amp;#8217;ll find the resume says a lot about the candidate- and the ones that stand out are easy to spot.  **If you think you&amp;#8217;re not getting any good resumes in, then you&amp;#8217;re writing a poor ad.&lt;/p&gt;

&lt;p&gt;**&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
