<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> </title>
    <link>http://blog.michaelhamrah.com/tags/docker/</link>
    <language>en-us</language>
    <author>Michael Hamrah</author>
    <rights>(C) 2015</rights>
    <updated>2015-06-27 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Fleet Unit Files for Kubernetes on CoreOS</title>
          <link>http://blog.michaelhamrah.com/2015/06/fleet-unit-files-for-kubernetes-on-coreos/</link>
          <pubDate>Sat, 27 Jun 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/06/fleet-unit-files-for-kubernetes-on-coreos/</guid>
          <description>&lt;p&gt;As I&amp;#8217;ve been leveraging CoreOS more and more for running docker containers, the limitations of Fleet have become apparent.&lt;/p&gt;

&lt;p&gt;Despite the benefits of &lt;a href=&#34;http://blog.michaelhamrah.com/2015/04/easy-scaling-with-fleet-and-coreos/&#34;&gt;dynamic unit files via the Fleet API&lt;/a&gt; there is still a need for fine-grained scheduling, discovery, and more complex dependencies across containers. Thus I&amp;#8217;ve been exploring Kubernetes. Although young, it shows promise for practical usage. The &lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes&#34;&gt;Kubernetes repository&lt;/a&gt; has a plethora of &amp;#8220;getting started&amp;#8221; examples across a variety of environments. There are a few CoreOS related already, but they embed the kubernetes units in a cloud-config file, which may not be what you want.&lt;/p&gt;

&lt;p&gt;My preference is to separate the CoreOS cluster setup from the Kubernetes installation. Keeping your CoreOS cloud-config minimal has many benefits, especially on AWS where you can&amp;#8217;t easily update your cloud-config. Plus, you may already have a CoreOS cluster and you just want to deploy Kubernetes on top of it.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/mhamrah/kubernetes-coreos-units&#34;&gt;Fleet Unit Files for Kubernetes on CoreOS are on GitHub&lt;/a&gt;. The unit files makes a few assumptions, mainly you are running with a &lt;a href=&#34;https://coreos.com/docs/cluster-management/setup/cluster-architectures/#production-cluster-with-central-services&#34;&gt;production setup using central services&lt;/a&gt;. For AWS &lt;a href=&#34;http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/&#34;&gt;you can use Cloudformation to manage multiple sets of CoreOS roles as distinct stacks, and join them together via input parameters&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Slimming down Dockerfiles: Decrease the size of Gitlabâ€™s CI runner from 900 to 420 mb</title>
          <link>http://blog.michaelhamrah.com/2015/03/slimming-down-dockerfiles-decrease-the-size-of-gitlabs-ci-runner-from-900-to-420-mb/</link>
          <pubDate>Sun, 22 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/slimming-down-dockerfiles-decrease-the-size-of-gitlabs-ci-runner-from-900-to-420-mb/</guid>
          <description>

&lt;p&gt;I&amp;#8217;ve been leveraging Gitlab CI for our continuous integration needs, running both the CI site and CI runners on our CoreOS cluster in docker containers. It&amp;#8217;s working well. On the runner side, after cloning the ci-runner repositroy and running a &lt;code&gt;docker build -t base-runner .&lt;/code&gt; , I was a little disappointed with the size of the runner. It weighed in at 900MB, a fairly hefty size for something that should be a lightweight process. I&amp;#8217;ve built the &lt;a href=&#34;https://github.com/gitlabhq/gitlab-ci-runner/blob/master/Dockerfile&#34;&gt;ci-runner dockerfile&lt;/a&gt; with the name &amp;#8220;base-runner&amp;#8221;:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;base-runner      latest      aaf8a1c6a6b8    2 weeks ago    901.1 MB
&lt;/pre&gt;

&lt;p&gt;The dockerfile is well documented and organized, but I immediately noticed some things which cause dockerfile bloat. There are some great resources on slimming down docker files, including &lt;a href=&#34;http://www.centurylinklabs.com/optimizing-docker-images/&#34;&gt;optimizing docker images&lt;/a&gt; and the &lt;a href=&#34;https://github.com/gliderlabs/docker-alpine&#34;&gt;docker-alpine&lt;/a&gt; project. The advice comes down to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use the smallest possible base layer (usually Ubuntu is not needed)&lt;/li&gt;
&lt;li&gt;Eliminate, or at least reduce, layers&lt;/li&gt;
&lt;li&gt;Avoid extraneous cruft, usually due to excessive packages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#8217;s make some minor changes to see if we can slim down this image. At the top of the dockerfile, we see the usual apt-get commands:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;# Update your packages and install the ones that are needed to compile Ruby
RUN apt-get update -y
RUN apt-get upgrade -y
RUN apt-get install -y curl libxml2-dev libxslt-dev libcurl4-openssl-dev libreadline6-dev libssl-dev patch build-essential zlib1g-dev openssh-server libyaml-dev libicu-dev

# Download Ruby and compile it
RUN mkdir /tmp/ruby
RUN cd /tmp/ruby &amp;#038;&amp;#038; curl --silent ftp://ftp.ruby-lang.org/pub/ruby/2.0/ruby-2.0.0-p481.tar.gz | tar xz
RUN cd /tmp/ruby/ruby-2.0.0-p481 &amp;#038;&amp;#038; ./configure --disable-install-rdoc &amp;#038;&amp;#038; make install
&lt;/pre&gt;

&lt;p&gt;Each &lt;code&gt;RUN&lt;/code&gt; command creates a separate layer, and nothing is cleaned up. These artifacts will stay with the container unnecessarily. Running another &lt;code&gt;RUN rm -rf /tmp&lt;/code&gt; won&amp;#8217;t help, because the history is still there. We need things gone and without a trace. We can &amp;#8220;flatten&amp;#8221; these commands and add some cleanup commands while preserving readability:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Update your packages and install the ones that are needed to compile Ruby
# Download Ruby and compile it
RUN apt-get update -y &amp;#038;&amp;#038; 
    apt-get upgrade -y &amp;#038;&amp;#038; 
    apt-get install -y curl libxml2-dev libxslt-dev libcurl4-openssl-dev libreadline6-dev libssl-dev patch build-essential zlib1g-dev openssh-server libyaml-dev libicu-dev &amp;#038;&amp;#038; 
    mkdir /tmp/ruby &amp;#038;&amp;#038; 
    cd /tmp/ruby &amp;#038;&amp;#038; curl --silent ftp://ftp.ruby-lang.org/pub/ruby/2.0/ruby-2.0.0-p481.tar.gz | tar xz &amp;#038;&amp;#038; 
    cd /tmp/ruby/ruby-2.0.0-p481 &amp;#038;&amp;#038; ./configure --disable-install-rdoc &amp;#038;&amp;#038; make install &amp;#038;&amp;#038; 
    apt-get clean &amp;#038;&amp;#038; 
    rm -rf /var/lib/apt/lists/* /tmp/*
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s only one run command, and the last two lines cleanup the apt-get downloads and &lt;code&gt;tmp&lt;/code&gt; space. Let&amp;#8217;s see how we well we do:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;$ docker images | grep base-runner
base-runner      latest      2a454f84e4e8      About a minute ago   566.9 MB
&lt;/pre&gt;

&lt;p&gt;Not bad; with one simple modification we went from 902mb to 566mb. This change comes at the cost of build speed. Because there&amp;#8217;s no previously cached layer, we always start from the beginning. When creating docker files, I usually start with multiple run commands so history is preserved while I&amp;#8217;m working on the file, but then concatenate everything at the end to minimize cruft.&lt;/p&gt;

&lt;p&gt;566mb is a good start, but can we do better? The goal of this build is to install the ci-runner. This requires Ruby and some dependencies, all documented on the ci-runner&amp;#8217;s readme. As long as we&amp;#8217;re meeting those requirements, we&amp;#8217;re good to go. Let&amp;#8217;s switch to debian:wheezy. We&amp;#8217;ll also need to tweak the locale setting for debian. Our updated dockerfile starts with this:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;# gitlab-ci-runner

FROM debian:wheezy
MAINTAINER Michael Hamrah &amp;lt;m@hamrah.com&gt;

# Get rid of the debconf messages
ENV DEBIAN_FRONTEND noninteractive

# Update your packages and install the ones that are needed to compile Ruby
# Download Ruby and compile it
RUN apt-get update -y &amp;#038;&amp;#038; 
    apt-get upgrade -y &amp;#038;&amp;#038; 
    apt-get install -y locales curl libxml2-dev libxslt-dev libcurl4-openssl-dev libreadline6-dev libssl-dev patch build-essential zlib1g-dev openssh-server libyaml-dev libicu-dev &amp;#038;&amp;#038; 
    mkdir /tmp/ruby &amp;#038;&amp;#038; 
    cd /tmp/ruby &amp;#038;&amp;#038; curl --silent ftp://ftp.ruby-lang.org/pub/ruby/2.0/ruby-2.0.0-p481.tar.gz | tar xz &amp;#038;&amp;#038; 
    cd /tmp/ruby/ruby-2.0.0-p481 &amp;#038;&amp;#038; ./configure --disable-install-rdoc &amp;#038;&amp;#038; make install &amp;#038;&amp;#038; 
    apt-get clean &amp;#038;&amp;#038; 
    rm -rf /var/lib/apt/lists/* /tmp/*
&lt;/pre&gt;

&lt;p&gt;Let&amp;#8217;s check this switch:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;base-runner      latest      40a1465ebaed      3 minutes ago      490.3 MB
&lt;/pre&gt;

&lt;p&gt;Better. A slight modification can slim this down some more; the dockerfile builds ruby from source. Not only does this take longer, it&amp;#8217;s not needed: we can just include the &lt;code&gt;ruby&lt;/code&gt; and &lt;code&gt;ruby-dev&lt;/code&gt; packages; on debian:wheezy these are good enough for running the ci-runner. By removing the install-from-source commands we can get the image down to:&lt;/p&gt;

&lt;pre class=&#34;syntax bash wrap:true&#34;&gt;base-runner      latest      bb4e6306811d      About a minute ago   423.6 MB
&lt;/pre&gt;

&lt;p&gt;This now more than 50% less then the original, with a minimal amount of tweaking.&lt;/p&gt;

&lt;h1 id=&#34;pushing-even-further:ef76312dca5f3c1992b296f85b019da1&#34;&gt;Pushing Even Further&lt;/h1&gt;

&lt;p&gt;Normally I&amp;#8217;m not looking for an absolute minimal container. I just want to avoid excessive bloat, and some simple commands can usually go a long way. I also find it best to avoid packages in favor of pre-built binaries. As an example I do a lot of work with Scala, and have an sbt container for builds. If I were to install the SBT package from debian I&amp;#8217;d get a container weighing in at a few hundred megabytes. That&amp;#8217;s because the SBT package pulls in a lot of dependencies: java, for one. But if I already have a jre, all I really need is the sbt jar file and a bash script to launch. That considerably shrinks down the dockerfile size.&lt;/p&gt;

&lt;p&gt;When selecting a base image, it&amp;#8217;s important to realize what you&amp;#8217;re getting. A linux distribution is simply the linux kernel and an opinionated configuration of packages, tools and binaries. Ubuntu uses aptitude for package management, Fedora uses Yum. Centos 6 uses a specific version of the kernel, while version 7 uses another. You get one set of packages with Debian, another with Ubuntu. That&amp;#8217;s the power of specific communities: how frequently things are updated, how well they&amp;#8217;re maintained, and what you get out-of-box. A docker container jails a process to only see a specific part of the filesystem; specifically, the container&amp;#8217;s file system. Using a distribution ensures that the required libraries and support binaries are there, in place, where they should be. But major distributions aren&amp;#8217;t designed to run specific applications; their general-purposes servers that are designed to run a variety of apps and processes. If you want to run a single process there&amp;#8217;s a lot that comes with an OS you don&amp;#8217;t need.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s a re-emergence of lightweight linux distributions in the docker world first popularized with embedded systems. You&amp;#8217;re probably familiar with &lt;a href=&#34;https://registry.hub.docker.com/_/busybox/&#34;&gt;busybox&lt;/a&gt; useful for running one-off bash commands. Because of busybox&amp;#8217;s embedded roots, it&amp;#8217;s not quite intended for packages. &lt;a href=&#34;https://www.alpinelinux.org&#34;&gt;Alpine Linux&lt;/a&gt; is another alternative which features its own &lt;a href=&#34;https://registry.hub.docker.com/_/alpine/&#34;&gt;official registry&lt;/a&gt;. It&amp;#8217;s still very small, based on busybox, and has its own package system. I tried getting gitlab&amp;#8217;s ci-runner working with alpine, but unfortunately some of the ruby gems used by ci-runner require GNU packages which aren&amp;#8217;t available, and I didn&amp;#8217;t want to compile them manually. In terms of time/benefit, I can live with 400mb and move on to something else. For most things you can probably do a lot with Alpine and keep your containers really small: great for doing continuous deploys to a bunch of servers.&lt;/p&gt;

&lt;p&gt;The bottom line is know what you need. If you want a minimal container, build up, rather than slim down. You usually need the runtime for your application (if any), your app, and supporting dependencies. Know those dependencies, and avoid cruft when you can.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Deploying Docker Containers on CoreOS with the Fleet API</title>
          <link>http://blog.michaelhamrah.com/2015/03/deploying-docker-containers-on-coreos-with-the-fleet-api/</link>
          <pubDate>Tue, 17 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/deploying-docker-containers-on-coreos-with-the-fleet-api/</guid>
          <description>

&lt;p&gt;I&amp;#8217;ve been spending a lot more time with CoreOS in search of a docker-filled utopian PaaS dreams. I haven&amp;#8217;t found quite what I&amp;#8217;m looking for, but with some bash scripting, a little http, good tools and a lotta love I&amp;#8217;m coming close. There are no shortage of solutions to this problem, and honestly, nobody&amp;#8217;s really figured this out yet in an easy, fluid, turn-key type of way. You&amp;#8217;ve probably read about CoreOS, Mesos, Marathon, Kubernetes&amp;#8230; maybe even dug into Deis, Flynn, Shipyard. You&amp;#8217;ve spun up a cluster, and are like&amp;#8230; This is great, now what.&lt;/p&gt;

&lt;p&gt;What I want is to go from an app on my laptop to running in a production environment with minimal fuss. I don&amp;#8217;t want to re-invent the wheel; there are too many people solving this problem in a similar way. I like CoreOS because it provides a bare-bones docker runtime with a solid set of low-level tools. Plus, a lot of people I&amp;#8217;m close with have been using it, so the cross-pollination of ideas helps overcome some hurdles.&lt;/p&gt;

&lt;p&gt;One of these hurdles is how you launch containers on a cluster. I really like &lt;a href=&#34;https://github.com/mesosphere/marathon&#34;&gt;Marathon&amp;#8217;s&lt;/a&gt; http api for Mesos, but I also like the simplicity of CoreOS as a platform. CoreOS&amp;#8217;s distributed init system is &lt;a href=&#34;https://github.com/coreos/fleet&#34;&gt;Fleet&lt;/a&gt;, which leverages systemd for running a process on a CoreOS node (it doesn&amp;#8217;t have to be a container). It has some nice features, but having to constantly write similar systemd files and run fleetctl to manage containers is somewhat annoying.&lt;/p&gt;

&lt;p&gt;Turns out, &lt;a href=&#34;https://github.com/coreos/fleet/blob/master/Documentation/api-v1.md&#34;&gt;Fleet has an http API&lt;/a&gt;. It&amp;#8217;s not quite as nice as Marathon&amp;#8217;s; you can&amp;#8217;t easily scale to N number of instances, but it does come close. There are a few examples of using the API to launch containers, but I wanted a more end-to-end solution that eliminated boilerplate.&lt;/p&gt;

&lt;h2 id=&#34;activate-the-fleet-api:f1075c253a77011bd480830af8403bf8&#34;&gt;Activate the Fleet API&lt;/h2&gt;

&lt;p&gt;The Fleet API isn&amp;#8217;t enabled out-of-the-box. That makes sense as the API is currently unsecured, so you shouldn&amp;#8217;t enable it unless you have the proper VPC set up. &lt;a href=&#34;https://coreos.com/docs/launching-containers/config/fleet-deployment-and-configuration/&#34;&gt;CoreOS has good documentation on getting the API running&lt;/a&gt;. For a quick start you can drop the following yaml snippet into your cloudconfig&amp;#8217;s units section:&lt;/p&gt;

&lt;pre class=&#34;syntax yaml&#34;&gt;- name: fleet.socket
  drop-ins:
    - name: 30-ListenStream.conf
      content: |
        [Socket]
        ListenStream=8080
        Service=fleet.service
        [Install]
        WantedBy=sockets.target
&lt;/pre&gt;

&lt;h2 id=&#34;exploring-the-api:f1075c253a77011bd480830af8403bf8&#34;&gt;Exploring the API&lt;/h2&gt;

&lt;p&gt;With the API enabled, it&amp;#8217;s time to get to work. The &lt;a href=&#34;https://github.com/coreos/fleet/blob/master/Documentation/api-v1.md&#34;&gt;API has some simple documentation&lt;/a&gt; but offers enough to get started. I personally like the minimal approach, although I wish it was more feature-rich (it is v1, and better than nothing).&lt;/p&gt;

&lt;p&gt;You can do a lot with curl, bash and jq. First, let&amp;#8217;s see what&amp;#8217;s running. All these examples assume you have a FLEET_ENDPOINT environment variable set with the host and port:&lt;/p&gt;

&lt;p&gt;On a side note, environment variables are key to reuse the same functionality across environments. In my opinion, they aren&amp;#8217;t used nearly enough. Check out the &lt;a href=&#34;http://12factor.net/config&#34;&gt;twelve-factor app&amp;#8217;s config section&lt;/a&gt; to understand the importance of environment variables.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -s $FLEET_ENDPOINT/fleet/v1/units | jq &#39;.units[] | { name: .name, currentState: .currentState}&#39;
&lt;/pre&gt;

&lt;p&gt;Sure, you can get the same data by running &lt;code&gt;fleetctl list-units&lt;/code&gt;, but the http command doesn&amp;#8217;t involve ssh, which can be a plus if you have a protected network, are are running from an application or CI server.&lt;/p&gt;

&lt;h2 id=&#34;creating-containers:f1075c253a77011bd480830af8403bf8&#34;&gt;Creating Containers&lt;/h2&gt;

&lt;p&gt;Instead of crafting a fleet template and running &lt;code&gt;fleetctl start sometemplate&lt;/code&gt; , we want to launch new units via http. This involves PUTting a resource to the /units/ endpoint under the name of your unit (it&amp;#8217;s actually /fleet/v1/units, it took me forever to find the path prefix). The Fleet API will build a corresponding systemd unit from the json payload, and the content closely corresponds to what you can do with &lt;a href=&#34;https://coreos.com/docs/launching-containers/launching/fleet-unit-files/&#34;&gt;a fleet unit file&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The schema takes in a &lt;code&gt;desiredState&lt;/code&gt; and an array of &lt;code&gt;options&lt;/code&gt; which specify the &lt;code&gt;section&lt;/code&gt;, &lt;code&gt;name&lt;/code&gt;, and &lt;code&gt;value&lt;/code&gt; for each line. Most Fleet templates follow a similar pattern as exemplified with &lt;a href=&#34;https://coreos.com/docs/launching-containers/launching/launching-containers-fleet/&#34;&gt;the launching containers with Fleet guide&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Cleanup potentially running containers&lt;/li&gt;
&lt;li&gt;Pull the container&lt;/li&gt;
&lt;li&gt;Run the container&lt;/li&gt;
&lt;li&gt;Define X-Fleet parameters, like conflicts.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Again we&amp;#8217;ll use curl, but writing json on the command line is really annoying. So let&amp;#8217;s create a &lt;code&gt;unit.json&lt;/code&gt; for our payload defining the tasks for CoreOS&amp;#8217;s apache container:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;{
  &#34;desiredState&#34;: &#34;launched&#34;,
  &#34;options&#34;: [
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/usr/bin/docker kill %p-i%&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/usr/bin/docker rm %p-%i&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker pull coreos/%p&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStart&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker run --rm --name %pi-%i -p 80 coreos/%p /usr/sbin/apache2ctl -D FOREGROUND&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStop&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker stop %p-%i&#34;
    },
    {
      &#34;section&#34;: &#34;X-Fleet&#34;,
      &#34;name&#34;: &#34;Conflicts&#34;,
      &#34;value&#34;: &#34;%p@*.service&#34;
    }
  ]
}
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a couple of things of note in this snippet:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We&amp;#8217;re adding a &amp;#8220;-&amp;#8221; in front of the docker kill and docker rm commands of the ExecStartPre tasks. This tells to Fleet to continue if there&amp;#8217;s an error; these tasks are precautionary to remove an existing phantom container if it will conflict with the newly launched one.&lt;/li&gt;
&lt;li&gt;We&amp;#8217;re using Fleet&amp;#8217;s systemd placeholders %p and %i to replace actual values in our template with values from the template name. This provides a level of agnosticism in our template; we can easily reuse this template to launch different containers by changing the name. Unfortunately this doesn&amp;#8217;t quite work in our example because it&amp;#8217;s apache specific, but if you were running a container with an entry point command specified, it would work fine. You&amp;#8217;ll also want to manage containers under your own namespace, either in a private or public registry.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can launch this file with curl:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -d @unit.json -w &#34;%{http_code}&#34; -H &#39;Content-Type: application/json&#39; $FLEETCTL_ENDPOINT/fleet/v1/units/apache@1.service
&lt;/pre&gt;

&lt;p&gt;If all goes well you&amp;#8217;ll get back a &lt;code&gt;201 Created&lt;/code&gt; response. Try running the &lt;code&gt;list units&lt;/code&gt; curl command to see your container task.&lt;/p&gt;

&lt;p&gt;We can run &lt;code&gt;fleetctl cat apache@1&lt;/code&gt; to view the generated systemd unit:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;[Service]
ExecStartPre=-/usr/bin/docker kill %p-%I
ExecStartPre=-/usr/bin/docker rm %p-%i
ExecStartPre=/usr/bin/docker pull coreos/%p
ExecStart=/usr/bin/docker run --rm --name %pi-%i -p 80 coreos/%p /usr/sbin/apache2ctl -D FOREGROUND
ExecStop=/usr/bin/docker stop %p-%i

[X-Fleet]
Conflicts=%p@*.service
&lt;/pre&gt;

&lt;p&gt;Want to launch a second task? Just post again, but change the instance number from 1 to 2:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -d @unit.json -w &#34;%{http_code}&#34; -H &#39;Content-Type: application/json&#39; $FLEETCTL_ENDPOINT/fleet/v1/units/apache@2.service
&lt;/pre&gt;

&lt;p&gt;When you&amp;#8217;re done with your container, you can simple issue a delete command to tear it down:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X DELETE -w &#34;%{http_code}&#34; $FLEET_ENDPOINT/fleet/v1/units/apache@1.service
&lt;/pre&gt;

&lt;h2 id=&#34;deploying-new-versions:f1075c253a77011bd480830af8403bf8&#34;&gt;Deploying New Versions&lt;/h2&gt;

&lt;p&gt;Launching individual containers is great, but for continuous delivery, you need deploy new versions with no downtime. The example above used systemd&amp;#8217;s placeholders for providing the name of the container, but left the apache commands in place. Let&amp;#8217;s use another CoreOS example container from the &lt;a href=&#34;https://coreos.com/blog/zero-downtime-frontend-deploys-vulcand/&#34;&gt;zero downtime frontend deploys&lt;/a&gt; blog post. This &lt;code&gt;coreos/example&lt;/code&gt; container uses an entrypoint and tagged docker versions to go from a v1 to a v2 version of the app. Instead of creating multiple, similar, fleet unit files like that blog post, can we make an agnostic http call that works across versions? Yes we can.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s conceptually figure out how this would work. We don&amp;#8217;t want to change the json payload across versions, so the body must be static. We could use some form of templating or find-and-replace, but let&amp;#8217;s try and avoid that complexity for now. Can we make due with the options provided us? We know that the %p parameter lets us pass in the template name to our body. So if we can specify the name and version of the container we want to launch in the name of the unit file we PUT, we&amp;#8217;re good to go.&lt;/p&gt;

&lt;p&gt;So we want to:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -d @unit.json -w &#34;%{http_code&#34;} -H &#39;Content-Type: application/json&#39; $FLEETCTL_ENDPOINT/fleet/v1/units/example:1.0.0@1.service
&lt;/pre&gt;

&lt;p&gt;I tried this with the above snippet, but replaced the pull and run commands above with the following:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;{
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStart&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker run --rm --name %p-%i -p 80 coreos/%p&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStop&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker stop %p-%i&#34;
    },
&lt;/pre&gt;

&lt;p&gt;Unfortunately, this didn&amp;#8217;t work because the colon, :, in example:1.0.0 make the name invalid for a container. I could forego the name, but then I wouldn&amp;#8217;t be able to easily stop, kill or rm the container. So we need to massage the %p parameter a little bit. Luckily, bash to the rescue.&lt;/p&gt;

&lt;p&gt;Unfortunately, systemd is a little wonky when it comes to scripting in a unit file. It&amp;#8217;s relatively hard to create and access environment variables, you need fully-qualified paths, and multiple lines for arbitrary scripts are discouraged. After googling how exactly to do bash scripting in a systemd file, or why an environment variable wasn&amp;#8217;t being set, I began to understand the frustration in the community on popular distros switching to systemd. But we can still make do with what we have by launching a &lt;code&gt;/bin/bash&lt;/code&gt; command instead of the vanilla &lt;code&gt;/usr/bin/docker&lt;/code&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;{
  &#34;desiredState&#34;: &#34;launched&#34;,
  &#34;options&#34;: [
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker kill $APP-%i\&#34;&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;-/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker rm $APP-%i\&#34;&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStartPre&#34;,
      &#34;value&#34;: &#34;/usr/bin/docker pull coreos/%p&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStart&#34;,
      &#34;value&#34;: &#34;/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker run --name $APP-%i -h $APP-%i -p 80 --rm coreos/%p\&#34;&#34;
    },
    {
      &#34;section&#34;: &#34;Service&#34;,
      &#34;name&#34;: &#34;ExecStop&#34;,
      &#34;value&#34;: &#34;/bin/bash -c \&#34;APP=`/bin/echo %p | sed &#39;s/:/-/&#39;`; /usr/bin/docker stop $APP-%i&#34;
    },
    {
      &#34;section&#34;: &#34;X-Fleet&#34;,
      &#34;name&#34;: &#34;Conflicts&#34;,
      &#34;value&#34;: &#34;%p@*.service&#34;
    }
  ]
}
&lt;/pre&gt;

&lt;p&gt;and we can submit with:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X PUT -d @unit.json -H &#39;Content-Type: application/json&#39;  $FLEET_ENDPOINT/fleet/v1/units/example:1.0.0@1.service
&lt;/pre&gt;

&lt;p&gt;More importantly, we can easily launch multiple containers of version two simultaneously:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X PUT -d @unit.json -H &#39;Content-Type: application/json&#39;  $FLEET_ENDPOINT/fleet/v1/units/example:2.0.0@1.service
curl -X PUT -d @unit.json -H &#39;Content-Type: application/json&#39;  $FLEET_ENDPOINT/fleet/v1/units/example:2.0.0@2.service
&lt;/pre&gt;

&lt;p&gt;and then destroy version one:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -X DELETE -w &#34;%{http_code}&#34; $FLEET_ENDPOINT/fleet/v1/units/example:1.0.0@1.service
&lt;/pre&gt;

&lt;h2 id=&#34;more-jq-and-bash-fun:f1075c253a77011bd480830af8403bf8&#34;&gt;More jq and bash fun&lt;/h2&gt;

&lt;p&gt;Let&amp;#8217;s say you do start multiple containers, and you want to cycle them out and delete them. In our above example, we&amp;#8217;ve started two containers. How will we easily go from v2 to v3, and remove the v3 nodes? The marathon API has a simple &amp;#8220;scale&amp;#8221; button which does just that. Can we do the same for CoreOS? Yes we can.&lt;/p&gt;

&lt;p&gt;Conceptually, let&amp;#8217;s think about what we want. We want to select all containers running a specific version, grab the full unit file name, and then curl a DELETE operation to that endpoint. We can use the Fleet API to get our information, jq to parse the response, and the bash pipe operator with xargs to call our curl command.&lt;/p&gt;

&lt;p&gt;Stringing this together like so:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl -s $FLEET_ENDPOINT/fleet/v1/units | jq &#39;.units[] | .name | select(startswith(&#34;example:1.0.0&#34;))&#39; | xargs -t -I{} curl -s -X DELETE $FLEET_ENDPOINT/fleet/v1/units/{}
&lt;/pre&gt;

&lt;p&gt;jq provides some very powerful json processing. We are pulling out the name field, and only selecting elements which start with our specific app and version, and then piping that to xargs. The -I{} flag for xargs is a substitution trick I learned. This allows you to do string placements rather than pass the field as an argument.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:f1075c253a77011bd480830af8403bf8&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I can pretty much guarantee no matter what you pick to run your Docker PaaS, it won&amp;#8217;t do exactly what you want. I can also guarantee that there will be a lot to learn: new apis, new commands, new tools. It&amp;#8217;s going to feel like pushing a round peg in a square hole. But that&amp;#8217;s okay; part of the experience is formulating opinions on how you want things to work. It&amp;#8217;s a blend of learning the patterns and practices of a tool versus configuring it to work the way you want. Always remember a few things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Keep It Simple&lt;/li&gt;
&lt;li&gt;Think about how it should work conceptually&lt;/li&gt;
&lt;li&gt;You can do a lot with command line.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With an API-enabled CoreOS cluster, you can easily plug deployment of containers to whatever build flow you use: your laptop, a github web hook, jenkins, or whatever flow you wish. Because all the above commands are bash, you can replace any part with a bash variable and execute appropriately. This makes parameterizing these commands into functions easy.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Running Consul on CoreOS</title>
          <link>http://blog.michaelhamrah.com/2014/11/running-consul-on-coreos/</link>
          <pubDate>Sat, 29 Nov 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/11/running-consul-on-coreos/</guid>
          <description>&lt;p&gt;I&amp;#8217;m a big fan of &lt;a href=&#34;consul.io&#34;&gt;Consul&lt;/a&gt;, Hashicorp&amp;#8217;s service discovery tool. I&amp;#8217;ve also become a fan of CoreOS, the cluster framework for running docker containers. Even though CoreOS comes with etcd for service discovery I find the feature set of Consul more compelling. And as a programmer I know I can have my cake and eat it too.&lt;/p&gt;

&lt;p&gt;My first take was to modify my &lt;a href=&#34;github.com/mhamrah/ansible-consul&#34;&gt;ansible-consul&lt;/a&gt; fork to run consul natively on CoreOS. Although this could work I find it defeats CoreOS&amp;#8217;s container-first approach with &lt;a href=&#34;https://github.com/coreos/fleet&#34;&gt;fleet&lt;/a&gt;. Jeff Lindsay created &lt;a href=&#34;https://github.com/progrium/docker-consul&#34;&gt;a consul docker container&lt;/a&gt; which does the job well. I created two fleet service files: one for launching the consul container and another for service discovery. At first the service discovery aspect seemed weird; I tried to pass ip addresses via the &amp;#8211;join parameter or use &lt;code&gt;ExecStartPost&lt;/code&gt; for running the join command. However I took a cue from the CoreOS cluster setup: sometimes you need a third party to get stuff done. In this case we the built in etcd server to manage the join ip address to kickstart the consul cluster.&lt;/p&gt;

&lt;p&gt;The second fleet service file acts as a sidekick:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For every running consul service there&amp;#8217;s a sidekick process&lt;/li&gt;
&lt;li&gt;The sidekick process writes the current IP to a key only if that key doesn&amp;#8217;t exist&lt;/li&gt;
&lt;li&gt;The sidekick process uses the value of that key to join the cluster with &lt;code&gt;docker exec&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The sidekick process removes the key if the consul service dies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two service files are below, but you should tweak for your needs.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You only need a 3 or 5 node server cluster. If your CoreOS deployment is large, use some form of restriction for the server nodes. You can do the same for the client nodes.&lt;/li&gt;
&lt;li&gt;The discovery script could be optimized. It will try and join whatever ip address is listed in the key. This avoids a few split brain scenarios, but needs to be tested.&lt;/li&gt;
&lt;li&gt;If you want DNS to work properly you need to set some Docker daemon options. Read the docker-consul README.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Accessing the Docker Host Server Within a Container</title>
          <link>http://blog.michaelhamrah.com/2014/06/accessing-the-docker-host-server-within-a-container/</link>
          <pubDate>Sun, 29 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/accessing-the-docker-host-server-within-a-container/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/userguide/dockerlinks/#working-with-links-names&#34;&gt;Docker links&lt;/a&gt; are a great way to link two containers together but sometimes you want to know more about the host and network from within a container. You have a couple of options:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can access the Docker host by the container&amp;#8217;s gateway.&lt;/li&gt;
&lt;li&gt;You can access the Docker host by its ip address from within a container.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-gateway-approach:d6e669ce9206f072ff43b8fdf5b03a0c&#34;&gt;The Gateway Approach&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dotcloud/docker/issues/1143&#34;&gt;This GitHub Issue&lt;/a&gt; outlines the solution. Essentially you&amp;#8217;re using netstat to parse the gateway the docker container uses to access the outside world. This is the docker0 bridge on the host.&lt;/p&gt;

&lt;p&gt;As an example, we&amp;#8217;ll run a simple docker container which returns the hostname of the container on port 8080:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run -d -p 8080:8080 mhamrah/mesos-sample
&lt;/pre&gt;

&lt;p&gt;Next we&amp;#8217;ll run /bin/bash in another container to do some discovery:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run -i -t ubuntu /bin/bash
#once in, install curl:
apt-get update
apt-get install -y curl
&lt;/pre&gt;

&lt;p&gt;We can use the following command to pull out the gateway from netstat:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;netstat -nr | grep &#39;^0\.0\.0\.0&#39; | awk &#39;{print $2}&#39;
#returns 172.17.42.1 for me.
&lt;/pre&gt;

&lt;p&gt;We can then curl our other docker container, and we should get that docker container&amp;#8217;s hostname:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl 172.17.42.1:8080
# returns 00b019ce188c
&lt;/pre&gt;

&lt;p&gt;Nothing exciting, but you get the picture: it doesn&amp;#8217;t matter that the service is inside another container, we&amp;#8217;re accessing it via the host, and we didn&amp;#8217;t need to use links. We just needed to know the port the other service was listening on. If you had a service running on some other port&amp;#8211;say Postgres on 5432&amp;#8211;not running in a Docker container&amp;#8211;you can access it via &lt;code&gt;172.17.42.1:5432&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have docker installed in your container you can also query the docker host:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# In a container with docker installed list other containers running on the host for other containers:
docker -H tcp://172.17.42.1:2375 ps
CONTAINER ID        IMAGE                         COMMAND                CREATED              STATUS              PORTS                     NAMES
09d035054988        ubuntu:14.04                  /bin/bash              About a minute ago   Up About a minute   0.0.0.0:49153-&gt;8080/tcp   angry_bardeen
00b019ce188c        mhamrah/mesos-sample:latest   /opt/delivery/bin/de   8 minutes ago        Up 8 minutes        0.0.0.0:8080-&gt;8080/tcp    suspicious_colden
&lt;/pre&gt;

&lt;p&gt;You can use this for some hakky service-discovery.&lt;/p&gt;

&lt;h2 id=&#34;the-ip-approach:d6e669ce9206f072ff43b8fdf5b03a0c&#34;&gt;The IP Approach&lt;/h2&gt;

&lt;p&gt;The gateway approach is great because you can figure out a way to access a host from entirely within a container. You also have the same access via the host&amp;#8217;s ip address. I&amp;#8217;m using boot2docker, and the boot2docker ip address is &lt;code&gt;192.168.59.103&lt;/code&gt; and I can accomplish the same tasks as the gateway approach:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Docker processes, via ip:
docker -H tcp://192.168.59.103:2375 ps
# Other docker containers, via ip:
curl 192.168.59.103:8080
&lt;/pre&gt;

&lt;p&gt;Although there&amp;#8217;s no way to introspect the host&amp;#8217;s ip address (AFAIK) you can pass this in via an environment variable:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker@boot2docker:~$  docker run -i -t -e DOCKER_HOST=192.168.59.103 ubuntu /bin/bash
root@07561b0607f4:/# env
HOSTNAME=07561b0607f4
DOCKER_HOST=192.168.59.103
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PWD=/
&lt;/pre&gt;

&lt;p&gt;If the container knows the ip address of its host, you can broadcast this out to other services via the container&amp;#8217;s application. Useful for service discovery tools run from within a container where you want to tell others the host IP so others can find you.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Service Discovery Options with Marathon and Deimos</title>
          <link>http://blog.michaelhamrah.com/2014/06/service-discovery-options-with-marathon-and-deimos/</link>
          <pubDate>Sun, 29 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/service-discovery-options-with-marathon-and-deimos/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve become a fan of Mesos and Marathon: combined with &lt;a href=&#34;https://github.com/mesosphere/deimos&#34;&gt;Deimos&lt;/a&gt; you can create a DIY PaaS for launching and scaling Docker containers across a number of nodes. Marathon supports a bare-bones service-discovery mechanism through its task API, but it would be nice for containers to register themselves with some service discovery tool themselves. In order to achieve this containers need to know their host ip address and the port Marathon assigned them so they could tell other interested services where they can be found.&lt;/p&gt;

&lt;p&gt;Deimos allows default parameters to be passed in when executing &lt;code&gt;docker run&lt;/code&gt; and Marathon adds assigned ports to a container&amp;#8217;s environment variables. If a container has this information it can register it with a service discovery tool.&lt;/p&gt;

&lt;p&gt;Here we assign the host&amp;#8217;s IP address as a default run option in our &lt;a href=&#34;https://github.com/mesosphere/deimos/#configuration&#34;&gt;Deimos config file&lt;/a&gt;.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#/etc/deimos.cfg
[containers.options]
append: [&#34;-e&#34;, &#34;HOST_IP=192.168.33.12&#34;]
&lt;/pre&gt;

&lt;p&gt;Now let&amp;#8217;s launch our mesos-sample container to our Mesos cluster via Marathon:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;// Post to http://192.168.33.12/v2/apps
{
  &#34;container&#34;: {
    &#34;image&#34;: &#34;docker:///mhamrah/mesos-sample&#34;
  },
  &#34;cpus&#34;: &#34;1&#34;,
  &#34;id&#34;: &#34;www&#34;,
  &#34;instances&#34;: 1,
  &#34;mem&#34;: 512,
  &#34;ports&#34;: [0],
  &#34;uris&#34;: [],
  &#34;cmd&#34;: &#34;&#34;
}
&lt;/pre&gt;

&lt;p&gt;Once our app is launch, we can inspect all the environment variables in our container with the &lt;code&gt;/env&lt;/code&gt; endpoint from &lt;code&gt;mhamrah/mesos-sample&lt;/code&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;curl http://192.168.33.12:31894/env
[ {
  &#34;HOSTNAME&#34; : &#34;a4305981619d&#34;
}, {
  &#34;PORT0&#34; : &#34;31894&#34;
}, {
  &#34;PATH&#34; : &#34;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&#34;
}, {
  &#34;PWD&#34; : &#34;/tmp/mesos-sandbox&#34;
}, {
  &#34;PORTS&#34; : &#34;31894&#34;
}, {
  &#34;HOST_IP&#34; : &#34;192.168.33.12&#34;
}, {
  &#34;PORT&#34; : &#34;31894&#34;
}]
&lt;/pre&gt;

&lt;p&gt;With this information some startup script could use the &lt;code&gt;PORT&lt;/code&gt; (or &lt;code&gt;PORT0&lt;/code&gt;) and &lt;code&gt;HOST_IP&lt;/code&gt; to register itself for direct point-to-point communication in a cluster.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Akka Clustering with SBT-Docker and SBT-Native-Packager</title>
          <link>http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/</link>
          <pubDate>Thu, 19 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/</guid>
          <description>

&lt;p&gt;Since my last post on &lt;a href=&#34;http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/&#34;&gt;akka clustering with docker containers&lt;/a&gt; a new plugin, &lt;a href=&#34;https://github.com/marcuslonnberg/sbt-docker&#34;&gt;SBT-Docker&lt;/a&gt;, has emerged which allows you to build docker containers directly from SBT. I&amp;#8217;ve updated my &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;akka-docker-cluster-example&lt;/a&gt; to leverage these two plugins for a smoother docker build experience.&lt;/p&gt;

&lt;h2 id=&#34;one-step-build:9dc58615474f52923afa41a9d5040e47&#34;&gt;One Step Build&lt;/h2&gt;

&lt;p&gt;The approach is basically the same as the previous example: we use SBT Native Packager to gather up the appropriate dependencies, upload them to the docker container, and create the entrypoint. I decided to keep the start script approach to &amp;#8220;prep&amp;#8221; any environment variables required before launching. With SBT Docker linked to Native Packager all you need to do is fire&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;docker
&lt;/pre&gt;

&lt;p&gt;from sbt and you have a docker container ready to launch or push.&lt;/p&gt;

&lt;h2 id=&#34;understanding-the-build:9dc58615474f52923afa41a9d5040e47&#34;&gt;Understanding the Build&lt;/h2&gt;

&lt;p&gt;SBT Docker requires a dockerfile defined in your build. I want to pass in artifacts from native packager to docker. This allows native packager to focus on application needs while docker is focused on docker. Docker turns into just another type of package for your app.&lt;/p&gt;

&lt;p&gt;We can pass in arguments by mapping the appropriate parameters to a function which returns the Dockerfile. In build.spt:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;// Define a dockerfile, using parameters from native-packager
dockerfile in docker &amp;lt;&amp;lt;= (name, stagingDirectory in Universal) map {
  case(appName, stageDir) =&gt;
    val workingDir = s&#34;/opt/${appName}&#34;
    new Dockerfile {
      //use java8 base box
      from(&#34;relateiq/oracle-java8&#34;)
      maintainer(&#34;Michael Hamrah&#34;)
      //expose our akka port
      expose(1600)
      //upload native-packager staging directory files
      add(stageDir, workingDir)
      //make files executable
      run(&#34;chmod&#34;, &#34;+x&#34;, s&#34;/opt/${appName}/bin/${appName}&#34;)
      run(&#34;chmod&#34;, &#34;+x&#34;, s&#34;/opt/${appName}/bin/start&#34;)
      //set working directory
      workDir(workingDir)
      //entrypoint into our start script
      entryPointShell(s&#34;bin/start&#34;, appName, &#34;$@&#34;)
    }
}
&lt;/pre&gt;

&lt;h3 id=&#34;linking-sbt-docker-to-sbt-native-packager:9dc58615474f52923afa41a9d5040e47&#34;&gt;Linking SBT Docker to SBT Native Packager&lt;/h3&gt;

&lt;p&gt;Because we&amp;#8217;re relying on Native Packager to assemble our runtime dependencies we need to ensure the native packager files are &amp;#8220;staged&amp;#8221; before docker tries to upload them. Luckily it&amp;#8217;s easy to create dependencies with SBT. We simply have docker depend on the native packager&amp;#8217;s stage task:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;docker &amp;lt;&amp;lt;= docker.dependsOn(com.typesafe.sbt.packager.universal.Keys.stage.in(Compile))
&lt;/pre&gt;

&lt;h3 id=&#34;adding-additional-files:9dc58615474f52923afa41a9d5040e47&#34;&gt;Adding Additional Files&lt;/h3&gt;

&lt;p&gt;The last step is to add our start script to the native packager build. Native packager has a &lt;code&gt;mappings&lt;/code&gt; key where we can add files to our package. I kept the start script in the docker folder and I want it in the bin directory within the docker container. Here&amp;rsquo;s the mapping:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;mappings in Universal += baseDirectory.value / &#34;docker&#34; / &#34;start&#34; -&gt; &#34;bin/start&#34;
&lt;/pre&gt;

&lt;p&gt;With this setting everything will be assembled as needed and we can package to any type we want. Setting up a cluster with docker is &lt;a href=&#34;http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/&#34;&gt;the same as before&lt;/a&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run --name seed -i -t clustering
docker run --name c1 -link seed:seed -i -t clustering
&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s interesting to note SBT Native Packager also has docker support, but it&amp;rsquo;s undocumented and doesn&amp;rsquo;t allow granular control over the Dockerfile output. Until SBT Native Packager fully supports docker output the SBT Docker plugin is a nice tool to package your sbt-based apps.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Running an Akka Cluster with Docker Containers</title>
          <link>http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/</link>
          <pubDate>Sun, 23 Mar 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/</guid>
          <description>

&lt;p&gt;&lt;strong&gt;Update! You can now use SBT-Docker with SBT-Native Packager for a better sbt/docker experience. &lt;a href=&#34;http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/&#34;&gt;Here&amp;#8217;s the new approach&lt;/a&gt; with an &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;updated GitHub repo&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We recently upgraded our vagrant environments to use &lt;a href=&#34;http://docker.io&#34;&gt;docker&lt;/a&gt;. One of our projects relies on &lt;a href=&#34;http://doc.akka.io/docs/akka/2.3.0/common/cluster.html&#34;&gt;akka&amp;#8217;s cluster functionality&lt;/a&gt;. I wanted to easily run an akka cluster locally using docker as sbt can be somewhat tedious. &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;The example project is on github&lt;/a&gt; and the solution is described below.&lt;/p&gt;

&lt;p&gt;The solution relies on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;Sbt Native Packager&lt;/a&gt; to package dependencies and create a startup file.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library for configuring the app&amp;#8217;s ip address and seed nodes. We setup cascading configurations that will look for docker link environment variables if present.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example/blob/master/bin/dockerize&#34;&gt;A simple bash script&lt;/a&gt; to package the app and build the docker container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;#8217;m a big fan of &lt;a href=&#34;http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library and the environment variable overrides come in handy for providing sensible defaults with optional overrides. It&amp;#8217;s the preferred way we configure our applications in upper environments.&lt;/p&gt;

&lt;p&gt;The tricky part of running an akka cluster with docker is knowing the ip address each remote node needs to listen on. An akka cluster relies on each node listening on a specific port and hostname or ip. It also needs to know the port and hostname/ip of a seed node the cluster. As there&amp;#8217;s no catch-all binding we need specific ip settings for our cluster.&lt;/p&gt;

&lt;p&gt;A simple bash script within the container will figure out the current IP for our cluster configuration and &lt;a href=&#34;http://docs.docker.io/en/latest/use/working_with_links_names/&#34;&gt;docker links&lt;/a&gt; pass seed node information to newly launched nodes.&lt;/p&gt;

&lt;h2 id=&#34;first-step-setup-application-configuration:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;First Step: Setup Application Configuration&lt;/h2&gt;

&lt;p&gt;The configuration is the same as that of a normal cluster, but I&amp;#8217;m using substitution to configure the ip address, port and seed nodes for the application. For simplicity I setup a &lt;code&gt;clustering&lt;/code&gt; block with defaults for running normally and environment variable overrides:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;clustering {
 ip = &#34;127.0.0.1&#34;
 ip = ${?CLUSTER_IP}
 port = 1600
 port = ${?CLUSTER_PORT}
 seed-ip = &#34;127.0.0.1&#34;
 seed-ip = ${?CLUSTER_IP}
 seed-ip = ${?SEED_PORT_1600_TCP_ADDR}
 seed-port = 1600
 seed-port = ${?SEED_PORT_1600_TCP_PORT}
 cluster.name = clustering-cluster
}

akka.remote {
    log-remote-lifecycle-events = on
    netty.tcp {
      hostname = ${clustering.ip}
      port = ${clustering.port}
    }
  }
  cluster {
    seed-nodes = [
       &#34;akka.tcp://&#34;${clustering.cluster.name}&#34;@&#34;${clustering.seed-ip}&#34;:&#34;${clustering.seed-port}
    ]
    auto-down-unreachable-after = 10s
  }
}
&lt;/pre&gt;

&lt;p&gt;As an example the &lt;code&gt;clustering.seed-ip&lt;/code&gt; setting will use &lt;em&gt;127.0.0.1&lt;/em&gt; as the default. If it can find a _CLUSTER&lt;em&gt;IP&lt;/em&gt; or a &lt;em&gt;SEED_PORT_1600_TCP_ADDR&lt;/em&gt; override it will use that instead. You&amp;#8217;ll notice the latter override is using docker&amp;#8217;s environment variable pattern for linking: that&amp;#8217;s how we set the cluster&amp;#8217;s seed node when using docker. You don&amp;#8217;t need the _CLUSTER&lt;em&gt;IP&lt;/em&gt; in this example but that&amp;#8217;s the environment variable we use in upper environments and I didn&amp;#8217;t want to change our infrastructure to conform to docker&amp;#8217;s pattern. The cascading settings are helpful if you&amp;#8217;re forced to follow one pattern depending on the environment. We do the same thing for the ip and port of the current node when launched.&lt;/p&gt;

&lt;p&gt;With this override in place we can use substitution to set the seed nodes in the akka cluster configuration block. The expression &lt;code&gt;&amp;quot;akka.tcp://&amp;quot;${clustering.cluster.name}&amp;quot;@&amp;quot;${clustering.seed-ip}&amp;quot;:&amp;quot;${clustering.seed-port}&lt;/code&gt; builds the proper akka URI so the current node can find the seed node in the cluster. Seed nodes avoid potential split-brain issues during network partitions. You&amp;#8217;ll want to run more than one in production but for local testing one is fine. On a final note the cluster-name setting is arbitrary. Because the name of the actor system and the uri must match I prefer not to hard code values in multiple places.&lt;/p&gt;

&lt;p&gt;I put these settings in resources/reference.conf. We could have named this file application.conf, but I prefer bundling configurations as reference.conf and reserving application.conf for external configuration files. A setting in application.conf will override a corresponding reference.conf setting and you probably want to manage application.conf files outside of the project&amp;#8217;s jar file.&lt;/p&gt;

&lt;h2 id=&#34;second-sbt-native-packager:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Second: SBT Native Packager&lt;/h2&gt;

&lt;p&gt;We use the native packager plugin to build a runnable script for our applications. For docker we just need to run &lt;code&gt;universal:stage&lt;/code&gt;, creating a folder with all dependencies in the &lt;code&gt;target/&lt;/code&gt; folder of our project. We&amp;#8217;ll move this into a staging directory for uploading to the docker container.&lt;/p&gt;

&lt;h2 id=&#34;third-the-dockerfile-and-start-script:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Third: The Dockerfile and Start script&lt;/h2&gt;

&lt;p&gt;The dockerfile is pretty simple:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;FROM dockerfile/java

MAINTAINER Michael Hamrah m@hamrah.com

ADD tmp/ /opt/app/
ADD start /opt/start
RUN chmod +x /opt/start

EXPOSE 1600

ENTRYPOINT [ &#34;/opt/start&#34; ]
&lt;/pre&gt;

&lt;p&gt;We start with Dockerfile&amp;#8217;s java base image. We then upload our staging &lt;code&gt;tmp/&lt;/code&gt; folder which has our application from sbt&amp;#8217;s native packager output and a corresponding executable start script described below. I opted for &lt;code&gt;ENTRYPOINT&lt;/code&gt; instead of &lt;code&gt;CMD&lt;/code&gt; so the container is treated like an executable. This makes it easier to pass in command line arguments into the sbt native packager script in case you want to set java system properties or override configuration settings via command line arguments.&lt;/p&gt;

&lt;p&gt;The start script is how we tee up the container&amp;#8217;s IP address for our cluster application:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

CLUSTER_IP=&lt;code&gt;/sbin/ifconfig eth0 | grep &#39;inet addr:&#39; | cut -d: -f2 | awk &#39;{ print $1}&#39;&lt;/code&gt; /opt/app/bin/clustering $@
&lt;/pre&gt;

&lt;p&gt;The script sets an inline environment variable by parsing &lt;code&gt;ifconfig&lt;/code&gt; output to get the container&amp;#8217;s ip. We then run the &lt;em&gt;clustering&lt;/em&gt; start script produced from sbt native packager. The &lt;code&gt;$@&lt;/code&gt; lets us pass along any command line settings set when launching the container into the sbt native packager script.&lt;/p&gt;

&lt;h2 id=&#34;fourth-putting-it-together:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Fourth: Putting It Together&lt;/h2&gt;

&lt;p&gt;The last part is a simple bash script named &lt;code&gt;dockerize&lt;/code&gt; to orchestrate each step. By running this script we run sbt native packager, move files to a staging directory, and build the container:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

echo &#34;Build docker container&#34;

#run sbt native packager
sbt universal:stage

#cleanup stage directory
rm -rf docker/tmp/

#copy output into staging area
cp -r target/universal/stage/ docker/tmp/

#build the container, remove intermediate nodes
docker build -rm -t clustering docker/

#remove staged files
rm -rf docker/tmp/
&lt;/pre&gt;

&lt;p&gt;With this in place we simply run&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;bin/dockerize
&lt;/pre&gt;

&lt;p&gt;to create our docker container named clustering.&lt;/p&gt;

&lt;h2 id=&#34;running-the-application-within-docker:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Running the Application within Docker&lt;/h2&gt;

&lt;p&gt;With our clustering container built we fire up our first instance. This will be our seed node for other containers:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -i -t -name seed clustering
2014-03-23 00:20:39,918 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:20:40,392 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:20:40,403 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:20:40,418 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:20:41,404 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
&lt;/pre&gt;

&lt;p&gt;Next we fire up a second node. Because of our reference.conf defaults all we need to do is link this container with the name &lt;em&gt;seed&lt;/em&gt;. Docker will set the environment variables we are looking for in the bundled reference.conf:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -name c1 -link seed:seed -i -t clustering
2014-03-23 00:22:49,332 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:22:49,788 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:22:49,797 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:22:50,238 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
2014-03-23 00:22:50,249 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:22:50,803 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.3:1600
&lt;/pre&gt;

&lt;p&gt;You&amp;#8217;ll see the current leader discovering new nodes and the appropriate broadcast messages sent out. We can even do this a third time and all nodes will react:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -name c2 -link seed:seed -i -t clustering
2014-03-23 00:24:52,768 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:24:53,224 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:24:53,235 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:24:53,470 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
2014-03-23 00:24:53,472 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.3:1600
2014-03-23 00:24:53,478 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:24:55,401 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.4:1600
&lt;/pre&gt;

&lt;p&gt;Try killing a node and see what happens!&lt;/p&gt;

&lt;h2 id=&#34;modifying-the-docker-start-script:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Modifying the Docker Start Script&lt;/h2&gt;

&lt;p&gt;There&amp;#8217;s another reason for the docker start script: it opens the door for different seed discovery options. Container linking works well if everything is running on the same host but not when running on multiple hosts. Also setting multiple seed nodes via docker links will get tedious via environment variables; it&amp;#8217;s doable but we&amp;#8217;re getting into coding-cruft territory. It would be better to discover seed nodes and set that configuration via command line parameters when launching the app.&lt;/p&gt;

&lt;p&gt;The start script gives us control over how we discover information. We could use &lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;etcd&lt;/a&gt;, &lt;a href=&#34;http://www.serfdom.io/&#34;&gt;serf&lt;/a&gt; or even zookeeper to manage how seed nodes are set and discovered, passing this to our application via environment variables or additional command line parameters. Seed nodes can easily be set via system properties set via the command line:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;-Dakka.cluster.seed-nodes.0=akka.tcp://ClusterSystem@host1:2552
-Dakka.cluster.seed-nodes.1=akka.tcp://ClusterSystem@host2:2552
&lt;/pre&gt;

&lt;p&gt;The start script can probably be configured via sbt native packager but I haven&amp;#8217;t looked into that option. Regardless this approach is (relatively) straight forward to run akka clusters with docker. The &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;full project is on github&lt;/a&gt;. If there&amp;#8217;s a better approach I&amp;#8217;d love to know!&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
