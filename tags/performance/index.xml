<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> </title>
    <link>http://blog.michaelhamrah.com/tags/performance/</link>
    <language>en-us</language>
    <author>Michael Hamrah</author>
    <rights>(C) 2015</rights>
    <updated>2013-10-12 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Overview on Web Performance and Scalability</title>
          <link>http://blog.michaelhamrah.com/2013/10/overview-on-web-performance-and-scalability/</link>
          <pubDate>Sat, 12 Oct 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/10/overview-on-web-performance-and-scalability/</guid>
          <description>&lt;p&gt;I recently gave a talk to some junior developers on performance and scalability. The talk is relatively high-level, providing an overview of non-programming topics which are important for performance and scalability. The &lt;a href=&#34;http://michaelhamrah.com/perf/#/&#34;&gt;original deck is here&lt;/a&gt; and on &lt;a href=&#34;https://speakerdeck.com/mhamrah/things-to-know-about-web-performance&#34;&gt;speaker deck&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A few months ago I also &lt;a href=&#34;http://michaelhamrah.com/spdy/&#34;&gt;gave a talk on spdy&lt;/a&gt; which is also on &lt;a href=&#34;https://speakerdeck.com/mhamrah/intro-to-spdy&#34;&gt;speaker deck&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>SPDY Slide Deck</title>
          <link>http://blog.michaelhamrah.com/2013/04/spdy-slide-deck/</link>
          <pubDate>Sun, 14 Apr 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/04/spdy-slide-deck/</guid>
          <description>&lt;p&gt;I recently gave a talk on &lt;a href=&#34;http://www.chromium.org/spdy&#34;&gt;SPDY&lt;/a&gt;, the new protocol which will serve as the foundation for HTTP 2.0. SPDY introduces some interesting features to solve current limitations with how HTTP 1.1 sits on top of TCP. &lt;a href=&#34;http://www.michaelhamrah.com/spdy/&#34;&gt;Check out the deck for a high-level overview, with links.&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Scalability comparison of WordPress with NGINX/PHP-FCM and Apache on an ec2-micro instance.</title>
          <link>http://blog.michaelhamrah.com/2013/03/scalability-comparison-of-wordpress-with-nginxphp-fcm-and-apache-on-an-ec2-micro-instance/</link>
          <pubDate>Sun, 17 Mar 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/03/scalability-comparison-of-wordpress-with-nginxphp-fcm-and-apache-on-an-ec2-micro-instance/</guid>
          <description>&lt;p&gt;For the past few years this blog ran apache + mod_php on an ec2-micro instance. It was time for a change; I&amp;#8217;ve enjoyed using nginx in other projects and thought I could get more out of my micro server. I went with a php-fpm/nginx combo and am very surprised with the results. The performance charts are below; for php the response times varied little under minimal load, but nginx handled heavy load far better than apache. Overall throughput with nginx was phenomenal from this tiny server. The result for static content was even more impressive: apache effectively died after ~2000 concurrent connections and 35k total pages killing the server; nginx handled the load to 10,000 very well and delivered 160k successful responses.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the &lt;a href=&#34;http://loader.io&#34;&gt;loader.io&lt;/a&gt; results from static content from &lt;a href=&#34;http://www.michaelhamrah.com&#34;&gt;http://www.michaelhamrah.com&lt;/a&gt;, comparing apache with nginx. I suggest clicking through and exploring the charts:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/f1c357b13b1f554eef534b79866eb5ce&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Apache only handled 33.5k successful responses up to about 1,300 concurrent connections, and died pretty quickly. Nginx did far better:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/9430bdfcab50f31dc66f3ea3014beb84&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;160k successful response with a 22% error rate and avg. response time of 142ms. Not too shabby. The apache run effectively killed the server and required a full reboot as ssh was unresponsive. Nginx barely hiccuped.&lt;/p&gt;

&lt;p&gt;The results of my wordpress/php performance is also interesting. I only did 1000 concurrent users hitting blog.michaelhamrah.com. Here&amp;#8217;s the apache result:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/210867953c97cdd2dd4308dce17bcae3&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;There was a 21% error rate with 13.7k request served and a 237ms average response time (I believe the lower average is due to errors). Overall not too bad for an ec2-micro instance, but the error rate was quite high and nginx again did far better:&lt;/p&gt;

&lt;div style=&#34;width: 600px;&#34;&gt;
  &lt;/p&gt; 
  
  &lt;div style=&#34;width: 100%; text-align: right;&#34;&gt;
    &lt;a href=&#34;http://loader.io/results/631e11ff9206c6c7a3820c891380c9a3&#34; target=&#34;_blank&#34;  style=&#34;padding: 0 10px 10px 0; font-family: Arial, &#39;Helvetica Neue&#39;, Helvetica, sans-serif; font-size: 14px;&#34;&gt;View on loader.io&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;A total of 19k successes with a 0% error rate. The average response time was a little higher than apache, but nginx did serve far more responses. I also get a kick out of the response time line between the two charts. Apache is fairly choppy as it scales up, while nginx increases smoothly and evens out when the concurrent connections plateaus. That&amp;#8217;s what scalability should look like!&lt;/p&gt;

&lt;p&gt;There are plenty of guides online showing how to get set up with nginx/php-fpm. &lt;a href=&#34;http://codex.wordpress.org/Nginx&#34;&gt;The Nginx guide on WordPress Codex&lt;/a&gt; is the most thorough, but there&amp;#8217;s a &lt;a href=&#34;http://todsul.com/install-configure-php-fpm&#34;&gt;straightforward nginx/php guide on Tod Sul&lt;/a&gt;. I also relied on an &lt;a href=&#34;http://dak1n1.com/blog/12-nginx-performance-tuning&#34;&gt;nginx tuning guide from Dakini&lt;/a&gt; and &lt;a href=&#34;http://calendar.perfplanet.com/2012/using-nginx-php-fpmapc-and-varnish-to-make-wordpress-websites-fly/&#34;&gt;this nginx/wordpress tuning guide from perfplanet&lt;/a&gt;. They both have excellent information. I also think you should check out the &lt;a href=&#34;https://github.com/h5bp/server-configs/blob/master/nginx/nginx.conf&#34;&gt;html5 boilerplate nginx conf files&lt;/a&gt; which have great bits of information.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re setting this up yourself, start simple and work your way up. The guides above have varying degrees of information and various configuration options which may conflict with each other. Here&amp;#8217;s some tips:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Decide if you&amp;#8217;re going with a socket or tcp/ip connection between nginx + php-fcm. A socket connection is slightly faster and local to the system, but a tcp/ip is (marginally) easier to set up and good if you are spanning multiple nodes (you could create a php app farm to compliment an nginx front-facing web farm).&lt;/p&gt;

&lt;p&gt;I chose to go with the socket approach between nginx/php-fpm. It was relatively painless, but I did hit a snag passing nginx requests to php. I kept getting a &amp;#8220;no input file specified&amp;#8221; error. It turns out it was a simple permissions issue: the default php-fpm user was different the nginx user the webserver runs under. Which leads me to:&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Plan your users. Security issues are annoying, so make sure file and app permissions are all in sync.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Check your settings! Read through default configuration options so you know what&amp;#8217;s going on. For instance you may end up running more worker processes in your nginx instance than available cpu&amp;#8217;s killing performance. Well documented configuration files are essential to tuning.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Plan for access and error logging. If things go wrong during the setup, you&amp;#8217;ll want to know what&amp;#8217;s going on and if your server is getting requests. You can turn access logs of later.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get your app running, test, and tune. If you do too many configuration settings at once you&amp;#8217;ll most likely hit a snag. I only did a moderate amount of tuning; nginx configuration files vary considerably, so again it&amp;#8217;s a good idea to read through the options and make your own call. Ditto for php-fcm.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I am really happy with the idea of running php as a separate process. Running php as a daemon has many benefits: you have a dedicate process you can monitor and recycle for php without effecting your web server. Pooling apps allows you to tune them individually. You&amp;#8217;re also not tying yourself to a particular web server; php-fpm can run fine with apache. In TCP mode you can even offload your web server to separate node. At the very least, you can distinguish php usage against web server usage.&lt;/p&gt;

&lt;p&gt;So my only question is why would anyone still use apache?&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to Handle a Super Bowl Size Spike in Web Traffic</title>
          <link>http://blog.michaelhamrah.com/2013/02/how-to-handle-a-super-bowl-size-spike-in-web-traffic/</link>
          <pubDate>Wed, 06 Feb 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/02/how-to-handle-a-super-bowl-size-spike-in-web-traffic/</guid>
          <description>

&lt;p&gt;I was shocked to learn the number of &lt;a href=&#34;http://www.yottaa.com/blog/bid/265815/Coke-SodaStream-the-13-Websites-That-Crashed-During-Super-Bowl-2013&#34;&gt;sites which failed to handle the spike in web traffic during the Super Bowl&lt;/a&gt;. Most of these sites served static content and should have scaled easily with the use of CDNs. Scaling sites, even dynamic ones, are achievable with well known tools and techniques.&lt;/p&gt;

&lt;h2 id=&#34;the-problem-is-simple:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;The Problem is Simple&lt;/h2&gt;

&lt;p&gt;At a basic level accessing a web page is when one computer, the client, connects to a server and downloads some content. A problem occurs when the number of people requesting content exceeds the ability to deliver content. It&amp;#8217;s just like a restaurant. When there are too many customers people must wait to be served. Staff becomes stressed and strained. Computers are the same. Excessive load causes things to break down.&lt;/p&gt;

&lt;h2 id=&#34;optimization-comes-in-three-forms:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Optimization Comes in Three Forms&lt;/h2&gt;

&lt;p&gt;To handle more requests there are three things you can do: produce (render) content faster, deliver (download) content faster and add more servers to handle more connections. Each of these solutions has a limit. Designing for these limits is architecting for scale.&lt;/p&gt;

&lt;p&gt;A page is composed of different types of content: html, css and js. This content is either dynamic (changes frequently) or static (changes infrequently). Static content is easier to scale because you create it once and deliver it repeatedly. The work of rendering is eliminated. Static content can be pushed out to CDNs or cached locally to avoid redownloading. Requests to origin servers are reduced or eliminated. You can also download content faster with small payload sizes. There is less to deliver if there is less markup and the content is compressed. Less to deliver means faster download.&lt;/p&gt;

&lt;p&gt;Dynamic content is trickier to cache because it is always changing. Reuse is difficult because pages must be regenerated for specific users at specific times. Scaling dynamic content involves database tuning, server side caching, and code optimization. If you can render a page quickly you can deliver more pages because the server can move on to new requests. Most often, at scale, you want to treat treat dynamic content like static content as best you can.&lt;/p&gt;

&lt;p&gt;Adding more servers is usually the easiest way to scale but breaks down quickly. The more servers you have the more you need to keep in sync and manage. You may be able to add more web servers, but those web servers must connect to database servers. Even powerful database servers can only handle so many connections and adding multiple database servers is complicated. You may be able to add specific types of servers, like cache servers, to achieve the results you need without increasing your entire topology.&lt;/p&gt;

&lt;p&gt;The more servers you have the harder it is to keep content fresh. You may feel increasing your servers will increase your load. It will become expensive to both manage and run. You may be able to achieve a similar result if you cut your response times which also gives the end user a better experience. If you understand the knobs and dials of your system you can tune properly.&lt;/p&gt;

&lt;h2 id=&#34;make-assumptions:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Make Assumptions&lt;/h2&gt;

&lt;p&gt;Don&amp;#8217;t be afraid to make assumptions about your traffic patterns. This will help you optimize for your particular situation. For most publicly facing websites traffic is anonymous. This is particularly true during spikes like the Super Bowl. Because you can deliver the same page to every anonymous user you effectively have static content for those users. Cache controls determine how long content is valid and powers HTTP accelerators and CDNs for distribution. You don&amp;#8217;t need to optimize for everyone; split your user base into groups and optimize for the majority. Even laxing cache rules on pages to a minute can shift the burden away from your application servers freeing valuable resources. Anonymous users will get the benefit of cached content with a quick download, dynamic users will have fast servers.&lt;/p&gt;

&lt;p&gt;You can also create specific rendering pipelines for anonymous and known users for highly dynamic content. If you can identify anonymous users early you may be able to avoid costly database queries, external API calls or page renders.&lt;/p&gt;

&lt;h2 id=&#34;understand-http:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Understand HTTP&lt;/h2&gt;

&lt;p&gt;HTTP powers the web. The better you understand HTTP the better you can leverage tools for optimizing the web. Specifically look at &lt;a href=&#34;http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html&#34;&gt;http cache headers&lt;/a&gt; which allow you to use web accelerators like Varnish and CDNs. The vary header will allow you to split anonymous and known users giving you fine grained control on who gets what. Expiration headers determine content freshness. The worst thing you can do is set cache headers to private on static content preventing browsers from caching locally.&lt;/p&gt;

&lt;h2 id=&#34;try-varnish-and-esi:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Try Varnish and ESI&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.varnish-cache.org&#34;&gt;Varnish&lt;/a&gt; is an HTTP accelerator. It caches dynamic content produced from your website for efficient delivery. Web frameworks usually have their own features for caching content, but Varnish allows you to bypass your application stack completely for faster response times. You can deliver a pre-rendered dynamic page as if it were a static page sitting in memory for a greater number of connections.&lt;/p&gt;

&lt;p&gt;Edge Side Includes allow you to mix static and dynamic content together. If a page is 90% similar for everyone, you can cache the 90% in Varnish and have your application server deliver the other 10%. This greatly reduces the work your app server needs to do. ESI&amp;#8217;s are just emerging into web frameworks. It will play a more prominent role in Rails 4.&lt;/p&gt;

&lt;h2 id=&#34;use-a-cdn-and-multiple-data-centers:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Use a CDN and Multiple Data Centers&lt;/h2&gt;

&lt;p&gt;You don&amp;#8217;t need to add more servers to your own data center. You can leverage the web to fan work out across the Internet. I talk more about CDN&amp;#8217;s, the importance of edge locations and latency in my post &lt;a href=&#34;http://www.michaelhamrah.com/blog/2012/01/building-for-the-web-understanding-the-network/&#34;&gt;Building for the Web: Understanding the Network&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Your application servers should be reserved for doing application-specific work which is unique to every request. There are more efficient ways of delivering the same content to multiple people than processing a request top-to-bottom via a web framework. Remember &amp;#8220;the same&amp;#8221; doesn&amp;#8217;t mean the same indefinitely; it&amp;#8217;s the same for whatever timeframe you specify.&lt;/p&gt;

&lt;p&gt;If you run Varnish servers in multiple data centers you can effectively create your own CDN. Your database and content may be on the east coast but if you run a Varnish server on the west coast an anonymous user in San Fransisco will have the benefit of a fast response time and you&amp;#8217;ve saved a connection to your app server. Even if Varnish has to deliver 10% dynamic content via an ESI on the east coast it can leverage the fast connection between data centers. This is much better then the end user hoping coast-to-coast themselves for an entire page.&lt;/p&gt;

&lt;p&gt;Amazon&amp;#8217;s Route 53 offers the ability to route requests to an optimal location. There are other geo-aware DNS solutions. If you have a multi-region setup you are not only building for resiliency your are horizontally scaling your requests across data centers. At massive scale even load balancers may become overloaded so round-robin via DNS becomes essential. DNS may be a bottleneck as well. If your DNS provider can&amp;#8217;t handle the flood of requests trying to map your URL to your IP address nobody can even get to your data center!&lt;/p&gt;

&lt;h2 id=&#34;use-auto-scaling-groups-or-alerting:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Use Auto Scaling Groups or Alerting&lt;/h2&gt;

&lt;p&gt;If you can take an action when things get rough you can better handle spikes. Auto scaling groups are a great feature of AWS when some threshold is maxed. If you&amp;#8217;re not on AWS good monitoring tools will help you take action when things hit a danger zone. If you design your application with auto-scaling in mind, leveraging load balancers for internal communication and avoiding state, you are in a better position to deal with traffic growth. Scaling on demand saves money as you don&amp;#8217;t need to run all your servers all the time. Pinterest gave a talk explaining how it saves money by reducing its server farm at night when traffic is low.&lt;/p&gt;

&lt;h2 id=&#34;compress-and-serialized-data-across-the-wire:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Compress and Serialized Data Across the Wire&lt;/h2&gt;

&lt;p&gt;Page sizes can be greatly reduced if you enable compression. Web traffic is mostly text which is easily compressible. A 100kb page is a lot faster to download than a 1mb page. Don&amp;#8217;t forget about internal communication as well. In todays API driven world using efficient serialization protocols like protocol buffers can greatly reduce network traffic. Most RPC tools support some form of optimal serialization. SOAP was the rage in the early 2000s but XML is one of the worst ways to serialize data for speed. Compressed content allows you to store more in cache and reduces network I/O as well.&lt;/p&gt;

&lt;h2 id=&#34;shut-down-features:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Shut Down Features&lt;/h2&gt;

&lt;p&gt;A performance bottleneck may be caused by one particular feature. When developing new features, especially on a high traffic site, the ability to shut down a misbehaving feature could be the quick solution to a bad problem. Most high-traffic websites &amp;#8220;leak&amp;#8221; new features by deploying them to only 10% of their users to monitor behavior. Once everything is okay they activate the feature everywhere. Similar to determining page freshness for caches, determining available features under load can keep a site alive. What&amp;#8217;s more important: one specific feature or the entire system?&lt;/p&gt;

&lt;h2 id=&#34;non-blocking-i-o:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Non-Blocking I/O&lt;/h2&gt;

&lt;p&gt;Asynchronous programming is a challenge and probably a last-resort for scaling. Sometimes servers break down without any visible threshold. You may have seen a slow request but memory, cpu, and network levels are all okay. This scenario is usually caused by blocking threads waiting on some form of I/O. Blocked threads are plugs that clog your application. They do nothing and prevent other things from happening. If you call external web services, run long database queries or perform disk I/O beware of synchronous operations. They are bottlenecks. Asynchronous based frameworks like node.js put asynchronous programming at the forefront of development making them attractive for handling numerous concurrent connections. Asynchronous programming also paves the way for queue-based architectures. If every request is routed through a queue and processed by a worker the queue will help even out spikes in traffic. The queue size will also determine how many workers you need. It may be trickier to code but it&amp;#8217;s how things scale.&lt;/p&gt;

&lt;h2 id=&#34;think-at-scale:9da68d5a6f235e570e3d7349e289ad5f&#34;&gt;Think at Scale&lt;/h2&gt;

&lt;p&gt;When dealing with a high-load environment nothing can be off the table. What works for a few thousand users will grow out of control for a few million. Even small issues will become exponentially problematic.&lt;/p&gt;

&lt;p&gt;Scaling isn&amp;#8217;t just about the tools to deal with load. It&amp;#8217;s about the decisions you make on how your application behaves. The most important thing is determining page freshness for users. The decisions for an up-to-the-second experience for every user are a lot different than an up-to-the-minute experience for anonymous users. When dealing with millions of concurrent requests one will involve a lot of engineering complexity and the other can be solved quickly.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Effective Caching Strategies: Understanding HTTP, Fragment and Object Caching</title>
          <link>http://blog.michaelhamrah.com/2012/08/effective-caching-strategies-understanding-http-fragment-and-object-caching/</link>
          <pubDate>Sat, 18 Aug 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/08/effective-caching-strategies-understanding-http-fragment-and-object-caching/</guid>
          <description>

&lt;p&gt;Caching is one of the most effective techniques to speed up a website and has become a staple of modern web architecture. Effective caching strategies will allow you to get the most out of your website, ease pressure on your database and offer a better experience for users. Yet as the old &lt;a href=&#34;http://martinfowler.com/bliki/TwoHardThings.html&#34;&gt;adage says&lt;/a&gt; caching&amp;#8211;especially invalidation&amp;#8211;is tricky. How to deal with dynamic pages, deciding what to cache, per-user personalization and invalidation are some of the challenges which come along with caching.&lt;/p&gt;

&lt;h3 id=&#34;caching-levels:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Caching Levels&lt;/h3&gt;

&lt;p&gt;There a three broad levels of caching:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html&#34;&gt;HTTP Caching&lt;/a&gt;&lt;/em&gt; allows for full-page caching via HTTP headers on URIs. This must be enabled on all static content and should be added to dynamic content when possible. It is the best form of caching, especially for dynamic pages, as you are serving generated html content and your application can effectively leverage reverse-proxies like &lt;a href=&#34;http://www.squid-cache.org/&#34;&gt;Squid&lt;/a&gt; and &lt;a href=&#34;https://www.varnish-cache.org/&#34;&gt;Varnish&lt;/a&gt;. &lt;a href=&#34;http://www.mnot.net/cache_docs/&#34;&gt;Mark Nottingham&amp;#8217;s great overview on HTTP Caching is worth a read&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Fragment Caching&lt;/em&gt; allows you to cache page fragments or partial templates. When you cannot cache an entire http response, fragment caching is your next best bet. You can quickly assemble your pages from pre-generated html snippets. For a page involving disparate dynamic content you can build your result page from cached html fragments for each section. For listing pages, like search results, you can build the page from html fragments for each id and not regenerate markup. For detail pages you can separate less-volatile or common sections from high-volatile or per-user sections.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Object Caching&lt;/em&gt; allows you to cache a full object (as in a model or viewmodel). When you must generate html for each user/request, or when your objects are shared across various views, object caching can be extremely helpful. It allows you to better deal with expensive queries and lessen hits to your database.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The goal is to make your response times as fast as possible while lessening load. The more html (or data) you can push closer to the end-user the better. HTTP caching is better than fragment caching: you are ready to return the rendered page. When combined with a CDN even dynamic pages can be pushed to edge locations for faster response times. Fragment caching is better than object caching: you already have the rendered html to build the page. Object caching is better than a database call: you already have the cached query result or denormalized object for your view. The deeper you get in the stack (the closer to the datastore) the more options you have to vary the output. Consequently the more expensive and longer the operation will take.&lt;/p&gt;

&lt;h3 id=&#34;break-content-down-cache-for-views:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Break Content Down; Cache for Views&lt;/h3&gt;

&lt;p&gt;A cache strategy is dependent on breaking content down to store and reuse later. The more granular you can get the more options you have to serve cached content. There are two main dimensions: what to cache and whom to cache for. It is difficult to HTTP cache a page with a &amp;#8220;Hello, {{ username }}&amp;#8221; in the header for all users. However if you break your users down into logged-in users and anonymous users you can easily HTTP cache your homepage for just anonymous users using the &lt;em&gt;vary&lt;/em&gt; http-header and defer to fragment caching for logged-in users.&lt;/p&gt;

&lt;p&gt;Cache key naming strategies allow you to vary the &lt;em&gt;what&lt;/em&gt; with the &lt;em&gt;who for&lt;/em&gt; in a robust way by creating multiple versions of the same resource. A cache key could include the role of the user and the page, such as &lt;em&gt;role:page:fragement:id&lt;/em&gt;, as in _anon:widget&lt;em&gt;detail:widget:1234&lt;/em&gt; and serve the &lt;em&gt;widget detail&lt;/em&gt; html fragment to anonymous users. The same widget could be represented in a search detail list via _anon:widget&lt;em&gt;search:widget:1234&lt;/em&gt;. When widget 1234 updates both keys are invalidated. Most people opt for object caching for an easy win with dynamic pages, specifically by caching via a primary key or id. This can be helpful, but if you break down your content into the &lt;em&gt;what&lt;/em&gt; and &lt;em&gt;who for&lt;/em&gt; with a good key naming strategy you can leverage fragment caching and save on rendering time.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;vary&lt;/em&gt; http header is very helpful for dealing with HTTP caching and is not used widely enough. By varying URIs based on certain headers (like authorization or a cookie value) you can cache different representations for the same resource in a similar way to creating multiple keys. Think of the cache key as the URI plus whatever is set in the &lt;em&gt;vary&lt;/em&gt; header. This opens up the power of HTTP caching for dynamic or per-user content.&lt;/p&gt;

&lt;p&gt;You are ready to deliver content quickly when you think about your cache in terms of views and not data. Cache a denormalized object with child associations for easy rendering without extra lookups. Store rendered html fragments for sections of a page that are common to users on otherwise specific content. &amp;#8220;Popular&amp;#8221; and &amp;#8220;Recent&amp;#8221; may be expensive queries; storing rendered html saves on processing time and can be injected into the main page. You can even reuse fragments across pages. A good cache key naming strategy allows for different representations of the same data which can easily be invalidated.&lt;/p&gt;

&lt;h3 id=&#34;cache-invalidation:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Cache Invalidation&lt;/h3&gt;

&lt;p&gt;Nobody likes stale data. As you think about caching think about what circumstances to invalidate the cache. Time-based expirations are convenient but can usually be avoided by invalidating caches on create and update commands. A good cache key naming strategy helps. Web frameworks usually have a notion of &amp;#8220;callbacks&amp;#8221; to perform secondary actions when a primary action takes place. A set of fragment and object caches for a widget could be invalidated when a record is updated. If cache values are granular enough you could invalidate sections of a page, like blog comments, when a comment is added and not expire the entire blog post.&lt;/p&gt;

&lt;p&gt;HTTP Etags provide a great mechanism for dealing with stale HTTP requests. Etags allow a more invalidation options than the basic if-modified-since headers. When dealing with Etags the most important thing is to avoid processing the entire request simply to generate the Etag to validate against (this saves network bandwidth but does not save processing time). Caching Etag values against URIs are a good way to see if an Etag is still valid to send the proper 304 NOT MODIFIED response as quickly as possible in the request cycle. Depending on your needs you can also cache sets of Etag values against URIs to handle various representations.&lt;/p&gt;

&lt;p&gt;If you must rely on time-based expiration try to add expiration callbacks to keep the cache fresh, especially for expensive queries in high-load scenarios.&lt;/p&gt;

&lt;h3 id=&#34;edge-side-includes-fragment-caching-for-http:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Edge Side Includes: Fragment Caching for HTTP&lt;/h3&gt;

&lt;p&gt;Edge Side Includes are a great way of pushing more dynamic content closer to users. ESIs essentially give you the benefits of fragment caching with the performance of HTTP caching. If you are considering using a tool like &lt;a href=&#34;http://www.squid-cache.org/&#34;&gt;Squid&lt;/a&gt; or &lt;a href=&#34;https://www.varnish-cache.org/&#34;&gt;Varnish&lt;/a&gt; ESIs are essential and will allow you to add customized content to otherwise similar pages. The &lt;em&gt;user panel&lt;/em&gt; in the header of a page is a classic example of an ESI usage. If the user panel is the only variant of an otherwise common page for all users, the common elements could be pulled from the reverse-proxy within milliseconds and the &amp;#8220;Welcome, {{USER}}&amp;#8221; injected dynamically as a fragment from the application server before sending everything to the client. This bypasses the application stack lightening load and decreasing processing time.&lt;/p&gt;

&lt;h3 id=&#34;distributed-or-centralized-caches-are-better:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Distributed or Centralized Caches are Better&lt;/h3&gt;

&lt;p&gt;Distributed and/or centralized caches are better than in-memory application server cache stores. By using a distributed cache like &lt;a href=&#34;http://memcached.org/&#34;&gt;Memcache&lt;/a&gt;, or a centralized cache store like &lt;a href=&#34;http://redis.io&#34;&gt;Redis&lt;/a&gt;, you can drop duplicate data caches to make caching and invalidating objects easier. Even though caching objects in a web app&amp;#8217;s memory space is convenient and reduces network i/o, it soon becomes impractical in a web farm. You do not want to build up caches per-server or steal memory space away from the web server. Nor do you want to have to hunt and gather objects across a farm to invalidate caches. If you do not want to support your own cache farm, there are plenty of SaaS services to deal with caching.&lt;/p&gt;

&lt;h3 id=&#34;compress-when-possible:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Compress When Possible&lt;/h3&gt;

&lt;p&gt;Compressing content helps. Memory is a far more valuable resource for web apps than cpu cycles. When possible, compress your serialized cache content. This lowers the memory footprint so you can put more stuff in cache, and lightens the transfer load (and time) between your cache server and application server. For HTTP caching the helpful &lt;em&gt;vary&lt;/em&gt; http header can also be used to cache content for browsers supporting compression and those that don&amp;#8217;t. For object caching, only store what you need in the cache. Even though compression helps reduce the footprint, not storing extraneous data further reduces the footprint and saves serialization time.&lt;/p&gt;

&lt;h3 id=&#34;nosql-to-the-rescue:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;NoSQL to the Rescue&lt;/h3&gt;

&lt;p&gt;One of the interesting trends I am reading about is how certain NoSQL stores are eliminating the need for separate cache farms. NoSQL solutions are beneficial for a variety of reasons even though they create significant data-modeling challenges. What NoSQL solutions lack in the flexibility of representing and accessing data (i.e. no joins, minimal search) they can make up in their distributed nature, fault-tolerance, end access efficiency. When you model your data for your views, putting the burden on storing data in the same way you want to get it out, you&amp;#8217;re essentially replacing your denormalized memory-caching tier with a more durable solution. Cassandra and other Dynamo/Bigtable type stores are key-value stores, similar to cache stores, with the value part offering some sort of structured data type (in the case of Cassandra, sorted lists via column families). MongoDb and Redis, (not Dynamo inspired) offer similar advantages; Redis&amp;#8217; sorted sets/sorted lists offer a variety of solutions for listing problems, MongoDb allows you to query objects.&lt;/p&gt;

&lt;p&gt;If you are okay with storing (and updating) multiple-versions of your data (again, you are caching for views) you can cut the two-layer approach of separate cache and data stores. The trick is storing everything you need to render a view for a given key. Searches could be handled by a search-server like Solr or ElasticSearch; listing results could be handled by maintaining your own index via a sorted-list value via another key. When using Cassandra you&amp;#8217;d get fast, masterless, and scalable persistant storage. In general this approach is only worthwhile if your views are well-defined. The worst thing you want to do is refactor your entire data model when your views change!&lt;/p&gt;

&lt;h3 id=&#34;how-web-frameworks-help:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;How Web Frameworks Help&lt;/h3&gt;

&lt;p&gt;There is always debate on differences between frameworks and languages. One of the things I always look for is how easy it is to add caching to your application. Rails offers great support for caching, and the &lt;a href=&#34;http://guides.rubyonrails.org/caching_with_rails.html&#34;&gt;Caching with Rails&lt;/a&gt; guide is worth a read no matter what framework or language you use. It easily supports fragment caching in views via content blocks, behind-the-scene action caching support, has a pluggable cache framework to use different stores, and most importantly has an extremely flexible invalidation framework via model observers and cache sweepers. When choosing any type of framework, &amp;#8220;how to cache&amp;#8221; should be a bullet point at the top of the list.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Solr: Improving Performance and Other Considerations</title>
          <link>http://blog.michaelhamrah.com/2011/11/solr-improving-performance-and-other-considerations/</link>
          <pubDate>Tue, 29 Nov 2011 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2011/11/solr-improving-performance-and-other-considerations/</guid>
          <description>

&lt;p&gt;We use &lt;a href=&#34;http://lucene.apache.org/solr/&#34;&gt;Solr&lt;/a&gt; as our search engine for one of our internal systems. It has been awesome; before, we had to deal with very messy sql statements to support many search criteria. Solr allows us to stick our denormalized data into an index and search on an arbitrary number of fields via an elegant, RESTful interface. It&amp;#8217;s extremely fast, easy to use, and easy to scale. I wanted to share some lessons learned from our experience with Solr.&lt;/p&gt;

&lt;h2 id=&#34;know-your-use-cases:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Know Your Use Cases&lt;/h2&gt;

&lt;p&gt;There are two worlds of Solr: writing data (committing) and reading data (querying). Solr should not be treated like a database or some nosql solution; it is a search indexer built on top of Lucene. Treat it like a search indexer and not a permanent data store; it doesn&amp;#8217;t behave like a database. There are plenty of tools to keep data in database in sync with Solr; the worst case scenario is you have to sync it yourself. You should know how heavy you will query it, how much you&amp;#8217;ll write to it, and have a rough idea what your schema will be (but it doesn&amp;#8217;t have to be 100%). Knowing your use cases will allow you to configure your instance and define your schema appropriately.&lt;/p&gt;

&lt;p&gt;Solr offers a variety of ways to index and parse data; when you&amp;#8217;re starting out, you don&amp;#8217;t need to pick one. Solr has a great copyField feature that allows you to index the same data in multiple ways. This can be great for trying out new things or doing A/B comparisons. Once your patterns are well defined, you can tune your index and configuration as needed.&lt;/p&gt;

&lt;p&gt;Our use cases are pretty straight forward; we simply need to search many different fields and aggregate results. We don&amp;#8217;t need to deal with lexical analyzation or sorting on score. Our biggest issue was actually commits because we didn&amp;#8217;t thoroughly vet our update patterns. Remember, Solr is about commits as much as it is about querying. You need to realize there will be some lag between when you update Solr and when you see the results. There are a large number of factors that go into how long that delay will be (it could be very quick), but it will be there, and you should design your system in knowing there will be a delay rather than trying to avoid it. The commit section covers why you shouldn&amp;#8217;t try and commit on every update, even under moderate load.&lt;/p&gt;

&lt;h2 id=&#34;know-configuration-options:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Know Configuration Options&lt;/h2&gt;

&lt;p&gt;Go through the solrconfig.xml and schema.xml files. It&amp;#8217;s well documented and there are lots of good bits in there (solrconfig.xml is often missed!). The caches are what matter most, and explained in later sections. If you know your usage patterns you can get a good sense of how you can tune your caches for optimal results. Autowarming is also important; it allows Solr to reuse caches from previous indexes when things change.&lt;/p&gt;

&lt;p&gt;Don&amp;#8217;t forget that Solr sits on top of Java, so you should also tune the JVM as appropriate. This probably will revolve around how much memory to allocate to the JVM. Be sure to give as much as possible, especially in production.&lt;/p&gt;

&lt;h2 id=&#34;understand-commits:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Understand Commits&lt;/h2&gt;

&lt;p&gt;You should control the number of commits being made to Solr. Load testing is important; you need to know how often and what happens when Solr will rebuilds an index. You shouldn&amp;#8217;t commit on every update; you will surely hit memory and performance issues. When a commit occurs, an index and search warmer need to be built. A search warmer is a view onto an index. Caches may need to be pre-populated. Locking occurs. You don&amp;#8217;t want to have that overhead if you don&amp;#8217;t need it. If you have any post commit listeners those will also run. Finally, updating without forcing a commit is a lot faster than forcing a commit on update. The downside is simply that data will not be immediately available.&lt;/p&gt;

&lt;p&gt;This is where autocommit comes into play. We use an autocommit every 5 seconds or 5000 docs. We never hit 5000 commits in less than 5 seconds; we just don&amp;#8217;t want data to be too stale. 5000 docs allows us to re-index in production if we need to without killing the system. This ratio provides a good enough index time for searches to work appropriately without causing too many commits from choking the system. Again, know your usage patterns and you can get this number right.&lt;/p&gt;

&lt;h2 id=&#34;search-warmers-and-cold-searches:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Search Warmers and Cold Searches&lt;/h2&gt;

&lt;p&gt;Solr caching works by creating a view on an index called a searcher. A commit will create a warming search to prep the index and the cache. How long this takes is tricky to say, but the more rows, indexable fields, and the more parsing that is done the longer it takes. The default is to only allow two warming searches at once, and depending on how you’re doing commits, you can easily surpass that limit. If you read the solrconfig.xml file you&amp;#8217;ll see that 1-2 is useful for read-only slaves. So you&amp;#8217;re going to want to increase this number on your main instance; but be aware, you can kill your available memory if you&amp;#8217;re committing so much you have a high number of warmers.&lt;/p&gt;

&lt;p&gt;By default Solr will block if a search warmer isn&amp;#8217;t available. Depending on how and when you&amp;#8217;re committing, you may not want this. For instance, if the first search is warming an index, it could be a while before it returns. Be sure to reuse old warmers and see if you can live with a semi-built index. This is all handled in the solrconfig.xml file. Read it!&lt;/p&gt;

&lt;h2 id=&#34;increase-cache-sizes:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Increase Cache Sizes&lt;/h2&gt;

&lt;p&gt;Don&amp;#8217;t forget out-of-the-box mode is not production mode. We&amp;#8217;ve touched on a committing and search warmers. Cache sizes are another important aspect and should be as big as possible. This allows more warmers to be reused and offers a greater opportunity to search against cached search results (fq parameters) versus new query results (q parameters). The more we can cache the better; it also allows Solr to carry over search warmers when rebuilding indices which is very helpful.&lt;/p&gt;

&lt;h2 id=&#34;lock-types:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Lock Types&lt;/h2&gt;

&lt;p&gt;Luckily the default lock type is now &amp;#8220;Native&amp;#8221; which means Solr uses OS level locking. Previously it was single and this killed the system in concurrent update scenarios. Go native.&lt;/p&gt;

&lt;h2 id=&#34;understand-and-leverage-q-and-fq-parameters:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Understand and Leverage Q and FQ parameters&lt;/h2&gt;

&lt;p&gt;Q is the original query, fq is a filter query.  For larger sets this is important. If the original query is cached an fq query will just search the cached original query, rather than the entire index.  So if you have an index with one million records, and a query returns 100k results, a q/fq combination will only search the 100k cached records. This is a big performance win. Ensure your cache settings are big enough for your usage patterns to create more cache hits.&lt;/p&gt;

&lt;h2 id=&#34;minimize-use-of-facets:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Minimize Use of Facets&lt;/h2&gt;

&lt;p&gt;Calculating facets is time consuming and can easily increase a search 2-5x than normal.  This is the slowest bottleneck we have with Solr (but still, it’s minimal compared to sql).  If you can avoid facets than do so.  If you can’t, only calculate them once on initial load, and design a UI that doesn’t need to refresh them (i.e. paging via ajax, etc).  When searching from a facet, use the fq parameter to minimize the set you&amp;#8217;re searching on from your q query. This also reduces the required number of entries that are calculated for a facet and greatly increases performance.&lt;/p&gt;

&lt;h2 id=&#34;avoid-dynamic-fields-in-solr:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Avoid Dynamic Fields in Solr&lt;/h2&gt;

&lt;p&gt;This is more of an application architectural decision rather than anything else, and probably somewhat controversial. I feel you should avoid the use of dynamic fields and focus on defining your schema. I feel you can easily lose control over your schema if your model changes often as you have no base to work from. That can have unintended consequences depending on how you wrap your Solr instance and how you serialize and deserialize Solr data. It’s not too much up front work to define your schema during development that would call for the use of dynamic fields in production, unless of course your app necessitates using dynamic fields for one reason or another.&lt;/p&gt;

&lt;p&gt;The other, more valid argument is that on a per-field level you can specify multi-values, required, and indexable fields. Solr handles multi valued and indexable fields differently on commits. If you are using dynamic fields and are indexing each one, and are not actually searching nor returning these fields, you have a really high and unnecessary commit cost. At the very least, consider turning off indexing for dynamic fields if you don&amp;#8217;t need it.&lt;/p&gt;

&lt;h2 id=&#34;use-field-lists:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Use Field Lists&lt;/h2&gt;

&lt;p&gt;You should always specify what data you want returned from the query with fl (field list). This is extremely important!  Depending on how you’ve set up your schema, you probably have a ton of fields you don’t actually need returned to the UI.  This is common when you are indexing the same field with different parsers via the copyField functionality. Use fl to get back only the data you need- this will greatly reduce the amount data (and network traffic) returned, and speed up the query because Solr will not have to fetch unnecessary fields from its internal storage. In a high-read environment, you can greatly reduce both memory and network load by trimming the fat from your dataset.&lt;/p&gt;

&lt;h2 id=&#34;have-a-reindex-strategy:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Have a Reindex strategy&lt;/h2&gt;

&lt;p&gt;There will come a time when you need to reindex your Solr instance. Most likely this will be when you&amp;#8217;re releasing a new feature. It&amp;#8217;s important to have a reindexing strategy ready to go. Let&amp;#8217;s say you add a new field to your UI which you want to search on. You release your code, but that field is not in Solr yet so you get no results. Or, you get a doc back from Solr, you deserialize it to your object model, and get an error because you expect the field to be there and it&amp;#8217;s not. You must prepare for that. You could change your schema file, reindex in a background process, and then release code when ready. In this scenario make sure you can reindex without killing the system. It&amp;#8217;s also important to know how long it will take. Having to reindex like this may not be practical if takes a couple of days. You could also reindex to a second, unused Solr instance, and when you deploy you cut over to the new instance. By looking at your db update timestamps you can sync any missed data. (Remember how I said Solr is not a data store? This is a reason.)&lt;/p&gt;

&lt;h2 id=&#34;final-thoughts:89a03406893d3e68a4e4c16c2b6641ca&#34;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;Remember that data in Solr needs to be stored, indexed and returned. If you are only using dynamic fields, indexing all of them, defining copyField settings left and right and returning all that data because you are not using field lists (and potentially calculating facets on everything), you are generating a lot of unnecessary overhead. Keep it small and keep it slim. You&amp;#8217;ll lower your storage needs, your memory requirements, and your result set. You&amp;#8217;ll speed up commits as well.&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
