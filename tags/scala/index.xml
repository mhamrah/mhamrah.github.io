<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> </title>
    <link>http://blog.michaelhamrah.com/tags/scala/</link>
    <language>en-us</language>
    <author>Michael Hamrah</author>
    <rights>(C) 2015</rights>
    <updated>2015-03-20 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Book Review: Go Programming Blueprints, and the beauty of a language.</title>
          <link>http://blog.michaelhamrah.com/2015/03/book-review-go-programming-blueprints-and-the-beauty-of-a-language/</link>
          <pubDate>Fri, 20 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/book-review-go-programming-blueprints-and-the-beauty-of-a-language/</guid>
          <description>&lt;p&gt;Just over two years ago my wife and I [traveled around Asia for several months)[thegreatbigadventure.tumblr.com]. I didn’t do any programming while I was gone &lt;a href=&#34;http://blog.michaelhamrah.com/2013/04/six-months-of-computer-science-without-computers/&#34;&gt;but I did a great deal of reading&lt;/a&gt;, gaining a new-found appreciation for programming and technology. I became deeply interested in Scala and Go for their respective approachs to statically typed languages. Scala for its functional programming aspects and Go for its refreshing and intentionally succinct approach to interfaces, types and its anti-inheritance. The criticism I most often here with Scala; that’s it too open, too free-for-fall in its paradigms is in stark contrast to the main criticisms I hear of Go: it’s too limiting, too constrained.&lt;/p&gt;

&lt;p&gt;Since returning a majority of my time is focused on Scala, yet I still keep a hand in the Go cookie jar. Both languages are incredibly productive, and I appreciate FP the more I use it and understand it. Scala’s criticism is legitimate; it can be a chaotic language. However, my personal opinion is the language shouldn’t constrain you: it’s the discipline of the programmer to write code well, not the language. A bad programmer is going to destroy any language; a good programmer can make any code beautiful. More importantly, no language is magical. A language is a tool, and it’s up to the programmer to use it effectively.&lt;/p&gt;

&lt;p&gt;Learning a language is more than just knowing how to write a class or function. Learning a language is about composing these together effectively and using the ecosystem around the language. Scala’s benefit is the ecosystem around the JVM; idiomatic Scala is contentious debate, as you have the functional programmers on one side and the more lenient anti-javaists on the other (Martin Odersky’s talk &lt;a href=&#34;https://www.youtube.com/watch?v=ecekSCX3B4Q&#34;&gt;Scala: The Simple Parts&lt;/a&gt; is a great overview of where Scala shines). Go, on the other hand, is truly effective when you embrace its opinions and leverage its ecosystem: understanding imports and go get, writing small, independent modules, reusing these modules, embracing interfaces, and understanding the power of goroutines.&lt;/p&gt;

&lt;p&gt;Last summer I had the great pleasure of being a technical reviewer for Mat Ryer’s &lt;a href=&#34;http://bit.ly/GoBb&#34;&gt;Go Programming Blueprints&lt;/a&gt;. I’ve read a great deal of programming books in my career and appreciated Mat’s approach to showcasing the power and simplicity of Go. It’s not for beginners programmers, but if you have some experience, not even with Go, you can kick-start a working knowledge easily with Mat’s book. My favorite aspect is it explains how to write idiomatic Go to build applications. One example application composes discrete services and links them with bitly’s NSQ library, another uses a routing library on top of Go’s httpRequest handler. The book isn’t just isolated to web programs, there’s a section on writing CLI apps which link together with standard in and standard out. For those criticizing Go’s terseness Mat’s book exemplifies what you can do with those terse systems: write scalable, composable apps that are also maintainable and readable. The books shows why so many exciting new tools are written in Go: you can do a lot with little, and they compile to statically linked, minimal binaries.&lt;/p&gt;

&lt;p&gt;As you develop your craft of writing code, you develop certain opinions on the way code should work. When your language is inline with your opinions, or you develop opinions based on the language, you are effectively using that language. If you are learning a new language, like Go, but still applying your existing opinions on how to develop applications (say, by wishing the language had Generics), you struggle. Worse, you are attempting to shape a new language to the one you know, effectively programming in the old language. You should embrace what the language offers, and honor its design decisions. Mat’s book shows how to apply Go’s design decisions effectively. The language itself will evolve and grow, but it will do it in a way that enhances and honors its design decisions. And if you still don’t like it, or Scala, well there’s always Rust.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Adding Http Server-Side Events to Akka-Streams</title>
          <link>http://blog.michaelhamrah.com/2015/01/adding-http-server-side-events-to-akka-streams/</link>
          <pubDate>Sun, 18 Jan 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/01/adding-http-server-side-events-to-akka-streams/</guid>
          <description>

&lt;p&gt;In my last blog post we pushed messages from RabbitMq to the console using Akka-Streams. We used the &lt;a href=&#34;https://github.com/ScalaConsultants/reactive-rabbit&#34;&gt;reactive-rabbit&lt;/a&gt; library to create an Akka-Streams &lt;code&gt;Source&lt;/code&gt; for our &lt;em&gt;streams-playground&lt;/em&gt; queue and mapped the stream to a &lt;code&gt;println&lt;/code&gt; statement before dropping it into an empty &lt;code&gt;Sink&lt;/code&gt;. All Akka-Streams need both a &lt;code&gt;Source&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; to be runnable; we created a complete stream blueprint to be run later.&lt;/p&gt;

&lt;p&gt;Printing to the console is somewhat boring, so let&amp;#8217;s take it up a notch. The excellent &lt;a href=&#34;spray.io&#34;&gt;Spray Web Service&lt;/a&gt; library is being merged into Akka as &lt;a href=&#34;http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M2/scala/http/index.html&#34;&gt;Akka-Http&lt;/a&gt;. It&amp;#8217;s essentially Spray built with Akka-Streams in mind. The routing dsl, immutable request/response model, and high-performance http server are all there; think of it as Spray vNext. Check out Mathias Doenitz&amp;#8217;s &lt;a href=&#34;http://spray.io/scaladays2014/#/&#34;&gt;excellent slide deck on kaka-http from Scala days&lt;/a&gt; to learn more on this evolution of Spray; it also highlights the back-pressure functionality Akka-Streams will give you for Http.&lt;/p&gt;

&lt;p&gt;Everyone&amp;#8217;s familiar with the Request/Response model of Http, but to show the power of Akka-Streams we&amp;#8217;ll add Heiko Seeberger&amp;#8217;s &lt;a href=&#34;https://github.com/hseeberger/akka-sse&#34;&gt;Akka-SSE&lt;/a&gt; library which brings Server-Side Events to Akka-Http. &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/eventsource/basics/&#34;&gt;Server-Side Events&lt;/a&gt; are a more efficient form of long-polling that&amp;#8217;s a lighter protocol to the bi-directional WebSocket API. It allows the client to easily register a handler which the server can then push events to. Akka-SSE adds an SSE-enabled completion marshaller to Akka-Http so your response can be SSE-aware. Instead of printing messages to the console, we&amp;#8217;ll push those messages to the browser with SSE. This shows one of my favorite features of stream-based programming: we simply connect the specific pipes to create more complex flows, without worrying about the how; the framework handles that for us.&lt;/p&gt;

&lt;h2 id=&#34;changing-the-original-example:a9237168f3920272915cff712cbdae5e&#34;&gt;Changing the Original Example&lt;/h2&gt;

&lt;p&gt;If you&amp;#8217;re interested in the code, simply &lt;a href=&#34;https://github.com/mhamrah/streams-playground&#34;&gt;clone the original repo&lt;/a&gt; with &lt;code&gt;git clone https://github.com/mhamrah/streams-playground.git&lt;/code&gt; and then &lt;code&gt;git checkout adding-sse&lt;/code&gt; to get to this step in the repo.&lt;/p&gt;

&lt;p&gt;To modify the original example we&amp;#8217;re going to remove the &lt;code&gt;println&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; calls from &lt;code&gt;RabbitMqConsumer&lt;/code&gt; so we can plug in our enhanced &lt;code&gt;Source&lt;/code&gt; to the Akka-Http sink.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;def consume() = {
    Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
  }
&lt;/pre&gt;

&lt;p&gt;This is now a partial flow: we build up the original RabbitMq &lt;code&gt;Source&lt;/code&gt; with our map function to get the message body. Now the &amp;#8220;other end&amp;#8221; of the stream needs to be connected, which we defer until later. This is the essence of stream composition. There are multiple ways we can cut this up: our &lt;code&gt;map&lt;/code&gt; call could be the only thing in this function, with our &lt;code&gt;RabbitMq&lt;/code&gt; source defined elsewhere.&lt;/p&gt;

&lt;h2 id=&#34;adding-akka-http:a9237168f3920272915cff712cbdae5e&#34;&gt;Adding Akka-Http&lt;/h2&gt;

&lt;p&gt;If you&amp;#8217;re familiar with Spray, Akka-Http won&amp;#8217;t look that much different. We want to create an &lt;code&gt;Actor&lt;/code&gt; for our http service. There are just a few different traits we extend our original Actor from, and a different way plug our routing functions into the Akka-Streams pipeline.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;class HttpService
  extends Actor
  with Directives
  with ImplicitFlowMaterializer
  with SseMarshalling {
  // implementation
  // ...
}
&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;Directives&lt;/code&gt; gives us the routing dsl, similar to &lt;a href=&#34;http://spray.io/documentation/1.2.2/spray-routing/&#34;&gt;spray-routing&lt;/a&gt; (the functions are pretty much the same). Because Akka-Http uses Akka-Streams, we need an implicit &lt;code&gt;FlowMaterializer&lt;/code&gt; in scope to run the stream. &lt;code&gt;ImplicitFlowMaterializer&lt;/code&gt; provides a default. Finally, the &lt;code&gt;SseMarshalling&lt;/code&gt; trait from Heiko Seeberger&amp;#8217;s library provides the SSE functionality we want for our app. &lt;em&gt;If you&amp;#8217;re interested in a robust Akka-Streams sample, Heiko&amp;#8217;s &lt;a href=&#34;https://github.com/hseeberger/reactive-flows&#34;&gt;Reactive-Flows&lt;/a&gt; is worth checking out.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;##Binding to Http&lt;/p&gt;

&lt;p&gt;Within our actor body we&amp;#8217;ll create our http stream by binding a routing function to an http port. This is a little different than Spray; there&amp;#8217;s just some syntactical sugar so we can plug our routing function directly into the http pipeline:&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;//need an ExecutionContext for Futures
    import context.dispatcher

    //There&#39;s no receive needed, this is implicit
    //by our routing dsl.
    override def receive: Receive = Actor.emptyBehavior

    //We bind to an interface and create a 
    //Flow with our routing function
    Http()(context.system)
      .bind(Config.interface, Config.port)
      .startHandlingWith(route)

    //Simple composition of basic routes
    private def route: Route = sse ~ assets

    //Defined later
    private def see: Route = ???
    private def assets: Route = ???
&lt;/pre&gt;

&lt;p&gt;If we weren&amp;#8217;t using the Routing DSL we&amp;#8217;d need to explicitly handling HttpRequest messages in our receive partial function. But the &lt;code&gt;startHandlingWith&lt;/code&gt; call will do this for us; like spray-routing it takes in a routing function, and will call the appropriate route handler. New http requests will be pumped into the route handler and completed with the completion function at the end of the route.&lt;/p&gt;

&lt;p&gt;##Adding SSE&lt;/p&gt;

&lt;p&gt;The last piece of the puzzle is adding a specific route for SSE. We need two pieces for SSE support: first, an implicit function which converts the type produced from our &lt;code&gt;Source&lt;/code&gt; to an &lt;code&gt;Sse.Message&lt;/code&gt;; in this case, we need to go from a &lt;code&gt;String&lt;/code&gt; to an &lt;code&gt;Sse.Message&lt;/code&gt;. Secondly we need a route where a client can subscribe to the stream of server-side events.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;//Convert a String (our RabbitMq output) to an SSE Message
 implicit def stringToSseMessage(event: String): Sse.Message = {
      Sse.Message(event, Some(&#34;published&#34;))
    }

 //add a route for our sse endpoint.
 private def sse: Route = {
      path(&#34;messages&#34;) {
        get {
          complete {
            RabbitMqConsumer.consume
          }
        }
      }
    }
&lt;/pre&gt;

&lt;p&gt;In order for SSE to work in the browser we need to produce a stream of SSE messages with a specific content-type: &lt;code&gt;Content-Type: text/event-stream&lt;/code&gt;. That&amp;#8217;s what Akka-SSE provides: the SSE Message case classes and serialization to &lt;code&gt;text/event-stream&lt;/code&gt;. Our implicit function &lt;code&gt;stringToSseMessage&lt;/code&gt; allows the Scala types to align so the &amp;#8220;stream pipes&amp;#8221; can be attached together. In our case, we produce a stream of &lt;code&gt;String&lt;/code&gt;s, our RabbitMq message body. We need to produce a stream of &lt;code&gt;SSE.Messages&lt;/code&gt; so we add a simple conversion function. When a new client connects, they&amp;#8217;ll attach themselves to the consuming RabbitMq &lt;code&gt;Source&lt;/code&gt;. Akka-Http lets you natively complete a route with a &lt;code&gt;Flow&lt;/code&gt;; Akka-Sse simply completes that &lt;code&gt;Flow&lt;/code&gt; with the proper Http response for SSE.&lt;/p&gt;

&lt;h2 id=&#34;trying-it-out:a9237168f3920272915cff712cbdae5e&#34;&gt;Trying It Out&lt;/h2&gt;

&lt;p&gt;Fire up SBT and run &lt;code&gt;~reStart&lt;/code&gt;, ensuring you have RabbitMq running and set up a queue named &lt;code&gt;streams-playground&lt;/code&gt; (&lt;a href=&#34;https://github.com/mhamrah/streams-playground/blob/master/README.md&#34;&gt;see the README&lt;/a&gt;). In your console, try a simple &lt;code&gt;curl&lt;/code&gt; command:&lt;/p&gt;

&lt;pre class=&#34;bash&#34;&gt;$ curl http://localhost:8080/messages
&lt;/pre&gt;

&lt;p&gt;The curl command won&amp;#8217;t return. Start sending messages via the RabbitMq Admin console and you&amp;#8217;ll see the SSE output in action:&lt;/p&gt;

&lt;pre class=&#34;bash&#34;&gt;$ curl localhost:8080/messages
event:published
data:woot!

event:published
data:another message!
&lt;/pre&gt;

&lt;p&gt;Close the curl command, and fire up your browser at &lt;code&gt;http://localhost:8080&lt;/code&gt; you&amp;#8217;ll see a simple web page (served from the &lt;code&gt;assets&lt;/code&gt; route). Continue sending messages via RabbitMq, and those messages will be added to the dom. Most modern browsers natively support SSE with the &lt;code&gt;EventSource&lt;/code&gt; object. The following gist creates an event listener on the &lt;code&gt;&#39;published&#39;&lt;/code&gt; event, which is produced from our &lt;code&gt;implicit string =&amp;gt; sse&lt;/code&gt; function above:&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s also handlers for opening the initial sse connection and any errors produced. You could also add more events; our simple conversion only goes from a &lt;code&gt;String&lt;/code&gt; to one specific SSE of type &lt;code&gt;published&lt;/code&gt;. You could map a set of case classes&amp;#8211;preferably an algebraic data type&amp;#8211;to a set of events for the client. Most modern browsers support &lt;code&gt;EventStream&lt;/code&gt;; there&amp;#8217;s no need a for an additional framework or library. The gist above includes a test I copied from the &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/eventsource/basics/&#34;&gt;html5 rocks page on SSE&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;a-naive-implementation:a9237168f3920272915cff712cbdae5e&#34;&gt;A Naive Implementation&lt;/h2&gt;

&lt;p&gt;If you open up multiple browsers to localhost, or &lt;code&gt;curl http://localhost:8080/messages&lt;/code&gt; a few times, you&amp;#8217;ll notice that a published message only goes to one client. This is because our initial RabbitMq &lt;code&gt;Source&lt;/code&gt; only consumes one message from a queue, and passes that down the stream pipeline. That single message will only go to one of the connected clients; there&amp;#8217;s no fanout or broadcasting. You can do that with either RabbitMq or Akka-Streams, try experimenting for yourself!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>A Gentle Introduction To Akka Streams</title>
          <link>http://blog.michaelhamrah.com/2015/01/a-gentle-introduction-to-akka-streams/</link>
          <pubDate>Tue, 13 Jan 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/01/a-gentle-introduction-to-akka-streams/</guid>
          <description>

&lt;p&gt;I&amp;#8217;m happy to see stream-based programming emerge as a paradigm in many languages. Streams have been around for a while: take a look at the good &amp;#8216;ol | operator in Unix. Streams offer an interesting conceptual model to processing pipelines that is very functional: you have an input, you produce an output. You string these little functions together to build bigger, more complex pipelines. Most of the time you can make these functions asynchronous and parallelize them over input data to maximize throughput and scale. With a Stream, handling data is almost hidden behind the scenes: it just &lt;em&gt;flows&lt;/em&gt; through &lt;em&gt;functions&lt;/em&gt;, producing a new output from some input. In the case of an Http server, the Request-Response model across all clients is a Stream-based process: You map a Request to a Response, passing it through various functions which act on an input. Forget about MVC, it&amp;#8217;s all middleware. No need to set variables, iterate over collections, orchestrate function calls. Just concatenate stream-enabled functions together, and run your code. Streams offer a succinct programming model for a process. The fact it also scales is a nice bonus.&lt;/p&gt;

&lt;p&gt;Stream based programming is possible in a variety of languages, and I encourage you to explore this space. There&amp;#8217;s an excellent &lt;a href=&#34;https://github.com/substack/stream-handbook&#34;&gt;stream handbook for Node&lt;/a&gt;, &lt;a href=&#34;https://github.com/matz/streem&#34;&gt;an exploratory stream language from Yukihiro &amp;#8220;Matz&amp;#8221; Matsumoto of Ruby fame&lt;/a&gt;, &lt;a href=&#34;https://spark.apache.org/streaming/&#34;&gt;Spark Streaming&lt;/a&gt; and of course &lt;a href=&#34;http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M2/index.html&#34;&gt;Akka-Streams&lt;/a&gt; which joins the existing &lt;a href=&#34;https://github.com/scalaz/scalaz-stream&#34;&gt;scalaz-stream&lt;/a&gt; library for Scala. Even Go&amp;#8217;s &lt;a href=&#34;http://golang.org/pkg/net/http/#HandleFunc&#34;&gt;HttpHandler function&lt;/a&gt; is Stream-esque: you can easily wrap one function around another, building up a flow, and manipulate the Response stream accordingly.&lt;/p&gt;

&lt;h2 id=&#34;why-akka-streams:9c0e63de68271e30d1a6e002245492be&#34;&gt;Why Akka-Streams?&lt;/h2&gt;

&lt;p&gt;Akka-Streams provide a higher-level abstraction over Akka&amp;#8217;s existing actor model. The Actor model provides an excellent primitive for writing concurrent, scalable software, but it still is a primitive; it&amp;#8217;s not hard to find a few critiques of the model. So is it possible to have your cake and eat it too? Can we abstract the functionality we want to achieve with Actors into a set of function calls? Can we treat Actor Messages as Inputs and Outputs to Functions, with type safety? Hello, Akka-Streams.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s an excellent &lt;a href=&#34;http://www.typesafe.com/activator/template/akka-stream-scala&#34;&gt;activator template for Akka-Streams&lt;/a&gt; offering an in-depth tutorial on several aspects of Akka-Streams. For a more a gentler introduction, read on.&lt;/p&gt;

&lt;h2 id=&#34;the-recipe:9c0e63de68271e30d1a6e002245492be&#34;&gt;The Recipe&lt;/h2&gt;

&lt;p&gt;To cook up a reasonable dish, we are going to consume messages from &lt;a href=&#34;https://www.rabbitmq.com&#34;&gt;RabbitMq&lt;/a&gt; with the &lt;a href=&#34;https://github.com/ScalaConsultants/reactive-rabbit&#34;&gt;reactive-rabbit&lt;/a&gt; library and output them to the console. The code is on &lt;a href=&#34;https://github.com/mhamrah/streams-playground&#34;&gt;GitHub&lt;/a&gt;. If you&amp;#8217;d like to follow along, &lt;code&gt;git clone&lt;/code&gt; and then &lt;code&gt;git checkout intro&lt;/code&gt;; hopefully I&amp;#8217;ll build up more functionality in later posts so the master branch may differ.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s start with a code snippet:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;object RabbitMqConsumer {
 def consume(implicit flowMaterializer: FlowMaterializer) = {
    Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
      .foreach(println(_))
  }
}
&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;We use a RabbitMq connection to consume messages off of a queue named &lt;code&gt;streams-playground&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;For each message, we pull out the message and decode the bytes as a UTF-8 string&lt;/li&gt;
&lt;li&gt;We print it to the console&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-ingredients:9c0e63de68271e30d1a6e002245492be&#34;&gt;The Ingredients&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;Source&lt;/code&gt; is something which produces exactly one output. If you need something that generates data, you need a &lt;code&gt;Source&lt;/code&gt;. Our source above is produced from the &lt;code&gt;connection.consume&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;Sink&lt;/code&gt; is something with exactly one input. A &lt;code&gt;Sink&lt;/code&gt; is the final stage of a Stream process. The &lt;code&gt;.foreach&lt;/code&gt; call is a Sink which writes the input (_) to the console via &lt;code&gt;println&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;Flow&lt;/code&gt; is something with exactly one input and one output. It allows data to flow through a function: like calling &lt;code&gt;map&lt;/code&gt; which also returns an element on a collection. The &lt;code&gt;map&lt;/code&gt; call above is a &lt;code&gt;Flow&lt;/code&gt;: it consumes a &lt;code&gt;Delivery&lt;/code&gt; message and outputs a &lt;code&gt;String&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to actually run something using Akka-Streams you must have both a &lt;code&gt;Source&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; attached to the same pipeline. This allows you to create a &lt;code&gt;RunnableFlow&lt;/code&gt; and begin processing the stream. Just as you can compose functions and classes, you can compose streams to build up richer functionality. It&amp;#8217;s a powerful abstraction allowing you to build your processing logic independently of its execution. Think of stream libraries where you &amp;#8220;plug in&amp;#8221; parts of streams together and customize accordingly.&lt;/p&gt;

&lt;h2 id=&#34;a-simple-flow:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Simple Flow&lt;/h2&gt;

&lt;p&gt;You&amp;#8217;ll notice the above snippet requires an &lt;code&gt;implicit flowMaterializer: FlowMaterializer&lt;/code&gt;. A &lt;code&gt;FlowMaterializer&lt;/code&gt; is required to actually run a &lt;code&gt;Flow&lt;/code&gt;. In the snippet above &lt;code&gt;foreach&lt;/code&gt; acts as both a &lt;code&gt;Sink&lt;/code&gt; and a &lt;code&gt;run()&lt;/code&gt; call to run the flow. If you look at the Main.scala file you&amp;#8217;ll see I start the stream easily in one call:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;implicit val flowMaterializer = FlowMaterializer()
  RabbitMqConsumer.consume
&lt;/pre&gt;

&lt;p&gt;Create a queue named &lt;code&gt;streams-playground&lt;/code&gt; via the RabbitMq Admin UI and run the application. You can use publish messages in the RabbitMq Admin UI and they will appear in the console. Try some UTF-8 characters, like åßç∂!&lt;/p&gt;

&lt;h2 id=&#34;a-variation:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Variation&lt;/h2&gt;

&lt;p&gt;The original snippet is nice, but it does require the implicit FlowMaterializer to build and run the stream in &lt;code&gt;consume&lt;/code&gt;. If you remove it, you&amp;#8217;ll get a compile error. Is there a way to separate the definition of the stream with the running of the stream? Yes, by simply removing the &lt;code&gt;foreach&lt;/code&gt; call. &lt;code&gt;foreach&lt;/code&gt; is just syntactical sugar for a &lt;code&gt;map&lt;/code&gt; with a &lt;code&gt;run()&lt;/code&gt; call. By explicitly setting a &lt;code&gt;Sink&lt;/code&gt; without a call to &lt;code&gt;run()&lt;/code&gt; we can construct our stream blueprint producing a new object of type &lt;code&gt;RunnableFlow&lt;/code&gt;. Intuitively, it&amp;#8217;s a &lt;code&gt;Flow&lt;/code&gt; which can be &lt;code&gt;run()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the variation:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;def consume() = {
     Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
      .map(println(_))
      .to(Sink.ignore) //won&#39;t start consuming until run() is called!
  }
&lt;/pre&gt;

&lt;p&gt;We got rid of our &lt;code&gt;flowMaterializer&lt;/code&gt; implicit by terminating our Stream with a &lt;code&gt;to()&lt;/code&gt; call and a simple Sink.ignore which discards messages. This stream will not be run when called. Instead we must call it explicitly in Main.scala:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;implicit val flowMaterializer = FlowMaterializer()
  RabbitMqConsumer.consume().run()
&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;ve separated out the entire pipeline into two stages: the build stage, via the &lt;code&gt;consume&lt;/code&gt; call, and the run stage, with &lt;code&gt;run()&lt;/code&gt;. Ideally you&amp;#8217;d want to compose your stream processing as you wire up the app, with each component, like RabbitMqConsumer, providing part of the overall stream process.&lt;/p&gt;

&lt;h2 id=&#34;a-counter-example:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Counter Example&lt;/h2&gt;

&lt;p&gt;As an alternative, explore the &lt;a href=&#34;http://www.rabbitmq.com/tutorials/tutorial-one-java.html&#34;&gt;rabbitmq tutorials&lt;/a&gt; for Java examples. Here&amp;#8217;s a snippet from the site:&lt;/p&gt;

&lt;pre class=&#34;lang:java&#34;&gt;QueueingConsumer consumer = new QueueingConsumer(channel);
    channel.basicConsume(QUEUE_NAME, true, consumer);

    while (true) {
      QueueingConsumer.Delivery delivery = consumer.nextDelivery();
      String message = new String(delivery.getBody());
      System.out.println(&#34; [x] Received &#39;&#34; + message + &#34;&#39;&#34;);
    }
&lt;/pre&gt;

&lt;p&gt;This is typical of an imperative style. Our flow is controlled by the while loop, we have to explicitly manage variables, and there&amp;#8217;s no flow control. We could separate out the body from the while loop, but we&amp;#8217;d have a crazy function signature. Alternatively on the Akka side there&amp;#8217;s the solid &lt;a href=&#34;https://github.com/sstone/amqp-client&#34;&gt;amqp-client library&lt;/a&gt; which provides an Actor based model over RabbitMq:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;// create an actor that will receive AMQP deliveries
  val listener = system.actorOf(Props(new Actor {
    def receive = {
      case Delivery(consumerTag, envelope, properties, body) =&gt; {
        println(&#34;got a message: &#34; + new String(body))
        sender ! Ack(envelope.getDeliveryTag)
      }
    }
  }))

  // create a consumer that will route incoming AMQP messages to our listener
  // it starts with an empty list of queues to consume from
  val consumer = ConnectionOwner.createChildActor(conn, Consumer.props(listener, channelParams = None, autoack = false))
&lt;/pre&gt;

&lt;p&gt;You get the concurrency primitives via configuration over the actor system, but we still enter imperative-programming land in the Actor&amp;#8217;s &lt;code&gt;receive&lt;/code&gt; blog (sure, this can be refactored to some degree). In general, if we can model our process as a set of streams, we achieve the same benefits we get with functional programming: clear composition on what is happening, not how it&amp;#8217;s doing it.&lt;/p&gt;

&lt;p&gt;Streams can be applied in a variety of contexts. I&amp;#8217;m happy to see the amazing and powerful &lt;a href=&#34;http://spray.io&#34;&gt;spray.io&lt;/a&gt; library for Restful web services will be merged into Akka as a stream enabled http toolkit. It&amp;#8217;s also not hard to find out what&amp;#8217;s been done with &lt;a href=&#34;https://github.com/scalaz/scalaz-stream#projects-using-scalaz-stream&#34;&gt;scalaz-streams&lt;/a&gt; or the plethora of tooling already available in other languages.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Clustering Akka Applications with Docker — Version 3</title>
          <link>http://blog.michaelhamrah.com/2014/11/clustering-akka-applications-with-docker-version-3/</link>
          <pubDate>Thu, 27 Nov 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/11/clustering-akka-applications-with-docker-version-3/</guid>
          <description>&lt;p&gt;The SBT Native Packager plugin now offers first-class Docker support for building Scala based applications. My last post involved combining SBT Native Packager, SBT Docker, and a custom start script to launch our application. We can simplify the process in two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Although the SBT Docker plugin allows for better customization of Dockerfiles it&amp;#8217;s unnecessary for our use case. SBT Native Packager is enough.&lt;/li&gt;
&lt;li&gt;A separate start script was required for IP address inspection so TCP traffic can be routed to the actor system. I recently contributed an update for &lt;a href=&#34;https://github.com/sbt/sbt-native-packager/pull/411&#34;&gt;better ENTRYPOINT support within SBT Native Packager&lt;/a&gt; which gives us options for launching our app in a container.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With this PR we can now add our IP address inspection snippet to our build removing the need for extraneous files. We could have added this snippet to &lt;code&gt;bashScriptExtraDefines&lt;/code&gt; but that is a global change, requiring &lt;code&gt;/sbin/ifconfig eth0&lt;/code&gt; to be available wherever the application is run. This is definitely infrastructure bleed-out and must be avoided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;The new code, on GitHub,&lt;/a&gt; uses a shell with ENTRYPOINT exec mode to set our environment variable before launching the application:&lt;/p&gt;

&lt;pre class=&#34;code scala&#34;&gt;dockerExposedPorts in Docker := Seq(1600)

dockerEntrypoint in Docker := Seq(&#34;sh&#34;, &#34;-c&#34;, &#34;CLUSTER_IP=`/sbin/ifconfig eth0 | grep &#39;inet addr:&#39; | cut -d: -f2 | awk &#39;{ print $1 }&#39;` bin/clustering $*&#34;)
&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;$*&lt;/code&gt; allows for command-line parameters to be honored when launching the container. Because the app leverages the Typesafe Config library we can also set via Java system properties:&lt;/p&gt;

&lt;pre class=&#34;code bash&#34;&gt;docker run --rm -i -t --name seed mhamrah/clustering:0.3 -Dclustering.cluster.name=example-cluster
&lt;/pre&gt;

&lt;p&gt;Launching the cluster is exactly as before:&lt;/p&gt;

&lt;pre class=&#34;code bash&#34;&gt;docker run --rm -d --name seed mhamrah/clustering:0.3
docker run --rm -d --name member1 --link seed:seed mhamrah/clustering:0.3
&lt;/pre&gt;

&lt;p&gt;For complex scripts it may be too messy to overload the ENTRYPOINT sequence. For those cases simply bake your own docker container as a base and use the ENTRYPOINT approach to call out to your script. SBT Native Packager will still upload all your dependencies and its bash script to &lt;code&gt;/opt/docker/bin/&amp;lt;your app&amp;gt;&lt;/code&gt;. The Docker &lt;code&gt;WORKDIR&lt;/code&gt; is set to &lt;code&gt;/opt/docker&lt;/code&gt; so you can drop the &lt;code&gt;/opt/docker&lt;/code&gt; as above.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Accelerate Team Development with your own SBT Plugin Defaults</title>
          <link>http://blog.michaelhamrah.com/2014/10/accelerate-team-development-with-your-own-sbt-plugin-defaults/</link>
          <pubDate>Mon, 13 Oct 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/10/accelerate-team-development-with-your-own-sbt-plugin-defaults/</guid>
          <description>

&lt;p&gt;My team manages several Scala services built with SBT. The setup of these projects are very similar, from included plugins, dependencies, and build-and-deploy configurations. At first we simply copied and paste these settings across projects but as the number of services increased the hunt-and-change strategy became laborious. Time to optimize.&lt;/p&gt;

&lt;p&gt;I heard of a few teams that created their own sbt plugins for default settings but couldn&amp;#8217;t find information on how this looked. The recent change to &lt;a href=&#34;http://www.scala-sbt.org/0.13/docs/Plugins.html&#34;&gt;AutoPlugins&lt;/a&gt; also didn&amp;#8217;t help existing documentation. I found Will Sargent&amp;#8217;s excellent post on &lt;a href=&#34;tersesystems.com/2014/06/24/writing-an-sbt-plugin&#34;&gt;writing an sbt plugin&lt;/a&gt; helpful but it wasn&amp;#8217;t what I was looking for. I want a plugin which included other plugins and set defaults for those plugins. The goal is to &amp;#8220;drop in&amp;#8221; this plugin and automatically have a set of defaults: using &lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;sbt-native-packager&lt;/a&gt;, a configured &lt;a href=&#34;https://github.com/sbt/sbt-release&#34;&gt;sbt-release&lt;/a&gt; and our nexus artifact server good-to-go.&lt;/p&gt;

&lt;h2 id=&#34;file-locations:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;File Locations&lt;/h2&gt;

&lt;p&gt;As an sbt refresher anything in the &lt;code&gt;project/&lt;/code&gt; folder relates to the build. If you want to develop your own plugin just for the current project you can simply add your .scala files to &lt;code&gt;project/&lt;/code&gt;. If you want to develop your own plugin as a standalone project you put those files in the &lt;code&gt;src/&lt;/code&gt; directory as usual. I mistakenly thought an sbt plugin project only required files in the &lt;code&gt;project/&lt;/code&gt; folder. Silly me.&lt;/p&gt;

&lt;h2 id=&#34;sbt-builds:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;SBT Builds&lt;/h2&gt;

&lt;p&gt;It&amp;#8217;s important to note that the project folder&amp;#8211;and the build itself&amp;#8211;is separate from how your source code is built. SBT uses Scala 2.10, so anything in the &lt;code&gt;project/&lt;/code&gt; folder will be built against 2.10 even if your project is set to 2.11. Thus when developing your plugin use Scala 2.10 to match sbt.&lt;/p&gt;

&lt;h2 id=&#34;dependencies:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;Dependencies&lt;/h2&gt;

&lt;p&gt;Usually when you include a plugin you specify it in the &lt;code&gt;project/plugins.sbt&lt;/code&gt;, right? But what if you&amp;#8217;re developing a plugin that uses other plugins? Your code is in &lt;code&gt;src/&lt;/code&gt; so it won&amp;#8217;t pick up anything in &lt;code&gt;project/&lt;/code&gt; as that only relates to your build. So you need to add whatever plugin you want as a &lt;code&gt;dependency&lt;/code&gt; in your build so its available in within your project, just like any other dependency. But there&amp;#8217;s a trick with sbt plugins. Originally I had the usual in &lt;code&gt;build.sbt&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;libraryDependencies += &amp;quot;com.typesafe.sbt&amp;quot; % &amp;quot;sbt-native-packager&amp;quot; % &amp;quot;0.8.0-M2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but kept getting unresolved dependency errors. This made no sense to me as the plugin is clearly available. It turns out if you want to include an sbt plugin as a project dependency you need to specify it in a special way, explicitly setting the sbt and scala version you want:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;libraryDependencies += sbtPluginExtra(&amp;quot;com.typesafe.sbt&amp;quot; % &amp;quot;sbt-native-packager&amp;quot; % &amp;quot;0.8.0-M2&amp;quot;, sbtV = &amp;quot;0.13&amp;quot;, scalaV = &amp;quot;2.10&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With that, your dependency will resolve and you can use include anything under sbt-native-packager when developing your plugin.&lt;/p&gt;

&lt;h2 id=&#34;specifying-your-plugin-defaults:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;Specifying your Plugin Defaults&lt;/h2&gt;

&lt;p&gt;With your separate project and dependencies satisfied you can now create your plugin which uses other plugins and defaults settings specific to you. This part is easy and follows the usual documentation. Declare an object which extends AutoPlugin and override &lt;code&gt;projectSettings&lt;/code&gt; or &lt;code&gt;buildSettings&lt;/code&gt;. This class looks exactly like it would if you were setting things manually in your build.&lt;/p&gt;

&lt;p&gt;For instance, here&amp;#8217;s how we&amp;#8217;d set the &lt;code&gt;java_server&lt;/code&gt; archetype as the default in our plugin:&lt;/p&gt;

&lt;pre class=&#34;code scala&#34;&gt;package com.hamrah.plugindefaults

import sbt._
import Keys._
import com.typesafe.sbt.SbtNativePackager._

object PluginDefaults extends AutoPlugin {
 override lazy val projectSettings = packageArchetype.java_server
}
&lt;/pre&gt;

&lt;p&gt;You can concatenate any other settings you want to project settings, like scalaVersion, scalacOptions, etc.&lt;/p&gt;

&lt;h2 id=&#34;using-the-plugin:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;Using the Plugin&lt;/h2&gt;

&lt;p&gt;You can build and publish your plugin to a repo and include it like you would any other plugin. Or you can include it locally for testing by putting this in your sbt file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lazy val root = project.in( file(&amp;quot;.&amp;quot;) ).dependsOn( defaultPluginSettings )
lazy val defaultPluginSettings = uri(&amp;quot;file:///&amp;lt;full path to your plugin directory&amp;gt;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your default settings can be explicitly added to your project if not automatically imported with a simple:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PluginDefaults.projectSettings
//or
settings = PluginDefaults.projectSettings // in a .scala file
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;in-closing:abbf4ae7a81d9cf247973d7e1dba3596&#34;&gt;In Closing&lt;/h2&gt;

&lt;p&gt;As an FYI there could be better ways to do this. A lot of the above was trial and error, but works. If you have feedback or better suggestions please leave a comment!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Spray Directives: Creating Your Own, Simple Directive</title>
          <link>http://blog.michaelhamrah.com/2014/05/spray-directives-creating-your-own-simple-directive/</link>
          <pubDate>Sat, 24 May 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/05/spray-directives-creating-your-own-simple-directive/</guid>
          <description>&lt;p&gt;The &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/&#34;&gt;spray-routing&lt;/a&gt; package provides an excellent dsl for creating restful api&amp;#8217;s with Scala and Akka. This dsl is powered by &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/key-concepts/directives/&#34;&gt;directives&lt;/a&gt;, small building blocks you compose to filter, process and compose requests and responses for your API. Building your own directives lets you create reusable components for your application and better organize your application.&lt;/p&gt;

&lt;p&gt;I recently refactored some code in a Spray API to leverage custom directives. The &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/advanced-topics/custom-directives/&#34;&gt;Spray documentation provides a good reference on custom directives&lt;/a&gt; but I found myself getting hung up in a few places.&lt;/p&gt;

&lt;p&gt;As an example we&amp;#8217;re going to write a custom directive which produces a UUID for each request. Here&amp;#8217;s how I want to use this custom directive:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;generateUUID { uuid =&gt;
  path(&#34;foo&#34;) {
   get {
     //log the uuid, pass it to your app, or maybe just return it
     complete { uuid.toString }
   }
  }
}
&lt;/pre&gt;

&lt;p&gt;Usually you leverage existing directives to build custom directives. I (incorrectly) started with the &lt;code&gt;provide&lt;/code&gt; directive to provide a value to an inner route:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;import spray.routing._
import java.util.UUID
import Directives._

trait UuidDirectives {
  def generateUuid: Directive1[UUID] = {
    provide(UUID.randomUUID)
  }
}
&lt;/pre&gt;

&lt;p&gt;Before I explain what&amp;#8217;s wrong, let&amp;#8217;s dig into the code. First, generateUuid is a function which returns a Directive1 wrapping a UUID value. Directive1 is just a type alias for &lt;code&gt;Directive[UUID :: HNil]&lt;/code&gt;. Directives are centered around a feature of the shapeless library called heterogeneous lists, or HLists. An &lt;code&gt;HList&lt;/code&gt; is simply a list, but each element in the list can be a different, specific type. Instead of a generic &lt;code&gt;List[Any]&lt;/code&gt;, your list can be composed of specific types of list of String, Int, String, UUID. The first element of this list is a String, not an Any, and the second is an Int, with all the features of an Int. In the directive above I just have an &lt;code&gt;HList&lt;/code&gt; with one element: &lt;code&gt;UUID&lt;/code&gt;. If I write &lt;code&gt;Directive[UUID :: String :: HNil]&lt;/code&gt; I have a two element list of &lt;code&gt;UUID&lt;/code&gt; and String, and the compiler will throw an error if I try to use this directive with anything other a &lt;code&gt;UUID&lt;/code&gt; and a String. HLists sound like a lightweight case class, but with an &lt;code&gt;HList&lt;/code&gt;, you get a lot of list-like features. HLists allow the compiler to do the heavy lifting of type safety, so you can have strongly-typed functions to compose together.&lt;/p&gt;

&lt;p&gt;Provide is a directive which (surprise surprise) will provide a value to an inner route. I thought this would be perfect for my directive, and the corresponding test ensures it works:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;import org.scalatest._
import org.scalatest.matchers._
import spray.testkit.ScalatestRouteTest
import spray.http._
import spray.routing.Directives._

class UuidDirectivesSpec
  extends FreeSpec
  with Matchers
  with UuidDirectives
  with ScalatestRouteTest {

  &#34;The UUID Directive&#34; - {
    &#34;can generate a UUID&#34; in {
      Get() ~&gt; generateUuid { uuid =&gt; complete(uuid.toString) } ~&gt; check  {
        responseAs[String].size shouldBe 36
      }
    }
  }
}
&lt;/pre&gt;

&lt;p&gt;But there&amp;#8217;s an issue! Spray directives are classes are composed when instantiated via an apply() function. The &lt;a href=&#34;http://spray.io/documentation/1.2.1/spray-routing/advanced-topics/understanding-dsl-structure/&#34;&gt;Spray docs on understanding the dsl structure&lt;/a&gt; explains it best, but in summary, generateUuid will only be called once when the routing tree is built, not on every request.&lt;/p&gt;

&lt;p&gt;A better unit test shows the issue:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;will generate different UUID per request&#34; in {
      //like the runtime, instantiate route once
      val uuidRoute =  generateUuid { uuid =&gt; complete(uuid.toString) }

      var uuid1: String = &#34;&#34;
      var uuid2: String = &#34;&#34;
      Get() ~&gt; uuidRoute ~&gt; check  {
        responseAs[String].size shouldBe 36
        uuid1 = responseAs[String]
      }
      Get() ~&gt; uuidRoute ~&gt; check  {
        responseAs[String].size shouldBe 36
        uuid2 = responseAs[String]
      }
      //fails!
      uuid1 shouldNot equal (uuid2)
    }
  }
&lt;/pre&gt;

&lt;p&gt;The fix is simple: we need to use the &lt;code&gt;extract&lt;/code&gt; directive which applies the current RequestContext to our route so it&amp;#8217;s called on every request. For our UUID directive we don&amp;#8217;t need anything from the request, just the function which is run for every request:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;trait UuidDirectives {
  def generateUuid: Directive[UUID :: HNil] = {
    extract(ctx =&gt;
        UUID.randomUUID)
  }
}
&lt;/pre&gt;

&lt;p&gt;With our randomUUID call wrapped in an extract directive we have a unique call per request, and our tests pass!&lt;/p&gt;

&lt;p&gt;In a following post we&amp;#8217;ll add some more complexity to our custom directive, stay tuned!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Spray Directives: Custom Directives, Part Two: flatMap</title>
          <link>http://blog.michaelhamrah.com/2014/05/spray-directives-custom-directives-part-two-flatmap/</link>
          <pubDate>Sat, 24 May 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/05/spray-directives-custom-directives-part-two-flatmap/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;http://blog.michaelhamrah.com/2014/05/spray-directives-creating-your-own-simple-directive/&#34;&gt;Our last post covered custom Spray Directives&lt;/a&gt;. We&amp;#8217;re going to expand our UUID directive a little further. Generating a unique ID per request is great, but what if we want the client to pass in an existing unique identifier to act as a correlation id between systems?&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll modify our existing directive by checking to see if the client supplied a correlation-id request-header using the existing &lt;code&gt;optionalHeaderValueByName&lt;/code&gt; directive:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;def generateUuid: Directive[UUID :: HNil] = {
    optionalHeaderValueByName(&#34;correlation-id&#34;) {
      case Some(value) =&gt; provide(UUID.fromString(value))
      case None =&gt; provide(UUID.randomUUID)
    }
  }
&lt;/pre&gt;

&lt;p&gt;Unfortunately this code doesn&amp;#8217;t compile! We get an error because Spray is looking for a Route, which is a function of RequestContext =&amp;gt; Unit:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;[error]  found   : spray.routing.Directive1
[error]     (which expands to)  spray.routing.Directive[shapeless.::]
[error]  required: spray.routing.RequestContext =&gt; Unit
[error]       case Some(value) =&gt; provide(UUID.fromString(value))
&lt;/pre&gt;

&lt;p&gt;What do we do? &lt;code&gt;flatMap&lt;/code&gt; comes to the rescue. Here&amp;#8217;s the deal: we need to transform one directive (&lt;code&gt;optionalHeaderValueByName&lt;/code&gt;) into another directive (one that provides a UUID). We do this by using flatMap to focus on the value in the first directive (the option returned from &lt;code&gt;optionalHeaderValueByName&lt;/code&gt;) and return another value (the UUID). With &lt;code&gt;flatMap&lt;/code&gt; we are basically &amp;#8220;repackaging&amp;#8221; one value into another package.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the updated code which properly compiles:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;def generateUuid: Directive[UUID :: HNil] = {
    //use flatMap to match on the Option returned and provide
    //a new value
    optionalHeaderValueByName(&#34;correlation-id&#34;).flatMap {
      case Some(value) =&gt; provide(UUID.fromString(value))
      case None =&gt; provide(UUID.randomUUID)
    }
  }
&lt;/pre&gt;

&lt;p&gt;and the test:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;can extract a uuid value from the header&#34; in {
      val uuid = java.util.UUID.randomUUID.toString

      Get() ~&gt; addHeader(&#34;correlation-id&#34;, uuid) ~&gt; uuidRoute ~&gt; check {
        responseAs[String] shouldEqual uuid
      }
    }
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a small tweak we&amp;#8217;ll make to our UUID directive to show another example of directive composition. If the client doesn&amp;#8217;t supply a UUID, and we call generateUUID multiple times, we&amp;#8217;ll get different uuids for the same request. This defeats the purpose of a single correlation id, and prevents us from extracting a uuid multiple times per request. A failing test shows the issue:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;can extract the same uuid twice per request&#34; in {
      var uuid1: String =&#34;&#34;
      var uuid2: String = &#34;&#34;
      Get() ~&gt; generateUuid { uuid =&gt;
        {
          uuid1 = uuid.toString
          generateUuid { another =&gt;
            uuid2 = another.toString
            complete(&#34;&#34;)
          }
        }
      } ~&gt; check {
        //fails
        uuid1 shouldEqual uuid2
      }
    }
&lt;/pre&gt;

&lt;p&gt;To fix the issue, if we generate a UUID, we will add it to the request header as if the client supplied it. We&amp;#8217;ll use the mapRequest directive to add the generated UUID to the header.&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;def generateUuid: Directive[UUID :: HNil] = {
    optionalHeaderValueByName(&#34;correlation-id&#34;).flatMap {
      case Some(value) =&gt; provide(UUID.fromString(value))
      case None =&gt;
        val id = UUID.randomUUID
        mapRequest(r =&gt; r.withHeaders(r.headers :+ RawHeader(&#34;correlation-id&#34;, id.toString))) &amp;#038; provide(id)
    }
  }
&lt;/pre&gt;

&lt;p&gt;In my first version I had the mapRequest call and the provide call on separate lines (there was no &amp;amp;). mapRequest was never being called, and it was because mapRequest was not being returned as a value- only the provide directive is returned. We need to &amp;#8220;merge&amp;#8221; these two directives with the &amp;amp; operator. &lt;code&gt;mapRequest&lt;/code&gt; is a no-op returning a Directive0 (a Directive with a Nil HList) so combining it with provide yields a Directive1[UUID], which is exactly what we want.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Running an Akka Cluster with Docker Containers</title>
          <link>http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/</link>
          <pubDate>Sun, 23 Mar 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/</guid>
          <description>

&lt;p&gt;&lt;strong&gt;Update! You can now use SBT-Docker with SBT-Native Packager for a better sbt/docker experience. &lt;a href=&#34;http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/&#34;&gt;Here&amp;#8217;s the new approach&lt;/a&gt; with an &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;updated GitHub repo&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We recently upgraded our vagrant environments to use &lt;a href=&#34;http://docker.io&#34;&gt;docker&lt;/a&gt;. One of our projects relies on &lt;a href=&#34;http://doc.akka.io/docs/akka/2.3.0/common/cluster.html&#34;&gt;akka&amp;#8217;s cluster functionality&lt;/a&gt;. I wanted to easily run an akka cluster locally using docker as sbt can be somewhat tedious. &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;The example project is on github&lt;/a&gt; and the solution is described below.&lt;/p&gt;

&lt;p&gt;The solution relies on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;Sbt Native Packager&lt;/a&gt; to package dependencies and create a startup file.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library for configuring the app&amp;#8217;s ip address and seed nodes. We setup cascading configurations that will look for docker link environment variables if present.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example/blob/master/bin/dockerize&#34;&gt;A simple bash script&lt;/a&gt; to package the app and build the docker container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;#8217;m a big fan of &lt;a href=&#34;http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library and the environment variable overrides come in handy for providing sensible defaults with optional overrides. It&amp;#8217;s the preferred way we configure our applications in upper environments.&lt;/p&gt;

&lt;p&gt;The tricky part of running an akka cluster with docker is knowing the ip address each remote node needs to listen on. An akka cluster relies on each node listening on a specific port and hostname or ip. It also needs to know the port and hostname/ip of a seed node the cluster. As there&amp;#8217;s no catch-all binding we need specific ip settings for our cluster.&lt;/p&gt;

&lt;p&gt;A simple bash script within the container will figure out the current IP for our cluster configuration and &lt;a href=&#34;http://docs.docker.io/en/latest/use/working_with_links_names/&#34;&gt;docker links&lt;/a&gt; pass seed node information to newly launched nodes.&lt;/p&gt;

&lt;h2 id=&#34;first-step-setup-application-configuration:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;First Step: Setup Application Configuration&lt;/h2&gt;

&lt;p&gt;The configuration is the same as that of a normal cluster, but I&amp;#8217;m using substitution to configure the ip address, port and seed nodes for the application. For simplicity I setup a &lt;code&gt;clustering&lt;/code&gt; block with defaults for running normally and environment variable overrides:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;clustering {
 ip = &#34;127.0.0.1&#34;
 ip = ${?CLUSTER_IP}
 port = 1600
 port = ${?CLUSTER_PORT}
 seed-ip = &#34;127.0.0.1&#34;
 seed-ip = ${?CLUSTER_IP}
 seed-ip = ${?SEED_PORT_1600_TCP_ADDR}
 seed-port = 1600
 seed-port = ${?SEED_PORT_1600_TCP_PORT}
 cluster.name = clustering-cluster
}

akka.remote {
    log-remote-lifecycle-events = on
    netty.tcp {
      hostname = ${clustering.ip}
      port = ${clustering.port}
    }
  }
  cluster {
    seed-nodes = [
       &#34;akka.tcp://&#34;${clustering.cluster.name}&#34;@&#34;${clustering.seed-ip}&#34;:&#34;${clustering.seed-port}
    ]
    auto-down-unreachable-after = 10s
  }
}
&lt;/pre&gt;

&lt;p&gt;As an example the &lt;code&gt;clustering.seed-ip&lt;/code&gt; setting will use &lt;em&gt;127.0.0.1&lt;/em&gt; as the default. If it can find a &lt;em&gt;CLUSTER_IP&lt;/em&gt; or a &lt;em&gt;SEED_PORT_1600_TCP_ADDR&lt;/em&gt; override it will use that instead. You&amp;#8217;ll notice the latter override is using docker&amp;#8217;s environment variable pattern for linking: that&amp;#8217;s how we set the cluster&amp;#8217;s seed node when using docker. You don&amp;#8217;t need the &lt;em&gt;CLUSTER_IP&lt;/em&gt; in this example but that&amp;#8217;s the environment variable we use in upper environments and I didn&amp;#8217;t want to change our infrastructure to conform to docker&amp;#8217;s pattern. The cascading settings are helpful if you&amp;#8217;re forced to follow one pattern depending on the environment. We do the same thing for the ip and port of the current node when launched.&lt;/p&gt;

&lt;p&gt;With this override in place we can use substitution to set the seed nodes in the akka cluster configuration block. The expression &lt;code&gt;&amp;quot;akka.tcp://&amp;quot;${clustering.cluster.name}&amp;quot;@&amp;quot;${clustering.seed-ip}&amp;quot;:&amp;quot;${clustering.seed-port}&lt;/code&gt; builds the proper akka URI so the current node can find the seed node in the cluster. Seed nodes avoid potential split-brain issues during network partitions. You&amp;#8217;ll want to run more than one in production but for local testing one is fine. On a final note the cluster-name setting is arbitrary. Because the name of the actor system and the uri must match I prefer not to hard code values in multiple places.&lt;/p&gt;

&lt;p&gt;I put these settings in resources/reference.conf. We could have named this file application.conf, but I prefer bundling configurations as reference.conf and reserving application.conf for external configuration files. A setting in application.conf will override a corresponding reference.conf setting and you probably want to manage application.conf files outside of the project&amp;#8217;s jar file.&lt;/p&gt;

&lt;h2 id=&#34;second-sbt-native-packager:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Second: SBT Native Packager&lt;/h2&gt;

&lt;p&gt;We use the native packager plugin to build a runnable script for our applications. For docker we just need to run &lt;code&gt;universal:stage&lt;/code&gt;, creating a folder with all dependencies in the &lt;code&gt;target/&lt;/code&gt; folder of our project. We&amp;#8217;ll move this into a staging directory for uploading to the docker container.&lt;/p&gt;

&lt;h2 id=&#34;third-the-dockerfile-and-start-script:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Third: The Dockerfile and Start script&lt;/h2&gt;

&lt;p&gt;The dockerfile is pretty simple:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;FROM dockerfile/java

MAINTAINER Michael Hamrah m@hamrah.com

ADD tmp/ /opt/app/
ADD start /opt/start
RUN chmod +x /opt/start

EXPOSE 1600

ENTRYPOINT [ &#34;/opt/start&#34; ]
&lt;/pre&gt;

&lt;p&gt;We start with Dockerfile&amp;#8217;s java base image. We then upload our staging &lt;code&gt;tmp/&lt;/code&gt; folder which has our application from sbt&amp;#8217;s native packager output and a corresponding executable start script described below. I opted for &lt;code&gt;ENTRYPOINT&lt;/code&gt; instead of &lt;code&gt;CMD&lt;/code&gt; so the container is treated like an executable. This makes it easier to pass in command line arguments into the sbt native packager script in case you want to set java system properties or override configuration settings via command line arguments.&lt;/p&gt;

&lt;p&gt;The start script is how we tee up the container&amp;#8217;s IP address for our cluster application:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

CLUSTER_IP=&lt;code&gt;/sbin/ifconfig eth0 | grep &#39;inet addr:&#39; | cut -d: -f2 | awk &#39;{ print $1}&#39;&lt;/code&gt; /opt/app/bin/clustering $@
&lt;/pre&gt;

&lt;p&gt;The script sets an inline environment variable by parsing &lt;code&gt;ifconfig&lt;/code&gt; output to get the container&amp;#8217;s ip. We then run the &lt;em&gt;clustering&lt;/em&gt; start script produced from sbt native packager. The &lt;code&gt;$@&lt;/code&gt; lets us pass along any command line settings set when launching the container into the sbt native packager script.&lt;/p&gt;

&lt;h2 id=&#34;fourth-putting-it-together:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Fourth: Putting It Together&lt;/h2&gt;

&lt;p&gt;The last part is a simple bash script named &lt;code&gt;dockerize&lt;/code&gt; to orchestrate each step. By running this script we run sbt native packager, move files to a staging directory, and build the container:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

echo &#34;Build docker container&#34;

#run sbt native packager
sbt universal:stage

#cleanup stage directory
rm -rf docker/tmp/

#copy output into staging area
cp -r target/universal/stage/ docker/tmp/

#build the container, remove intermediate nodes
docker build -rm -t clustering docker/

#remove staged files
rm -rf docker/tmp/
&lt;/pre&gt;

&lt;p&gt;With this in place we simply run&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;bin/dockerize
&lt;/pre&gt;

&lt;p&gt;to create our docker container named clustering.&lt;/p&gt;

&lt;h2 id=&#34;running-the-application-within-docker:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Running the Application within Docker&lt;/h2&gt;

&lt;p&gt;With our clustering container built we fire up our first instance. This will be our seed node for other containers:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -i -t -name seed clustering
2014-03-23 00:20:39,918 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:20:40,392 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:20:40,403 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:20:40,418 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:20:41,404 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
&lt;/pre&gt;

&lt;p&gt;Next we fire up a second node. Because of our reference.conf defaults all we need to do is link this container with the name &lt;em&gt;seed&lt;/em&gt;. Docker will set the environment variables we are looking for in the bundled reference.conf:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -name c1 -link seed:seed -i -t clustering
2014-03-23 00:22:49,332 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:22:49,788 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:22:49,797 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:22:50,238 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
2014-03-23 00:22:50,249 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:22:50,803 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.3:1600
&lt;/pre&gt;

&lt;p&gt;You&amp;#8217;ll see the current leader discovering new nodes and the appropriate broadcast messages sent out. We can even do this a third time and all nodes will react:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -name c2 -link seed:seed -i -t clustering
2014-03-23 00:24:52,768 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:24:53,224 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:24:53,235 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:24:53,470 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
2014-03-23 00:24:53,472 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.3:1600
2014-03-23 00:24:53,478 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:24:55,401 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.4:1600
&lt;/pre&gt;

&lt;p&gt;Try killing a node and see what happens!&lt;/p&gt;

&lt;h2 id=&#34;modifying-the-docker-start-script:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Modifying the Docker Start Script&lt;/h2&gt;

&lt;p&gt;There&amp;#8217;s another reason for the docker start script: it opens the door for different seed discovery options. Container linking works well if everything is running on the same host but not when running on multiple hosts. Also setting multiple seed nodes via docker links will get tedious via environment variables; it&amp;#8217;s doable but we&amp;#8217;re getting into coding-cruft territory. It would be better to discover seed nodes and set that configuration via command line parameters when launching the app.&lt;/p&gt;

&lt;p&gt;The start script gives us control over how we discover information. We could use &lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;etcd&lt;/a&gt;, &lt;a href=&#34;http://www.serfdom.io/&#34;&gt;serf&lt;/a&gt; or even zookeeper to manage how seed nodes are set and discovered, passing this to our application via environment variables or additional command line parameters. Seed nodes can easily be set via system properties set via the command line:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;-Dakka.cluster.seed-nodes.0=akka.tcp://ClusterSystem@host1:2552
-Dakka.cluster.seed-nodes.1=akka.tcp://ClusterSystem@host2:2552
&lt;/pre&gt;

&lt;p&gt;The start script can probably be configured via sbt native packager but I haven&amp;#8217;t looked into that option. Regardless this approach is (relatively) straight forward to run akka clusters with docker. The &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;full project is on github&lt;/a&gt;. If there&amp;#8217;s a better approach I&amp;#8217;d love to know!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Testing Akka’s FSM: Using setState for unit testing</title>
          <link>http://blog.michaelhamrah.com/2014/03/testing-akkas-fsm-using-setstate-for-unit-testing/</link>
          <pubDate>Fri, 21 Mar 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/03/testing-akkas-fsm-using-setstate-for-unit-testing/</guid>
          <description>&lt;p&gt;I wrote &lt;a href=&#34;http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/&#34;&gt;about Akka&amp;#8217;s Finite State Machine&lt;/a&gt; as a way to model a process. One thing I didn&amp;#8217;t discuss was testing an FSM. Akka has &lt;a href=&#34;http://doc.akka.io/docs/akka/snapshot/scala/testing.html&#34;&gt;great testing support&lt;/a&gt; and FSM&amp;#8217;s can easily be tested using the &lt;a href=&#34;http://doc.akka.io/api/akka/snapshot/index.html#akka.testkit.TestFSMRef&#34;&gt;&lt;code&gt;TestFSMRef&lt;/code&gt;&lt;/a&gt; class.&lt;/p&gt;

&lt;p&gt;An FSM is defined by its states and the data stored between those states. For each state in the machine you can match on both an incoming message and current state data. Our previous example modeled a process to check data integrity across two systems. We&amp;#8217;ll continue that example by adding tests to ensure the FSM is working correctly. &lt;a href=&#34;http://blog.michaelhamrah.com/2013/02/embracing-test-driven-development-for-speed/&#34;&gt;These should have been before we developed the FSM&lt;/a&gt; but late tests are (arguably) better than no tests.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s important to test combinations of messages against various states and data. You don&amp;#8217;t want to be in a position to run through a state machine to the state you want for every test. Luckily, there&amp;#8217;s a handy &lt;code&gt;setState&lt;/code&gt; method to explicitly set the current state and data of the FSM. This lets you &amp;#8220;fast forward&amp;#8221; the FSM to the exact state you want to test against.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s say we want to test a &lt;code&gt;DataRetrieved&lt;/code&gt; message in the &lt;code&gt;PendingComparison&lt;/code&gt; state. We also want to test this message against various &lt;code&gt;Data&lt;/code&gt; combinations. We can set this state explicitly:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;The ComparisonEngine&#34; - {
  &#34;in the PendingComparison state&#34; - {
    &#34;when a DataRetrieved message arrives from the old system&#34; - {
      &#34;stays in PendingComparison with updated data when no other data is present&#34; in {
        val fsm = TestFSMRef(new ComparisonEngine())
        
        //set our initial state with setState
        fsm.setState(PendingComparison, ComparisonStatus(&#34;someid&#34;, None, None))

        fsm ! DataRetrieved(&#34;oldSystem&#34;, &#34;oldData&#34;)

        fsm.stateName should be (PendingComparison)
        fsm.stateData should be (ComparisonStatus(&#34;someid&#34;, Some(&#34;oldData&#34;), None))
      }
    }
  }
}
&lt;/pre&gt;

&lt;p&gt;It may be tempting to send more messages to continue verifying the FSM is working correctly. This will yield a large, unwieldy and brittle test. It will make refactoring difficult and make it harder to understand what &lt;em&gt;should&lt;/em&gt; be happening.&lt;/p&gt;

&lt;p&gt;Instead, to further test the FSM, be explicit about the current state and what should happen next. Add a test for it:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;when a DataRetrieved message arrives from the old system&#34; - {
  &#34;moves to AllRetrieved when data from the new system is already present&#34; in {
    val fsm = TestFSMRef(new ComparisonEngine())
        
    //set our initial state with setState
    fsm.setState(PendingComparison, ComparisonStatus(&#34;someid&#34;, None, Some(&#34;newData&#34;)))

    fsm ! DataRetrieved(&#34;oldSystem&#34;, &#34;oldData&#34;)
    fsm.stateName should be (AllRetrieved)
    fsm.stateData should be (ComparisonStatus(&#34;someid&#34;, Some(&#34;oldData&#34;), Some(&#34;newData&#34;)))
  }
}
&lt;/pre&gt;

&lt;p&gt;By mixing in ImplicitSender or using TestProbes we can also verify messages the FSM should be sending in response to incoming messages.&lt;/p&gt;

&lt;p&gt;Testing is an essential part of developing applications. Unit tests should be explicit and granular. For higher level orchestration integration tests, taking a black-box approach, provide ways to oversee entire processes. Don&amp;#8217;t let your code become too unwieldy to manage: use the tools at your disposal and good coding practices to stay lean. Akka&amp;#8217;s FSM provides ways of programming transitional behavior over time and Akka&amp;#8217;s FSM Testkit support provides a way of ensuring that code works over time.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Using Typesafe’s Config for Scala (and Java) for Application Configuration</title>
          <link>http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/</link>
          <pubDate>Sun, 23 Feb 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/</guid>
          <description>

&lt;p&gt;I recently leveraged &lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library to refactor configuration settings for a project. I was very pleased with the API and functionality of the library.&lt;/p&gt;

&lt;p&gt;The documentation is pretty solid so there&amp;#8217;s no need to go over basics. One feature I like is the clear hierarchy when specifying configuration values. I find it helpful to put as much as possible in a reference.conf file in the /resources directory for an application or library. These can get overridden in a variety of ways, primarily by adding an application.conf file to the bundled output&amp;#8217;s classpath. The &lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;sbt native packager&lt;/a&gt;, helpful for deploying applications, makes it easy to attach a configuration file to an output. This is helpful if you have settings which you normally wouldn&amp;#8217;t want to use during development, say using remote actors with akka. I find placing a reasonable set of defaults in a reference.conf file allows you to easily transport a configuration around while still overriding it as necessary. Otherwise you can get into copy and paste hell by duplicating configurations across multiple files for multiple environments.&lt;/p&gt;

&lt;h2 id=&#34;alternative-overrides:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Alternative Overrides&lt;/h2&gt;

&lt;p&gt;There are two other interesting ways you can override configuration settings: using environment variables or java system properties. The environment variable approach comes in very handy when pushing to cloud environments where you don&amp;#8217;t know what a configuration is beforehand. Using the ${?VALUE} pattern a property will only be set if a value exists. This allows you to provide an option for overriding a value without actually having to specify one.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s an example in a conf file using substitution leveraging this technique:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http {
 port = 8080
 port = ${?HTTP_PORT}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;re setting a default port of 8080. If the configuration can find a valid substitute it will replace the port value with the substitute; otherwise, it will keep it at 8080. The configuration library will look up its hierarchy for an HTTP_PORT value, checking other configuration files, Java system properties, and finally environment variables. Environment variables aren&amp;#8217;t perfect, but they&amp;#8217;re easy to set and leveraged in a lot of places. If you leave out the ? and just have ${HTTP_PORT} then the application will throw an exception if it can&amp;#8217;t find a value. But by using the ? you can override as many times as you want. This can be helpful when running apps on Heroku where environment variables are set for third party services.&lt;/p&gt;

&lt;h3 id=&#34;using-java-system-properties:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Using Java System Properties&lt;/h3&gt;

&lt;p&gt;Java system properties provide another option for setting config values. The shell script created by sbt-native-packager supports java system properties, so you can also set the http port via the command line using the -D flag:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/bash_script_from_native_packager -Dhttp.port=8081
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be helpful if you want to run an akka based application with a different log level to see what&amp;#8217;s going on in production:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/some_akka_app_script -Dakka.loglevel=debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately sbt run doesn&amp;#8217;t support java system properties so you can&amp;#8217;t tweak settings with the command line when running sbt. The &lt;a href=&#34;https://github.com/spray/sbt-revolver&#34;&gt;sbt-revolver&lt;/a&gt; plugin, which allows you to run your app in a forked JVM, does allow you to pass java arguments using the command line. Once you&amp;#8217;re set up with this plugin you can change settings by adding your Java overrides after &lt;code&gt;---&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;re-start --- -Dhttp.port=8081
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;with-c3p0:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;With c3p0&lt;/h3&gt;

&lt;p&gt;I was really excited to see that the &lt;a href=&#34;http://www.mchange.com/projects/c3p0/#c3p0_conf&#34;&gt;c3p0 connection pool library also supports Typesafe Config&lt;/a&gt;. So you can avoid those annoying xml-based files and merge your c3p0 settings directly with your regular configuration files. I&amp;#8217;ve migrated an application to a &lt;a href=&#34;docker.io&#34;&gt;docker&lt;/a&gt; based development environment and used this c3p0 feature with &lt;a href=&#34;http://docs.docker.io/en/latest/use/working_with_links_names/&#34;&gt;docker links&lt;/a&gt; to set mysql settings:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app {
 db {
  host = localhost
  host = ${?DB_PORT_3306_TCP_ADDR}
  port = &amp;quot;3306&amp;quot;
  port = ${?DB_PORT_3306_TCP_PORT}
 }
}

c3p0 {
 named-configs {
  myapp {
      jdbcUrl = &amp;quot;jdbc:mysql://&amp;quot;${app.db.host}&amp;quot;:&amp;quot;${app.db.port}&amp;quot;/MyDatabase&amp;quot;
  }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When I link a mysql container to my app container with &lt;code&gt;--link mysql:db&lt;/code&gt; Docker will inject the DB_PORT_3306_TCP_* environment variables which are pulled by the above settings.&lt;/p&gt;

&lt;h3 id=&#34;accessing-values-from-code:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Accessing Values From Code&lt;/h3&gt;

&lt;p&gt;One other practice I like is having a single &amp;#8220;Config&amp;#8221; class for an application. It can be very tempting to load a configuration node from anywhere in your app but that can get messy fast. Instead, create a config class and access everything you need through that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;object MyAppConfig {
  private val config =  ConfigFactory.load()

  private lazy val root = config.getConfig(&amp;quot;my_app&amp;quot;)

  object HttpConfig {
    private val httpConfig = config.getConfig(&amp;quot;http&amp;quot;)

    lazy val interface = httpConfig.getString(&amp;quot;interface&amp;quot;)
    lazy val port = httpConfig.getInt(&amp;quot;port&amp;quot;)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Type safety, Single Responsibility, and no strings all over the place.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;When dealing with configuration think about what environments you have and what the actual differences are between those environments. Usually this is a small set of differing values for only a few properties. Make it easy to change just those settings without changing&amp;#8211;or duplicating&amp;#8211;anything else. This could done via environment variables, command line flags, even loading configuration files from a url. Definitely avoid copying the same value across multiple configurations: just distill that value down to a lower setting in a hierarchy. By minimizing configuration files you&amp;#8217;ll be making your life a lot easier.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re developing an app for distribution, or writing a library, providing a well-documented configuration file (&lt;a href=&#34;https://github.com/spray/spray/blob/master/spray-can/src/main/resources/reference.conf&#34;&gt;spray&amp;#8217;s spray-can reference.conf is an excellent example&lt;/a&gt;) you can allow users to override defaults easily in a manner that is suitable for them and their runtimes.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Using Akka’s ClusterClient</title>
          <link>http://blog.michaelhamrah.com/2014/02/using-akkas-clusterclient/</link>
          <pubDate>Wed, 05 Feb 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/02/using-akkas-clusterclient/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve been spending some time implementing a feature which leverages Akka&amp;#8217;s &lt;a href=&#34;http://doc.akka.io/docs/akka/2.2.3/contrib/cluster-client.html&#34;&gt;ClusterClient&lt;/a&gt; (&lt;a href=&#34;http://doc.akka.io/api/akka/2.2.3/index.html#akka.contrib.pattern.ClusterClient&#34;&gt;api docs&lt;/a&gt;). A ClusterClient can be useful if:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You are running a service which needs to talk to another service in a cluster, but you don&amp;#8217;t that service to be in the cluster (cluster roles are another option for interconnecting services where separate hosts are necessary but I&amp;#8217;m not sold on them just yet).&lt;/li&gt;
&lt;li&gt;You don&amp;#8217;t want the overhead of running an http client/server interaction model between these services, but you&amp;#8217;d like similar semantics. Spray is a great akka framework for api services but you may not want to write a Spray API or use an http client library.&lt;/li&gt;
&lt;li&gt;You want to use the same transparency of local-to-remote actors but don&amp;#8217;t want to deal with remote actorref configurations to specific hosts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The documentation was a little thin on some specifics so getting started wasn&amp;#8217;t as smooth sailing as I&amp;#8217;d like. Here are some gotchas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You only need &lt;code&gt;akka.extensions = [&amp;quot;akka.contrib.pattern.ClusterReceptionistExtension&amp;quot;]&lt;/code&gt; on the Host (Server) Cluster. (If your client isn&amp;#8217;t a cluster, you&amp;#8217;ll get a runtime exception).&lt;/li&gt;
&lt;li&gt;&amp;#8220;Receptionist&amp;#8221; is the default name for the Host Cluster actor managing ClusterClient connections. Your ClusterClient connects first to the receptionist (via the set of initial contacts) then can start sending messages to actors in the Host Cluster. The name is configurable.&lt;/li&gt;
&lt;li&gt;The client actor system using the ClusterClient needs to have a Netty port open. You must use either actor.cluster.ClusterActorRefProvider or actor.remote.RemoteActorRefProvider. Otherwise the Host Cluster and ClusterClient can&amp;#8217;t establish proper communication. You can use the ClusterActorRefProvider on the client even you&amp;#8217;re not running a cluster.&lt;/li&gt;
&lt;li&gt;As a ClusterClient you wrap messages with a ClusterClient.send (or sendAll) message first. (I was sending vanilla messages and they weren&amp;#8217;t going through, but this is in the docs).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ClusterClients are worth checking out if you want to create physically separate yet interconnected systems but don&amp;#8217;t want to go through the whole load-balancer or http-layer setup. Just another tool in the Akka toolbelt!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Programming Akka’s Finite State Machines in Scala</title>
          <link>http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/</link>
          <pubDate>Thu, 16 Jan 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/</guid>
          <description>&lt;p&gt;Over the past few months my team has been building a new suite of services using Scala and Akka. An interesting aspect of Akka&lt;/p&gt;

&lt;p&gt;we leverage is its &lt;a href=&#34;http://doc.akka.io/docs/akka/2.2.3/scala/fsm.html&#34;&gt;Finite State Machine&lt;/a&gt; support. Finite State Machines&lt;/p&gt;

&lt;p&gt;are a staple of computer programming although not often used in practice. A conceptual process can usually be represented with a finite state machine: there are a defined number of states with explicit transitions between states. If we have a vocabulary&lt;/p&gt;

&lt;p&gt;around these states and transitions we can program the state machine.&lt;/p&gt;

&lt;p&gt;A traditional implementation of an FSM is to check and maintain state explicitly via if/else conditions, use the state design pattern, or implement some other construct. Using Akka&amp;#8217;s FSM support, which explicitly defines states and offers transition hooks, allows us to easily implement our conceptual model of a process. FSM is built on top of Akka&amp;#8217;s actor model giving excellent concurrency controls so we can run many of these state machines simultaneously. You can implement your own FSM with Akka&amp;#8217;s normal actor behavior with the &lt;em&gt;become&lt;/em&gt; method to change the partial function handling messages. However FSM offers some nice hooks plus data management in addition to just changing behavior.&lt;/p&gt;

&lt;p&gt;As an example we will use Akka&amp;#8217;s FSM support to check data in two systems. Our initial process is fairly simplistic but provides a good overview of leveraging Finite State Machines. Say we are rolling out a new system and we want to ensure data flows to both the old and new system. We need a process which waits a certain&lt;/p&gt;

&lt;p&gt;amount of time for data to appear in both places. If data is found in both systems we will check the data for consistency,&lt;/p&gt;

&lt;p&gt;if data is never found after a threshold we will alert data is out of sync.&lt;/p&gt;

&lt;p&gt;Based on our description we have four states. We define our states using a sealed trait:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;sealed trait ComparisonStates
case object AwaitingComparison extends ComparisonStates
case object PendingComparison extends ComparisonStates
case object AllRetrieved extends ComparisonStates
case object DataUnavailable extends ComparisonStates
&lt;/pre&gt;

&lt;p&gt;Next we define the data we manage between state transitions. We need to manage an identifier with data from&lt;/p&gt;

&lt;p&gt;the old and new system. Again we use a sealed trait:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;sealed trait Data
case object Uninitialized extends Data
case class ComparisonStatus(id: String, oldSystem: Option[SomeData] = None, newSystem: Option[SomeData] = None) extends Data
&lt;/pre&gt;

&lt;p&gt;A state machine is just a normal actor with the FSM trait mixed in. We declare our&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;ComparisonEngine&lt;/pre&gt;

&lt;p&gt;actor with FSM support,&lt;/p&gt;

&lt;p&gt;specifying our applicable state and data types:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;class ComparisonEngine extends Actor with FSM[ComparisonStates, Data] {
}
&lt;/pre&gt;

&lt;p&gt;Instead of handling messages directly in a receive method FSM support creates an additional layer of messaging handling.&lt;/p&gt;

&lt;p&gt;When using FSM you match on both message and current state. Our FSM only handles two messages: &lt;em&gt;Compare(id: Int)&lt;/em&gt; and&lt;/p&gt;

&lt;p&gt;&lt;em&gt;DataRetrieved(system: String, someData: SomeData)&lt;/em&gt;. You can construct your data types and messages any way&lt;/p&gt;

&lt;p&gt;you please. I like to keep states abstract as we can generalize on message handling. This&lt;/p&gt;

&lt;p&gt;prevents us from dealing with too many states and state transitions.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s start implementing the body of our &lt;em&gt;ComparisonEngine&lt;/em&gt;. We will start with our initial state:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;startWith(AwaitingComparison, Uninitialized)

when(AwaitingComparison) {
  case Event(Compare(id), Uninitialized) =&gt;
    goto(PendingComparison) using ComparisonStatus(id)
}
&lt;/pre&gt;

&lt;p&gt;We simply declare our initial state is AwaitingComparison, and the only message we are willing to process is a Compare.&lt;/p&gt;

&lt;p&gt;When we receive this message we go to a new state&amp;#8211;PendingComparison&amp;#8211;and set some data. Notice how we aren&amp;#8217;t actually doing anything else?&lt;/p&gt;

&lt;p&gt;A great aspect of FSM is the ability to listen on state transitions. This allows us to separate state transition logic from state transition&lt;/p&gt;

&lt;p&gt;actions. When we transition from an initial state to a PendingComparison state we want to ask our two systems for data. We simply match&lt;/p&gt;

&lt;p&gt;on state transitions and add our applicable logic:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;onTransition {
    case AwaitingComparison -&gt; PendingComparison =&gt;
      nextStateData match {
        case ComparisonStatus(id, old, new) =&gt; {
          oldSystemChecker ! VerifyData(id)
          newSystemChecker ! VerifyData(id)
        }
      }
    }
&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;oldSystemChecker&lt;/em&gt; and &lt;em&gt;newSystemChecker&lt;/em&gt; are actors responsible for verifying data in their respective systems. These can be passed in to the FSM as constructor arguments, or you can have the FSM create the actors and supervise their lifecycle.&lt;/p&gt;

&lt;p&gt;These two actors will send a DataRetrieved message back to our FSM when data is present. Because we are now in the &lt;em&gt;PendingComparison&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;state we specify our new state transition actions against a set of possible scenarios:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;when(PendingComparison, stateTimeout = 15 minutes) {
  case Event(DataRetrieved(&#34;old&#34;, old), ComparisonStatus(id, _, None)) =&gt; {
    stay using ComparisonStatus(id, Some(old), None)
  }
  case Event(DataRetrieved(&#34;new&#34;, new), ComparisonStatus(id, None, _)) =&gt; {
    stay using ComparisonStatus(id, None, Some(new))
  }
  case Event(StateTimeout, c: ComparisonStatus) =&gt; {
    goto(IdUnavailable) using c
  }
  case Event(DataRetrieved(system, data), cs @ ComparisonStatus(_, _, _)) =&gt; {
    system match {
      case &#34;old&#34; =&gt; goto(AllRetrieved) using cs.copy(old = Some(data))
      case &#34;new&#34; =&gt; goto(AllRetrieved) using cs.copy(new = Some(data))
    }
  }
}
&lt;/pre&gt;

&lt;p&gt;Our snippet says we will wait 15 minutes for our systemChecker actors to return with data, otherwise, we&amp;#8217;ll timeout and go to the unavailable state. Either the old&lt;/p&gt;

&lt;p&gt;system or new system will return first, in which case, one set of data in our ComparisonStatus will be None. So we stay in the PendingComparison state until&lt;/p&gt;

&lt;p&gt;the other system returns. If our previous pattern matches do not match, we know the current message we are processing is the final message. Notice how we don&amp;#8217;t care how these actors are getting their data. That&amp;#8217;s the responsibility of the child actors.&lt;/p&gt;

&lt;p&gt;Once we have all our data,&lt;/p&gt;

&lt;p&gt;so we go to the AllRetrieved state with the data from the final message.&lt;/p&gt;

&lt;p&gt;There are a couple of ways we could have defined our states. We could have a state for the oldSystem returned or newSystem returned. I find it easier to&lt;/p&gt;

&lt;p&gt;create a generic &lt;em&gt;PendingComparison&lt;/em&gt; state to keep our pattern matching for pending comparisons consolidated in a single partial function.&lt;/p&gt;

&lt;p&gt;Our final states are pretty simple: we just stop our state machine!&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;when(IdUnavailable) {
  case Event(_, _) =&gt; {
    stop
  }
}
when(AllRetrieved) {
  case Event(_, _) =&gt; {
    stop
  }
}
&lt;/pre&gt;

&lt;p&gt;Our last step is to add some more onTransition checks to handle our final states:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;case PendingComparison -&gt; AllRetrieved =&gt;
    nextStateData match {
      case ComparisonStatus(id, old, new) =&gt; {
        //Verification logic
     }
   }
 case _ -&gt; IdUnavailable =&gt;
   nextStateData match {
     case ComparisonStatus(id, old, new) =&gt; {
      //Handle timeout
      }
   }
&lt;/pre&gt;

&lt;p&gt;We don&amp;#8217;t care how we got to the &lt;em&gt;AllRetrieved&lt;/em&gt; state; we just know we are there and we have the data we need. We can offload our verification logic&lt;/p&gt;

&lt;p&gt;to another actor or inline it within our FSM as necessary.&lt;/p&gt;

&lt;p&gt;Implementing processing workflows can be tricky involving a lot of boilerplate code. Conditions must be checked, timeouts handled, error handling implemented.&lt;/p&gt;

&lt;p&gt;The Akka FSM approach provides a foundation for implementing workflow based processes on top of Akka&amp;#8217;s great supervision support. We create a ComparisonEngine&lt;/p&gt;

&lt;p&gt;for every piece of data we need to check. If an engine dies we can supervise and restart. My favorite feature is the separation of what causes a state transition&lt;/p&gt;

&lt;p&gt;with what happens during a state transition. Combined with isolated behavior amongst actors this creates a cleaner, isolated and composable application to manage.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Spray API Development: Getting Started with a Spray Web Service Using JSON</title>
          <link>http://blog.michaelhamrah.com/2013/06/scala-web-apis-up-and-running-with-spray-and-akka/</link>
          <pubDate>Sat, 22 Jun 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/06/scala-web-apis-up-and-running-with-spray-and-akka/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;spray.io&#34;&gt;Spray&lt;/a&gt; is a great library for building http api&amp;#8217;s with Scala. Just like &lt;a href=&#34;playframework.com&#34;&gt;Play!&lt;/a&gt; it&amp;#8217;s built with &lt;a href=&#34;akka.io&#34;&gt;Akka&lt;/a&gt; and provides numerous low and high level tools for http servers and clients. It puts Akka and Scala&amp;#8217;s asynchronous programming model first for high performance, composable application development.&lt;/p&gt;

&lt;p&gt;I wanted to highlight the &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-routing/&#34;&gt;spray-routing&lt;/a&gt; library which provides a nice DSL for defining web services. The routing library can be used with the standalone &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-can/#spray-can&#34;&gt;spray-can&lt;/a&gt; http server or in any servlet container.&lt;/p&gt;

&lt;p&gt;We&amp;#8217;ll highlight a simple entity endpoint, unmarshalling Json data into an object and deferring actual process to another Akka actor. To get started with your own spray-routing project, I created a &lt;a href=&#34;https://github.com/n8han/giter8&#34;&gt;giter8&lt;/a&gt; template to bootstrap your app:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$g8 mhamrah/sbt -b spray&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://spray.io/documentation/&#34;&gt;The documentation&lt;/a&gt; is quite good and &lt;a href=&#34;https://github.com/spray/spray&#34;&gt;the source code is worth browsing&lt;/a&gt;. For a richer routing example check out &lt;a href=&#34;https://github.com/spray/spray/tree/release/1.1/examples/spray-routing/on-spray-can&#34;&gt;Spray&amp;#8217;s own routing project&lt;/a&gt; which shows off http-streaming and a few other goodies.&lt;/p&gt;

&lt;h2 id=&#34;creating-a-server:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Creating a Server&lt;/h2&gt;

&lt;p&gt;We are going to create three main structures: An actor which contains our Http Service, a trait which contains our route definition, and a Worker actor that will do the work of the request.&lt;/p&gt;

&lt;p&gt;The service actor is launched in your application&amp;#8217;s main method. Here we are using Scala&amp;#8217;s App class to launch our server feeding in values from &lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;typesafe config&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
val service= system.actorOf(Props[SpraySampleActor], &amp;quot;spray-sample-service&amp;quot;)
IO(Http) ! Http.Bind(service, system.settings.config.getString(&amp;quot;app.interface&amp;quot;), system.settings.config.getInt(&amp;quot;app.port&amp;quot;))

println(&amp;quot;Hit any key to exit.&amp;quot;)
val result = readLine()
system.shutdown()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because Spray is based on Akka, we are just creating a standard actor system and passing our service to &lt;a href=&#34;http://doc.akka.io/docs/akka/snapshot/scala/io.html&#34;&gt;Akka&amp;#8217;s new IO library&lt;/a&gt;. This is the high performance foundation for our service built on the spray-can server.&lt;/p&gt;

&lt;h2 id=&#34;the-service-actor:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;The Service Actor&lt;/h2&gt;

&lt;p&gt;Our service actor is pretty lightweight, as the functionality is deferred to our route definition in the HttpService trait. We only need to set the actorRefFactory and call runRoutes from our trait. You could simply set routes directly in this class, but the separation has its benefits, primarily for testing.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
class SpraySampleActor extends Actor with SpraySampleService with SprayActorLogging {
  def actorRefFactory = context
  def receive = runRoute(spraysampleRoute)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-service-trait-8211-spray-8217-s-routing-dsl:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;The Service Trait &amp;#8211; Spray&amp;#8217;s Routing DSL&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-routing/key-concepts/routes/&#34;&gt;Spray&amp;#8217;s Routing DSL&lt;/a&gt; is where Spray really shines. It is similar to Sinatra inspired web frameworks like Scalatra, but relies on composable function elements so requests pass through a series of actions similar to &lt;a href=&#34;http://unfiltered.databinder.net/&#34;&gt;Unfiltered&lt;/a&gt;. The result is an easy to read syntax for routing and the Dont-Repeat-Yourself of composable functions.&lt;/p&gt;

&lt;p&gt;To start things off, we&amp;#8217;ll create a simple get/post operation at the /entity path:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
trait SpraySampleService extends HttpService {
  val spraysampleRoute = {
    path(&amp;quot;entity&amp;quot;) {
      get { 
        complete(&amp;quot;list&amp;quot;)
      } ~
      post {
        complete(&amp;quot;create&amp;quot;)
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The path, get and complete operations are &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-routing/key-concepts/directives/#directives&#34;&gt;Directives&lt;/a&gt;, the building blocks of Spray routing. Directives take the current http request and process a particular action against it. The above snippet doesn&amp;#8217;t much except filter the request on the current path and the http action. The path directive also lets you pull out path elements:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
path (&amp;quot;entity&amp;quot; / Segment) { id =&amp;gt;
    get {
      complete(s&amp;quot;detail ${id}&amp;quot;)
    } ~
    post {
      complete(s&amp;quot;update ${id}&amp;quot;)
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a number ways to pull out elements from a path. Spray&amp;#8217;s unit tests are the best way to explore the possibilities.&lt;/p&gt;

&lt;p&gt;You can use curl to test the service so far:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!bash
curl -v http://localhost:8080/entity
curl -v http://localhost:8080/entity/1234
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;unmarshalling:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Unmarshalling&lt;/h2&gt;

&lt;p&gt;One of the nice things about Spray&amp;#8217;s DSL is how function composition allows you to build up request handling. In this snippet we use json4s support to unmarshall the http request into a JObject:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
/* We need an implicit formatter to be mixed in to our trait */
object Json4sProtocol extends Json4sSupport {
  implicit def json4sFormats: Formats = DefaultFormats
}

trait SpraySampleService extends HttpService {
  import Json4sProtocol._

  val spraysampleRoute = {
    path(&amp;quot;entity&amp;quot;) {
      /* ... */
      post {
        entity(as[JObject]) { someObject =&amp;gt;
          doCreate(someObject)
        }
      } 
     /* ... */
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We use the Entity to directive to unmarshall the request, which finds the implicit json4s serializer we specified earlier. SomeObject is set to the JObject produced, which is passed to our yet-to-be-built doCreate method. If Spray can&amp;#8217;t unmarshall the entity an error is returned to the client.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s a curl command that sets the http method to POST and applies the appropriate header and json body:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!bash
curl -v -X POST http://localhost:8080/entity -H &amp;quot;Content-Type: application/json&amp;quot; -d &amp;quot;{ \&amp;quot;property\&amp;quot; : \&amp;quot;value\&amp;quot; }&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;leveraging-akka-and-futures:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Leveraging Akka and Futures&lt;/h2&gt;

&lt;p&gt;We want to keep our route structure clean, so we defer actual work to another Akka worker. Because Spray is built with Akka this is pretty seamless. We need to create our ActorRef to send a message. We&amp;#8217;ll also implement our doCreate function called within the earlier POST /entity directive:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
//Our worker Actor handles the work of the request.
val worker = actorRefFactory.actorOf(Props[WorkerActor], &amp;quot;worker&amp;quot;)

def doCreate[T](json: JObject) = {
  //all logic must be in the complete directive
  //otherwise it will be run only once on launch
  complete {
    //We use the Ask pattern to return
    //a future from our worker Actor,
    //which then gets passed to the complete
    //directive to finish the request.
    (worker ? Create(json))
                .mapTo[Ok]
                .map(result =&amp;gt; s&amp;quot;I got a response: ${result}&amp;quot;)
                .recover { case _ =&amp;gt; &amp;quot;error&amp;quot; }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a couple of things going on here. Our worker class is looking for a Create message, which we send to the actor with the ask (?) pattern. The ask pattern lets us know the task completed so we call then tell the client. When we get the Ok message we simply return the result; in the case of an error we return a short message. The response future returned is passed to Spray&amp;#8217;s complete directive, which will then complete the request to the client. There&amp;#8217;s no blocking occurring in this snippet: we are just wiring up futures and functions.&lt;/p&gt;

&lt;p&gt;Our worker doesn&amp;#8217;t do much but out the message contents and return a random number:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!scala
class WorkerActor extends Actor with ActorLogging {
import WorkerActor._

def receive = {
  case Create(json) =&amp;gt; {
    log.info(s&amp;quot;Create ${json}&amp;quot;)
    sender ! Ok(util.Random.nextInt(10000))
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can view how the entire request is handled &lt;a href=&#34;https://github.com/mhamrah/spray-sample/blob/master/src/main/scala/Boot.scala&#34;&gt;by viewing the source file&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;wrapping-up:e4c25fe8ca10d79600b3f827b4d5c8dd&#34;&gt;Wrapping Up&lt;/h2&gt;

&lt;p&gt;Reading the documentation and exploring the unit tests are the best way to understand the power of Spray&amp;#8217;s routing DSL. The performance of the standalone &lt;a href=&#34;http://spray.io/documentation/1.1-M8/spray-can/&#34;&gt;spray-can&lt;/a&gt; service is outstanding, and the Akka platform adds resiliency through its lifecycle management tools. Akka&amp;#8217;s remoting feature allows systems to build out their app tiers. A project I&amp;#8217;m working on is using Spray and Akka to publish messages to a pub/sub system for downstream request handling. It&amp;#8217;s an excellent platform for high performance API development. &lt;a href=&#34;https://github.com/mhamrah/spray-sample&#34;&gt;Full spray-sample is on GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Updating Flickr Photos with Gpx Data using Scala: Getting Started</title>
          <link>http://blog.michaelhamrah.com/2013/05/updating-flickr-photos-with-gpx-data-using-scala-getting-started/</link>
          <pubDate>Sun, 05 May 2013 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2013/05/updating-flickr-photos-with-gpx-data-using-scala-getting-started/</guid>
          <description>

&lt;p&gt;If you read this blog you know I&amp;#8217;ve just returned from six months of travels around Asia, documented on our tumblr, &lt;a href=&#34;http://thegreatbigadventure.tumblr.com&#34;&gt;The Great Big Adventure&lt;/a&gt; with photos on &lt;a href=&#34;http://flickr.com/hamrah&#34;&gt;Flickr&lt;/a&gt;. Even though my camera doesn&amp;#8217;t have a GPS, I realized toward the second half of the trip I could mark GPS waypoints and write a program to link that data later. I decided to write this little app in Scala, a language I&amp;#8217;ve been learning since my return. The app is still a work in progress, but instead of one long post I&amp;#8217;ll spread it out as I go along.&lt;/p&gt;

&lt;h2 id=&#34;the-workflow:57d2935fd637ebe85b97477296b70272&#34;&gt;The Workflow&lt;/h2&gt;

&lt;p&gt;When I took a photo I usually marked the location with a waypoint in my GPS. I accumulated a set of around 1000 of these points spread out over three gpx (xml) files. My plan is to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read in the three gpx files and combine them into a distinct list.&lt;/li&gt;
&lt;li&gt;For each day I have at least one gpx point, get all of my flickr images for that data.&lt;/li&gt;
&lt;li&gt;For each image, find the waypoint timestamp with the least difference in time.&lt;/li&gt;
&lt;li&gt;Update that image with the waypoint data on Flickr.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;getting-started:57d2935fd637ebe85b97477296b70272&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;If you&amp;#8217;re going to be doing anything with Scala, learning &lt;a href=&#34;http://scala-sbt.org&#34;&gt;sbt&lt;/a&gt; is essential. Luckily, it&amp;#8217;s pretty straightforward, but the documentation across the internet is somewhat inconsistent. As of this writing, &lt;a href=&#34;http://twitter.github.io/scala_school/sbt.html&#34;&gt;Twitter&amp;#8217;s Scala School SBT Documentation&lt;/a&gt;, which I used as a reference to get started, incorrectly states that SBT creates a template for you. It no longer does, with the preferred approach to use &lt;a href=&#34;https://github.com/n8han/giter8&#34;&gt;giter8&lt;/a&gt;, an excellent templating tool. I created &lt;a href=&#34;https://github.com/mhamrah/sbt.g8&#34;&gt;my own simplified version&lt;/a&gt; which is based off of the excellently documented &lt;a href=&#34;https://github.com/ymasory/sbt.g8&#34;&gt;template by Yuvi Masory&lt;/a&gt;. Some of the versions in build.sbt are a outdated, but it&amp;#8217;s worthwhile reading through the code to get a feel for the Scala and SBT ecosystem. The g8 project also contains a good working example of custom sbt commands (like g8-test). One gotcha with SBT: if you change your build.sbt file, you must call &lt;em&gt;reload&lt;/em&gt; in the sbt console. Otherwise, your new dependencies will not be picked up. For rubyists this is similar to running &lt;em&gt;bundle update&lt;/em&gt; after changing your gemfile.&lt;/p&gt;

&lt;h2 id=&#34;testing:57d2935fd637ebe85b97477296b70272&#34;&gt;Testing&lt;/h2&gt;

&lt;p&gt;I&amp;#8217;m a big fan of TDD, and strive for a test-first approach. It&amp;#8217;s easy to get a feel for the small stuff in the scala repl, but orchestration is what programming is all about, and TDD allows you to design and throughly test functionality in a repeatable way. The two main libraries are &lt;a href=&#34;https://code.google.com/p/specs/&#34;&gt;specs&lt;/a&gt; (actually, it&amp;#8217;s now &lt;a href=&#34;http://etorreborre.github.io/specs2/&#34;&gt;specs2&lt;/a&gt;) and &lt;a href=&#34;http://www.scalatest.org/&#34;&gt;ScalaTest&lt;/a&gt;. I originally went with specs2. It was fine, but I wasn&amp;#8217;t too impressed with the output and not thrilled with the matchers. I believe these are all customizable, but to get a better feel for the ecosystem I switched to ScalaTest. I like ScalaTest&amp;#8217;s default output better and the flexible composition of testing styles (I&amp;#8217;m using FreeSpec) and matchers (ShouldMatchers) provide a great platform for testing. Luckily, both specs2 and scalatest integrate with SBT which provides continuous testing and growl support, so you don&amp;#8217;t need to fully commit to either one too early.&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
