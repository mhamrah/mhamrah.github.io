<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> </title>
    <link>http://blog.michaelhamrah.com/tags/devops/</link>
    <language>en-us</language>
    <author>Michael Hamrah</author>
    <rights>(C) 2015</rights>
    <updated>2015-04-10 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Easy Scaling with Fleet and CoreOS</title>
          <link>http://blog.michaelhamrah.com/2015/04/easy-scaling-with-fleet-and-coreos/</link>
          <pubDate>Fri, 10 Apr 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/04/easy-scaling-with-fleet-and-coreos/</guid>
          <description>&lt;p&gt;One element of a successful production deployment is the ability to easily scale the number of instances your process is running. Many cloud providers, both on the PaaS and IaaS front, offer such functionality: AWS Auto Scaling Groups, Heroku&amp;#8217;s process size, Marathon&amp;#8217;s instance count. I was hoping for something similar in the CoreOS world. &lt;a href=&#34;http://deis.io&#34;&gt;Deis&lt;/a&gt;, the PaaS-on-CoreOS service, offers Heroku-like scaling, but I don&amp;#8217;t want to commit to the Deis layer nor its build pack approach (for no other reason than personal preference). Fleet, CoreOS&amp;#8217;s distributed systemd service, offers service templating, but you cannot say &amp;#8220;now run three instances of service x&amp;#8221;. Being programmers we can do whatever we want, and luckily, we&amp;#8217;re only a little bash script away from replicating the &amp;#8220;scale to x instances&amp;#8221; functionality of popular providers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.michaelhamrah.com/2015/03/deploying-docker-containers-on-coreos-with-the-fleet-api/&#34;&gt;You&amp;#8217;ll want to enable the Fleet HTTP Api&lt;/a&gt; for this script to work. You can easily port this to the Fleet CLI, but I much prefer the http api because it doesn&amp;#8217;t involve ssh, and provides more versatility into how and where you run the script.&lt;/p&gt;

&lt;p&gt;Conceptually the flow is straightforward:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Given a process we want to set the number of running instances to some &lt;code&gt;desired_count&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;desired_count&lt;/code&gt; is less than &lt;code&gt;current_count&lt;/code&gt;, scale down.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;desired_count&lt;/code&gt; is more than &lt;code&gt;current_count&lt;/code&gt;, scale up.&lt;/li&gt;
&lt;li&gt;If they are the same, do nothing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fleet offers service templating so you can have a service unit named &lt;code&gt;my_awesome_app@.service&lt;/code&gt; with specific copies named &lt;code&gt;my_awesome_app@1, my_awesome_app@2, my_awesome_app@N&lt;/code&gt; representing specific running instances. Currently Fleet doesn&amp;#8217;t offer a way to group these related services together but we can easily pattern match on the service name to control specific running instances. The steps are straightforward:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Query the Fleet API for all instances&lt;/li&gt;
&lt;li&gt;Filter by all services matching the specified name&lt;/li&gt;
&lt;li&gt;See how many instances we have running for the given service&lt;/li&gt;
&lt;li&gt;Destroy or create instances using specific service names until we match the &lt;code&gt;desired_size&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these steps are easily achievable with Fleet&amp;#8217;s HTTP Api (or fleetctl) and a little bash. To give our script some context, let&amp;#8217;s start with how we want to use the script. Ideally it will look like this:&lt;/p&gt;

&lt;pre class=&#34;toolbar-overlay:false syntax bash&#34;&gt;./scale-fleet my_awesome_app 5
&lt;/pre&gt;

&lt;p&gt;First, let&amp;#8217;s set up our script &lt;code&gt;scale-fleet&lt;/code&gt; and set the command line arguments:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

FLEET_HOST=&amp;lt;YOUR FLEET API HOST&gt;

# You may want to consider cli flags 
SERVICE_NAME=$1
DESIRED_SIZE=$2
&lt;/pre&gt;

&lt;p&gt;Next we want to query the Fleet API and filter on all units with a prefix of &lt;code&gt;SERVICE_NAME&lt;/code&gt; which have a process number. This will give us an array of units matching &lt;code&gt;my_awesome_app@1.service&lt;/code&gt;, not the base template of &lt;code&gt;my_awesome_app@.service&lt;/code&gt;. These are the units we will either add to or destroy as appropriate. The latest 1.5 version of jq supports regex expressions, but as of this writing 1.4 is the common release version, so we&amp;#8217;ll parse the json response with jq, and then filter with grep. Finally some bash trickery will parse the result into an array we can loop through later.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Curls the API and filter on a specific pattern, storing results in an array
INSTANCES=($(curl -s $FLEET_HOST/fleet/v1/units | jq &#34;.units[].name | select(startswith(\&#34;$SERVICE@\&#34;))&#34; | grep &#39;\w@\d\.service&#39;))

# A bash trick to get size of array
CURRENT_SIZE=${#INSTANCES[@]}
echo &#34;Current instance count for $SERVICE is: $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;Next let&amp;#8217;s scaffold the various scenarios for matching &lt;code&gt;CURRENT_SIZE&lt;/code&gt; with &lt;code&gt;DESIRED_SIZE&lt;/code&gt;, which boils down to some if statements.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;if [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; then
  echo &#34;doing nothing, current size is equal desired size&#34;
elif [[ $DESIRED_SIZE &amp;lt; $CURRENT_SIZE ]]; then
  echo &#34;going to scale down instance $CURRENT_SIZE&#34;
  # More stuff here
else 
  echo &#34;going to scale up to $DESIRED_SIZE&#34;
  # More stuff here
fi
&lt;/pre&gt;

&lt;p&gt;When the desired size equals the current size we don&amp;rsquo;t need to do anything. Scaling down is easy, we simply loop, deleting the specific instance, until the desired and current states match. You can drop in the following snippet for scaling down:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;until [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; do
    curl -X DELETE $FLEET_HOST/fleet/v1/units/${SERVICE}@${CURRENT_SIZE}.service

    let CURRENT_SIZE = CURRENT_SIZE-1
  done
  echo &#34;new instance count is $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;Scaling up is a bit trickier. Unfortunately you can&amp;rsquo;t simply create a new unit from a template like you can with the fleetctl CLI. But you can do exactly what the fleetctl does: copy the body from the base template and create a new one with the specific full unit name. With the body we can loop, creating instances, until our current size matches the desired size. Let&amp;rsquo;s walk it through step-by-step:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;echo &#34;going to scale up to $desired_size&#34;
 # Get payload by parsing the options field from the base template
 # And build our new payload for PUTing later
 payload=`curl -s $FLEET_HOST/fleet/v1/units/${SERVICE}@.service | jq &#39;. | { &#34;desiredState&#34;:&#34;launched&#34;, &#34;options&#34;: .options }&#39;`

 #Loop, PUTing our new template with the appropriate name
 until [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; do
   let current_size=current_size+1

   curl -X PUT -d &#34;${payload}&#34; -H &#39;Content-Type: application/json&#39; $FLEET_HOST/fleet/v1/units/${SERVICE}@${CURRENT_SIZE}.service 
 done
 echo &#34;new instance count is $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;With our script in place we can scale away:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Scale up to 5 instances
$ ./scale-fleet my_awesome_app 5

# Scale down
$ ./scale-fleet my_awesome_app 3
&lt;/pre&gt;

&lt;p&gt;Because this all comes down to a simple bash script you can easily run it from a variety of places. It can be part of a parameterized Jambi job to scale manually with a UI, part of an &lt;a href=&#34;http://github.com/hashicorp/envconsul&#34;&gt;envconsul&lt;/a&gt; setup with a key set in &lt;a href=&#34;consul.io&#34;&gt;Consul&lt;/a&gt;, or it can fit into a larger script that reads performance characteristics from some monitoring tool and reacts accordingly. You can also combine this with AWS Cloudformation or another cloud provider: if you&amp;rsquo;re CPU&amp;rsquo;s hit a certain threshold, you can scale the specific worker role running your instances, and have your &lt;code&gt;desired_size&lt;/code&gt; be some factor of that number.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been on a bash kick lately. It&amp;rsquo;s a versatile scripting language that easily portable. The syntax can be somewhat mystic, but as long as you have a shell, you have all you need to run your script.&lt;/p&gt;

&lt;p&gt;The final, complete script is here:&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Managing CoreOS Clusters on AWS with CloudFormation</title>
          <link>http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/</link>
          <pubDate>Wed, 25 Mar 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/</guid>
          <description>&lt;p&gt;Personally, I find CloudFormation a somewhat annoying tool, yet I haven&amp;#8217;t replaced it with anything else. Those json files can get so ugly and unwieldy. Alternatives exist; you can try an abstraction like &lt;a href=&#34;https://github.com/cloudtools/troposphere&#34;&gt;troposphere&lt;/a&gt; or &lt;a href=&#34;https://jclouds.apache.org&#34;&gt;jclouds&lt;/a&gt;, or ditch cfn completely with something like &lt;a href=&#34;https://www.terraform.io&#34;&gt;terraform&lt;/a&gt;. These are interesting tools but somehow I find myself sticking with the straight-up json approach, the aws cli, and some bash scripting: the pieces are already there, they just need to be strung together. In the end it&amp;#8217;s not that bad, and there are some tools and techniques I&amp;#8217;ve picked up which really help out. I recently applied these to managing CoreOS clusters with CFN, and wanted to share a simplified version of the approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://coreos.com/docs/running-coreos/cloud-providers/ec2/&#34;&gt;CoreOS provides a default CloudFormation template&lt;/a&gt; which is a great start for cluster experimentation. But scaling out, where nodes are coming and going, can be disastrous for etcd&amp;#8217;s quorum consensus if you&amp;#8217;re not careful. You just don&amp;#8217;t want to remove nodes from a formed etcd cluster. &lt;a href=&#34;https://coreos.com/docs/cluster-management/setup/cluster-architectures/&#34;&gt;CoreOS&amp;#8217;s cluster documentation&lt;/a&gt; has a section on production configuration: you want a core set of nodes for running central services, with various worker nodes for specific purposes. We can elaborate this with a short-list of requirements:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;You want to tag sets of instances with specific roles so you can group dependencies and isolate apps when needed.&lt;/strong&gt;&lt;/em&gt; Although possible, it&amp;#8217;s unrealistic to actually run any app on any node. More likely you want to group apps into front-facing and back-facing and treat those nodes differently. For instance, you could map the IP&amp;#8217;s of front-facing nodes to a Route53 endpoint.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;You want a cluster of heterogeneous instances for different workloads&lt;/strong&gt;&lt;/em&gt; Certain apps require certain characteristics. Even though you&amp;#8217;re running everything in docker containers, you still want to have c4&amp;#8217;s for compute-intensive loads, r3&amp;#8217;s for memory-intensive loads, etc. Look at your applications and map them to a system topology. You can also scale these groups of instances differently, but you want to see your entire system as a whole: not as independent, discrete parts.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;At some point, you&amp;#8217;ll need to update the configuration of your instances. You want to do this surgically, without accidentally destroying your cluster&lt;/strong&gt;&lt;/em&gt;. You may be one bad cfn update from relaunching an auto scaling group or misconfiguring an instance which causes a replacement. Just like normal instances you want to apply updates and reconfiguration of nodes in a sane, logical way. If you only had one cfn template for your entire cluster, it&amp;#8217;s all or nothing. That&amp;#8217;s not a choice we want to make.&lt;/p&gt;

&lt;p&gt;CoreOS won&amp;#8217;t let you forget about the underlying nodes; it just adds a little abstraction so you don&amp;#8217;t need to deal with specific nodes as much.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m assuming you&amp;#8217;re familiar with CloudFormation and the basics of a template. For our setup we&amp;#8217;ll start with the &lt;a href=&#34;https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template&#34;&gt;us-east-1 hvm CoreOS template&lt;/a&gt; and modify it along the way. This template create a straight-up CoreOS cluster launched in an Auto Scaling Group, uses a LaunchConfig&amp;#8217;s UserData to set some Cloud-Config settings. Like most templates you need a few parameters to launch. The non-default ones are your keypair and the etcd Discovery Url for forming the cluster. We are going to launch this stack with the CLI (who needs user interfaces?)&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s create a bash script, &lt;code&gt;coreos-cfn.sh&lt;/code&gt;, to call our create stack (don&amp;#8217;t forget to chmod +x). We need a DiscoveryUrl so we&amp;#8217;ll get a new one in our script and pass it as a parameter to CFN.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash 

DISCOVERY_URL=`curl -s -w &#34;\n&#34; https://discovery.etcd.io/new`
#Check to make sure the above command worked, or exit
[[ $? -ne 0 ]] &amp;#038;&amp;#038; echo &#34;Could not generate discovery url.&#34; &amp;#038;&amp;#038; exit 1

if [ -z &#34;$COREOS_KEYPAIR&#34; ]; then
  KEYPAIR=yourkey.pem
fi

# Create the CloudFormation stack
aws cloudformation create-stack \
    --stack-name coreos-test \
    --template-body file://coreos-stable-hvm.template \
    --capabilities CAPABILITY_IAM \
    --tags Key=Name,Value=CoreOS \
    --parameters \
        ParameterKey=DiscoveryURL,ParameterValue=${DISCOVERY_URL} \
        ParameterKey=KeyPair,ParameterValue=${KEYPAIR}
&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-z $KEYPAIR&lt;/code&gt; tests to see if there&amp;#8217;s a keypair set as an environment variable; if not, it uses the specified one. If you run &lt;code&gt;coreos-cfn.sh&lt;/code&gt; you should see the CLI spit out the ARN for the stack. Before we do that, let&amp;#8217;s make two minor tweaks.&lt;/p&gt;

&lt;p&gt;There are two key pieces of information we want to remember from this cluster: The DiscoveryUrl, so can access cluster state, and the AutoScalingGroup, so we can easily inspect instances in the future. Because the DiscoveryUrl is a parameter the aws cli will remember it for you. We need to add the auto scaling group as an output:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;&#34;Outputs&#34;: {
    &#34;AutoScalingGroup&#34; : {
      &#34;Value&#34;: { &#34;Ref&#34;: &#34;CoreOSServerAutoScale&#34; }
    }
  }
&lt;/pre&gt;

&lt;p&gt;After launching the cluster we can use the CLI and some jq to get back these parameters. It&amp;#8217;s a simple built-in storage mechanism of AWS, and all you need is the original stack name:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Get back the DiscoveryURL: Describe the stack, select the parameter list
DISCOVERY_URL=`aws cloudformation describe-stacks --stack-name coreos-test | \
  jq -r &#39;[.Stacks[].Parameters[]][] | select (.ParameterKey == &#34;DiscoveryURL&#34;) | .ParameterValue&#39;`

# Get back the auto-scaling-group-id
LEADER_ASG=`aws cloudformation describe-stacks --stack-name coreos-test | \
  jq -r &#39;[.Stacks[].Outputs[]][] | select (.OutputKey == &#34;AutoScalingGroup&#34;) | .OutputValue&#39;`

echo &#34;Discovery Url is $DISCOVERY_URL and Leader ASG is $LEADER_ASG&#34;
&lt;/pre&gt;

&lt;p&gt;Why is this important? Because now we can either inspect the state of the cluster via the disovery url service, or query the ASG to inspect running nodes directly:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Query AWS for Leader Nodes
$aws ec2 describe-instances --filters Name=tag-value,Values=$LEADER_ASG | \
  jq &#39;.Reservations[].Instances[].NetworkInterfaces[].PrivateIpAddress&#39;

# Inspect the Discovery Url for nodes, trimming port. 
$ `curl -s $DISCOVERY_URL | jq &#39;.node.nodes[].value[0:-5]&#39;

# Taking the latter one step further, we can build an Etcd Peers string using Jq, xargs and tr
$ ETCD_PEERS=`curl -s $DISCOVERY_URL | jq &#39;.node.nodes[].value[0:-5]&#39; | xargs -I{}  echo &#34;{}:4001&#34; | tr &#34;\\n&#34; &#34;,&#34;`
# Drop the last ,
$ ETCD_PEERS=${ETCD_PEERS%?}
&lt;/pre&gt;

&lt;p&gt;Armed with this information we are now able to spin up new CoreOS nodes and have it use our CoreOS leader cluster for management. The &lt;a href=&#34;https://coreos.com/docs/cluster-management/setup/cluster-architectures/&#34;&gt;CoreOS Cluster Architecture page&lt;/a&gt; has the specific &lt;code&gt;cloud-config&lt;/code&gt; settings which amount to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Disable etcd, we don&amp;#8217;t need it&lt;/li&gt;
&lt;li&gt;Set etcd peer settings to a comma delimited list of nodes for Fleet, Locksmith&lt;/li&gt;
&lt;li&gt;Set environment variables for fleet and etcd in start scripts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;#8217;ll make the etcd peer list a parameter for our template. We can duplicate our leader template, replace the &lt;code&gt;UserData&lt;/code&gt; portion of the &lt;code&gt;LaunchConfig&lt;/code&gt; with the updated settings from the link above, and add &lt;code&gt;{ Ref: }&lt;/code&gt; parameters where appropriate. Let&amp;#8217;s also add a metadata parameter as well:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;&#34;Parameters&#34;: {
    &#34;EtcdPeers&#34; : {
      &#34;Description&#34; : &#34;A comma delimited list of etcd endpoints to use for state management.&#34;,
      &#34;Type&#34; : &#34;String&#34;
    },
    &#34;FleetMetadata&#34; : {
      &#34;Description&#34; : &#34;A comma delimited list of key=value attributes to apply for fleet&#34;,
      &#34;Type&#34; : &#34;String&#34;
    }
  }
&lt;/pre&gt;

&lt;p&gt;We can use the &lt;code&gt;Ref&lt;/code&gt; functionality to pass these to our &lt;code&gt;UserData&lt;/code&gt; script of the &lt;code&gt;LaunchConfig&lt;/code&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;//other config above
  &#34;UserData&#34; : { &#34;Fn::Base64&#34;:
          { &#34;Fn::Join&#34;: [ &#34;&#34;, [
            &#34;#cloud-config\n\n&#34;,
            &#34;coreos:\n&#34;,
            &#34;  fleet:\n&#34;,
            &#34;    metadata: &#34;, { &#34;Ref&#34;: &#34;FleetMetadata&#34; }, &#34;\n&#34;,
            &#34;    etcd_servers: $&#34;, { &#34;Ref&#34;: &#34;EtcdPeers&#34; }, &#34;\n&#34;,
            &#34;  locksmith:\n&#34;,
            &#34;    endpoint: &#34;, { &#34;Ref&#34;: &#34;EtcdPeers&#34; }, &#34;\n&#34;
            ] ]
          }

// Other config below
&lt;/pre&gt;

&lt;p&gt;Finally we need a bash script which lets us inspect the existing stack information to pass as parameters to this new template. I also appreciate a CLI tool with a sane set of explicit flags. When I launch a secondary set of CoreOS nodes, I&amp;#8217;d like something simple to set the name, type, metadata and where I want to join to:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ launch-worker-group.sh -n r3-workers -t r3.large -j coreos-test -m &#34;instancetype=r3,role=worker&#34;
&lt;/pre&gt;

&lt;p&gt;Bash has a flag-parsing abilities in its &lt;code&gt;getopts&lt;/code&gt; function which we&amp;#8217;ll simply use to set variables:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

while getopts n:j:m:s: FLAG; do
  case $FLAG in
    n)  STACK_NAME=${OPTARG};;
    j)  JOIN=${OPTARG};;
    m)  METADATA=${OPTARG};;
    t)  INSTANCE_TYPE =${OPTARG};;
    [?])
      print &gt;&amp;#038;2 &#34;Usage: $0 [ -n stack-name ] [ -j join to leader] [ -m fleet-metadata ] [ -t instance-type ]&#34;
      exit 1;;
  esac
done

shift $((OPTIND-1))

# You can set defaults, too:
if [ -z $INSTANCE_TYPE ]; then 
  INSTANCE_TYPE =&#34;m3.medium&#34;
fi
&lt;/pre&gt;

&lt;p&gt;With this in place it&amp;#8217;s just a matter of calling the AWS CLI with our new template and updated parameters. The only thing we&amp;#8217;re doing differently than the original script is using CloudFormation&amp;#8217;s json parameter functionality. This allows for more structured data in variables. Otherwise the comma-delimited list for etcd peers will throw off the CLI call.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;DISCOVERY_URL=`aws cloudformation describe-stacks --stack-name $JOIN | \
  jq -r &#39;[.Stacks[].Parameters[]][] | select (.ParameterKey == &#34;DiscoveryURL&#34;) | .ParameterValue&#39;`
# Taking the latter one step further, we can build an Etcd 
# Peers string using jq, xargs and tr to flatten
ETCD_PEERS=`curl -s $DISCOVERY_URL | jq &#39;.node.nodes[].value[0:-5]&#39; | \
  xargs -I{}  echo &#34;{}:4001&#34; | tr &#34;\\n&#34; &#34;,&#34;`

# Drop the last ,
ETCD_PEERS=${ETCD_PEERS%?}

 # Create the CloudFormation stack
 aws cloudformation create-stack \
    --stack-name STACK_NAME \
    --template-body file://coreos-worker-hvm.template \
    --capabilities CAPABILITY_IAM \
    --tags Key=Name,Value=CoreOS Key=Role,Value=Worker \
    --parameters &#34;[
      { \&#34;ParameterKey\&#34;:\&#34;FleetMetadata\&#34;,\&#34;ParameterValue\&#34;:\&#34;${METADATA}\&#34; },
      { \&#34;ParameterKey\&#34;:\&#34;InstanceType\&#34;,\&#34;ParameterValue\&#34;:\&#34;${INSTANCE_TYPE}\&#34; },
      { \&#34;ParameterKey\&#34;:\&#34;EtcdPeers\&#34;,\&#34;ParameterValue\&#34;:\&#34;${ETCD_PEERS%?}\&#34; },
      { \&#34;ParameterKey\&#34;:\&#34;KeyPair\&#34;,\&#34;ParameterValue\&#34;:\&#34;${KEYPAIR}\&#34; }
    ]&#34;
&lt;/pre&gt;

&lt;p&gt;And launch it! This will create a new stack for your worker nodes with whatever metadata you want, with whatever instance type you want.&lt;/p&gt;

&lt;p&gt;There are a few ways to extend this. For one, we haven&amp;#8217;t dealt with updating or destroying the stack. You can create separate shell scripts or combine them together with flags for determining which action to take. I prefer the latter as it keeps all related scripts in one file, but you can break out accordingly. You can use the AWS CLI and the Stack Name to query for private ip&amp;#8217;s and update Route 53 accordingly, bypassing the need for an ELB.&lt;/p&gt;

&lt;p&gt;You can do a lot with bash and other CLI tools like jq. You don&amp;#8217;t need to scour GitHub for open source tools, or frameworks that have bells and whistles. The core components are there, you just need to glue them together. Yes, your scripts may get out of hand, but at that point it&amp;#8217;s worth looking for alternatives because there&amp;#8217;s probably a specific problem you need to solve. Remember, be opinionated and let those choices guide you. At some point in the future I may be raving about Terraform; friends say it&amp;#8217;s a great tool, but it&amp;#8217;s just not one that I need-or particularly want-to use now.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Accessing the Docker Host Server Within a Container</title>
          <link>http://blog.michaelhamrah.com/2014/06/accessing-the-docker-host-server-within-a-container/</link>
          <pubDate>Sun, 29 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/accessing-the-docker-host-server-within-a-container/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/userguide/dockerlinks/#working-with-links-names&#34;&gt;Docker links&lt;/a&gt; are a great way to link two containers together but sometimes you want to know more about the host and network from within a container. You have a couple of options:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can access the Docker host by the container&amp;#8217;s gateway.&lt;/li&gt;
&lt;li&gt;You can access the Docker host by its ip address from within a container.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-gateway-approach:d6e669ce9206f072ff43b8fdf5b03a0c&#34;&gt;The Gateway Approach&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dotcloud/docker/issues/1143&#34;&gt;This GitHub Issue&lt;/a&gt; outlines the solution. Essentially you&amp;#8217;re using netstat to parse the gateway the docker container uses to access the outside world. This is the docker0 bridge on the host.&lt;/p&gt;

&lt;p&gt;As an example, we&amp;#8217;ll run a simple docker container which returns the hostname of the container on port 8080:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run -d -p 8080:8080 mhamrah/mesos-sample
&lt;/pre&gt;

&lt;p&gt;Next we&amp;#8217;ll run /bin/bash in another container to do some discovery:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run -i -t ubuntu /bin/bash
#once in, install curl:
apt-get update
apt-get install -y curl
&lt;/pre&gt;

&lt;p&gt;We can use the following command to pull out the gateway from netstat:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;netstat -nr | grep &#39;^0\.0\.0\.0&#39; | awk &#39;{print $2}&#39;
#returns 172.17.42.1 for me.
&lt;/pre&gt;

&lt;p&gt;We can then curl our other docker container, and we should get that docker container&amp;#8217;s hostname:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;curl 172.17.42.1:8080
# returns 00b019ce188c
&lt;/pre&gt;

&lt;p&gt;Nothing exciting, but you get the picture: it doesn&amp;#8217;t matter that the service is inside another container, we&amp;#8217;re accessing it via the host, and we didn&amp;#8217;t need to use links. We just needed to know the port the other service was listening on. If you had a service running on some other port&amp;#8211;say Postgres on 5432&amp;#8211;not running in a Docker container&amp;#8211;you can access it via &lt;code&gt;172.17.42.1:5432&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have docker installed in your container you can also query the docker host:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# In a container with docker installed list other containers running on the host for other containers:
docker -H tcp://172.17.42.1:2375 ps
CONTAINER ID        IMAGE                         COMMAND                CREATED              STATUS              PORTS                     NAMES
09d035054988        ubuntu:14.04                  /bin/bash              About a minute ago   Up About a minute   0.0.0.0:49153-&gt;8080/tcp   angry_bardeen
00b019ce188c        mhamrah/mesos-sample:latest   /opt/delivery/bin/de   8 minutes ago        Up 8 minutes        0.0.0.0:8080-&gt;8080/tcp    suspicious_colden
&lt;/pre&gt;

&lt;p&gt;You can use this for some hakky service-discovery.&lt;/p&gt;

&lt;h2 id=&#34;the-ip-approach:d6e669ce9206f072ff43b8fdf5b03a0c&#34;&gt;The IP Approach&lt;/h2&gt;

&lt;p&gt;The gateway approach is great because you can figure out a way to access a host from entirely within a container. You also have the same access via the host&amp;#8217;s ip address. I&amp;#8217;m using boot2docker, and the boot2docker ip address is &lt;code&gt;192.168.59.103&lt;/code&gt; and I can accomplish the same tasks as the gateway approach:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Docker processes, via ip:
docker -H tcp://192.168.59.103:2375 ps
# Other docker containers, via ip:
curl 192.168.59.103:8080
&lt;/pre&gt;

&lt;p&gt;Although there&amp;#8217;s no way to introspect the host&amp;#8217;s ip address (AFAIK) you can pass this in via an environment variable:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker@boot2docker:~$  docker run -i -t -e DOCKER_HOST=192.168.59.103 ubuntu /bin/bash
root@07561b0607f4:/# env
HOSTNAME=07561b0607f4
DOCKER_HOST=192.168.59.103
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PWD=/
&lt;/pre&gt;

&lt;p&gt;If the container knows the ip address of its host, you can broadcast this out to other services via the container&amp;#8217;s application. Useful for service discovery tools run from within a container where you want to tell others the host IP so others can find you.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Setting up a Multi-Node Mesos Cluster running Docker, HAProxy and Marathon with Ansible</title>
          <link>http://blog.michaelhamrah.com/2014/06/setting-up-a-multi-node-mesos-cluster-running-docker-haproxy-and-marathon-with-ansible/</link>
          <pubDate>Thu, 26 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/setting-up-a-multi-node-mesos-cluster-running-docker-haproxy-and-marathon-with-ansible/</guid>
          <description>

&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;With Mesos 0.20 Docker support is now native, and Deimos has been deprecated. The ansible-mesos-playbook has been updated appropriately, and most of this blog post still holds true. There are slight variations with how you post to Marathon.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf&#34;&gt;Google Omega Paper&lt;/a&gt; has given birth to cloud vNext: cluster schedulers managing containers. You can make a bunch of nodes appear as one big computer and deploy anything to your own private cloud; just like Docker, but across any number of nodes. &lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes&#34;&gt;Google&amp;#8217;s Kubernetes&lt;/a&gt;, &lt;a href=&#34;flynn.io&#34;&gt;Flynn&lt;/a&gt;, &lt;a href=&#34;https://github.com/coreos/fleet&#34;&gt;Fleet&lt;/a&gt; and &lt;a href=&#34;http://mesos.apache.org/&#34;&gt;Apache Mesos&lt;/a&gt;, originally from Twitter, are implementations of Omega with the goal of abstracting away discrete nodes and optimizing compute resources. Each implementation has its own tweak, but they all follow the same basic setup: leaders, for coordination and scheduling; some service discovery component; some underlying cluster tool (like Zookeeper); followers, for processing.&lt;/p&gt;

&lt;p&gt;In this post we&amp;#8217;ll use &lt;a href=&#34;http://www.ansible.com/home&#34;&gt;Ansible&lt;/a&gt; to install a multi-node Mesos cluster using packages from &lt;a href=&#34;http://mesosphere.io/&#34;&gt;Mesosphere&lt;/a&gt;. Mesos, as a cluster framework, allows you to run a variety of cluster-enabled software, including &lt;a href=&#34;http://spark.apache.org/&#34;&gt;Spark&lt;/a&gt;, &lt;a href=&#34;https://github.com/mesosphere/storm-mesos&#34;&gt;Storm&lt;/a&gt; and &lt;a href=&#34;https://github.com/mesos/hadoop&#34;&gt;Hadoop&lt;/a&gt;. You can also run &lt;a href=&#34;https://github.com/jenkinsci/mesos-plugin&#34;&gt;Jenkins&lt;/a&gt;, schedule tasks with &lt;a href=&#34;https://github.com/airbnb/chronos&#34;&gt;Chronos&lt;/a&gt;, even run &lt;a href=&#34;https://github.com/mesosphere/elasticsearch-mesos&#34;&gt;ElasticSearch&lt;/a&gt; and &lt;a href=&#34;https://github.com/mesosphere/cassandra-mesos&#34;&gt;Cassandra&lt;/a&gt; without having to double to specific servers. We&amp;#8217;ll also set up &lt;a href=&#34;https://github.com/mesosphere/marathon&#34;&gt;Marathon&lt;/a&gt; for running services with &lt;a href=&#34;https://github.com/mesosphere/deimos&#34;&gt;Deimos&lt;/a&gt; support for Docker containers.&lt;/p&gt;

&lt;p&gt;Mesos, even with Marathon, doesn&amp;#8217;t offer the holistic integration of some other tools, namely Kubernetes, but at this point it&amp;#8217;s easier to set up on your own set of servers. Although young Mesos is one of the oldest projects of the group and allows more of a DIY approach on service composition.&lt;/p&gt;

&lt;h3 id=&#34;tl-dr:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;TL;DR&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://github.com/mhamrah/ansible-mesos-playbook&#34;&gt;The playbook is on github, just follow the readme!&lt;/a&gt;&lt;/em&gt;. If you want to simply try out Mesos, Marathon, and Docker &lt;a href=&#34;http://mesosphere.io/learn/run-docker-on-mesosphere/&#34;&gt;mesosphere has an excellent tutorial to get you started on a single node&lt;/a&gt;. This tutorial outlines the creation of a more complex multi-node setup.&lt;/p&gt;

&lt;h3 id=&#34;system-setup:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;System Setup&lt;/h3&gt;

&lt;p&gt;The system is divided into two parts: a set of masters, which handle scheduling and task distribution, with a set of slaves providing compute power. Mesos uses Zookeeper for cluster coordination and leader election. A key component is service discovery: you don&amp;#8217;t know which host or port will be assigned to a task, which makes, say, accessing a website running on a slave difficult. The Marathon API allows you to query task information, and we use this feature to configure HAProxy frontend/backend resources.&lt;/p&gt;

&lt;p&gt;Our masters run:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Zookeeper&lt;/li&gt;
&lt;li&gt;Mesos-Master&lt;/li&gt;
&lt;li&gt;HAProxy&lt;/li&gt;
&lt;li&gt;Marathon&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and our slaves run:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mesos-Slave&lt;/li&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;li&gt;Deimos, the Mesos -&amp;gt; Docker bridge&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ansible:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Ansible&lt;/h3&gt;

&lt;p&gt;Ansible works by running a playbook, composed of roles, against a set of hosts, organized into groups. My &lt;a href=&#34;https://github.com/mhamrah/ansible-mesos-playbook&#34;&gt;Ansible-Mesos-Playbook&lt;/a&gt; on GitHub has an example hosts file with some EC2 instances listed. You should be able to replace these with your own EC2 instances running Ubuntu 14.04, our your own private instances running Ubuntu 14.04. Ansible allows us to pass node information around so we can configure multiple servers to properly set up our masters, zookeeper set, point slaves to masters, and configure Marathon for high availability.&lt;/p&gt;

&lt;p&gt;We want at least three servers in our master group for a proper zookeeper quorum. We use host variables to specify the zookeeper id for each node.&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;[mesos_masters]
ec2-54-204-214-172.compute-1.amazonaws.com zoo_id=1
ec2-54-235-59-210.compute-1.amazonaws.com zoo_id=2
ec2-54-83-161-83.compute-1.amazonaws.com zoo_id=3
&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/mhamrah/ansible-mesos&#34;&gt;mesos-ansible&lt;/a&gt; playbook will use nodes in the &lt;code&gt;mesos_masters&lt;/code&gt; for a variety of configuration options. First, the &lt;code&gt;/etc/zookeeper/conf/zoo.cfg&lt;/code&gt; will list all master nodes, with &lt;code&gt;/etc/zookeeper/conf/myid&lt;/code&gt; being set appropriately. It will also set up upstart scripts in &lt;code&gt;/etc/init/mesos-master.conf&lt;/code&gt;, &lt;code&gt;/etc/init/mesos-slave.conf&lt;/code&gt; with default configuration files in &lt;code&gt;/etc/defaults/mesos.conf&lt;/code&gt;. Mesos 0.19 supports external executors, so we use Deimos to run docker containers. This is only required on slaves, but the configuration options are set in the shared &lt;code&gt;/etc/defaults/mesos.conf&lt;/code&gt; file.&lt;/p&gt;

&lt;h3 id=&#34;marathon-and-haproxy:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Marathon and HAProxy&lt;/h3&gt;

&lt;p&gt;The playbook leverages an &lt;code&gt;ansible-marathon&lt;/code&gt; role to install a custom build of marathon with Deimos support. If Mesos is the OS for the data center, Marathon is the init system. Marathoin allows us to &lt;code&gt;http post&lt;/code&gt; new tasks, containing docker container configurations, which will run on Mesos slaves. With HAProxy we can use the masters as a load balancing proxy server routing traffic from known hosts (the masters) to whatever node/port is running the marathon task. HAProxy is configured via a cron job running &lt;a href=&#34;https://github.com/mhamrah/ansible-marathon/blob/master/files/haproxy_dns_cfg&#34;&gt;a custom bash script&lt;/a&gt;. The script queries the marathon API and will route to the appropriate backend by matching a host header prefix to the marathon job name.&lt;/p&gt;

&lt;h3 id=&#34;mesos-followers-slaves:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Mesos Followers (Slaves)&lt;/h3&gt;

&lt;p&gt;The slaves are pretty straightforward. We don&amp;#8217;t need any host variables, so we just list whatever slave nodes you&amp;#8217;d like to configure:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;[mesos_slaves]
ec2-54-91-78-105.compute-1.amazonaws.com
ec2-54-82-227-223.compute-1.amazonaws.com 
&lt;/pre&gt;

&lt;p&gt;Mesos-Slave will be configured with Deimos support.&lt;/p&gt;

&lt;h3 id=&#34;the-result:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;The Result&lt;/h3&gt;

&lt;p&gt;With all this set up you can set up a wildcard domain name, say &lt;code&gt;*.example.com&lt;/code&gt;, to point to all of your master node ip addresses. If you launch a task like &amp;#8220;www&amp;#8221; you can visit www.example.com and you&amp;#8217;ll hit whatever server is running your application. Let&amp;#8217;s try launching a simple web server which returns the docker container&amp;#8217;s hostname:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;POST to one of our masters:

POST /v2/apps

{
  &#34;container&#34;: {
    &#34;image&#34;: &#34;docker:///mhamrah/mesos-sample&#34;
  },
  &#34;cpus&#34;: &#34;.25&#34;,
  &#34;id&#34;: &#34;www&#34;,
  &#34;instances&#34;: 4,
  &#34;mem&#34;: 512,
  &#34;ports&#34;: [0],
  &#34;uris&#34;: []
}
&lt;/pre&gt;

&lt;p&gt;We run four instances allocating 25% of a cpu with an application name of &lt;code&gt;www&lt;/code&gt;. If we hit &lt;code&gt;www.example.com&lt;/code&gt;, we&amp;#8217;ll get the hostname of the docker container running on whatever slave node is hosting the task. Deimos will inspect whatever ports are &lt;code&gt;EXPOSE&lt;/code&gt;d in the docker container and assign a port for Mesos to use. Even though the config script only works on port 80 you can easily reconfigure for your own needs.&lt;/p&gt;

&lt;p&gt;To view marathon tasks, simply go to one of your master hosts on port 8080. Marathon will proxy to the correct master. To view mesos tasks, navigate to port 5050 and you&amp;#8217;ll be redirected to the appropriate master. You can also inspect the STDOUT and STDERR of Mesos tasks.&lt;/p&gt;

&lt;h3 id=&#34;notes:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;Notes&lt;/h3&gt;

&lt;p&gt;In my testing I noticed, on rare occasion, the cluster didn&amp;#8217;t have a leader or marathon wasn&amp;#8217;t running. You can simply restart zookeeper, mesos, or marathon via ansible:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#Restart Zookeeper
ansible mesos_masters -a &#34;sudo service zookeeper restart&#34;
&lt;/pre&gt;

&lt;p&gt;There&amp;#8217;s a high probability something won&amp;#8217;t work. Check the logs, it took me a while to get things working: grepping &lt;code&gt;/var/log/syslog&lt;/code&gt; will help, along with &lt;code&gt;/var/log/upstart/mesos-master.conf&lt;/code&gt;, &lt;code&gt;mesos-slave.conf&lt;/code&gt; and &lt;code&gt;marathon.conf&lt;/code&gt;, along with the &lt;code&gt;/var/log/mesos/&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;what-8217-s-next:eb793d553e2cf336944f7f93c5d3c089&#34;&gt;What&amp;#8217;s Next&lt;/h3&gt;

&lt;p&gt;Cluster schedulers are an exciting tool for running production applications. It&amp;#8217;s never been easier to build, package and deploy services on public, private clouds or bare metal servers. Mesos, with Marathon, offers a cool combination for running docker containers&amp;#8211;and other mesos-based services&amp;#8211;in production. &lt;a href=&#34;https://engineering.twitter.com/university/videos/docker-mesos&#34;&gt;This Twitter U video highlights how OpenTable uses Mesos for production&lt;/a&gt;. The HAProxy approach, albeit simple, offers a way to route traffic to the correct container. HAProxy will detect failures and reroute traffic accordingly.&lt;/p&gt;

&lt;p&gt;I didn&amp;#8217;t cover inter-container communication (say, a website requiring a database) but you can use your service-discovery tool of choice to solve the problem. The Mesos-Master nodes provide good &amp;#8220;anchor points&amp;#8221; for known locations to look up stuff; you can always query the marathon api for service discovery. Ansible provides a way to automate the install and configuration of mesos-related tools across multiple nodes so you can have a serious mesos-based platform for testing or production use.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Using Typesafe’s Config for Scala (and Java) for Application Configuration</title>
          <link>http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/</link>
          <pubDate>Sun, 23 Feb 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/</guid>
          <description>

&lt;p&gt;I recently leveraged &lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library to refactor configuration settings for a project. I was very pleased with the API and functionality of the library.&lt;/p&gt;

&lt;p&gt;The documentation is pretty solid so there&amp;#8217;s no need to go over basics. One feature I like is the clear hierarchy when specifying configuration values. I find it helpful to put as much as possible in a reference.conf file in the /resources directory for an application or library. These can get overridden in a variety of ways, primarily by adding an application.conf file to the bundled output&amp;#8217;s classpath. The &lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;sbt native packager&lt;/a&gt;, helpful for deploying applications, makes it easy to attach a configuration file to an output. This is helpful if you have settings which you normally wouldn&amp;#8217;t want to use during development, say using remote actors with akka. I find placing a reasonable set of defaults in a reference.conf file allows you to easily transport a configuration around while still overriding it as necessary. Otherwise you can get into copy and paste hell by duplicating configurations across multiple files for multiple environments.&lt;/p&gt;

&lt;h2 id=&#34;alternative-overrides:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Alternative Overrides&lt;/h2&gt;

&lt;p&gt;There are two other interesting ways you can override configuration settings: using environment variables or java system properties. The environment variable approach comes in very handy when pushing to cloud environments where you don&amp;#8217;t know what a configuration is beforehand. Using the ${?VALUE} pattern a property will only be set if a value exists. This allows you to provide an option for overriding a value without actually having to specify one.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s an example in a conf file using substitution leveraging this technique:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http {
 port = 8080
 port = ${?HTTP_PORT}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;re setting a default port of 8080. If the configuration can find a valid substitute it will replace the port value with the substitute; otherwise, it will keep it at 8080. The configuration library will look up its hierarchy for an HTTP_PORT value, checking other configuration files, Java system properties, and finally environment variables. Environment variables aren&amp;#8217;t perfect, but they&amp;#8217;re easy to set and leveraged in a lot of places. If you leave out the ? and just have ${HTTP_PORT} then the application will throw an exception if it can&amp;#8217;t find a value. But by using the ? you can override as many times as you want. This can be helpful when running apps on Heroku where environment variables are set for third party services.&lt;/p&gt;

&lt;h3 id=&#34;using-java-system-properties:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Using Java System Properties&lt;/h3&gt;

&lt;p&gt;Java system properties provide another option for setting config values. The shell script created by sbt-native-packager supports java system properties, so you can also set the http port via the command line using the -D flag:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/bash_script_from_native_packager -Dhttp.port=8081
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be helpful if you want to run an akka based application with a different log level to see what&amp;#8217;s going on in production:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/some_akka_app_script -Dakka.loglevel=debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately sbt run doesn&amp;#8217;t support java system properties so you can&amp;#8217;t tweak settings with the command line when running sbt. The &lt;a href=&#34;https://github.com/spray/sbt-revolver&#34;&gt;sbt-revolver&lt;/a&gt; plugin, which allows you to run your app in a forked JVM, does allow you to pass java arguments using the command line. Once you&amp;#8217;re set up with this plugin you can change settings by adding your Java overrides after &lt;code&gt;---&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;re-start --- -Dhttp.port=8081
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;with-c3p0:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;With c3p0&lt;/h3&gt;

&lt;p&gt;I was really excited to see that the &lt;a href=&#34;http://www.mchange.com/projects/c3p0/#c3p0_conf&#34;&gt;c3p0 connection pool library also supports Typesafe Config&lt;/a&gt;. So you can avoid those annoying xml-based files and merge your c3p0 settings directly with your regular configuration files. I&amp;#8217;ve migrated an application to a &lt;a href=&#34;docker.io&#34;&gt;docker&lt;/a&gt; based development environment and used this c3p0 feature with &lt;a href=&#34;http://docs.docker.io/en/latest/use/working_with_links_names/&#34;&gt;docker links&lt;/a&gt; to set mysql settings:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app {
 db {
  host = localhost
  host = ${?DB_PORT_3306_TCP_ADDR}
  port = &amp;quot;3306&amp;quot;
  port = ${?DB_PORT_3306_TCP_PORT}
 }
}

c3p0 {
 named-configs {
  myapp {
      jdbcUrl = &amp;quot;jdbc:mysql://&amp;quot;${app.db.host}&amp;quot;:&amp;quot;${app.db.port}&amp;quot;/MyDatabase&amp;quot;
  }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When I link a mysql container to my app container with &lt;code&gt;--link mysql:db&lt;/code&gt; Docker will inject the DB_PORT_3306_TCP_* environment variables which are pulled by the above settings.&lt;/p&gt;

&lt;h3 id=&#34;accessing-values-from-code:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Accessing Values From Code&lt;/h3&gt;

&lt;p&gt;One other practice I like is having a single &amp;#8220;Config&amp;#8221; class for an application. It can be very tempting to load a configuration node from anywhere in your app but that can get messy fast. Instead, create a config class and access everything you need through that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;object MyAppConfig {
  private val config =  ConfigFactory.load()

  private lazy val root = config.getConfig(&amp;quot;my_app&amp;quot;)

  object HttpConfig {
    private val httpConfig = config.getConfig(&amp;quot;http&amp;quot;)

    lazy val interface = httpConfig.getString(&amp;quot;interface&amp;quot;)
    lazy val port = httpConfig.getInt(&amp;quot;port&amp;quot;)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Type safety, Single Responsibility, and no strings all over the place.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:25b0f7af66c88453ba07c790c20bec8d&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;When dealing with configuration think about what environments you have and what the actual differences are between those environments. Usually this is a small set of differing values for only a few properties. Make it easy to change just those settings without changing&amp;#8211;or duplicating&amp;#8211;anything else. This could done via environment variables, command line flags, even loading configuration files from a url. Definitely avoid copying the same value across multiple configurations: just distill that value down to a lower setting in a hierarchy. By minimizing configuration files you&amp;#8217;ll be making your life a lot easier.&lt;/p&gt;

&lt;p&gt;If you&amp;#8217;re developing an app for distribution, or writing a library, providing a well-documented configuration file (&lt;a href=&#34;https://github.com/spray/spray/blob/master/spray-can/src/main/resources/reference.conf&#34;&gt;spray&amp;#8217;s spray-can reference.conf is an excellent example&lt;/a&gt;) you can allow users to override defaults easily in a manner that is suitable for them and their runtimes.&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
