<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> </title>
    <link>http://blog.michaelhamrah.com/tags/akka/</link>
    <language>en-us</language>
    <author>Michael Hamrah</author>
    <rights>(C) 2015</rights>
    <updated>2015-01-18 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Adding Http Server-Side Events to Akka-Streams</title>
          <link>http://blog.michaelhamrah.com/2015/01/adding-http-server-side-events-to-akka-streams/</link>
          <pubDate>Sun, 18 Jan 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/01/adding-http-server-side-events-to-akka-streams/</guid>
          <description>

&lt;p&gt;In my last blog post we pushed messages from RabbitMq to the console using Akka-Streams. We used the &lt;a href=&#34;https://github.com/ScalaConsultants/reactive-rabbit&#34;&gt;reactive-rabbit&lt;/a&gt; library to create an Akka-Streams &lt;code&gt;Source&lt;/code&gt; for our &lt;em&gt;streams-playground&lt;/em&gt; queue and mapped the stream to a &lt;code&gt;println&lt;/code&gt; statement before dropping it into an empty &lt;code&gt;Sink&lt;/code&gt;. All Akka-Streams need both a &lt;code&gt;Source&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; to be runnable; we created a complete stream blueprint to be run later.&lt;/p&gt;

&lt;p&gt;Printing to the console is somewhat boring, so let&amp;#8217;s take it up a notch. The excellent &lt;a href=&#34;spray.io&#34;&gt;Spray Web Service&lt;/a&gt; library is being merged into Akka as &lt;a href=&#34;http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M2/scala/http/index.html&#34;&gt;Akka-Http&lt;/a&gt;. It&amp;#8217;s essentially Spray built with Akka-Streams in mind. The routing dsl, immutable request/response model, and high-performance http server are all there; think of it as Spray vNext. Check out Mathias Doenitz&amp;#8217;s &lt;a href=&#34;http://spray.io/scaladays2014/#/&#34;&gt;excellent slide deck on kaka-http from Scala days&lt;/a&gt; to learn more on this evolution of Spray; it also highlights the back-pressure functionality Akka-Streams will give you for Http.&lt;/p&gt;

&lt;p&gt;Everyone&amp;#8217;s familiar with the Request/Response model of Http, but to show the power of Akka-Streams we&amp;#8217;ll add Heiko Seeberger&amp;#8217;s &lt;a href=&#34;https://github.com/hseeberger/akka-sse&#34;&gt;Akka-SSE&lt;/a&gt; library which brings Server-Side Events to Akka-Http. &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/eventsource/basics/&#34;&gt;Server-Side Events&lt;/a&gt; are a more efficient form of long-polling that&amp;#8217;s a lighter protocol to the bi-directional WebSocket API. It allows the client to easily register a handler which the server can then push events to. Akka-SSE adds an SSE-enabled completion marshaller to Akka-Http so your response can be SSE-aware. Instead of printing messages to the console, we&amp;#8217;ll push those messages to the browser with SSE. This shows one of my favorite features of stream-based programming: we simply connect the specific pipes to create more complex flows, without worrying about the how; the framework handles that for us.&lt;/p&gt;

&lt;h2 id=&#34;changing-the-original-example:a9237168f3920272915cff712cbdae5e&#34;&gt;Changing the Original Example&lt;/h2&gt;

&lt;p&gt;If you&amp;#8217;re interested in the code, simply &lt;a href=&#34;https://github.com/mhamrah/streams-playground&#34;&gt;clone the original repo&lt;/a&gt; with &lt;code&gt;git clone https://github.com/mhamrah/streams-playground.git&lt;/code&gt; and then &lt;code&gt;git checkout adding-sse&lt;/code&gt; to get to this step in the repo.&lt;/p&gt;

&lt;p&gt;To modify the original example we&amp;#8217;re going to remove the &lt;code&gt;println&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; calls from &lt;code&gt;RabbitMqConsumer&lt;/code&gt; so we can plug in our enhanced &lt;code&gt;Source&lt;/code&gt; to the Akka-Http sink.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;def consume() = {
    Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
  }
&lt;/pre&gt;

&lt;p&gt;This is now a partial flow: we build up the original RabbitMq &lt;code&gt;Source&lt;/code&gt; with our map function to get the message body. Now the &amp;#8220;other end&amp;#8221; of the stream needs to be connected, which we defer until later. This is the essence of stream composition. There are multiple ways we can cut this up: our &lt;code&gt;map&lt;/code&gt; call could be the only thing in this function, with our &lt;code&gt;RabbitMq&lt;/code&gt; source defined elsewhere.&lt;/p&gt;

&lt;h2 id=&#34;adding-akka-http:a9237168f3920272915cff712cbdae5e&#34;&gt;Adding Akka-Http&lt;/h2&gt;

&lt;p&gt;If you&amp;#8217;re familiar with Spray, Akka-Http won&amp;#8217;t look that much different. We want to create an &lt;code&gt;Actor&lt;/code&gt; for our http service. There are just a few different traits we extend our original Actor from, and a different way plug our routing functions into the Akka-Streams pipeline.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;class HttpService
  extends Actor
  with Directives
  with ImplicitFlowMaterializer
  with SseMarshalling {
  // implementation
  // ...
}
&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;Directives&lt;/code&gt; gives us the routing dsl, similar to &lt;a href=&#34;http://spray.io/documentation/1.2.2/spray-routing/&#34;&gt;spray-routing&lt;/a&gt; (the functions are pretty much the same). Because Akka-Http uses Akka-Streams, we need an implicit &lt;code&gt;FlowMaterializer&lt;/code&gt; in scope to run the stream. &lt;code&gt;ImplicitFlowMaterializer&lt;/code&gt; provides a default. Finally, the &lt;code&gt;SseMarshalling&lt;/code&gt; trait from Heiko Seeberger&amp;#8217;s library provides the SSE functionality we want for our app. &lt;em&gt;If you&amp;#8217;re interested in a robust Akka-Streams sample, Heiko&amp;#8217;s &lt;a href=&#34;https://github.com/hseeberger/reactive-flows&#34;&gt;Reactive-Flows&lt;/a&gt; is worth checking out.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;##Binding to Http&lt;/p&gt;

&lt;p&gt;Within our actor body we&amp;#8217;ll create our http stream by binding a routing function to an http port. This is a little different than Spray; there&amp;#8217;s just some syntactical sugar so we can plug our routing function directly into the http pipeline:&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;//need an ExecutionContext for Futures
    import context.dispatcher

    //There&#39;s no receive needed, this is implicit
    //by our routing dsl.
    override def receive: Receive = Actor.emptyBehavior

    //We bind to an interface and create a 
    //Flow with our routing function
    Http()(context.system)
      .bind(Config.interface, Config.port)
      .startHandlingWith(route)

    //Simple composition of basic routes
    private def route: Route = sse ~ assets

    //Defined later
    private def see: Route = ???
    private def assets: Route = ???
&lt;/pre&gt;

&lt;p&gt;If we weren&amp;#8217;t using the Routing DSL we&amp;#8217;d need to explicitly handling HttpRequest messages in our receive partial function. But the &lt;code&gt;startHandlingWith&lt;/code&gt; call will do this for us; like spray-routing it takes in a routing function, and will call the appropriate route handler. New http requests will be pumped into the route handler and completed with the completion function at the end of the route.&lt;/p&gt;

&lt;p&gt;##Adding SSE&lt;/p&gt;

&lt;p&gt;The last piece of the puzzle is adding a specific route for SSE. We need two pieces for SSE support: first, an implicit function which converts the type produced from our &lt;code&gt;Source&lt;/code&gt; to an &lt;code&gt;Sse.Message&lt;/code&gt;; in this case, we need to go from a &lt;code&gt;String&lt;/code&gt; to an &lt;code&gt;Sse.Message&lt;/code&gt;. Secondly we need a route where a client can subscribe to the stream of server-side events.&lt;/p&gt;

&lt;pre class=&#34;scala&#34;&gt;//Convert a String (our RabbitMq output) to an SSE Message
 implicit def stringToSseMessage(event: String): Sse.Message = {
      Sse.Message(event, Some(&#34;published&#34;))
    }

 //add a route for our sse endpoint.
 private def sse: Route = {
      path(&#34;messages&#34;) {
        get {
          complete {
            RabbitMqConsumer.consume
          }
        }
      }
    }
&lt;/pre&gt;

&lt;p&gt;In order for SSE to work in the browser we need to produce a stream of SSE messages with a specific content-type: &lt;code&gt;Content-Type: text/event-stream&lt;/code&gt;. That&amp;#8217;s what Akka-SSE provides: the SSE Message case classes and serialization to &lt;code&gt;text/event-stream&lt;/code&gt;. Our implicit function &lt;code&gt;stringToSseMessage&lt;/code&gt; allows the Scala types to align so the &amp;#8220;stream pipes&amp;#8221; can be attached together. In our case, we produce a stream of &lt;code&gt;String&lt;/code&gt;s, our RabbitMq message body. We need to produce a stream of &lt;code&gt;SSE.Messages&lt;/code&gt; so we add a simple conversion function. When a new client connects, they&amp;#8217;ll attach themselves to the consuming RabbitMq &lt;code&gt;Source&lt;/code&gt;. Akka-Http lets you natively complete a route with a &lt;code&gt;Flow&lt;/code&gt;; Akka-Sse simply completes that &lt;code&gt;Flow&lt;/code&gt; with the proper Http response for SSE.&lt;/p&gt;

&lt;h2 id=&#34;trying-it-out:a9237168f3920272915cff712cbdae5e&#34;&gt;Trying It Out&lt;/h2&gt;

&lt;p&gt;Fire up SBT and run &lt;code&gt;~reStart&lt;/code&gt;, ensuring you have RabbitMq running and set up a queue named &lt;code&gt;streams-playground&lt;/code&gt; (&lt;a href=&#34;https://github.com/mhamrah/streams-playground/blob/master/README.md&#34;&gt;see the README&lt;/a&gt;). In your console, try a simple &lt;code&gt;curl&lt;/code&gt; command:&lt;/p&gt;

&lt;pre class=&#34;bash&#34;&gt;$ curl http://localhost:8080/messages
&lt;/pre&gt;

&lt;p&gt;The curl command won&amp;#8217;t return. Start sending messages via the RabbitMq Admin console and you&amp;#8217;ll see the SSE output in action:&lt;/p&gt;

&lt;pre class=&#34;bash&#34;&gt;$ curl localhost:8080/messages
event:published
data:woot!

event:published
data:another message!
&lt;/pre&gt;

&lt;p&gt;Close the curl command, and fire up your browser at &lt;code&gt;http://localhost:8080&lt;/code&gt; you&amp;#8217;ll see a simple web page (served from the &lt;code&gt;assets&lt;/code&gt; route). Continue sending messages via RabbitMq, and those messages will be added to the dom. Most modern browsers natively support SSE with the &lt;code&gt;EventSource&lt;/code&gt; object. The following gist creates an event listener on the &lt;code&gt;&#39;published&#39;&lt;/code&gt; event, which is produced from our &lt;code&gt;implicit string =&amp;gt; sse&lt;/code&gt; function above:&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s also handlers for opening the initial sse connection and any errors produced. You could also add more events; our simple conversion only goes from a &lt;code&gt;String&lt;/code&gt; to one specific SSE of type &lt;code&gt;published&lt;/code&gt;. You could map a set of case classes&amp;#8211;preferably an algebraic data type&amp;#8211;to a set of events for the client. Most modern browsers support &lt;code&gt;EventStream&lt;/code&gt;; there&amp;#8217;s no need a for an additional framework or library. The gist above includes a test I copied from the &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/eventsource/basics/&#34;&gt;html5 rocks page on SSE&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;a-naive-implementation:a9237168f3920272915cff712cbdae5e&#34;&gt;A Naive Implementation&lt;/h2&gt;

&lt;p&gt;If you open up multiple browsers to localhost, or &lt;code&gt;curl http://localhost:8080/messages&lt;/code&gt; a few times, you&amp;#8217;ll notice that a published message only goes to one client. This is because our initial RabbitMq &lt;code&gt;Source&lt;/code&gt; only consumes one message from a queue, and passes that down the stream pipeline. That single message will only go to one of the connected clients; there&amp;#8217;s no fanout or broadcasting. You can do that with either RabbitMq or Akka-Streams, try experimenting for yourself!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>A Gentle Introduction To Akka Streams</title>
          <link>http://blog.michaelhamrah.com/2015/01/a-gentle-introduction-to-akka-streams/</link>
          <pubDate>Tue, 13 Jan 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/01/a-gentle-introduction-to-akka-streams/</guid>
          <description>

&lt;p&gt;I&amp;#8217;m happy to see stream-based programming emerge as a paradigm in many languages. Streams have been around for a while: take a look at the good &amp;#8216;ol | operator in Unix. Streams offer an interesting conceptual model to processing pipelines that is very functional: you have an input, you produce an output. You string these little functions together to build bigger, more complex pipelines. Most of the time you can make these functions asynchronous and parallelize them over input data to maximize throughput and scale. With a Stream, handling data is almost hidden behind the scenes: it just &lt;em&gt;flows&lt;/em&gt; through &lt;em&gt;functions&lt;/em&gt;, producing a new output from some input. In the case of an Http server, the Request-Response model across all clients is a Stream-based process: You map a Request to a Response, passing it through various functions which act on an input. Forget about MVC, it&amp;#8217;s all middleware. No need to set variables, iterate over collections, orchestrate function calls. Just concatenate stream-enabled functions together, and run your code. Streams offer a succinct programming model for a process. The fact it also scales is a nice bonus.&lt;/p&gt;

&lt;p&gt;Stream based programming is possible in a variety of languages, and I encourage you to explore this space. There&amp;#8217;s an excellent &lt;a href=&#34;https://github.com/substack/stream-handbook&#34;&gt;stream handbook for Node&lt;/a&gt;, &lt;a href=&#34;https://github.com/matz/streem&#34;&gt;an exploratory stream language from Yukihiro &amp;#8220;Matz&amp;#8221; Matsumoto of Ruby fame&lt;/a&gt;, &lt;a href=&#34;https://spark.apache.org/streaming/&#34;&gt;Spark Streaming&lt;/a&gt; and of course &lt;a href=&#34;http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M2/index.html&#34;&gt;Akka-Streams&lt;/a&gt; which joins the existing &lt;a href=&#34;https://github.com/scalaz/scalaz-stream&#34;&gt;scalaz-stream&lt;/a&gt; library for Scala. Even Go&amp;#8217;s &lt;a href=&#34;http://golang.org/pkg/net/http/#HandleFunc&#34;&gt;HttpHandler function&lt;/a&gt; is Stream-esque: you can easily wrap one function around another, building up a flow, and manipulate the Response stream accordingly.&lt;/p&gt;

&lt;h2 id=&#34;why-akka-streams:9c0e63de68271e30d1a6e002245492be&#34;&gt;Why Akka-Streams?&lt;/h2&gt;

&lt;p&gt;Akka-Streams provide a higher-level abstraction over Akka&amp;#8217;s existing actor model. The Actor model provides an excellent primitive for writing concurrent, scalable software, but it still is a primitive; it&amp;#8217;s not hard to find a few critiques of the model. So is it possible to have your cake and eat it too? Can we abstract the functionality we want to achieve with Actors into a set of function calls? Can we treat Actor Messages as Inputs and Outputs to Functions, with type safety? Hello, Akka-Streams.&lt;/p&gt;

&lt;p&gt;There&amp;#8217;s an excellent &lt;a href=&#34;http://www.typesafe.com/activator/template/akka-stream-scala&#34;&gt;activator template for Akka-Streams&lt;/a&gt; offering an in-depth tutorial on several aspects of Akka-Streams. For a more a gentler introduction, read on.&lt;/p&gt;

&lt;h2 id=&#34;the-recipe:9c0e63de68271e30d1a6e002245492be&#34;&gt;The Recipe&lt;/h2&gt;

&lt;p&gt;To cook up a reasonable dish, we are going to consume messages from &lt;a href=&#34;https://www.rabbitmq.com&#34;&gt;RabbitMq&lt;/a&gt; with the &lt;a href=&#34;https://github.com/ScalaConsultants/reactive-rabbit&#34;&gt;reactive-rabbit&lt;/a&gt; library and output them to the console. The code is on &lt;a href=&#34;https://github.com/mhamrah/streams-playground&#34;&gt;GitHub&lt;/a&gt;. If you&amp;#8217;d like to follow along, &lt;code&gt;git clone&lt;/code&gt; and then &lt;code&gt;git checkout intro&lt;/code&gt;; hopefully I&amp;#8217;ll build up more functionality in later posts so the master branch may differ.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s start with a code snippet:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;object RabbitMqConsumer {
 def consume(implicit flowMaterializer: FlowMaterializer) = {
    Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
      .foreach(println(_))
  }
}
&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;We use a RabbitMq connection to consume messages off of a queue named &lt;code&gt;streams-playground&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;For each message, we pull out the message and decode the bytes as a UTF-8 string&lt;/li&gt;
&lt;li&gt;We print it to the console&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-ingredients:9c0e63de68271e30d1a6e002245492be&#34;&gt;The Ingredients&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;Source&lt;/code&gt; is something which produces exactly one output. If you need something that generates data, you need a &lt;code&gt;Source&lt;/code&gt;. Our source above is produced from the &lt;code&gt;connection.consume&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;Sink&lt;/code&gt; is something with exactly one input. A &lt;code&gt;Sink&lt;/code&gt; is the final stage of a Stream process. The &lt;code&gt;.foreach&lt;/code&gt; call is a Sink which writes the input (_) to the console via &lt;code&gt;println&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;Flow&lt;/code&gt; is something with exactly one input and one output. It allows data to flow through a function: like calling &lt;code&gt;map&lt;/code&gt; which also returns an element on a collection. The &lt;code&gt;map&lt;/code&gt; call above is a &lt;code&gt;Flow&lt;/code&gt;: it consumes a &lt;code&gt;Delivery&lt;/code&gt; message and outputs a &lt;code&gt;String&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to actually run something using Akka-Streams you must have both a &lt;code&gt;Source&lt;/code&gt; and &lt;code&gt;Sink&lt;/code&gt; attached to the same pipeline. This allows you to create a &lt;code&gt;RunnableFlow&lt;/code&gt; and begin processing the stream. Just as you can compose functions and classes, you can compose streams to build up richer functionality. It&amp;#8217;s a powerful abstraction allowing you to build your processing logic independently of its execution. Think of stream libraries where you &amp;#8220;plug in&amp;#8221; parts of streams together and customize accordingly.&lt;/p&gt;

&lt;h2 id=&#34;a-simple-flow:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Simple Flow&lt;/h2&gt;

&lt;p&gt;You&amp;#8217;ll notice the above snippet requires an &lt;code&gt;implicit flowMaterializer: FlowMaterializer&lt;/code&gt;. A &lt;code&gt;FlowMaterializer&lt;/code&gt; is required to actually run a &lt;code&gt;Flow&lt;/code&gt;. In the snippet above &lt;code&gt;foreach&lt;/code&gt; acts as both a &lt;code&gt;Sink&lt;/code&gt; and a &lt;code&gt;run()&lt;/code&gt; call to run the flow. If you look at the Main.scala file you&amp;#8217;ll see I start the stream easily in one call:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;implicit val flowMaterializer = FlowMaterializer()
  RabbitMqConsumer.consume
&lt;/pre&gt;

&lt;p&gt;Create a queue named &lt;code&gt;streams-playground&lt;/code&gt; via the RabbitMq Admin UI and run the application. You can use publish messages in the RabbitMq Admin UI and they will appear in the console. Try some UTF-8 characters, like åßç∂!&lt;/p&gt;

&lt;h2 id=&#34;a-variation:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Variation&lt;/h2&gt;

&lt;p&gt;The original snippet is nice, but it does require the implicit FlowMaterializer to build and run the stream in &lt;code&gt;consume&lt;/code&gt;. If you remove it, you&amp;#8217;ll get a compile error. Is there a way to separate the definition of the stream with the running of the stream? Yes, by simply removing the &lt;code&gt;foreach&lt;/code&gt; call. &lt;code&gt;foreach&lt;/code&gt; is just syntactical sugar for a &lt;code&gt;map&lt;/code&gt; with a &lt;code&gt;run()&lt;/code&gt; call. By explicitly setting a &lt;code&gt;Sink&lt;/code&gt; without a call to &lt;code&gt;run()&lt;/code&gt; we can construct our stream blueprint producing a new object of type &lt;code&gt;RunnableFlow&lt;/code&gt;. Intuitively, it&amp;#8217;s a &lt;code&gt;Flow&lt;/code&gt; which can be &lt;code&gt;run()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here&amp;#8217;s the variation:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;def consume() = {
     Source(connection.consume(&#34;streams-playground&#34;))
      .map(_.message.body.utf8String)
      .map(println(_))
      .to(Sink.ignore) //won&#39;t start consuming until run() is called!
  }
&lt;/pre&gt;

&lt;p&gt;We got rid of our &lt;code&gt;flowMaterializer&lt;/code&gt; implicit by terminating our Stream with a &lt;code&gt;to()&lt;/code&gt; call and a simple Sink.ignore which discards messages. This stream will not be run when called. Instead we must call it explicitly in Main.scala:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;implicit val flowMaterializer = FlowMaterializer()
  RabbitMqConsumer.consume().run()
&lt;/pre&gt;

&lt;p&gt;We&amp;#8217;ve separated out the entire pipeline into two stages: the build stage, via the &lt;code&gt;consume&lt;/code&gt; call, and the run stage, with &lt;code&gt;run()&lt;/code&gt;. Ideally you&amp;#8217;d want to compose your stream processing as you wire up the app, with each component, like RabbitMqConsumer, providing part of the overall stream process.&lt;/p&gt;

&lt;h2 id=&#34;a-counter-example:9c0e63de68271e30d1a6e002245492be&#34;&gt;A Counter Example&lt;/h2&gt;

&lt;p&gt;As an alternative, explore the &lt;a href=&#34;http://www.rabbitmq.com/tutorials/tutorial-one-java.html&#34;&gt;rabbitmq tutorials&lt;/a&gt; for Java examples. Here&amp;#8217;s a snippet from the site:&lt;/p&gt;

&lt;pre class=&#34;lang:java&#34;&gt;QueueingConsumer consumer = new QueueingConsumer(channel);
    channel.basicConsume(QUEUE_NAME, true, consumer);

    while (true) {
      QueueingConsumer.Delivery delivery = consumer.nextDelivery();
      String message = new String(delivery.getBody());
      System.out.println(&#34; [x] Received &#39;&#34; + message + &#34;&#39;&#34;);
    }
&lt;/pre&gt;

&lt;p&gt;This is typical of an imperative style. Our flow is controlled by the while loop, we have to explicitly manage variables, and there&amp;#8217;s no flow control. We could separate out the body from the while loop, but we&amp;#8217;d have a crazy function signature. Alternatively on the Akka side there&amp;#8217;s the solid &lt;a href=&#34;https://github.com/sstone/amqp-client&#34;&gt;amqp-client library&lt;/a&gt; which provides an Actor based model over RabbitMq:&lt;/p&gt;

&lt;pre class=&#34;lang:scala&#34;&gt;// create an actor that will receive AMQP deliveries
  val listener = system.actorOf(Props(new Actor {
    def receive = {
      case Delivery(consumerTag, envelope, properties, body) =&gt; {
        println(&#34;got a message: &#34; + new String(body))
        sender ! Ack(envelope.getDeliveryTag)
      }
    }
  }))

  // create a consumer that will route incoming AMQP messages to our listener
  // it starts with an empty list of queues to consume from
  val consumer = ConnectionOwner.createChildActor(conn, Consumer.props(listener, channelParams = None, autoack = false))
&lt;/pre&gt;

&lt;p&gt;You get the concurrency primitives via configuration over the actor system, but we still enter imperative-programming land in the Actor&amp;#8217;s &lt;code&gt;receive&lt;/code&gt; blog (sure, this can be refactored to some degree). In general, if we can model our process as a set of streams, we achieve the same benefits we get with functional programming: clear composition on what is happening, not how it&amp;#8217;s doing it.&lt;/p&gt;

&lt;p&gt;Streams can be applied in a variety of contexts. I&amp;#8217;m happy to see the amazing and powerful &lt;a href=&#34;http://spray.io&#34;&gt;spray.io&lt;/a&gt; library for Restful web services will be merged into Akka as a stream enabled http toolkit. It&amp;#8217;s also not hard to find out what&amp;#8217;s been done with &lt;a href=&#34;https://github.com/scalaz/scalaz-stream#projects-using-scalaz-stream&#34;&gt;scalaz-streams&lt;/a&gt; or the plethora of tooling already available in other languages.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Clustering Akka Applications with Docker — Version 3</title>
          <link>http://blog.michaelhamrah.com/2014/11/clustering-akka-applications-with-docker-version-3/</link>
          <pubDate>Thu, 27 Nov 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/11/clustering-akka-applications-with-docker-version-3/</guid>
          <description>&lt;p&gt;The SBT Native Packager plugin now offers first-class Docker support for building Scala based applications. My last post involved combining SBT Native Packager, SBT Docker, and a custom start script to launch our application. We can simplify the process in two ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Although the SBT Docker plugin allows for better customization of Dockerfiles it&amp;#8217;s unnecessary for our use case. SBT Native Packager is enough.&lt;/li&gt;
&lt;li&gt;A separate start script was required for IP address inspection so TCP traffic can be routed to the actor system. I recently contributed an update for &lt;a href=&#34;https://github.com/sbt/sbt-native-packager/pull/411&#34;&gt;better ENTRYPOINT support within SBT Native Packager&lt;/a&gt; which gives us options for launching our app in a container.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With this PR we can now add our IP address inspection snippet to our build removing the need for extraneous files. We could have added this snippet to &lt;code&gt;bashScriptExtraDefines&lt;/code&gt; but that is a global change, requiring &lt;code&gt;/sbin/ifconfig eth0&lt;/code&gt; to be available wherever the application is run. This is definitely infrastructure bleed-out and must be avoided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;The new code, on GitHub,&lt;/a&gt; uses a shell with ENTRYPOINT exec mode to set our environment variable before launching the application:&lt;/p&gt;

&lt;pre class=&#34;code scala&#34;&gt;dockerExposedPorts in Docker := Seq(1600)

dockerEntrypoint in Docker := Seq(&#34;sh&#34;, &#34;-c&#34;, &#34;CLUSTER_IP=`/sbin/ifconfig eth0 | grep &#39;inet addr:&#39; | cut -d: -f2 | awk &#39;{ print $1 }&#39;` bin/clustering $*&#34;)
&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;$*&lt;/code&gt; allows for command-line parameters to be honored when launching the container. Because the app leverages the Typesafe Config library we can also set via Java system properties:&lt;/p&gt;

&lt;pre class=&#34;code bash&#34;&gt;docker run --rm -i -t --name seed mhamrah/clustering:0.3 -Dclustering.cluster.name=example-cluster
&lt;/pre&gt;

&lt;p&gt;Launching the cluster is exactly as before:&lt;/p&gt;

&lt;pre class=&#34;code bash&#34;&gt;docker run --rm -d --name seed mhamrah/clustering:0.3
docker run --rm -d --name member1 --link seed:seed mhamrah/clustering:0.3
&lt;/pre&gt;

&lt;p&gt;For complex scripts it may be too messy to overload the ENTRYPOINT sequence. For those cases simply bake your own docker container as a base and use the ENTRYPOINT approach to call out to your script. SBT Native Packager will still upload all your dependencies and its bash script to &lt;code&gt;/opt/docker/bin/&amp;lt;your app&amp;gt;&lt;/code&gt;. The Docker &lt;code&gt;WORKDIR&lt;/code&gt; is set to &lt;code&gt;/opt/docker&lt;/code&gt; so you can drop the &lt;code&gt;/opt/docker&lt;/code&gt; as above.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Akka Clustering with SBT-Docker and SBT-Native-Packager</title>
          <link>http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/</link>
          <pubDate>Thu, 19 Jun 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/</guid>
          <description>

&lt;p&gt;Since my last post on &lt;a href=&#34;http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/&#34;&gt;akka clustering with docker containers&lt;/a&gt; a new plugin, &lt;a href=&#34;https://github.com/marcuslonnberg/sbt-docker&#34;&gt;SBT-Docker&lt;/a&gt;, has emerged which allows you to build docker containers directly from SBT. I&amp;#8217;ve updated my &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;akka-docker-cluster-example&lt;/a&gt; to leverage these two plugins for a smoother docker build experience.&lt;/p&gt;

&lt;h2 id=&#34;one-step-build:9dc58615474f52923afa41a9d5040e47&#34;&gt;One Step Build&lt;/h2&gt;

&lt;p&gt;The approach is basically the same as the previous example: we use SBT Native Packager to gather up the appropriate dependencies, upload them to the docker container, and create the entrypoint. I decided to keep the start script approach to &amp;#8220;prep&amp;#8221; any environment variables required before launching. With SBT Docker linked to Native Packager all you need to do is fire&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;docker
&lt;/pre&gt;

&lt;p&gt;from sbt and you have a docker container ready to launch or push.&lt;/p&gt;

&lt;h2 id=&#34;understanding-the-build:9dc58615474f52923afa41a9d5040e47&#34;&gt;Understanding the Build&lt;/h2&gt;

&lt;p&gt;SBT Docker requires a dockerfile defined in your build. I want to pass in artifacts from native packager to docker. This allows native packager to focus on application needs while docker is focused on docker. Docker turns into just another type of package for your app.&lt;/p&gt;

&lt;p&gt;We can pass in arguments by mapping the appropriate parameters to a function which returns the Dockerfile. In build.spt:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;// Define a dockerfile, using parameters from native-packager
dockerfile in docker &amp;lt;&amp;lt;= (name, stagingDirectory in Universal) map {
  case(appName, stageDir) =&gt;
    val workingDir = s&#34;/opt/${appName}&#34;
    new Dockerfile {
      //use java8 base box
      from(&#34;relateiq/oracle-java8&#34;)
      maintainer(&#34;Michael Hamrah&#34;)
      //expose our akka port
      expose(1600)
      //upload native-packager staging directory files
      add(stageDir, workingDir)
      //make files executable
      run(&#34;chmod&#34;, &#34;+x&#34;, s&#34;/opt/${appName}/bin/${appName}&#34;)
      run(&#34;chmod&#34;, &#34;+x&#34;, s&#34;/opt/${appName}/bin/start&#34;)
      //set working directory
      workDir(workingDir)
      //entrypoint into our start script
      entryPointShell(s&#34;bin/start&#34;, appName, &#34;$@&#34;)
    }
}
&lt;/pre&gt;

&lt;h3 id=&#34;linking-sbt-docker-to-sbt-native-packager:9dc58615474f52923afa41a9d5040e47&#34;&gt;Linking SBT Docker to SBT Native Packager&lt;/h3&gt;

&lt;p&gt;Because we&amp;#8217;re relying on Native Packager to assemble our runtime dependencies we need to ensure the native packager files are &amp;#8220;staged&amp;#8221; before docker tries to upload them. Luckily it&amp;#8217;s easy to create dependencies with SBT. We simply have docker depend on the native packager&amp;#8217;s stage task:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;docker &amp;lt;&amp;lt;= docker.dependsOn(com.typesafe.sbt.packager.universal.Keys.stage.in(Compile))
&lt;/pre&gt;

&lt;h3 id=&#34;adding-additional-files:9dc58615474f52923afa41a9d5040e47&#34;&gt;Adding Additional Files&lt;/h3&gt;

&lt;p&gt;The last step is to add our start script to the native packager build. Native packager has a &lt;code&gt;mappings&lt;/code&gt; key where we can add files to our package. I kept the start script in the docker folder and I want it in the bin directory within the docker container. Here&amp;rsquo;s the mapping:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;mappings in Universal += baseDirectory.value / &#34;docker&#34; / &#34;start&#34; -&gt; &#34;bin/start&#34;
&lt;/pre&gt;

&lt;p&gt;With this setting everything will be assembled as needed and we can package to any type we want. Setting up a cluster with docker is &lt;a href=&#34;http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/&#34;&gt;the same as before&lt;/a&gt;:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;docker run --name seed -i -t clustering
docker run --name c1 -link seed:seed -i -t clustering
&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s interesting to note SBT Native Packager also has docker support, but it&amp;rsquo;s undocumented and doesn&amp;rsquo;t allow granular control over the Dockerfile output. Until SBT Native Packager fully supports docker output the SBT Docker plugin is a nice tool to package your sbt-based apps.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Running an Akka Cluster with Docker Containers</title>
          <link>http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/</link>
          <pubDate>Sun, 23 Mar 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/03/running-an-akka-cluster-with-docker-containers/</guid>
          <description>

&lt;p&gt;&lt;strong&gt;Update! You can now use SBT-Docker with SBT-Native Packager for a better sbt/docker experience. &lt;a href=&#34;http://blog.michaelhamrah.com/2014/06/akka-clustering-with-sbt-docker-and-sbt-native-packager/&#34;&gt;Here&amp;#8217;s the new approach&lt;/a&gt; with an &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;updated GitHub repo&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We recently upgraded our vagrant environments to use &lt;a href=&#34;http://docker.io&#34;&gt;docker&lt;/a&gt;. One of our projects relies on &lt;a href=&#34;http://doc.akka.io/docs/akka/2.3.0/common/cluster.html&#34;&gt;akka&amp;#8217;s cluster functionality&lt;/a&gt;. I wanted to easily run an akka cluster locally using docker as sbt can be somewhat tedious. &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;The example project is on github&lt;/a&gt; and the solution is described below.&lt;/p&gt;

&lt;p&gt;The solution relies on:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sbt/sbt-native-packager&#34;&gt;Sbt Native Packager&lt;/a&gt; to package dependencies and create a startup file.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/typesafehub/config&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library for configuring the app&amp;#8217;s ip address and seed nodes. We setup cascading configurations that will look for docker link environment variables if present.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example/blob/master/bin/dockerize&#34;&gt;A simple bash script&lt;/a&gt; to package the app and build the docker container.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;#8217;m a big fan of &lt;a href=&#34;http://blog.michaelhamrah.com/2014/02/leveraging-typesafes-config-library-across-environments/&#34;&gt;Typesafe&amp;#8217;s Config&lt;/a&gt; library and the environment variable overrides come in handy for providing sensible defaults with optional overrides. It&amp;#8217;s the preferred way we configure our applications in upper environments.&lt;/p&gt;

&lt;p&gt;The tricky part of running an akka cluster with docker is knowing the ip address each remote node needs to listen on. An akka cluster relies on each node listening on a specific port and hostname or ip. It also needs to know the port and hostname/ip of a seed node the cluster. As there&amp;#8217;s no catch-all binding we need specific ip settings for our cluster.&lt;/p&gt;

&lt;p&gt;A simple bash script within the container will figure out the current IP for our cluster configuration and &lt;a href=&#34;http://docs.docker.io/en/latest/use/working_with_links_names/&#34;&gt;docker links&lt;/a&gt; pass seed node information to newly launched nodes.&lt;/p&gt;

&lt;h2 id=&#34;first-step-setup-application-configuration:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;First Step: Setup Application Configuration&lt;/h2&gt;

&lt;p&gt;The configuration is the same as that of a normal cluster, but I&amp;#8217;m using substitution to configure the ip address, port and seed nodes for the application. For simplicity I setup a &lt;code&gt;clustering&lt;/code&gt; block with defaults for running normally and environment variable overrides:&lt;/p&gt;

&lt;pre class=&#34;syntax json&#34;&gt;clustering {
 ip = &#34;127.0.0.1&#34;
 ip = ${?CLUSTER_IP}
 port = 1600
 port = ${?CLUSTER_PORT}
 seed-ip = &#34;127.0.0.1&#34;
 seed-ip = ${?CLUSTER_IP}
 seed-ip = ${?SEED_PORT_1600_TCP_ADDR}
 seed-port = 1600
 seed-port = ${?SEED_PORT_1600_TCP_PORT}
 cluster.name = clustering-cluster
}

akka.remote {
    log-remote-lifecycle-events = on
    netty.tcp {
      hostname = ${clustering.ip}
      port = ${clustering.port}
    }
  }
  cluster {
    seed-nodes = [
       &#34;akka.tcp://&#34;${clustering.cluster.name}&#34;@&#34;${clustering.seed-ip}&#34;:&#34;${clustering.seed-port}
    ]
    auto-down-unreachable-after = 10s
  }
}
&lt;/pre&gt;

&lt;p&gt;As an example the &lt;code&gt;clustering.seed-ip&lt;/code&gt; setting will use &lt;em&gt;127.0.0.1&lt;/em&gt; as the default. If it can find a &lt;em&gt;CLUSTER_IP&lt;/em&gt; or a &lt;em&gt;SEED_PORT_1600_TCP_ADDR&lt;/em&gt; override it will use that instead. You&amp;#8217;ll notice the latter override is using docker&amp;#8217;s environment variable pattern for linking: that&amp;#8217;s how we set the cluster&amp;#8217;s seed node when using docker. You don&amp;#8217;t need the &lt;em&gt;CLUSTER_IP&lt;/em&gt; in this example but that&amp;#8217;s the environment variable we use in upper environments and I didn&amp;#8217;t want to change our infrastructure to conform to docker&amp;#8217;s pattern. The cascading settings are helpful if you&amp;#8217;re forced to follow one pattern depending on the environment. We do the same thing for the ip and port of the current node when launched.&lt;/p&gt;

&lt;p&gt;With this override in place we can use substitution to set the seed nodes in the akka cluster configuration block. The expression &lt;code&gt;&amp;quot;akka.tcp://&amp;quot;${clustering.cluster.name}&amp;quot;@&amp;quot;${clustering.seed-ip}&amp;quot;:&amp;quot;${clustering.seed-port}&lt;/code&gt; builds the proper akka URI so the current node can find the seed node in the cluster. Seed nodes avoid potential split-brain issues during network partitions. You&amp;#8217;ll want to run more than one in production but for local testing one is fine. On a final note the cluster-name setting is arbitrary. Because the name of the actor system and the uri must match I prefer not to hard code values in multiple places.&lt;/p&gt;

&lt;p&gt;I put these settings in resources/reference.conf. We could have named this file application.conf, but I prefer bundling configurations as reference.conf and reserving application.conf for external configuration files. A setting in application.conf will override a corresponding reference.conf setting and you probably want to manage application.conf files outside of the project&amp;#8217;s jar file.&lt;/p&gt;

&lt;h2 id=&#34;second-sbt-native-packager:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Second: SBT Native Packager&lt;/h2&gt;

&lt;p&gt;We use the native packager plugin to build a runnable script for our applications. For docker we just need to run &lt;code&gt;universal:stage&lt;/code&gt;, creating a folder with all dependencies in the &lt;code&gt;target/&lt;/code&gt; folder of our project. We&amp;#8217;ll move this into a staging directory for uploading to the docker container.&lt;/p&gt;

&lt;h2 id=&#34;third-the-dockerfile-and-start-script:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Third: The Dockerfile and Start script&lt;/h2&gt;

&lt;p&gt;The dockerfile is pretty simple:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;FROM dockerfile/java

MAINTAINER Michael Hamrah m@hamrah.com

ADD tmp/ /opt/app/
ADD start /opt/start
RUN chmod +x /opt/start

EXPOSE 1600

ENTRYPOINT [ &#34;/opt/start&#34; ]
&lt;/pre&gt;

&lt;p&gt;We start with Dockerfile&amp;#8217;s java base image. We then upload our staging &lt;code&gt;tmp/&lt;/code&gt; folder which has our application from sbt&amp;#8217;s native packager output and a corresponding executable start script described below. I opted for &lt;code&gt;ENTRYPOINT&lt;/code&gt; instead of &lt;code&gt;CMD&lt;/code&gt; so the container is treated like an executable. This makes it easier to pass in command line arguments into the sbt native packager script in case you want to set java system properties or override configuration settings via command line arguments.&lt;/p&gt;

&lt;p&gt;The start script is how we tee up the container&amp;#8217;s IP address for our cluster application:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

CLUSTER_IP=&lt;code&gt;/sbin/ifconfig eth0 | grep &#39;inet addr:&#39; | cut -d: -f2 | awk &#39;{ print $1}&#39;&lt;/code&gt; /opt/app/bin/clustering $@
&lt;/pre&gt;

&lt;p&gt;The script sets an inline environment variable by parsing &lt;code&gt;ifconfig&lt;/code&gt; output to get the container&amp;#8217;s ip. We then run the &lt;em&gt;clustering&lt;/em&gt; start script produced from sbt native packager. The &lt;code&gt;$@&lt;/code&gt; lets us pass along any command line settings set when launching the container into the sbt native packager script.&lt;/p&gt;

&lt;h2 id=&#34;fourth-putting-it-together:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Fourth: Putting It Together&lt;/h2&gt;

&lt;p&gt;The last part is a simple bash script named &lt;code&gt;dockerize&lt;/code&gt; to orchestrate each step. By running this script we run sbt native packager, move files to a staging directory, and build the container:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

echo &#34;Build docker container&#34;

#run sbt native packager
sbt universal:stage

#cleanup stage directory
rm -rf docker/tmp/

#copy output into staging area
cp -r target/universal/stage/ docker/tmp/

#build the container, remove intermediate nodes
docker build -rm -t clustering docker/

#remove staged files
rm -rf docker/tmp/
&lt;/pre&gt;

&lt;p&gt;With this in place we simply run&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;bin/dockerize
&lt;/pre&gt;

&lt;p&gt;to create our docker container named clustering.&lt;/p&gt;

&lt;h2 id=&#34;running-the-application-within-docker:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Running the Application within Docker&lt;/h2&gt;

&lt;p&gt;With our clustering container built we fire up our first instance. This will be our seed node for other containers:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -i -t -name seed clustering
2014-03-23 00:20:39,918 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:20:40,392 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:20:40,403 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:20:40,418 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:20:41,404 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
&lt;/pre&gt;

&lt;p&gt;Next we fire up a second node. Because of our reference.conf defaults all we need to do is link this container with the name &lt;em&gt;seed&lt;/em&gt;. Docker will set the environment variables we are looking for in the bundled reference.conf:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -name c1 -link seed:seed -i -t clustering
2014-03-23 00:22:49,332 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:22:49,788 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:22:49,797 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:22:50,238 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
2014-03-23 00:22:50,249 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:22:50,803 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.3:1600
&lt;/pre&gt;

&lt;p&gt;You&amp;#8217;ll see the current leader discovering new nodes and the appropriate broadcast messages sent out. We can even do this a third time and all nodes will react:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;$ docker run -name c2 -link seed:seed -i -t clustering
2014-03-23 00:24:52,768 INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-03-23 00:24:53,224 DEBUG com.mlh.clustering.ClusterListener - starting up cluster listener...
2014-03-23 00:24:53,235 DEBUG com.mlh.clustering.ClusterListener - Current members:
2014-03-23 00:24:53,470 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.2:1600
2014-03-23 00:24:53,472 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.3:1600
2014-03-23 00:24:53,478 INFO  com.mlh.clustering.ClusterListener - Leader changed: Some(akka.tcp://clustering-cluster@172.17.0.2:1600)
2014-03-23 00:24:55,401 DEBUG com.mlh.clustering.ClusterListener - Member is Up: akka.tcp://clustering-cluster@172.17.0.4:1600
&lt;/pre&gt;

&lt;p&gt;Try killing a node and see what happens!&lt;/p&gt;

&lt;h2 id=&#34;modifying-the-docker-start-script:bc2a8c6fccabe9b5fedfd70d57effb88&#34;&gt;Modifying the Docker Start Script&lt;/h2&gt;

&lt;p&gt;There&amp;#8217;s another reason for the docker start script: it opens the door for different seed discovery options. Container linking works well if everything is running on the same host but not when running on multiple hosts. Also setting multiple seed nodes via docker links will get tedious via environment variables; it&amp;#8217;s doable but we&amp;#8217;re getting into coding-cruft territory. It would be better to discover seed nodes and set that configuration via command line parameters when launching the app.&lt;/p&gt;

&lt;p&gt;The start script gives us control over how we discover information. We could use &lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;etcd&lt;/a&gt;, &lt;a href=&#34;http://www.serfdom.io/&#34;&gt;serf&lt;/a&gt; or even zookeeper to manage how seed nodes are set and discovered, passing this to our application via environment variables or additional command line parameters. Seed nodes can easily be set via system properties set via the command line:&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;-Dakka.cluster.seed-nodes.0=akka.tcp://ClusterSystem@host1:2552
-Dakka.cluster.seed-nodes.1=akka.tcp://ClusterSystem@host2:2552
&lt;/pre&gt;

&lt;p&gt;The start script can probably be configured via sbt native packager but I haven&amp;#8217;t looked into that option. Regardless this approach is (relatively) straight forward to run akka clusters with docker. The &lt;a href=&#34;https://github.com/mhamrah/akka-docker-cluster-example&#34;&gt;full project is on github&lt;/a&gt;. If there&amp;#8217;s a better approach I&amp;#8217;d love to know!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Testing Akka’s FSM: Using setState for unit testing</title>
          <link>http://blog.michaelhamrah.com/2014/03/testing-akkas-fsm-using-setstate-for-unit-testing/</link>
          <pubDate>Fri, 21 Mar 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/03/testing-akkas-fsm-using-setstate-for-unit-testing/</guid>
          <description>&lt;p&gt;I wrote &lt;a href=&#34;http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/&#34;&gt;about Akka&amp;#8217;s Finite State Machine&lt;/a&gt; as a way to model a process. One thing I didn&amp;#8217;t discuss was testing an FSM. Akka has &lt;a href=&#34;http://doc.akka.io/docs/akka/snapshot/scala/testing.html&#34;&gt;great testing support&lt;/a&gt; and FSM&amp;#8217;s can easily be tested using the &lt;a href=&#34;http://doc.akka.io/api/akka/snapshot/index.html#akka.testkit.TestFSMRef&#34;&gt;&lt;code&gt;TestFSMRef&lt;/code&gt;&lt;/a&gt; class.&lt;/p&gt;

&lt;p&gt;An FSM is defined by its states and the data stored between those states. For each state in the machine you can match on both an incoming message and current state data. Our previous example modeled a process to check data integrity across two systems. We&amp;#8217;ll continue that example by adding tests to ensure the FSM is working correctly. &lt;a href=&#34;http://blog.michaelhamrah.com/2013/02/embracing-test-driven-development-for-speed/&#34;&gt;These should have been before we developed the FSM&lt;/a&gt; but late tests are (arguably) better than no tests.&lt;/p&gt;

&lt;p&gt;It&amp;#8217;s important to test combinations of messages against various states and data. You don&amp;#8217;t want to be in a position to run through a state machine to the state you want for every test. Luckily, there&amp;#8217;s a handy &lt;code&gt;setState&lt;/code&gt; method to explicitly set the current state and data of the FSM. This lets you &amp;#8220;fast forward&amp;#8221; the FSM to the exact state you want to test against.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s say we want to test a &lt;code&gt;DataRetrieved&lt;/code&gt; message in the &lt;code&gt;PendingComparison&lt;/code&gt; state. We also want to test this message against various &lt;code&gt;Data&lt;/code&gt; combinations. We can set this state explicitly:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;The ComparisonEngine&#34; - {
  &#34;in the PendingComparison state&#34; - {
    &#34;when a DataRetrieved message arrives from the old system&#34; - {
      &#34;stays in PendingComparison with updated data when no other data is present&#34; in {
        val fsm = TestFSMRef(new ComparisonEngine())
        
        //set our initial state with setState
        fsm.setState(PendingComparison, ComparisonStatus(&#34;someid&#34;, None, None))

        fsm ! DataRetrieved(&#34;oldSystem&#34;, &#34;oldData&#34;)

        fsm.stateName should be (PendingComparison)
        fsm.stateData should be (ComparisonStatus(&#34;someid&#34;, Some(&#34;oldData&#34;), None))
      }
    }
  }
}
&lt;/pre&gt;

&lt;p&gt;It may be tempting to send more messages to continue verifying the FSM is working correctly. This will yield a large, unwieldy and brittle test. It will make refactoring difficult and make it harder to understand what &lt;em&gt;should&lt;/em&gt; be happening.&lt;/p&gt;

&lt;p&gt;Instead, to further test the FSM, be explicit about the current state and what should happen next. Add a test for it:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;&#34;when a DataRetrieved message arrives from the old system&#34; - {
  &#34;moves to AllRetrieved when data from the new system is already present&#34; in {
    val fsm = TestFSMRef(new ComparisonEngine())
        
    //set our initial state with setState
    fsm.setState(PendingComparison, ComparisonStatus(&#34;someid&#34;, None, Some(&#34;newData&#34;)))

    fsm ! DataRetrieved(&#34;oldSystem&#34;, &#34;oldData&#34;)
    fsm.stateName should be (AllRetrieved)
    fsm.stateData should be (ComparisonStatus(&#34;someid&#34;, Some(&#34;oldData&#34;), Some(&#34;newData&#34;)))
  }
}
&lt;/pre&gt;

&lt;p&gt;By mixing in ImplicitSender or using TestProbes we can also verify messages the FSM should be sending in response to incoming messages.&lt;/p&gt;

&lt;p&gt;Testing is an essential part of developing applications. Unit tests should be explicit and granular. For higher level orchestration integration tests, taking a black-box approach, provide ways to oversee entire processes. Don&amp;#8217;t let your code become too unwieldy to manage: use the tools at your disposal and good coding practices to stay lean. Akka&amp;#8217;s FSM provides ways of programming transitional behavior over time and Akka&amp;#8217;s FSM Testkit support provides a way of ensuring that code works over time.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Using Akka’s ClusterClient</title>
          <link>http://blog.michaelhamrah.com/2014/02/using-akkas-clusterclient/</link>
          <pubDate>Wed, 05 Feb 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/02/using-akkas-clusterclient/</guid>
          <description>&lt;p&gt;I&amp;#8217;ve been spending some time implementing a feature which leverages Akka&amp;#8217;s &lt;a href=&#34;http://doc.akka.io/docs/akka/2.2.3/contrib/cluster-client.html&#34;&gt;ClusterClient&lt;/a&gt; (&lt;a href=&#34;http://doc.akka.io/api/akka/2.2.3/index.html#akka.contrib.pattern.ClusterClient&#34;&gt;api docs&lt;/a&gt;). A ClusterClient can be useful if:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You are running a service which needs to talk to another service in a cluster, but you don&amp;#8217;t that service to be in the cluster (cluster roles are another option for interconnecting services where separate hosts are necessary but I&amp;#8217;m not sold on them just yet).&lt;/li&gt;
&lt;li&gt;You don&amp;#8217;t want the overhead of running an http client/server interaction model between these services, but you&amp;#8217;d like similar semantics. Spray is a great akka framework for api services but you may not want to write a Spray API or use an http client library.&lt;/li&gt;
&lt;li&gt;You want to use the same transparency of local-to-remote actors but don&amp;#8217;t want to deal with remote actorref configurations to specific hosts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The documentation was a little thin on some specifics so getting started wasn&amp;#8217;t as smooth sailing as I&amp;#8217;d like. Here are some gotchas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You only need &lt;code&gt;akka.extensions = [&amp;quot;akka.contrib.pattern.ClusterReceptionistExtension&amp;quot;]&lt;/code&gt; on the Host (Server) Cluster. (If your client isn&amp;#8217;t a cluster, you&amp;#8217;ll get a runtime exception).&lt;/li&gt;
&lt;li&gt;&amp;#8220;Receptionist&amp;#8221; is the default name for the Host Cluster actor managing ClusterClient connections. Your ClusterClient connects first to the receptionist (via the set of initial contacts) then can start sending messages to actors in the Host Cluster. The name is configurable.&lt;/li&gt;
&lt;li&gt;The client actor system using the ClusterClient needs to have a Netty port open. You must use either actor.cluster.ClusterActorRefProvider or actor.remote.RemoteActorRefProvider. Otherwise the Host Cluster and ClusterClient can&amp;#8217;t establish proper communication. You can use the ClusterActorRefProvider on the client even you&amp;#8217;re not running a cluster.&lt;/li&gt;
&lt;li&gt;As a ClusterClient you wrap messages with a ClusterClient.send (or sendAll) message first. (I was sending vanilla messages and they weren&amp;#8217;t going through, but this is in the docs).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ClusterClients are worth checking out if you want to create physically separate yet interconnected systems but don&amp;#8217;t want to go through the whole load-balancer or http-layer setup. Just another tool in the Akka toolbelt!&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Programming Akka’s Finite State Machines in Scala</title>
          <link>http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/</link>
          <pubDate>Thu, 16 Jan 2014 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2014/01/programming-akkas-finite-state-machines-in-scala/</guid>
          <description>&lt;p&gt;Over the past few months my team has been building a new suite of services using Scala and Akka. An interesting aspect of Akka&lt;/p&gt;

&lt;p&gt;we leverage is its &lt;a href=&#34;http://doc.akka.io/docs/akka/2.2.3/scala/fsm.html&#34;&gt;Finite State Machine&lt;/a&gt; support. Finite State Machines&lt;/p&gt;

&lt;p&gt;are a staple of computer programming although not often used in practice. A conceptual process can usually be represented with a finite state machine: there are a defined number of states with explicit transitions between states. If we have a vocabulary&lt;/p&gt;

&lt;p&gt;around these states and transitions we can program the state machine.&lt;/p&gt;

&lt;p&gt;A traditional implementation of an FSM is to check and maintain state explicitly via if/else conditions, use the state design pattern, or implement some other construct. Using Akka&amp;#8217;s FSM support, which explicitly defines states and offers transition hooks, allows us to easily implement our conceptual model of a process. FSM is built on top of Akka&amp;#8217;s actor model giving excellent concurrency controls so we can run many of these state machines simultaneously. You can implement your own FSM with Akka&amp;#8217;s normal actor behavior with the &lt;em&gt;become&lt;/em&gt; method to change the partial function handling messages. However FSM offers some nice hooks plus data management in addition to just changing behavior.&lt;/p&gt;

&lt;p&gt;As an example we will use Akka&amp;#8217;s FSM support to check data in two systems. Our initial process is fairly simplistic but provides a good overview of leveraging Finite State Machines. Say we are rolling out a new system and we want to ensure data flows to both the old and new system. We need a process which waits a certain&lt;/p&gt;

&lt;p&gt;amount of time for data to appear in both places. If data is found in both systems we will check the data for consistency,&lt;/p&gt;

&lt;p&gt;if data is never found after a threshold we will alert data is out of sync.&lt;/p&gt;

&lt;p&gt;Based on our description we have four states. We define our states using a sealed trait:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;sealed trait ComparisonStates
case object AwaitingComparison extends ComparisonStates
case object PendingComparison extends ComparisonStates
case object AllRetrieved extends ComparisonStates
case object DataUnavailable extends ComparisonStates
&lt;/pre&gt;

&lt;p&gt;Next we define the data we manage between state transitions. We need to manage an identifier with data from&lt;/p&gt;

&lt;p&gt;the old and new system. Again we use a sealed trait:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;sealed trait Data
case object Uninitialized extends Data
case class ComparisonStatus(id: String, oldSystem: Option[SomeData] = None, newSystem: Option[SomeData] = None) extends Data
&lt;/pre&gt;

&lt;p&gt;A state machine is just a normal actor with the FSM trait mixed in. We declare our&lt;/p&gt;

&lt;pre class=&#34;brush: plain; title: ; notranslate&#34; title=&#34;&#34;&gt;ComparisonEngine&lt;/pre&gt;

&lt;p&gt;actor with FSM support,&lt;/p&gt;

&lt;p&gt;specifying our applicable state and data types:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;class ComparisonEngine extends Actor with FSM[ComparisonStates, Data] {
}
&lt;/pre&gt;

&lt;p&gt;Instead of handling messages directly in a receive method FSM support creates an additional layer of messaging handling.&lt;/p&gt;

&lt;p&gt;When using FSM you match on both message and current state. Our FSM only handles two messages: &lt;em&gt;Compare(id: Int)&lt;/em&gt; and&lt;/p&gt;

&lt;p&gt;&lt;em&gt;DataRetrieved(system: String, someData: SomeData)&lt;/em&gt;. You can construct your data types and messages any way&lt;/p&gt;

&lt;p&gt;you please. I like to keep states abstract as we can generalize on message handling. This&lt;/p&gt;

&lt;p&gt;prevents us from dealing with too many states and state transitions.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s start implementing the body of our &lt;em&gt;ComparisonEngine&lt;/em&gt;. We will start with our initial state:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;startWith(AwaitingComparison, Uninitialized)

when(AwaitingComparison) {
  case Event(Compare(id), Uninitialized) =&gt;
    goto(PendingComparison) using ComparisonStatus(id)
}
&lt;/pre&gt;

&lt;p&gt;We simply declare our initial state is AwaitingComparison, and the only message we are willing to process is a Compare.&lt;/p&gt;

&lt;p&gt;When we receive this message we go to a new state&amp;#8211;PendingComparison&amp;#8211;and set some data. Notice how we aren&amp;#8217;t actually doing anything else?&lt;/p&gt;

&lt;p&gt;A great aspect of FSM is the ability to listen on state transitions. This allows us to separate state transition logic from state transition&lt;/p&gt;

&lt;p&gt;actions. When we transition from an initial state to a PendingComparison state we want to ask our two systems for data. We simply match&lt;/p&gt;

&lt;p&gt;on state transitions and add our applicable logic:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;onTransition {
    case AwaitingComparison -&gt; PendingComparison =&gt;
      nextStateData match {
        case ComparisonStatus(id, old, new) =&gt; {
          oldSystemChecker ! VerifyData(id)
          newSystemChecker ! VerifyData(id)
        }
      }
    }
&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;oldSystemChecker&lt;/em&gt; and &lt;em&gt;newSystemChecker&lt;/em&gt; are actors responsible for verifying data in their respective systems. These can be passed in to the FSM as constructor arguments, or you can have the FSM create the actors and supervise their lifecycle.&lt;/p&gt;

&lt;p&gt;These two actors will send a DataRetrieved message back to our FSM when data is present. Because we are now in the &lt;em&gt;PendingComparison&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;state we specify our new state transition actions against a set of possible scenarios:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;when(PendingComparison, stateTimeout = 15 minutes) {
  case Event(DataRetrieved(&#34;old&#34;, old), ComparisonStatus(id, _, None)) =&gt; {
    stay using ComparisonStatus(id, Some(old), None)
  }
  case Event(DataRetrieved(&#34;new&#34;, new), ComparisonStatus(id, None, _)) =&gt; {
    stay using ComparisonStatus(id, None, Some(new))
  }
  case Event(StateTimeout, c: ComparisonStatus) =&gt; {
    goto(IdUnavailable) using c
  }
  case Event(DataRetrieved(system, data), cs @ ComparisonStatus(_, _, _)) =&gt; {
    system match {
      case &#34;old&#34; =&gt; goto(AllRetrieved) using cs.copy(old = Some(data))
      case &#34;new&#34; =&gt; goto(AllRetrieved) using cs.copy(new = Some(data))
    }
  }
}
&lt;/pre&gt;

&lt;p&gt;Our snippet says we will wait 15 minutes for our systemChecker actors to return with data, otherwise, we&amp;#8217;ll timeout and go to the unavailable state. Either the old&lt;/p&gt;

&lt;p&gt;system or new system will return first, in which case, one set of data in our ComparisonStatus will be None. So we stay in the PendingComparison state until&lt;/p&gt;

&lt;p&gt;the other system returns. If our previous pattern matches do not match, we know the current message we are processing is the final message. Notice how we don&amp;#8217;t care how these actors are getting their data. That&amp;#8217;s the responsibility of the child actors.&lt;/p&gt;

&lt;p&gt;Once we have all our data,&lt;/p&gt;

&lt;p&gt;so we go to the AllRetrieved state with the data from the final message.&lt;/p&gt;

&lt;p&gt;There are a couple of ways we could have defined our states. We could have a state for the oldSystem returned or newSystem returned. I find it easier to&lt;/p&gt;

&lt;p&gt;create a generic &lt;em&gt;PendingComparison&lt;/em&gt; state to keep our pattern matching for pending comparisons consolidated in a single partial function.&lt;/p&gt;

&lt;p&gt;Our final states are pretty simple: we just stop our state machine!&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;when(IdUnavailable) {
  case Event(_, _) =&gt; {
    stop
  }
}
when(AllRetrieved) {
  case Event(_, _) =&gt; {
    stop
  }
}
&lt;/pre&gt;

&lt;p&gt;Our last step is to add some more onTransition checks to handle our final states:&lt;/p&gt;

&lt;pre class=&#34;syntax scala&#34;&gt;case PendingComparison -&gt; AllRetrieved =&gt;
    nextStateData match {
      case ComparisonStatus(id, old, new) =&gt; {
        //Verification logic
     }
   }
 case _ -&gt; IdUnavailable =&gt;
   nextStateData match {
     case ComparisonStatus(id, old, new) =&gt; {
      //Handle timeout
      }
   }
&lt;/pre&gt;

&lt;p&gt;We don&amp;#8217;t care how we got to the &lt;em&gt;AllRetrieved&lt;/em&gt; state; we just know we are there and we have the data we need. We can offload our verification logic&lt;/p&gt;

&lt;p&gt;to another actor or inline it within our FSM as necessary.&lt;/p&gt;

&lt;p&gt;Implementing processing workflows can be tricky involving a lot of boilerplate code. Conditions must be checked, timeouts handled, error handling implemented.&lt;/p&gt;

&lt;p&gt;The Akka FSM approach provides a foundation for implementing workflow based processes on top of Akka&amp;#8217;s great supervision support. We create a ComparisonEngine&lt;/p&gt;

&lt;p&gt;for every piece of data we need to check. If an engine dies we can supervise and restart. My favorite feature is the separation of what causes a state transition&lt;/p&gt;

&lt;p&gt;with what happens during a state transition. Combined with isolated behavior amongst actors this creates a cleaner, isolated and composable application to manage.&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
