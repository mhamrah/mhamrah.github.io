<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> </title>
    <link>http://blog.michaelhamrah.com/tags/scaling/</link>
    <language>en-us</language>
    <author>Michael Hamrah</author>
    <rights>(C) 2015</rights>
    <updated>2015-04-10 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Easy Scaling with Fleet and CoreOS</title>
          <link>http://blog.michaelhamrah.com/2015/04/easy-scaling-with-fleet-and-coreos/</link>
          <pubDate>Fri, 10 Apr 2015 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2015/04/easy-scaling-with-fleet-and-coreos/</guid>
          <description>&lt;p&gt;One element of a successful production deployment is the ability to easily scale the number of instances your process is running. Many cloud providers, both on the PaaS and IaaS front, offer such functionality: AWS Auto Scaling Groups, Heroku&amp;#8217;s process size, Marathon&amp;#8217;s instance count. I was hoping for something similar in the CoreOS world. &lt;a href=&#34;http://deis.io&#34;&gt;Deis&lt;/a&gt;, the PaaS-on-CoreOS service, offers Heroku-like scaling, but I don&amp;#8217;t want to commit to the Deis layer nor its build pack approach (for no other reason than personal preference). Fleet, CoreOS&amp;#8217;s distributed systemd service, offers service templating, but you cannot say &amp;#8220;now run three instances of service x&amp;#8221;. Being programmers we can do whatever we want, and luckily, we&amp;#8217;re only a little bash script away from replicating the &amp;#8220;scale to x instances&amp;#8221; functionality of popular providers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.michaelhamrah.com/2015/03/deploying-docker-containers-on-coreos-with-the-fleet-api/&#34;&gt;You&amp;#8217;ll want to enable the Fleet HTTP Api&lt;/a&gt; for this script to work. You can easily port this to the Fleet CLI, but I much prefer the http api because it doesn&amp;#8217;t involve ssh, and provides more versatility into how and where you run the script.&lt;/p&gt;

&lt;p&gt;Conceptually the flow is straightforward:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Given a process we want to set the number of running instances to some &lt;code&gt;desired_count&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;desired_count&lt;/code&gt; is less than &lt;code&gt;current_count&lt;/code&gt;, scale down.&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;desired_count&lt;/code&gt; is more than &lt;code&gt;current_count&lt;/code&gt;, scale up.&lt;/li&gt;
&lt;li&gt;If they are the same, do nothing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fleet offers service templating so you can have a service unit named &lt;code&gt;my_awesome_app@.service&lt;/code&gt; with specific copies named &lt;code&gt;my_awesome_app@1, my_awesome_app@2, my_awesome_app@N&lt;/code&gt; representing specific running instances. Currently Fleet doesn&amp;#8217;t offer a way to group these related services together but we can easily pattern match on the service name to control specific running instances. The steps are straightforward:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Query the Fleet API for all instances&lt;/li&gt;
&lt;li&gt;Filter by all services matching the specified name&lt;/li&gt;
&lt;li&gt;See how many instances we have running for the given service&lt;/li&gt;
&lt;li&gt;Destroy or create instances using specific service names until we match the &lt;code&gt;desired_size&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these steps are easily achievable with Fleet&amp;#8217;s HTTP Api (or fleetctl) and a little bash. To give our script some context, let&amp;#8217;s start with how we want to use the script. Ideally it will look like this:&lt;/p&gt;

&lt;pre class=&#34;toolbar-overlay:false syntax bash&#34;&gt;./scale-fleet my_awesome_app 5
&lt;/pre&gt;

&lt;p&gt;First, let&amp;#8217;s set up our script &lt;code&gt;scale-fleet&lt;/code&gt; and set the command line arguments:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;#!/bin/bash

FLEET_HOST=&amp;lt;YOUR FLEET API HOST&gt;

# You may want to consider cli flags 
SERVICE_NAME=$1
DESIRED_SIZE=$2
&lt;/pre&gt;

&lt;p&gt;Next we want to query the Fleet API and filter on all units with a prefix of &lt;code&gt;SERVICE_NAME&lt;/code&gt; which have a process number. This will give us an array of units matching &lt;code&gt;my_awesome_app@1.service&lt;/code&gt;, not the base template of &lt;code&gt;my_awesome_app@.service&lt;/code&gt;. These are the units we will either add to or destroy as appropriate. The latest 1.5 version of jq supports regex expressions, but as of this writing 1.4 is the common release version, so we&amp;#8217;ll parse the json response with jq, and then filter with grep. Finally some bash trickery will parse the result into an array we can loop through later.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Curls the API and filter on a specific pattern, storing results in an array
INSTANCES=($(curl -s $FLEET_HOST/fleet/v1/units | jq &#34;.units[].name | select(startswith(\&#34;$SERVICE@\&#34;))&#34; | grep &#39;\w@\d\.service&#39;))

# A bash trick to get size of array
CURRENT_SIZE=${#INSTANCES[@]}
echo &#34;Current instance count for $SERVICE is: $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;Next let&amp;#8217;s scaffold the various scenarios for matching &lt;code&gt;CURRENT_SIZE&lt;/code&gt; with &lt;code&gt;DESIRED_SIZE&lt;/code&gt;, which boils down to some if statements.&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;if [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; then
  echo &#34;doing nothing, current size is equal desired size&#34;
elif [[ $DESIRED_SIZE &amp;lt; $CURRENT_SIZE ]]; then
  echo &#34;going to scale down instance $CURRENT_SIZE&#34;
  # More stuff here
else 
  echo &#34;going to scale up to $DESIRED_SIZE&#34;
  # More stuff here
fi
&lt;/pre&gt;

&lt;p&gt;When the desired size equals the current size we don&amp;rsquo;t need to do anything. Scaling down is easy, we simply loop, deleting the specific instance, until the desired and current states match. You can drop in the following snippet for scaling down:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;until [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; do
    curl -X DELETE $FLEET_HOST/fleet/v1/units/${SERVICE}@${CURRENT_SIZE}.service

    let CURRENT_SIZE = CURRENT_SIZE-1
  done
  echo &#34;new instance count is $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;Scaling up is a bit trickier. Unfortunately you can&amp;rsquo;t simply create a new unit from a template like you can with the fleetctl CLI. But you can do exactly what the fleetctl does: copy the body from the base template and create a new one with the specific full unit name. With the body we can loop, creating instances, until our current size matches the desired size. Let&amp;rsquo;s walk it through step-by-step:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;echo &#34;going to scale up to $desired_size&#34;
 # Get payload by parsing the options field from the base template
 # And build our new payload for PUTing later
 payload=`curl -s $FLEET_HOST/fleet/v1/units/${SERVICE}@.service | jq &#39;. | { &#34;desiredState&#34;:&#34;launched&#34;, &#34;options&#34;: .options }&#39;`

 #Loop, PUTing our new template with the appropriate name
 until [[ $DESIRED_SIZE = $CURRENT_SIZE ]]; do
   let current_size=current_size+1

   curl -X PUT -d &#34;${payload}&#34; -H &#39;Content-Type: application/json&#39; $FLEET_HOST/fleet/v1/units/${SERVICE}@${CURRENT_SIZE}.service 
 done
 echo &#34;new instance count is $CURRENT_SIZE&#34;
&lt;/pre&gt;

&lt;p&gt;With our script in place we can scale away:&lt;/p&gt;

&lt;pre class=&#34;syntax bash&#34;&gt;# Scale up to 5 instances
$ ./scale-fleet my_awesome_app 5

# Scale down
$ ./scale-fleet my_awesome_app 3
&lt;/pre&gt;

&lt;p&gt;Because this all comes down to a simple bash script you can easily run it from a variety of places. It can be part of a parameterized Jambi job to scale manually with a UI, part of an &lt;a href=&#34;http://github.com/hashicorp/envconsul&#34;&gt;envconsul&lt;/a&gt; setup with a key set in &lt;a href=&#34;consul.io&#34;&gt;Consul&lt;/a&gt;, or it can fit into a larger script that reads performance characteristics from some monitoring tool and reacts accordingly. You can also combine this with AWS Cloudformation or another cloud provider: if you&amp;rsquo;re CPU&amp;rsquo;s hit a certain threshold, you can scale the specific worker role running your instances, and have your &lt;code&gt;desired_size&lt;/code&gt; be some factor of that number.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been on a bash kick lately. It&amp;rsquo;s a versatile scripting language that easily portable. The syntax can be somewhat mystic, but as long as you have a shell, you have all you need to run your script.&lt;/p&gt;

&lt;p&gt;The final, complete script is here:&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Effective Caching Strategies: Understanding HTTP, Fragment and Object Caching</title>
          <link>http://blog.michaelhamrah.com/2012/08/effective-caching-strategies-understanding-http-fragment-and-object-caching/</link>
          <pubDate>Sat, 18 Aug 2012 00:00:00 UTC</pubDate>
          <author>Michael Hamrah</author>
          <guid>http://blog.michaelhamrah.com/2012/08/effective-caching-strategies-understanding-http-fragment-and-object-caching/</guid>
          <description>

&lt;p&gt;Caching is one of the most effective techniques to speed up a website and has become a staple of modern web architecture. Effective caching strategies will allow you to get the most out of your website, ease pressure on your database and offer a better experience for users. Yet as the old &lt;a href=&#34;http://martinfowler.com/bliki/TwoHardThings.html&#34;&gt;adage says&lt;/a&gt; caching&amp;#8211;especially invalidation&amp;#8211;is tricky. How to deal with dynamic pages, deciding what to cache, per-user personalization and invalidation are some of the challenges which come along with caching.&lt;/p&gt;

&lt;h3 id=&#34;caching-levels:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Caching Levels&lt;/h3&gt;

&lt;p&gt;There a three broad levels of caching:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html&#34;&gt;HTTP Caching&lt;/a&gt;&lt;/em&gt; allows for full-page caching via HTTP headers on URIs. This must be enabled on all static content and should be added to dynamic content when possible. It is the best form of caching, especially for dynamic pages, as you are serving generated html content and your application can effectively leverage reverse-proxies like &lt;a href=&#34;http://www.squid-cache.org/&#34;&gt;Squid&lt;/a&gt; and &lt;a href=&#34;https://www.varnish-cache.org/&#34;&gt;Varnish&lt;/a&gt;. &lt;a href=&#34;http://www.mnot.net/cache_docs/&#34;&gt;Mark Nottingham&amp;#8217;s great overview on HTTP Caching is worth a read&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Fragment Caching&lt;/em&gt; allows you to cache page fragments or partial templates. When you cannot cache an entire http response, fragment caching is your next best bet. You can quickly assemble your pages from pre-generated html snippets. For a page involving disparate dynamic content you can build your result page from cached html fragments for each section. For listing pages, like search results, you can build the page from html fragments for each id and not regenerate markup. For detail pages you can separate less-volatile or common sections from high-volatile or per-user sections.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Object Caching&lt;/em&gt; allows you to cache a full object (as in a model or viewmodel). When you must generate html for each user/request, or when your objects are shared across various views, object caching can be extremely helpful. It allows you to better deal with expensive queries and lessen hits to your database.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The goal is to make your response times as fast as possible while lessening load. The more html (or data) you can push closer to the end-user the better. HTTP caching is better than fragment caching: you are ready to return the rendered page. When combined with a CDN even dynamic pages can be pushed to edge locations for faster response times. Fragment caching is better than object caching: you already have the rendered html to build the page. Object caching is better than a database call: you already have the cached query result or denormalized object for your view. The deeper you get in the stack (the closer to the datastore) the more options you have to vary the output. Consequently the more expensive and longer the operation will take.&lt;/p&gt;

&lt;h3 id=&#34;break-content-down-cache-for-views:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Break Content Down; Cache for Views&lt;/h3&gt;

&lt;p&gt;A cache strategy is dependent on breaking content down to store and reuse later. The more granular you can get the more options you have to serve cached content. There are two main dimensions: what to cache and whom to cache for. It is difficult to HTTP cache a page with a &amp;#8220;Hello, {{ username }}&amp;#8221; in the header for all users. However if you break your users down into logged-in users and anonymous users you can easily HTTP cache your homepage for just anonymous users using the &lt;em&gt;vary&lt;/em&gt; http-header and defer to fragment caching for logged-in users.&lt;/p&gt;

&lt;p&gt;Cache key naming strategies allow you to vary the &lt;em&gt;what&lt;/em&gt; with the &lt;em&gt;who for&lt;/em&gt; in a robust way by creating multiple versions of the same resource. A cache key could include the role of the user and the page, such as &lt;em&gt;role:page:fragement:id&lt;/em&gt;, as in &lt;em&gt;anon:widget_detail:widget:1234&lt;/em&gt; and serve the &lt;em&gt;widget detail&lt;/em&gt; html fragment to anonymous users. The same widget could be represented in a search detail list via &lt;em&gt;anon:widget_search:widget:1234&lt;/em&gt;. When widget 1234 updates both keys are invalidated. Most people opt for object caching for an easy win with dynamic pages, specifically by caching via a primary key or id. This can be helpful, but if you break down your content into the &lt;em&gt;what&lt;/em&gt; and &lt;em&gt;who for&lt;/em&gt; with a good key naming strategy you can leverage fragment caching and save on rendering time.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;vary&lt;/em&gt; http header is very helpful for dealing with HTTP caching and is not used widely enough. By varying URIs based on certain headers (like authorization or a cookie value) you can cache different representations for the same resource in a similar way to creating multiple keys. Think of the cache key as the URI plus whatever is set in the &lt;em&gt;vary&lt;/em&gt; header. This opens up the power of HTTP caching for dynamic or per-user content.&lt;/p&gt;

&lt;p&gt;You are ready to deliver content quickly when you think about your cache in terms of views and not data. Cache a denormalized object with child associations for easy rendering without extra lookups. Store rendered html fragments for sections of a page that are common to users on otherwise specific content. &amp;#8220;Popular&amp;#8221; and &amp;#8220;Recent&amp;#8221; may be expensive queries; storing rendered html saves on processing time and can be injected into the main page. You can even reuse fragments across pages. A good cache key naming strategy allows for different representations of the same data which can easily be invalidated.&lt;/p&gt;

&lt;h3 id=&#34;cache-invalidation:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Cache Invalidation&lt;/h3&gt;

&lt;p&gt;Nobody likes stale data. As you think about caching think about what circumstances to invalidate the cache. Time-based expirations are convenient but can usually be avoided by invalidating caches on create and update commands. A good cache key naming strategy helps. Web frameworks usually have a notion of &amp;#8220;callbacks&amp;#8221; to perform secondary actions when a primary action takes place. A set of fragment and object caches for a widget could be invalidated when a record is updated. If cache values are granular enough you could invalidate sections of a page, like blog comments, when a comment is added and not expire the entire blog post.&lt;/p&gt;

&lt;p&gt;HTTP Etags provide a great mechanism for dealing with stale HTTP requests. Etags allow a more invalidation options than the basic if-modified-since headers. When dealing with Etags the most important thing is to avoid processing the entire request simply to generate the Etag to validate against (this saves network bandwidth but does not save processing time). Caching Etag values against URIs are a good way to see if an Etag is still valid to send the proper 304 NOT MODIFIED response as quickly as possible in the request cycle. Depending on your needs you can also cache sets of Etag values against URIs to handle various representations.&lt;/p&gt;

&lt;p&gt;If you must rely on time-based expiration try to add expiration callbacks to keep the cache fresh, especially for expensive queries in high-load scenarios.&lt;/p&gt;

&lt;h3 id=&#34;edge-side-includes-fragment-caching-for-http:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Edge Side Includes: Fragment Caching for HTTP&lt;/h3&gt;

&lt;p&gt;Edge Side Includes are a great way of pushing more dynamic content closer to users. ESIs essentially give you the benefits of fragment caching with the performance of HTTP caching. If you are considering using a tool like &lt;a href=&#34;http://www.squid-cache.org/&#34;&gt;Squid&lt;/a&gt; or &lt;a href=&#34;https://www.varnish-cache.org/&#34;&gt;Varnish&lt;/a&gt; ESIs are essential and will allow you to add customized content to otherwise similar pages. The &lt;em&gt;user panel&lt;/em&gt; in the header of a page is a classic example of an ESI usage. If the user panel is the only variant of an otherwise common page for all users, the common elements could be pulled from the reverse-proxy within milliseconds and the &amp;#8220;Welcome, {{USER}}&amp;#8221; injected dynamically as a fragment from the application server before sending everything to the client. This bypasses the application stack lightening load and decreasing processing time.&lt;/p&gt;

&lt;h3 id=&#34;distributed-or-centralized-caches-are-better:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Distributed or Centralized Caches are Better&lt;/h3&gt;

&lt;p&gt;Distributed and/or centralized caches are better than in-memory application server cache stores. By using a distributed cache like &lt;a href=&#34;http://memcached.org/&#34;&gt;Memcache&lt;/a&gt;, or a centralized cache store like &lt;a href=&#34;http://redis.io&#34;&gt;Redis&lt;/a&gt;, you can drop duplicate data caches to make caching and invalidating objects easier. Even though caching objects in a web app&amp;#8217;s memory space is convenient and reduces network i/o, it soon becomes impractical in a web farm. You do not want to build up caches per-server or steal memory space away from the web server. Nor do you want to have to hunt and gather objects across a farm to invalidate caches. If you do not want to support your own cache farm, there are plenty of SaaS services to deal with caching.&lt;/p&gt;

&lt;h3 id=&#34;compress-when-possible:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;Compress When Possible&lt;/h3&gt;

&lt;p&gt;Compressing content helps. Memory is a far more valuable resource for web apps than cpu cycles. When possible, compress your serialized cache content. This lowers the memory footprint so you can put more stuff in cache, and lightens the transfer load (and time) between your cache server and application server. For HTTP caching the helpful &lt;em&gt;vary&lt;/em&gt; http header can also be used to cache content for browsers supporting compression and those that don&amp;#8217;t. For object caching, only store what you need in the cache. Even though compression helps reduce the footprint, not storing extraneous data further reduces the footprint and saves serialization time.&lt;/p&gt;

&lt;h3 id=&#34;nosql-to-the-rescue:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;NoSQL to the Rescue&lt;/h3&gt;

&lt;p&gt;One of the interesting trends I am reading about is how certain NoSQL stores are eliminating the need for separate cache farms. NoSQL solutions are beneficial for a variety of reasons even though they create significant data-modeling challenges. What NoSQL solutions lack in the flexibility of representing and accessing data (i.e. no joins, minimal search) they can make up in their distributed nature, fault-tolerance, end access efficiency. When you model your data for your views, putting the burden on storing data in the same way you want to get it out, you&amp;#8217;re essentially replacing your denormalized memory-caching tier with a more durable solution. Cassandra and other Dynamo/Bigtable type stores are key-value stores, similar to cache stores, with the value part offering some sort of structured data type (in the case of Cassandra, sorted lists via column families). MongoDb and Redis, (not Dynamo inspired) offer similar advantages; Redis&amp;#8217; sorted sets/sorted lists offer a variety of solutions for listing problems, MongoDb allows you to query objects.&lt;/p&gt;

&lt;p&gt;If you are okay with storing (and updating) multiple-versions of your data (again, you are caching for views) you can cut the two-layer approach of separate cache and data stores. The trick is storing everything you need to render a view for a given key. Searches could be handled by a search-server like Solr or ElasticSearch; listing results could be handled by maintaining your own index via a sorted-list value via another key. When using Cassandra you&amp;#8217;d get fast, masterless, and scalable persistant storage. In general this approach is only worthwhile if your views are well-defined. The worst thing you want to do is refactor your entire data model when your views change!&lt;/p&gt;

&lt;h3 id=&#34;how-web-frameworks-help:16e4ca2f3d8139ba5bf41842ddca5f95&#34;&gt;How Web Frameworks Help&lt;/h3&gt;

&lt;p&gt;There is always debate on differences between frameworks and languages. One of the things I always look for is how easy it is to add caching to your application. Rails offers great support for caching, and the &lt;a href=&#34;http://guides.rubyonrails.org/caching_with_rails.html&#34;&gt;Caching with Rails&lt;/a&gt; guide is worth a read no matter what framework or language you use. It easily supports fragment caching in views via content blocks, behind-the-scene action caching support, has a pluggable cache framework to use different stores, and most importantly has an extremely flexible invalidation framework via model observers and cache sweepers. When choosing any type of framework, &amp;#8220;how to cache&amp;#8221; should be a bullet point at the top of the list.&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
